<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sekyoro的博客小屋</title>
  
  
  <link href="https://www.sekyoro.top/atom.xml" rel="self"/>
  
  <link href="https://www.sekyoro.top/"/>
  <updated>2023-10-04T08:31:01.778Z</updated>
  <id>https://www.sekyoro.top/</id>
  
  <author>
    <name>Sekyoro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>stable_diffusion学习</title>
    <link href="https://www.sekyoro.top/2023/10/04/stable-diffusion%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/10/04/stable-diffusion%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-10-04T06:30:57.000Z</published>
    <updated>2023-10-04T08:31:01.778Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>比较火的text-img的模型,看一下其中的模块和流程.<br><span id="more"></span><br><a href="https://course.fast.ai/Lessons/lesson9.html以及[The">https://course.fast.ai/Lessons/lesson9.html以及[The</a> Illustrated Stable Diffusion – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)](<a href="https://jalammar.github.io/illustrated-stable-diffusion/)都是比较好的资料">https://jalammar.github.io/illustrated-stable-diffusion/)都是比较好的资料</a>.</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>stable diffusion前身是latent diffusion<a href="https://arxiv.org/abs/2112.10752">[2112.10752] High-Resolution Image Synthesis with Latent Diffusion Models (arxiv.org)</a></p><blockquote><p>扩散模型已经显示出在生成图像数据方面实现了最先进的结果。但扩散模型的一个缺点是反向去噪过程很慢。此外，这些模型消耗了大量内存，因为它们在像素空间中工作，在生成高分辨率图像时，像素空间变得不合理地昂贵。因此，训练这些模型并将其用于推理是具有挑战性的。</p></blockquote><p>latent diffusion可以通过在<strong>低维潜在空间上应用扩散过程</strong>而不是使用实际像素空间来降低内存和计算复杂性。这是标准扩散和潜在扩散模型之间的关键区别：<strong>在潜在扩散中，模型被训练以生成图像的潜在（压缩）表示</strong></p><p>潜在扩散有三个主要成分。 一种自动编码器(VAE),U-Net,text-encoder，例如CLIP的文本编码器。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/article-Figure3-1-1536x762.png" alt="img"></p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>VAE模型有两个部分，一个编码器和一个解码器。</p><p>编码器用于将图像转换为低维的潜在表示，该低维潜在表示将用作U-Net模型的输入。相反，解码器将潜在的表示转换回图像。</p><p>在潜在扩散训练期间，编码器用于获得前向扩散过程的图像的潜在表示（latent），前向扩散在每一步应用越来越多的噪声。</p><p>在推断过程中，使用VAE解码器将反向扩散过程生成的去噪潜伏时间转换回图像。正如我们将在推理过程中看到的，我们只需要VAE解码器。</p><h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p><img data-src="https://img1.imgtp.com/2023/10/04/0K8yjheX.png" alt="image-20231004151321726"></p><p>U-Net具有编码器部分和解码器部分，两者都由ResNet块组成。编码器将图像表示压缩成较低分辨率的图像表示，并且解码器将较低分辨率图像表示解码回假定噪声较小的原始较高分辨率图像表示。更具体地，<strong>U-Net输出预测可用于计算预测的去噪图像表示的噪声残差</strong>。</p><p>该体系结构的一些亮点包括：</p><ul><li>该模型预测与输入大小相同的图像</li><li>该模型使输入图像经过几个ResNet层块</li><li>这些层将图像大小减半2然后通过相同数量的块再次对其进行上采样。</li><li>skip connections将下采样路径上的特征链接到上采样路径中的相应层。</li></ul><p>为了防止U-Net在下采样时丢失重要信息，<strong>通常在编码器的下采样ResNet和解码器的上采样ResNets之间添加short-cut connections</strong>。</p><p>此外，稳定扩散U-Net能够通过跨注意力层将其输出条件设置在文本嵌入上。交叉注意层被添加到U-Net的编码器和解码器部分，通常在ResNet块之间。</p><h3 id="Text-encoder"><a href="#Text-encoder" class="headerlink" title="Text-encoder"></a>Text-encoder</h3><p>Text-encoder负责将输入提示（例如“一个骑马的天文数字”）转换为U-Net可以理解的嵌入空间。它通常是一个简单的基于转换器的编码器，将输入token序列映射到潜在文本嵌入序列。</p><p>Stable Diffusion在训练期间不训练Text-encoder，而是简单地使用CLIP的已经训练的文本编码器</p><p>由于潜在扩散模型的U-Net在低维空间上运行，与像素空间扩散模型相比，它大大降低了内存和计算需求。</p><h2 id="diffusion-process"><a href="#diffusion-process" class="headerlink" title="diffusion process"></a>diffusion process</h2><p>扩散过程包括<strong>取所需输出大小的随机噪声</strong>，并将其通过模型多次。该过程在<strong>给定数量的步骤</strong>后结束，并且输出图像应该表示根据模型的训练数据分布的样本，例如蝴蝶的图像。</p><p>在训练过程中，我们展示了许多给定分布的样本，例如蝴蝶的图像。经过训练后，该模型将能够处理随机噪声，生成类似的蝴蝶图像。</p><p>该模型通常不会被训练成直接预测噪声稍低的图像，而是预测“噪声残差”，即<strong>噪声较低的图像和输入图像之间的差异</strong>（对于称为“DDPM”的扩散模型），或者类似地，两个时间步长之间的梯度（如称为“Score VE”的扩散模式）。也就是说Unet被训练为一个去噪器,被用于输出噪声差异.</p><p>因此，为了进行去噪过程，需要一种特定的噪声调度算法，并包裹(wrap)模型，以定义推理需要多少扩散步骤，以及如何从模型的输出中计算噪声较小的图像。扩散器库的不同<strong>调度器</strong>在这里发挥作用。</p><p>在训练时,我们给定text,图像</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-autoencoder.png" alt="img"></p><p>首先将图像通过encoder得到潜在变量用于后续处理,这也是为了内存和计算资源.这种压缩（以及后来的解压缩）是通过自动编码器完成的。自动编码器使用其编码器将图像压缩到潜在空间中，然后使用解码器仅使用压缩信息来重建图像。</p><p>现在，正向扩散过程是在压缩的compressed latents上完成的。噪声应用于那些latents的噪声，而不是应用于本身的像素图像的噪声。因此，<strong>噪声预测器实际上被训练来预测压缩表示（潜在空间）中的噪声</strong>。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-latent-forward-process-v2.png" alt="img"></p><p>正向过程（使用自动编码器的编码器）是我们生成数据以训练噪声预测器的方式。一旦经过训练，我们就可以通过运行反向过程（使用自动编码器的解码器）来生成图像。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-forward-and-reverse-process-v2.png" alt="img"></p><p>通过一张图片(潜在变量)以及添加与一个值(noise amount)相关的噪声,得到新的数据.</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-forward-diffusion-training-example-2.png" alt="img"></p><p>有了这个数据集，我们可以训练噪声预测器，并最终获得一个出色的噪声预测器。当在特定配置下运行时，它实际上可以创建图像</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-u-net-noise-training-step.png" alt="img"></p><p>我们目的是得到一个噪声预测器,通过之前得到的带有噪声的图像数据集以及相应的加噪声的值,这个预测器将这些作为输入,输出就是噪声,将带噪声图像数据集减去噪声看是否与原图一样,这样就得到一个noise predictor.</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-denoising-step-2v2.png" alt="img" style="zoom:50%;" /></p><p>Dall-E2和谷歌的Imagen也是类似原理.</p><p>注意上面的输入并没有加入text embedding,Transformer语言模型被用作语言理解组件，该组件接受文本提示并生成 token embeddings。发布的stable diffusion使用ClipText（一种基于GPT的模型），而论文中使用的BERT。</p><h3 id="CLIP训练"><a href="#CLIP训练" class="headerlink" title="CLIP训练"></a>CLIP训练</h3><p>CLIP是在图像及其字幕的数据集上进行训练的。想象一下这样的数据集，只有4亿张图像和它们的标题(captions)</p><blockquote><p>事实上，CLIP是根据从网络上抓取的图像及其“alt”标签进行训练的。</p></blockquote><p>CLIP是图像编码器和文本编码器的组合。它的训练过程可以简化为拍摄图像及其说明(caption)。我们分别用图像和文本编码器对它们进行编码。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/clip-training-step-1.png" alt="img" style="zoom: 50%;" /></p><p>然后，我们使用<strong>余弦相似性</strong>来比较结果嵌入。当我们开始训练过程时，即使文本正确地描述了图像，相似度也会很低。我们更新这两个模型，以便下次嵌入它们时，得到的嵌入是相似的。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/clip-training-step-3.png" alt="img" style="zoom:50%;" /></p><p>通过在数据集中重复，并使用大批量，我们最终发现编码器能够生成狗的图像和句子“狗的照片”相似的嵌入。就像在word2vec中一样，训练过程也需要包括不匹配的图像和描述(captions )的负面示例，并且模型需要为它们分配低相似度分数。</p><blockquote><p>captions本身是字幕的意思,这里表示图像的类似标签,标题,名字的含义.</p></blockquote><h3 id="加入text信息"><a href="#加入text信息" class="headerlink" title="加入text信息"></a>加入text信息</h3><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-unet-inputs-v2.png" alt="img" style="zoom:67%;" /></p><p>为了使文本成为图像生成过程的一部分，我们必须调整我们的噪声预测器，以使用文本作为输入。</p><p>现在的数据集包括编码文本。由于我们在潜在空间中操作，输入图像和预测噪声都在潜在空间内。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-text-dataset-v2.png" alt="img"></p><h4 id="如果不包含text信息"><a href="#如果不包含text信息" class="headerlink" title="如果不包含text信息"></a>如果不包含text信息</h4><p>在Unet中,如果没有text信息,那么结构如下</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unet-inputs-outputs-v2.png" alt="img"></p><ol><li>Unet是一系列用于转换latents数组的层</li><li>每个层对上一层的输出进行操作</li><li>一些输出（通过residual connections）被馈送到网络中稍后的处理中</li><li>时间步长(timestep)被转换为时间步长嵌入向量( time step embedding vector)，</li></ol><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unit-resnet-steps-v2.png" alt="img"></p><h4 id="如果包含text信息"><a href="#如果包含text信息" class="headerlink" title="如果包含text信息"></a>如果包含text信息</h4><p>需要对结构进行的主要更改是在ResNet块之间添加一个注意力层，以添加对文本输入的支持。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unet-with-text-steps-v2.png" alt="img"></p><p>请注意，ResNet块不会直接查看文本。但注意力层将这些文本表示合并到了latents中。现在，下一个ResNet可以在处理过程中利用合并的文本信息。</p><h2 id="Stable-Diffusion-during-inference"><a href="#Stable-Diffusion-during-inference" class="headerlink" title="Stable Diffusion during inference"></a>Stable Diffusion during inference</h2><p>训练时通过一个prompt,也就是text文本,然后通过一个laten seed随机生成一个潜变量图像(正态分布噪声),这些作为unet的输入</p><p><img data-src="https://img1.imgtp.com/2023/10/04/kHc7pOee.png" alt="image-20231004150708585"></p><p>代码如下,使用hugging face的diffusers库.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch_device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPTextModel, CLIPTokenizer</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoencoderKL, UNet2DConditionModel, PNDMScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load the autoencoder model which will be used to decode the latents into image space.</span></span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;vae&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Load the tokenizer and text encoder to tokenize and encode the text.</span></span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. The UNet model for generating the latents.</span></span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;unet&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line">scheduler = LMSDiscreteScheduler.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;scheduler&quot;</span>)</span><br><span class="line">vae = vae.to(torch_device)</span><br><span class="line">text_encoder = text_encoder.to(torch_device)</span><br><span class="line">unet = unet.to(torch_device)</span><br><span class="line">prompt = [<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>]</span><br><span class="line"></span><br><span class="line">height = <span class="number">512</span>                        <span class="comment"># default height of Stable Diffusion</span></span><br><span class="line">width = <span class="number">512</span>                         <span class="comment"># default width of Stable Diffusion</span></span><br><span class="line"></span><br><span class="line">num_inference_steps = <span class="number">100</span>            <span class="comment"># Number of denoising steps</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">7.5</span>                <span class="comment"># Scale for classifier-free guidance</span></span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">32</span>)   <span class="comment"># Seed generator to create the inital latent noise</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">text_input = tokenizer(prompt, padding=<span class="string">&quot;max_length&quot;</span>, max_length=tokenizer.model_max_length, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br><span class="line">  max_length = text_input.input_ids.shape[-<span class="number">1</span>]</span><br><span class="line">uncond_input = tokenizer(</span><br><span class="line">    [<span class="string">&quot;&quot;</span>] * batch_size, padding=<span class="string">&quot;max_length&quot;</span>, max_length=max_length, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br><span class="line">  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br><span class="line">  latents = torch.randn(</span><br><span class="line">  (batch_size, unet.in_channels, height // <span class="number">8</span>, width // <span class="number">8</span>),</span><br><span class="line">  generator=generator,</span><br><span class="line">)</span><br><span class="line">latents = latents.to(torch_device)</span><br><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br><span class="line">latents = latents * scheduler.init_noise_sigma</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm(scheduler.timesteps):</span><br><span class="line">  <span class="comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span><br><span class="line">  latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  latent_model_input = scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># predict the noise residual</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">  <span class="comment"># perform guidance</span></span><br><span class="line">  noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">  latents = scheduler.step(noise_pred, t, latents).prev_sample</span><br><span class="line">  <span class="comment"># scale and decode the image latents with vae</span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  image = vae.decode(latents).sample</span><br><span class="line">  image = (image / <span class="number">2</span> + <span class="number">0.5</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">image = image.detach().cpu().permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).numpy()</span><br><span class="line">images = (image * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line">pil_images = [Image.fromarray(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br></pre></td></tr></table></figure><p>本人水平有限,如果上文中关于diffusion过程有错误,还请斧正.</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ol><li><a href="https://colab.research.google.com/drive/1MOY7vkpKUosqPSk993g0o4VLjJKAJrwK#scrollTo=BBsdAj9pDPOv">stable_diffusion.ipynb - Colaboratory (google.com)</a></li><li><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">Diffusers.ipynb - Colaboratory (google.com)</a></li><li><a href="https://jalammar.github.io/illustrated-stable-diffusion/">The Illustrated Stable Diffusion – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></li><li><a href="https://rekil156.github.io/rekilblog/posts/lesson9_stableDissufion/Lesson9.html">rekilblog - A different way to look at Stable Diffusion (rekil156.github.io)</a></li><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil’Log (lilianweng.github.io)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;比较火的text-img的模型,看一下其中的模块和流程.&lt;br&gt;</summary>
    
    
    
    
    <category term="stable diffusion" scheme="https://www.sekyoro.top/tags/stable-diffusion/"/>
    
  </entry>
  
  <entry>
    <title>深度学习训练tricks</title>
    <link href="https://www.sekyoro.top/2023/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83tricks/"/>
    <id>https://www.sekyoro.top/2023/10/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83tricks/</id>
    <published>2023-10-04T02:47:08.000Z</published>
    <updated>2023-10-04T04:28:39.371Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近在看微软的AI for Beginners,质量比较高,这里相当于将其中的一篇文章写过来加点自己的理解.</p><span id="more"></span><p>模型的训练的一个主要问题是梯度爆炸或者梯度消失,前者会导致训练不稳定,表现出来就是损失值不稳定,一直都处在较高值降不下去,后者就是更新缓慢.下面介绍一些技巧</p><h3 id="将值保持在合理的范围"><a href="#将值保持在合理的范围" class="headerlink" title="将值保持在合理的范围"></a>将值保持在合理的范围</h3><p>为了让数值运算更稳定,我们希望确保神经网络中的所有值都在合理的范围内，通常为[-1,1]或[0,1]. <strong>浮点计算的本质是，不同大小的值不能精确地一起操作</strong>.这样做是为了避免原本非常大或非常小的值进行传播时使得梯度爆炸或者消失,当保持在[-1,1]后数值比较稳定.</p><blockquote><p>例如，如果我们将10^-10^和10^10^相加，我们很可能得到10^10^，因为较小的值将被“转换”为与较大的值相同的order(这涉及到计算机的浮点数运算)，因此尾数将丢失。</p></blockquote><p>此外,大多数激活函数在[-1,1]附近具有非线性，因此将所有输入数据缩放到[-1,1]或[0,1]间隔是有意义的。</p><p>所以在预处理数据时通常会Normalize.</p><h2 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h2><p>理想情况下,我们希望这些值在通过网络层后处于相同的范围内。因此,初始化权重以保持输入值前后的分布方式是很重要的。</p><p>我的理解就是weight不能太大或太小或者其他情况,这影响到输出的分布.</p><p>正态分布N(0,1)不是一个好主意，因为如果我们有n个输入,输出的标准差将是n，并且值可能跳出[0,1]区间。也就是说如果输入是(B,C)的数据,B表示一个batch,C是特征数,进行正态分布权重初始化,假设w矩阵shape是(C,2)服从标准正态分布,得到值标准差为n(因为有n个特征,每个特征乘以一个正态分布,然后将每个特征相加)</p><p>所以不能使用标准正态分布初始化权重,通常使用以下初始化:</p><ul><li>均匀分布—uniform,也就是在一个范围内值相同.</li><li>N（0,1/N） 正态分布,使得输出尽量符合N(0,1)</li><li>N（0,1/√N_in）保证对于N(0,1)的输入，将保持相同的平均值和标准差.</li><li>N（0，√2/（N_in+N_out））——所谓的Xavier初始化（glorot）,它有助于在前向和后向传播过程中保持信号在一定范围内</li></ul><p>在pytorch中默认是N（0,1/√N_in）初始化,当然这可以自己设置.</p><h3 id="批量规范化"><a href="#批量规范化" class="headerlink" title="批量规范化"></a>批量规范化</h3><p>即使进行了适当的权重初始化，在训练过程中权重也可能变得任意大或小，并且它们会使信号超出适当的范围。</p><p>我们可以通过使用一种归一化技术来恢复信号。虽然有几种（权重规格化、层规格化），但最常用的是批量规格化。批量归一化的思想是考虑整个小批量的所有值，并根据这些值执行归一化（即减去平均值并除以标准差）。它被实现为网络层，在应用权重之后但在激活函数之前进行这种归一化。因此，我们可能会看到更高的最终准确性和更快的训练。</p><p>除了batchnormal之外还有layernormal和instancenormal等等,这些不同的方式应用在不同的任务上.</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是一种有趣的技术，可以在训练过程中去除一定比例的随机神经元。它也被实现为一个具有一个参数（要去除的神经元的百分比，通常为10%-50%）的层,在训练过程中，它将输入向量的随机元素归零，然后将其传递到下一层。</p><p>这种影响可以用几种方式来解释：</p><ul><li>它可以被认为是模型的一个随机冲击因素，它使优化超出了局部最小值</li><li>它可以被认为是隐式模型平均(implicit model averaging)，因为我们可以说，在dropout时，我们训练的模型略有不同</li></ul><p>在Pytorch实现中,在训练过程会去除一定比例神经元然后将剩余的神经元权重乘以去除比例的倒数,测试时正常传播.</p><p>下图横线不同的值表示dropout的比例</p><p><img data-src="https://img1.imgtp.com/2023/10/04/bB2APob6.png" alt="image-20231004113324997"></p><p>这东西没有好的数学支撑,但是效果不错.我没记错的话就是hinton提出的,这人有认知心理学领域的知识,所以会提出这些东西.</p><h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><p>深度学习的一个非常重要的方面就是能够防止过度拟合。虽然使用非常强大的神经网络模型可能很诱人，但我们应该始终<strong>平衡模型参数的数量与训练样本的数量</strong>。</p><p>有几种方法可以防止过度拟合：</p><ol><li><p>早期停止(early stopping)——持续监控验证集上的错误，并在验证错误开始增加时停止训练。</p></li><li><p>显式权重衰减/正则化(explicit weight decay/regularization)-<strong>为权重的高绝对值的损失函数添加额外的惩罚</strong>，这可以防止模型获得非常不稳定的结果. </p><p>权重衰减/正则化指的是损失函数加上weight的正则化系数,以希望权重减小.</p><blockquote><p>在WGAN中其实还有gradient penalty,将梯度的Norm加到损失函数中</p></blockquote></li><li><p>模型平均(model averaging)——训练几个模型，然后对结果进行平均。这有助于最小化差异。</p></li><li><p>Dropout（隐式模型平均值）</p></li></ol><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>训练的另一个重要方面是选择好的训练算法。虽然经典的梯度下降(每次选取一个样本)是一个合理的选择，但它有时可能太慢，或导致其他问题.</p><p>在深度学习中，我们使用随机梯度下降（SGD），这是一种应用于从训练集中随机选择的小批量的梯度下降。使用以下公式调整权重：</p><script type="math/tex; mode=display">w^{t+1}=w^{t}-\eta\nabla{\cal L}</script><h3 id="引入动量"><a href="#引入动量" class="headerlink" title="引入动量"></a>引入动量</h3><p>在动量SGD中，我们保持了之前步骤的一部分梯度。这类似于当我们带着惯性在某个地方移动时，我们受到了不同方向的冲击，我们的轨迹不会立即改变，而是保持了原始运动的一部分。在这里，我们引入另一个向量v来表示速度：</p><script type="math/tex; mode=display">\begin{array}{cc}{\bullet}&{ {v^{t+1}=\gamma v^{t}-\eta\nabla{\cal L}  } }\\{\bullet}&{ {w^{t+1}=w^{t}+v^{t+1} } }\\\end{array}</script><p>这里，参数γ表示我们考虑惯性的程度：γ=0对应于经典SGD；γ=1是一个纯运动方程</p><h3 id="Adam-Adagrad"><a href="#Adam-Adagrad" class="headerlink" title="Adam,Adagrad"></a>Adam,Adagrad</h3><p>由于在每一层中，我们将信号乘以某个矩阵W~i~，这取决于||Wi||，因此梯度可以减小并接近0，也可以无限上升。它是梯度消失/爆炸问题的本质。</p><p>一种解决方法就是计算梯度时,只使用损失梯度的方向，忽略绝对值.</p><script type="math/tex; mode=display">w^{t+1}=w^t-\eta(\nabla\mathcal{L}/||\nabla\mathcal{L}||)\text{,where}||\nabla\mathcal{L}||=\sqrt{\Sigma(\nabla\mathcal{L})^2}</script><p>还是要学好数学基础,这里梯度矩阵求一个L-2 Norm,计算类似归一化值(像单位向量一样).这样保持了梯度的分布,同时减小了梯度的值避免了梯度爆炸.</p><p>这个算法被称为Adagrad。另一个使用相同思想的算法：RMSProp，Adam.</p><p>其实优化器的选择还是要看具体任务.</p><h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><p>梯度剪裁是上述思想的扩展。当ℒ|| ≤ θ、 我们在权重优化中考虑原始梯度，当ℒ|| &gt; θ-我们将梯度除以它的范数。这里θ是一个参数，在大多数情况下，我们可以取θ=1或θ=10。</p><h3 id="学习率递减"><a href="#学习率递减" class="headerlink" title="学习率递减"></a>学习率递减</h3><p>训练的成功往往取决于学习率参数η。合理的假设是，η的值越大，训练越快，这是我们在训练开始时通常想要的，然后η的值就越小，我们就可以微调网络。因此，在大多数情况下，我们希望在训练过程中降低η。这可以通过在每个训练时期后将η乘以某个数字（例如0.98）来实现，或者通过使用更复杂的学习率scheduler来实现。</p><p>所以学习率一开始可以设置大一点,后面进行减小.</p><p>Pytorch中有learning rate scheduler用于调度学习率.</p><h2 id="使用不同的网络架构"><a href="#使用不同的网络架构" class="headerlink" title="使用不同的网络架构"></a>使用不同的网络架构</h2><p><a href="https://www.topbots.com/a-brief-history-of-neural-network-architectures/">A Brief History Of Neural Network Architectures (topbots.com)</a></p><p>网络架构需要跟具体任务相关,通常，我们会采用一种已被证明适用于我们特定任务（或类似任务）的架构</p><p><img data-src="https://topb0ts.wpenginepowered.com/wp-content/uploads/2017/06/neural_network_architectures_800x350px_web-1.png" alt="Neural Network Architectures Eugenio Culurciello" style="zoom:67%;" /></p><p>另一个好方法是使用能够自动调整到所需复杂性的体系结构。在某种程度上，ResNet架构和Inception是自我调整的.</p><p>本人能力有限,如果上面写的有误,还请评论指出,敬请赐教.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在看微软的AI for Beginners,质量比较高,这里相当于将其中的一篇文章写过来加点自己的理解.&lt;/p&gt;</summary>
    
    
    
    
    <category term="DeepLearning" scheme="https://www.sekyoro.top/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>HFNLP学习</title>
    <link href="https://www.sekyoro.top/2023/09/29/HFNLP%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/09/29/HFNLP%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-09-29T04:57:12.000Z</published>
    <updated>2023-10-04T07:54:51.216Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>深度学习中常用的技术,这里结合一些tutorial简单学习一下.<br><span id="more"></span><br>首先需要了解一些简单概念<a href="https://www.zhihu.com/tardis/zm/art/138310401?source_id=1005">深度学习推荐系统 | Embedding，从哪里来，到哪里去 (zhihu.com)</a>这里使用hugging face的相关库,我也建议多看看这个社区以及其相关的工具,我觉得这些工具很棒,这个开源社区也很棒.此外也有LangChain等工具.</p><h2 id="transformers模型"><a href="#transformers模型" class="headerlink" title="transformers模型"></a>transformers模型</h2><p>transformers库是利用基于transformer模型处理任务的库.transfomers模型架构上有过很多大模型.</p><p><img data-src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Architecture of a Transformers models"></p><p><img data-src="https://img1.imgtp.com/2023/09/29/bxBzfQw2.png" alt="image-20230929145450463" style="zoom:80%;" /></p><p>在hugging face中,有transformers库帮助我们处理一系列nlp任务.下面介绍一下这个库.</p><p>transformers库中最基本的对象.Transformers库是pipeline（）函数。它将模型与其必要的预处理和后处理步骤连接起来，使我们能够直接输入任何文本并获得可理解的答案.默认情况下，此pipeline选择一个特定的预训练模型，该模型已针对英语情感分析进行了微调。创建<strong>classifier</strong>对象时，将下载并缓存模型。如果您重新运行该命令，则将使用缓存的模型，无需再次下载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">classifier = pipeline(<span class="string">&quot;sentiment-analysis&quot;</span>)</span><br><span class="line">classifier(</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;I hate this so much!&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="symbol">&#x27;label</span><span class="symbol">&#x27;:</span> <span class="symbol">&#x27;POSITIVE</span>&#x27;, <span class="symbol">&#x27;score</span><span class="symbol">&#x27;:</span> <span class="number">0.9598048329353333</span>&#125;,</span><br><span class="line"> &#123;<span class="symbol">&#x27;label</span><span class="symbol">&#x27;:</span> <span class="symbol">&#x27;NEGATIVE</span>&#x27;, <span class="symbol">&#x27;score</span><span class="symbol">&#x27;:</span> <span class="number">0.9994558691978455</span>&#125;]</span><br></pre></td></tr></table></figure><p>将一些文本传递到pipeline时，主要涉及三个步骤：</p><ol><li>文本被预处理为模型能够理解的格式。</li><li>经过预处理的输入被传递给模型。 </li><li>模型的预测是经过后处理的，因此您可以理解它们。</li></ol><p>pipeline函数可用的任务参数有</p><ul><li><p><code>feature-extraction</code> (get the vector representation of a text)</p></li><li><p><code>fill-mask</code></p></li><li><p><code>ner</code> (named entity recognition)</p></li><li><p><code>question-answering</code></p></li><li><p><code>sentiment-analysis</code></p></li><li><p><code>summarization</code></p></li><li><p><code>text-generation</code></p></li><li><p><code>translation</code></p></li><li><p><code>zero-shot-classification</code></p><p><img data-src="https://img1.imgtp.com/2023/09/29/Q6Mjp3xk.png" alt="image-20230929151703649"></p></li></ul><p>由于NLP的一些任务我不是很熟悉,有必要做一些大概了解.</p><h4 id="Zero-shot-classification"><a href="#Zero-shot-classification" class="headerlink" title="Zero-shot classification"></a>Zero-shot classification</h4><p>我们需要对尚未标记的文本进行分类。这是实际项目中的常见场景，因为注释文本通常很耗时并且需要领域专业知识。对于这项任务<strong>zero-shot-classification</strong>pipeline非常强大：它允许您直接指定用于分类的标签，因此您不必依赖预训练模型的标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">classifier = pipeline(<span class="string">&quot;zero-shot-classification&quot;</span>)</span><br><span class="line">classifier(</span><br><span class="line">    <span class="string">&quot;This is a course about the Transformers library&quot;</span>,</span><br><span class="line">    candidate_labels=[<span class="string">&quot;education&quot;</span>, <span class="string">&quot;politics&quot;</span>, <span class="string">&quot;business&quot;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>由于pipeline没有指定model和tokenizer,会默认下载model和tokenizer.这里模型本身携带了tokenizer.后者就是生成embedding的.</p><h4 id="Text-generation"><a href="#Text-generation" class="headerlink" title="Text generation"></a>Text generation</h4><p>模型将通过生成剩余的文本来自动完成整段话。这类似于许多手机上的预测文本功能。文本生成涉及随机性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">generator = pipeline(<span class="string">&quot;text-generation&quot;</span>)</span><br><span class="line">generator(<span class="string">&quot;In this course, we will teach you how to&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="comment"># 指定模型</span></span><br><span class="line">generator = pipeline(<span class="string">&quot;text-generation&quot;</span>, model=<span class="string">&quot;distilgpt2&quot;</span>)</span><br><span class="line">generator(</span><br><span class="line">    <span class="string">&quot;In this course, we will teach you how to&quot;</span>,</span><br><span class="line">    max_length=<span class="number">30</span>,</span><br><span class="line">    num_return_sequences=<span class="number">2</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="Mask-filling"><a href="#Mask-filling" class="headerlink" title="Mask filling"></a>Mask filling</h4><p>这项任务的目的是填补给定文本中的空白：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">unmasker = pipeline(<span class="string">&quot;fill-mask&quot;</span>)</span><br><span class="line">unmasker(<span class="string">&quot;This course will teach you all about &lt;mask&gt; models.&quot;</span>, top_k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="Named-entity-recognition"><a href="#Named-entity-recognition" class="headerlink" title="Named entity recognition"></a>Named entity recognition</h4><p>命名实体识别 (NER) 是一项任务，其中模型必须找到输入文本的哪些部分对应于诸如人员、位置或组织之类的实体</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">ner = pipeline(<span class="string">&quot;ner&quot;</span>, grouped_entities=<span class="literal">True</span>)</span><br><span class="line">ner(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Question-answering"><a href="#Question-answering" class="headerlink" title="Question answering"></a>Question answering</h4><p>给定上下文中的信息回答问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">question_answerer = pipeline(<span class="string">&quot;question-answering&quot;</span>)</span><br><span class="line">question_answerer(</span><br><span class="line">    question=<span class="string">&quot;Where do I work?&quot;</span>,</span><br><span class="line">    context=<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="Summarization"><a href="#Summarization" class="headerlink" title="Summarization"></a>Summarization</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">summarizer = pipeline(<span class="string">&quot;summarization&quot;</span>)</span><br><span class="line">summarizer(</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    America has changed dramatically during recent years. Not only has the number of </span></span><br><span class="line"><span class="string">    graduates in traditional engineering disciplines such as mechanical, civil, </span></span><br><span class="line"><span class="string">    electrical, chemical, and aeronautical engineering declined, but in most of </span></span><br><span class="line"><span class="string">    the premier American universities engineering curricula now concentrate on </span></span><br><span class="line"><span class="string">    and encourage largely the study of engineering science. As a result, there </span></span><br><span class="line"><span class="string">    are declining offerings in engineering subjects dealing with infrastructure, </span></span><br><span class="line"><span class="string">    the environment, and related issues, and greater concentration on high </span></span><br><span class="line"><span class="string">    technology subjects, largely supporting increasingly complex scientific </span></span><br><span class="line"><span class="string">    developments. While the latter is important, it should not be at the expense </span></span><br><span class="line"><span class="string">    of more traditional engineering.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Rapidly developing economies such as China and India, as well as other </span></span><br><span class="line"><span class="string">    industrial countries in Europe and Asia, continue to encourage and advance </span></span><br><span class="line"><span class="string">    the teaching of engineering. Both China and India, respectively, graduate </span></span><br><span class="line"><span class="string">    six and eight times as many traditional engineers as does the United States. </span></span><br><span class="line"><span class="string">    Other industrial countries at minimum maintain their output, while America </span></span><br><span class="line"><span class="string">    suffers an increasingly serious decline in the number of engineering graduates </span></span><br><span class="line"><span class="string">    and a lack of well-educated engineers.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">translator = pipeline(<span class="string">&quot;translation&quot;</span>, model=<span class="string">&quot;Helsinki-NLP/opus-mt-fr-en&quot;</span>)</span><br><span class="line">translator(<span class="string">&quot;Ce cours est produit par Hugging Face.&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p><img data-src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."></p><h4 id="Preprocessing-with-a-tokenizer"><a href="#Preprocessing-with-a-tokenizer" class="headerlink" title="Preprocessing with a tokenizer"></a>Preprocessing with a tokenizer</h4><p>与其他神经网络一样，Transformer模型无法直接处理原始文本， 因此我们pipeline的第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用<em>tokenizer</em>(标记器)，负责：</p><ul><li>将输入拆分为单词、子单词或符号（如标点符号），称为标记(<em>token</em>)</li><li>将每个标记(token)映射到一个整数</li><li>添加可能对模型有用的其他输入</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from transformers import AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">raw_inputs = [</span><br><span class="line">    &quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;,</span><br><span class="line">    &quot;I hate this so much!&quot;,</span><br><span class="line">]</span><br><span class="line">inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=&quot;pt&quot;)</span><br><span class="line">print(inputs)</span><br></pre></td></tr></table></figure><h4 id="Going-through-the-model"><a href="#Going-through-the-model" class="headerlink" title="Going through the model"></a>Going through the model</h4><p>我们可以像下载标记器一样下载我们的预训练模型。 Transformers提供了一个AutoModel类，该类还具有from_pretrained（）方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">model = AutoModel.from_pretrained(checkpoint)</span><br></pre></td></tr></table></figure><p>这里说一下embedding现在常用的生成方式,这与transformer有关,关于Hugging Face的使用以及其教程后续继续更新.</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>在语义上用数字表示,模型能够理解.</p><p><img data-src="https://img1.imgtp.com/2023/09/29/ivB86Tl2.png" alt="image-20230929171253690"></p><p>一句话如何embedding</p><p>简单的方法:单独地embed每个word,然后计算所有word的embedding的均值或和.但这样无法区分顺序.</p><p>现在常用的办法:使用transformer网络计算每个word的context-aware的表示,然后计算每个表示的均值.或者为每个token计算embedding而不是word.或者再利用transformer在自己的数据上继续训练,使得相似的句子相似度更高.</p><p>多模态嵌入也是比较新的东西,将图像与文本都嵌入到一个域内.比如CLIP模型.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://huggingface.co/learn/nlp-course">https://huggingface.co/learn/nlp-course</a></li><li><a href="https://learn.deeplearning.ai/google-cloud-vertex-ai">https://learn.deeplearning.ai/google-cloud-vertex-ai</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;深度学习中常用的技术,这里结合一些tutorial简单学习一下.&lt;br&gt;</summary>
    
    
    
    
    <category term="Embedding" scheme="https://www.sekyoro.top/tags/Embedding/"/>
    
  </entry>
  
  <entry>
    <title>大模型微调</title>
    <link href="https://www.sekyoro.top/2023/09/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/"/>
    <id>https://www.sekyoro.top/2023/09/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</id>
    <published>2023-09-28T12:15:47.000Z</published>
    <updated>2023-10-01T13:56:36.604Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这段时间非常火的topic,大模型参数多,占用体积大训练困难,而且一般需要微调技术用于特定任务.<br><span id="more"></span></p><p><a href="https://colab.research.google.com/drive/14MCMope8tjZkg5H5mXKSr_g2lYi5woVU#scrollTo=_Gw1sj7sagEE&amp;uniqifier=1">AnimeBot.ipynb - Colaboratory (google.com)</a>我的完整代码</p><h2 id="什么是大模型LLM"><a href="#什么是大模型LLM" class="headerlink" title="什么是大模型LLM"></a>什么是大模型LLM</h2><blockquote><p>LLM是大型语言模型的缩写，是人工智能和机器学习领域的最新创新。2022年12月，随着ChatGPT的发布，这种强大的新型人工智能在网上疯传。对于那些足够开明的人来说，生活在人工智能的嗡嗡声和科技新闻周期之外，ChatGPT是一个在名为GPT-3的LLM上运行的聊天界面。</p></blockquote><p>最近的大模型就是Meta的llama2当然还有openai的GPT4,google的PaLM2.国内有清华的ChatGLM等等.</p><p>而大模型微调就是在此基础上更改其参数或者一些层使得更好应对一些下游任务.当你想将预先存在的模型适应特定的任务或领域时，微调模型在机器学习中至关重要。微调模型的决定取决于您的目标，这些目标通常是特定于领域或任务的。</p><p><img data-src="https://img1.imgtp.com/2023/09/28/NmFebCcD.png" alt="image-20230928220820907" style="zoom:50%;" /></p><p>现在关于微调的技术有很多,这些技术都是为了解决自己的specified task,一般需要特定的数据.</p><p>一般涉及三种方法。Prompt Engineering,embedding以及finetune也就是微调.</p><h3 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h3><p>简单来说就是跟模型对话时提前给一些已知的信息.</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*aeek418WCndtt1491JHK7w.png" alt="Chat GPT responds to the query using custom data (revenue numbers) provided in the prompt." style="zoom:67%;" /></p><p>这种方法简单,但是由于将大文本传递到LLM的提示大小和相关成本的限制，使用大文档集或网页作为LLM的输入不是最佳方式。</p><h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><p>嵌入是一种将信息（无论是文本、图像还是音频）表示为数字形式的方式</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*bYy116KZAanbxXta4PCkjQ.png" alt="img"></p><p>当需要将大量文档或网页传递给LLM时，嵌入效果很好。例如，当聊天机器人被构建为向用户提供一组策略文档的响应时，这种方法会很好地工作。</p><p>使用时需要将文本等内容生成embedding,这就需要seq2seq模型得到嵌入了.当用户想要查询LLM时，嵌入将从向量存储中检索并传递给LLM。LLM使用嵌入从自定义数据生成响应。</p><h3 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine tuning"></a>Fine tuning</h3><p><img data-src="https://img1.imgtp.com/2023/09/28/SHpKrgIW.png" alt="image-20230928205851057" style="zoom:67%;" /></p><p>微调是教模型如何处理输入查询以及如何表示响应的一种方式。例如，LLM可以通过提供有关客户评价和相应情绪的数据来进行微调。</p><p>微调通常用于为特定任务调整LLM，并在该范围内获得响应。该任务可以是电子邮件分类、情绪分析、实体提取、基于规格生成产品描述等</p><p>具体的微调技术有Lora,QLora,Peft等等</p><h4 id="Fine-tuning技术"><a href="#Fine-tuning技术" class="headerlink" title="Fine tuning技术"></a>Fine tuning技术</h4><h4 id="old-school"><a href="#old-school" class="headerlink" title="old school"></a>old school</h4><p>在老派的方法中，有各种方法可以微调预先训练的语言模型，每种方法都是根据特定需求和资源限制量身定制的。</p><ul><li>基于特征：它使用预先训练的LLM作为特征提取器，将输入文本转换为固定大小的数组。一个单独的分类器网络预测NLP任务中文本的分类概率。在训练中，只有分类器的权重会改变，这使得它对资源友好，但可能性能较差。</li><li>微调I：微调I通过添加额外的密集层来增强预先训练的LLM。在训练期间，只调整新添加的层的权重，同时保持预先训练的LLM权重冻结。在实验中，<strong>它显示出比基于特征的方法略好的性能</strong>。</li><li>微调II：在这种方法中，整个模型，包括预先训练的语言模型（LLM），都被解冻进行训练，允许更新所有模型权重。然而，它可能会导致<strong>灾难性的遗忘</strong>，新的特征会覆盖旧的知识。微调II是资源密集型的，但在需要最大性能时可提供卓越的结果。通用语言模型微调</li><li>ULMFiT是一种可应用于NLP任务的迁移学习方法。它涉及一个3层的AWD-LSTM体系结构来进行表示。ULMFiT是一种用于为特定下游任务微调预先训练的语言模型的方法。</li><li>基于梯度的参数重要性排序：这些方法用于对模型中特征或参数的重要性进行排序。在基于梯度的排序中，参数的重要性取决于排除参数时精度降低的程度。在基于随机森林的排序中，可以对每个特征的杂质减少进行平均，并根据该度量对特征进行排序。</li></ul><p><img data-src="https://img1.imgtp.com/2023/09/28/j3Yk0Qtv.png" alt="image-20230928210840072"></p><h4 id="LLM微调的前沿策略"><a href="#LLM微调的前沿策略" class="headerlink" title="LLM微调的前沿策略"></a>LLM微调的前沿策略</h4><ul><li>低秩自适应（LoRA）：LoRA是一种微调大型语言模型的技术。它使用低秩近似方法来降低将具有数十亿参数的模型（如GPT-3）适应特定任务或领域的计算和财务成本。</li><li>量化LoRA（QLoRA）：QLoRA是一种适用于大型语言模型（LLM）的高效微调方法，可显著减少内存使用，同时保持完整的16位微调性能。它通过将冻结的4位量化预训练语言模型的梯度反向传播到低秩适配器中来实现这一点。</li><li>参数高效微调（PEFT）：PEFT是一种NLP技术，<strong>通过只微调一小组参数</strong>，降低计算和存储成本，使预先训练的语言模型有效地适应各种应用。<strong>它可以消除灾难性的遗忘，为特定任务调整关键参数，并提供与图像分类和稳定扩散dreambooth等模式的全面微调相当的性能。这是一种在最小可训练参数的情况下实现高性能的有价值的方法。</strong></li><li>DeepSpeed:DeepSpeed是一个深度学习软件库，用于加速大型语言模型的训练。它包括ZeRO（零冗余优化器），这是一种用于分布式训练的内存高效方法。DeepSpeed可以自动优化使用Hugging Face的Trainer API的微调作业，并提供一个替代脚本来运行现有的微调脚本。</li><li>ZeRO：ZeRO是一组内存优化技术，能够有效训练具有数万亿参数的大型模型，如GPT-2和图灵NLG 17B。ZeRO的一个主要吸引力是不需要修改模型代码。这是一种内存高效的数据并行形式，可以让您访问所有可用GPU设备的聚合GPU内存，而不会因数据并行中的数据复制而导致效率低下。</li></ul><p>现在一般用lora及其衍生方法以及PEFT.</p><p>微调用的数据集可以自己做也可以到处找,比如hugging face上或者Google dataset,github上.</p><p>至于模型一般使用hugging face或者langchain等工具库直接调用,没有必要手动下载.获取到一般的语言或者其他类型的数据之后,一般都需要embedding等预处理步骤.embedding模型一般要与处理任务的模型有一定对应关系.</p><p>下面使用Hugging Face的transformers等库进行大模型微调.常常使用<code>AutoModel</code>,<code>AutoTokenizer</code>以及<code>AutoConfig</code>,通过调用<code>from_pretrained</code>获取相关信息.下面是一般训练流程.</p><h4 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transformers installation</span></span><br><span class="line">pip install transformers datasets</span><br><span class="line"><span class="comment"># To install from source instead of the last release, comment the command above and uncomment the following one.</span></span><br><span class="line">pip install git+https://github.com/huggingface/transformers.git</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;yelp_review_full&quot;</span>)</span><br><span class="line"><span class="comment">#dataset[&quot;train&quot;][100]</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">examples</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_datasets = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(output_dir=<span class="string">&quot;test_trainer&quot;</span>)</span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=small_train_dataset,</span><br><span class="line">    eval_dataset=small_eval_dataset,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure><p>上面的<code>compute_metrics</code>用于评估模型.<code>training_args</code>是训练时设置参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_metrics</span>(<span class="params">eval_pred</span>):</span></span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure><p>可以使用<code>trainer.push_to_hub()</code>推送到自己的仓库.这样会自动将训练超参数、训练结果和框架版本添加到您的模型卡中</p><h4 id="PEFT训练adapters"><a href="#PEFT训练adapters" class="headerlink" title="PEFT训练adapters"></a>PEFT训练adapters</h4><p>使用PEFT训练的适配器通常也比完整模型小一个数量级，便于共享、存储和加载。通常搭配Lora模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">peft_model_id = <span class="string">&quot;ybelkada/opt-350m-lora&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(peft_model_id)</span><br></pre></td></tr></table></figure><p>加载和使用PEFT适配器型,请确保Hub存储库或本地目录包含adapter_config.json文件和adapter weights.</p><p>也可以先加载基础model,再使用<code>load_adapter</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;facebook/opt-350m&quot;</span></span><br><span class="line">peft_model_id = <span class="string">&quot;ybelkada/opt-350m-lora&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id)</span><br><span class="line">model.load_adapter(peft_model_id)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>load_in_8bit</code>以及<code>device_map</code>涉及到将模型放哪和占用大小.</p><p>增加<code>adapter</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftConfig</span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;facebook/opt-350m&quot;</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>],</span><br><span class="line">    init_lora_weights=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.add_adapter(lora_config, adapter_name=<span class="string">&quot;adapter_1&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>训练一个adapter</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig</span><br><span class="line"></span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">    lora_alpha=<span class="number">16</span>,</span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,</span><br><span class="line">    r=<span class="number">64</span>,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">)</span><br><span class="line">model.add_adapter(peft_config)</span><br><span class="line">trainer = Trainer(model=model, ...)</span><br><span class="line">trainer.train()</span><br><span class="line">model.save_pretrained(save_dir)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(save_dir)</span><br></pre></td></tr></table></figure><p>每个 PEFT方法由<code>PeftConfig</code>类定义，该类存储用于构建<code>PeftModel</code>的所有重要参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, TaskType</span><br><span class="line"></span><br><span class="line">peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=<span class="literal">False</span>, r=<span class="number">8</span>, lora_alpha=<span class="number">32</span>, lora_dropout=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">    r=lora_r,</span><br><span class="line">    lora_alpha=lora_alpha,</span><br><span class="line">    lora_dropout=lora_dropout,</span><br><span class="line">    target_modules=lora_target_modules,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>使用<code>get_peft_model</code>函数包装基本模型和peft_config以创建PeftModel.并使用<code>print_trainable_parameters</code>打印需要更新的参数.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> get_peft_model</span><br><span class="line"></span><br><span class="line">model_name_or_path = <span class="string">&quot;bigscience/mt0-large&quot;</span></span><br><span class="line">tokenizer_name_or_path = <span class="string">&quot;bigscience/mt0-large&quot;</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)</span><br><span class="line"></span><br><span class="line">model = get_peft_model(model, peft_config)</span><br><span class="line">model.print_trainable_parameters()</span><br></pre></td></tr></table></figure><p><strong>保存并推送模型到仓库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">&quot;output_dir&quot;</span>)</span><br><span class="line">model.push_to_hub(<span class="string">&quot;my_awesome_peft_model&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>这只会保存增量经过训练的PEFT重量，这意味着它在存储、转移和装载方面非常高效。例如，在RAFT数据集的twitter_complaints子集上使用LoRA训练的bigscience/To_3B模型只包含两个文件：adapter_config.json和adapter_model.bin。</p></blockquote><p><strong>下载模型</strong></p><p>下面的方法是逻辑是首先通过PeftConfig得到peft的配置,从中得到基础模型位置,利用基础模型得到其模型和tokenizer,最后利用PeftModel得到model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel, PeftConfig</span><br><span class="line"></span><br><span class="line">peft_model_id = <span class="string">&quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;</span></span><br><span class="line">config = PeftConfig.from_pretrained(peft_model_id)</span><br><span class="line">  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)</span><br><span class="line">model = PeftModel.from_pretrained(model, peft_model_id)</span><br><span class="line">  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)</span><br><span class="line"></span><br><span class="line">  model = model.to(device)</span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br><span class="line">  inputs = tokenizer(<span class="string">&quot;Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">      outputs = model.generate(input_ids=inputs[<span class="string">&quot;input_ids&quot;</span>].to(<span class="string">&quot;cuda&quot;</span>), max_new_tokens=<span class="number">10</span>)</span><br><span class="line">      <span class="built_in">print</span>(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=<span class="literal">True</span>)[<span class="number">0</span>])</span><br><span class="line">  <span class="string">&#x27;complaint&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以简单地使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> AutoPeftModelForCausalLM</span><br><span class="line">peft_model = AutoPeftModelForCausalLM.from_pretrained(<span class="string">&quot;ybelkada/opt-350m-lora&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> AutoPeftModel</span><br><span class="line">model = AutoPeftModel.from_pretrained(peft_model_id)</span><br></pre></td></tr></table></figure><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><h4 id="下载所需包"><a href="#下载所需包" class="headerlink" title="下载所需包"></a>下载所需包</h4><p>一般是hugging face的transformers,datasets以及xformers,accelerate,trl,bitsandbytes,peft等库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!pip install -Uqqq pip --progress-bar off</span><br><span class="line">!pip install -qqq torch==2.0.1 --progress-bar off</span><br><span class="line">!pip install -qqq transformers==4.32.1 --progress-bar off</span><br><span class="line">!pip install -qqq datasets==2.14.4 --progress-bar off</span><br><span class="line">!pip install -qqq peft==0.5.0 --progress-bar off</span><br><span class="line">!pip install -qqq bitsandbytes==0.41.1 --progress-bar off</span><br><span class="line">!pip install -qqq trl==0.7.1 --progress-bar off</span><br></pre></td></tr></table></figure><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>数据处理方式特别多,有很多实现方式.这里主要使用pandas与datasets处理csv数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">animes_dataset = load_dataset(<span class="string">&quot;csv&quot;</span>, data_files = <span class="string">&quot;/content/animes.csv&quot;</span>) </span><br><span class="line">reviews_dataset = load_dataset(<span class="string">&quot;csv&quot;</span>, data_files = <span class="string">&quot;/content/reviews.csv&quot;</span>) </span><br><span class="line">animes_df = pd.DataFrame(animes_dataset[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">reviews_df = pd.DataFrame(reviews_dataset[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">merged_df = pd.merge(animes_df,reviews_df,left_on=<span class="string">&quot;uid&quot;</span>,right_on=<span class="string">&quot;anime_uid&quot;</span>)</span><br><span class="line"><span class="comment"># remove /n/r</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span>(<span class="params">x</span>):</span></span><br><span class="line">  <span class="comment">#remove multiple whitespace</span></span><br><span class="line">  new_string = <span class="built_in">str</span>(x).strip()</span><br><span class="line">  pattern = <span class="string">r&quot;\s&#123;3,&#125;&quot;</span></span><br><span class="line">  new_string = re.sub(pattern, <span class="string">&quot; &quot;</span>, new_string)</span><br><span class="line">  <span class="comment">#remove \r \n \t</span></span><br><span class="line">  pattern = <span class="string">r&quot;[\n\r\t]&quot;</span></span><br><span class="line">  new_string  = re.sub(pattern,<span class="string">&quot;&quot;</span>, new_string)</span><br><span class="line">  <span class="keyword">return</span> new_string</span><br><span class="line">merged_df[<span class="string">&quot;synopsis&quot;</span>] = merged_df[<span class="string">&quot;synopsis&quot;</span>].<span class="built_in">map</span>(clean_text)</span><br><span class="line">merged_df[<span class="string">&quot;text&quot;</span>] = merged_df[<span class="string">&quot;text&quot;</span>].<span class="built_in">map</span>(clean_text)</span><br><span class="line"><span class="comment"># split merged_df into train and test</span></span><br><span class="line">train_df, test_df = train_test_split(merged_df, test_size=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">dataset_dict = DatasetDict(&#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: Dataset.from_pandas(train_df),</span><br><span class="line">    <span class="string">&quot;validation&quot;</span>: Dataset.from_pandas(test_df)</span><br><span class="line">&#125;)</span><br><span class="line">DEFAULT_SYSTEM_PROMPT = <span class="string">&quot;Below is a name of an anime,write some intro about it&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">DEFAULT_SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT.strip()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_training_prompt</span>(<span class="params">data_point</span>):</span></span><br><span class="line">  <span class="comment"># 去除字符串中的方括号和空格</span></span><br><span class="line">  genres = data_point[<span class="string">&quot;genre&quot;</span>].strip(<span class="string">&quot;[]&quot;</span>).replace(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;\&#x27;&quot;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line">  synopsis_len = <span class="built_in">len</span>(data_point[<span class="string">&quot;synopsis&quot;</span>])</span><br><span class="line">  split_len = random.randint(<span class="number">1</span>,synopsis_len)</span><br><span class="line">  synopsis_input = data_point[<span class="string">&quot;synopsis&quot;</span>][<span class="number">1</span>:split_len]</span><br><span class="line"></span><br><span class="line">  <span class="built_in">input</span> = data_point[<span class="string">&quot;title&quot;</span>]+genres+synopsis_input</span><br><span class="line">  output = data_point[<span class="string">&quot;synopsis&quot;</span>]+data_point[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">  <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="string">&quot;text&quot;</span>:<span class="string">f&quot;&quot;&quot;### Instruction: <span class="subst">&#123;DEFAULT_SYSTEM_PROMPT&#125;</span></span></span><br><span class="line"><span class="string">            ### Input:</span></span><br><span class="line"><span class="string">            <span class="subst">&#123;<span class="built_in">input</span>.strip()&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ### Response:</span></span><br><span class="line"><span class="string">            <span class="subst">&#123;output.strip()&#125;</span></span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.strip()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_dataset</span>(<span class="params">data: Dataset</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        data.shuffle(seed=<span class="number">42</span>)</span><br><span class="line">        .<span class="built_in">map</span>(generate_training_prompt)</span><br><span class="line">        .remove_columns(</span><br><span class="line">              [</span><br><span class="line">                <span class="string">&quot;uid_x&quot;</span>,</span><br><span class="line">                <span class="string">&quot;aired&quot;</span>,</span><br><span class="line">                <span class="string">&quot;members&quot;</span>,</span><br><span class="line">                <span class="string">&quot;img_url&quot;</span>,</span><br><span class="line">                <span class="string">&quot;uid_y&quot;</span>,</span><br><span class="line">                <span class="string">&quot;profile&quot;</span>,</span><br><span class="line">                <span class="string">&quot;anime_uid&quot;</span>,</span><br><span class="line">                <span class="string">&quot;score_y&quot;</span>,</span><br><span class="line">                <span class="string">&quot;link_y&quot;</span></span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">dataset_dict[<span class="string">&quot;train&quot;</span>] = process_dataset(dataset_dict[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">dataset_dict[<span class="string">&quot;validation&quot;</span>] = process_dataset(dataset_dict[<span class="string">&quot;validation&quot;</span>])</span><br></pre></td></tr></table></figure><p>这里处理逻辑其实复杂了,只需要使用pandas读取数据,然后分为训练集和测试集然后转为Dataset即可.中间需要对dataframe的数据去除一些空白字符等.</p><h4 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h4><p>由于使用了PEFT</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">lora_r = <span class="number">16</span></span><br><span class="line">lora_alpha = <span class="number">64</span></span><br><span class="line">lora_dropout = <span class="number">0.1</span></span><br><span class="line">lora_target_modules = [</span><br><span class="line">    <span class="string">&quot;q_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;up_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;k_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;down_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;gate_proj&quot;</span>,</span><br><span class="line">    <span class="string">&quot;v_proj&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">    r=lora_r,</span><br><span class="line">    lora_alpha=lora_alpha,</span><br><span class="line">    lora_dropout=lora_dropout,</span><br><span class="line">    target_modules=lora_target_modules,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>设置trainingArgument,使用trl进行训练.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">OUTPUT_DIR = <span class="string">&quot;experiments&quot;</span></span><br><span class="line">training_arguments = TrainingArguments(</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">    optim=<span class="string">&quot;paged_adamw_32bit&quot;</span>,</span><br><span class="line">    logging_steps=<span class="number">1</span>,</span><br><span class="line">    learning_rate=<span class="number">1e-4</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>,</span><br><span class="line">    max_grad_norm=<span class="number">0.3</span>,</span><br><span class="line">    num_train_epochs=<span class="number">2</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;steps&quot;</span>,</span><br><span class="line">    eval_steps=<span class="number">0.2</span>,</span><br><span class="line">    warmup_ratio=<span class="number">0.05</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    group_by_length=<span class="literal">True</span>,</span><br><span class="line">    output_dir=OUTPUT_DIR,</span><br><span class="line">    report_to=<span class="string">&quot;tensorboard&quot;</span>,</span><br><span class="line">    save_safetensors=<span class="literal">True</span>,</span><br><span class="line">    lr_scheduler_type=<span class="string">&quot;cosine&quot;</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">)</span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model=model,</span><br><span class="line">    train_dataset=dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=dataset[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    peft_config=peft_config,</span><br><span class="line">    dataset_text_field=<span class="string">&quot;text&quot;</span>,</span><br><span class="line">    max_seq_length=<span class="number">4096</span>,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    args=training_arguments,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="训练与后续评估测试"><a href="#训练与后续评估测试" class="headerlink" title="训练与后续评估测试"></a>训练与后续评估测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> AutoPeftModelForCausalLM</span><br><span class="line"><span class="comment"># Load Lora adapter</span></span><br><span class="line"><span class="comment"># model = PeftModel.from_pretrained(</span></span><br><span class="line"><span class="comment">#     base_model,</span></span><br><span class="line"><span class="comment">#     &quot;/content/Finetuned_adapter&quot;,</span></span><br><span class="line"><span class="comment">#     )</span></span><br><span class="line"><span class="comment"># merged_model = model.merge_and_unload()</span></span><br><span class="line">trained_model = AutoPeftModelForCausalLM.from_pretrained(</span><br><span class="line">    OUTPUT_DIR,</span><br><span class="line">    low_cpu_mem_usage=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">merged_model = base_model.merge_and_unload()</span><br><span class="line">merged_model.save_pretrained(<span class="string">&quot;merged_model&quot;</span>, safe_serialization=<span class="literal">True</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;merged_model&quot;</span>)</span><br><span class="line"><span class="comment"># trainer.push_to_hub(&quot;anime_chatbot&quot;)</span></span><br><span class="line">merged_model.push_to_hub(<span class="string">&quot;anime_chatbot&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Pushed to hub&quot;</span>)</span><br><span class="line"><span class="comment"># @title test fine tune model</span></span><br><span class="line"><span class="comment"># @title test base model</span></span><br><span class="line">DEFAULT_SYSTEM_PROMPT = <span class="string">&quot;Below is a name of an anime,write some intro about it&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">DEFAULT_SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT.strip()</span><br><span class="line">user_prompt = <span class="keyword">lambda</span> <span class="built_in">input</span>:<span class="string">f&quot;&quot;&quot;### Instruction: <span class="subst">&#123;DEFAULT_SYSTEM_PROMPT&#125;</span></span></span><br><span class="line"><span class="string">            ### Input:</span></span><br><span class="line"><span class="string">            <span class="subst">&#123;<span class="built_in">input</span>.strip()&#125;</span></span></span><br><span class="line"><span class="string">            ### Response:</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span>.strip()</span><br><span class="line">pipe = pipeline(<span class="string">&#x27;text-generation&#x27;</span>,model=merged_model,tokenizer=tokenizer,max_length=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">result = pipe(user_prompt(<span class="string">&quot;please introduce shingekinokyojin&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(result[<span class="number">0</span>][<span class="string">&#x27;generated_text&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model_base = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;facebook/opt-350m&quot;</span>, torch_dtype=torch.bfloat16)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;facebook/opt-350m&quot;</span>)</span><br></pre></td></tr></table></figure><p>这里model_base是</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">OPTForCausalLM(</span><br><span class="line">  (model): OPTModel(</span><br><span class="line">    (decoder): OPTDecoder(</span><br><span class="line">      (embed_tokens): Embedding(50272, 512, <span class="attribute">padding_idx</span>=1)</span><br><span class="line">      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)</span><br><span class="line">      (project_out): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=512, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">      (project_in): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (0-23): 24 x OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=4096, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(<span class="attribute">in_features</span>=4096, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (lm_head): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=50272, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> get_peft_model</span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>],</span><br><span class="line">    init_lora_weights=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line">peft_model = get_peft_model(peft_model_base, lora_config)</span><br><span class="line">peft_model.print_trainable_parameters()</span><br></pre></td></tr></table></figure><p>使用lora_config获取到peft_model</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">PeftModel(</span><br><span class="line">  (base_model): LoraModel(</span><br><span class="line">    (model): OPTForCausalLM(</span><br><span class="line">      (model): OPTModel(</span><br><span class="line">        (decoder): OPTDecoder(</span><br><span class="line">          (embed_tokens): Embedding(50272, 512, <span class="attribute">padding_idx</span>=1)</span><br><span class="line">          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)</span><br><span class="line">          (project_out): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=512, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">          (project_in): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">          (layers): ModuleList(</span><br><span class="line">            (0-23): 24 x OPTDecoderLayer(</span><br><span class="line">              (self_attn): OPTAttention(</span><br><span class="line">                (k_proj): Linear(</span><br><span class="line">                  <span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span></span><br><span class="line">                  (lora_dropout): ModuleDict(</span><br><span class="line">                    (default): Identity()</span><br><span class="line">                  )</span><br><span class="line">                  (lora_A): ModuleDict(</span><br><span class="line">                    (default): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">                  )</span><br><span class="line">                  (lora_B): ModuleDict(</span><br><span class="line">                    (default): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">                  )</span><br><span class="line">                  (lora_embedding_A): ParameterDict()</span><br><span class="line">                  (lora_embedding_B): ParameterDict()</span><br><span class="line">                )</span><br><span class="line">                (v_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">                (q_proj): Linear(</span><br><span class="line">                  <span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span></span><br><span class="line">                  (lora_dropout): ModuleDict(</span><br><span class="line">                    (default): Identity()</span><br><span class="line">                  )</span><br><span class="line">                  (lora_A): ModuleDict(</span><br><span class="line">                    (default): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=8, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">                  )</span><br><span class="line">                  (lora_B): ModuleDict(</span><br><span class="line">                    (default): Linear(<span class="attribute">in_features</span>=8, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">                  )</span><br><span class="line">                  (lora_embedding_A): ParameterDict()</span><br><span class="line">                  (lora_embedding_B): ParameterDict()</span><br><span class="line">                )</span><br><span class="line">                (out_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">              )</span><br><span class="line">              (activation_fn): ReLU()</span><br><span class="line">              (self_attn_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">              (fc1): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=4096, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">              (fc2): Linear(<span class="attribute">in_features</span>=4096, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">              (final_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (lm_head): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=50272, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>使用<code>peft_model.merge_and_unload()</code>得到融合后的model</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">OPTForCausalLM(</span><br><span class="line">  (model): OPTModel(</span><br><span class="line">    (decoder): OPTDecoder(</span><br><span class="line">      (embed_tokens): Embedding(50272, 512, <span class="attribute">padding_idx</span>=1)</span><br><span class="line">      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)</span><br><span class="line">      (project_out): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=512, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">      (project_in): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (0-23): 24 x OPTDecoderLayer(</span><br><span class="line">          (self_attn): OPTAttention(</span><br><span class="line">            (k_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (v_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (q_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">            (out_proj): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          )</span><br><span class="line">          (activation_fn): ReLU()</span><br><span class="line">          (self_attn_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">          (fc1): Linear(<span class="attribute">in_features</span>=1024, <span class="attribute">out_features</span>=4096, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          (fc2): Linear(<span class="attribute">in_features</span>=4096, <span class="attribute">out_features</span>=1024, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">          (final_layer_norm): LayerNorm((1024,), <span class="attribute">eps</span>=1e-05, <span class="attribute">elementwise_affine</span>=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (lm_head): Linear(<span class="attribute">in_features</span>=512, <span class="attribute">out_features</span>=50272, <span class="attribute">bias</span>=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h4><ol><li>数据集的处理,微调的template该如何写</li></ol><p>找到的例子</p><p><a href="https://colab.research.google.com/github/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain/blob/master/14.fine-tuning-llama-2-7b-on-custom-dataset.ipynb#scrollTo=eRbskn48QNfW">14.fine-tuning-llama-2-7b-on-custom-dataset.ipynb - Colaboratory (google.com)</a></p><p><a href="https://colab.research.google.com/drive/1Ud2vVdjxs18qPCXG334E5rZfvo9b3nUN?usp=sharing#scrollTo=KfPuoMSrDD5-">Fine_tuned_Llama_PEFT_QLora.ipynb - Colaboratory (google.com)</a></p><p>在训练时使用一个template</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DEFAULT_SYSTEM_PROMPT = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Below is a conversation between a human and an AI agent. Write a summary of the conversation.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>.strip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_training_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    conversation: <span class="built_in">str</span>, summary: <span class="built_in">str</span>, system_prompt: <span class="built_in">str</span> = DEFAULT_SYSTEM_PROMPT</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;### Instruction: <span class="subst">&#123;system_prompt&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Input:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;conversation.strip()&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Response:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;summary&#125;</span></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>.strip()</span><br></pre></td></tr></table></figure><p>测试时</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    conversation: <span class="built_in">str</span>, system_prompt: <span class="built_in">str</span> = DEFAULT_SYSTEM_PROMPT</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;### Instruction: <span class="subst">&#123;system_prompt&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Input:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;conversation.strip()&#125;</span></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Response:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>.strip()</span><br></pre></td></tr></table></figure><ol><li><p>训练后得到的model是peftmodel还是什么类型的模型</p><p>一种方法是</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">repo_id = <span class="string">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span></span><br><span class="line">use_ram_optimized_load=<span class="literal">False</span></span><br><span class="line"></span><br><span class="line">base_model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    repo_id,</span><br><span class="line">    device_map=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">base_model.config.use_cache = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p><code>base_model</code>是一个<code>LlamaForCausalLM</code>类,训练完后使用</p><p><code>trainer.save_model(&quot;Finetuned_adapter&quot;)</code>保存模型,然后使用<code>PeftModel.from_pretrained</code>得到PeftModel</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = PeftModel.from_pretrained(</span><br><span class="line">    base_model,</span><br><span class="line">    <span class="string">&quot;/content/Finetuned_adapter&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">merged_model = model.merge_and_unload()</span><br></pre></td></tr></table></figure><p>然后保存模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">merged_model.save_pretrained(<span class="string">&quot;/content/Merged_model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;/content/Merged_model&quot;</span>)</span><br></pre></td></tr></table></figure><p>另一种是使用<code>AutoPeftModelForCausalLM</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> AutoPeftModelForCausalLM</span><br><span class="line"></span><br><span class="line">trained_model = AutoPeftModelForCausalLM.from_pretrained(</span><br><span class="line">    OUTPUT_DIR,</span><br><span class="line">    low_cpu_mem_usage=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">merged_model = model.merge_and_unload()</span><br><span class="line">merged_model.save_pretrained(<span class="string">&quot;merged_model&quot;</span>, safe_serialization=<span class="literal">True</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;merged_model&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://medium.com/walmartglobaltech/training-large-language-model-llm-on-your-data-2139eaad5f4f">Training Large Language Model (LLM) on your data | by Mohit Soni | Walmart Global Tech Blog | Aug, 2023 | Medium</a></li><li><a href="https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148">A Practical Introduction to LLMs | By: Shawhin Talebi | Towards Data Science</a></li><li><a href="https://www.lakera.ai/insights/llm-fine-tuning-guide">The Ultimate Guide to LLM Fine Tuning: Best Practices &amp; Tools | Lakera – Protecting AI teams that disrupt the world.</a></li><li>tutorial <a href="https://learn.deeplearning.ai/finetuning-large-language-models">https://learn.deeplearning.ai/finetuning-large-language-models</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这段时间非常火的topic,大模型参数多,占用体积大训练困难,而且一般需要微调技术用于特定任务.&lt;br&gt;</summary>
    
    
    
    
    <category term="llm" scheme="https://www.sekyoro.top/tags/llm/"/>
    
    <category term="finetune" scheme="https://www.sekyoro.top/tags/finetune/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移</title>
    <link href="https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    <id>https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/</id>
    <published>2023-09-26T03:00:48.000Z</published>
    <updated>2023-10-01T10:31:13.253Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>将一个图像中的风格应用在另一图像之上，即<em>风格迁移</em>（style transfer）这里我们需要两张输入图像：一张是<em>内容图像</em>，另一张是<em>风格图像</em>。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。 </p><span id="more"></span><h2 id="使用预训练模型进行迁移"><a href="#使用预训练模型进行迁移" class="headerlink" title="使用预训练模型进行迁移"></a>使用预训练模型进行迁移</h2><p><img data-src="https://zh.d2l.ai/_images/neural-style.svg" alt="风格迁移"></p><p>首先，我们初始化合成图像，例如将其初始化为内容图像。 </p><p>该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。 </p><p>接下来，我们通过前向传播（实线箭头方向）计算风格迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。 风格迁移常用的损失函数由3部分组成：</p><ol><li><em>内容损失</em>使合成图像与内容图像在内容特征上接近；</li><li><em>风格损失</em>使合成图像与风格图像在风格特征上接近；</li><li><em>全变分损失</em>则有助于减少合成图像中的噪点。</li></ol><p>最后，当模型训练结束时，我们输出风格迁移的模型参数，即得到最终的合成图像。</p><p>使用一个预训练模型,比如VGG提取内容与特征.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.vgg19(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出。</p><p> 一般来说，<strong>越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。</strong> <strong>为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，即<em>内容层</em>，来输出图像的内容特征</strong>。</p><p> 我们从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为<em>风格层</em>。 VGG网络使用了5个卷积块。可以选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">style_layers, content_layers = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">19</span>, <span class="number">28</span>], [<span class="number">25</span>]</span><br><span class="line">net = nn.Sequential(*[pretrained_net.features[i] <span class="keyword">for</span> i <span class="keyword">in</span></span><br><span class="line">                      <span class="built_in">range</span>(<span class="built_in">max</span>(content_layers + style_layers) + <span class="number">1</span>)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span>(<span class="params">X, content_layers, style_layers</span>):</span></span><br><span class="line">    contents = []</span><br><span class="line">    styles = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net)):</span><br><span class="line">        X = net[i](X)</span><br><span class="line">        <span class="comment"># forward pass 如果是选定的风格层或者是内容层 添加到列表中</span></span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> style_layers:</span><br><span class="line">            styles.append(X)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> content_layers:</span><br><span class="line">            contents.append(X)</span><br><span class="line">    <span class="keyword">return</span> contents, styles</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contents</span>(<span class="params">image_shape, device</span>):</span></span><br><span class="line">    content_X = preprocess(content_img, image_shape).to(device)</span><br><span class="line">    contents_Y, _ = extract_features(content_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> content_X, contents_Y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_styles</span>(<span class="params">image_shape, device</span>):</span></span><br><span class="line">    style_X = preprocess(style_img, image_shape).to(device) <span class="comment"># 得到tensor数据 放入网络中 将输出这张图像的内容和风格</span></span><br><span class="line">    _, styles_Y = extract_features(style_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> style_X, styles_Y</span><br></pre></td></tr></table></figure><p><code>get_contents</code>函数对内容图像抽取内容特征； <code>get_styles</code>函数对风格图像抽取风格特征。 因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容特征和风格特征。 </p><p>由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用<code>extract_features</code>函数来抽取合成图像的内容特征和风格特征.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgb_mean = torch.tensor([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = torch.tensor([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span>(<span class="params">img, image_shape</span>):</span></span><br><span class="line">    transforms = torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.Resize(image_shape),</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])</span><br><span class="line">    <span class="keyword">return</span> transforms(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess</span>(<span class="params">img</span>):</span></span><br><span class="line">    img = img[<span class="number">0</span>].to(rgb_std.device)</span><br><span class="line">    img = torch.clamp(img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) * rgb_std + rgb_mean, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torchvision.transforms.ToPILImage()(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>preprocess与postprocess分别将PIL数据转为tensor,tensor转为PIL数据.</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><strong>内容损失</strong></p><p>容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。 平方误差函数的两个输入均为<code>extract_features</code>函数计算所得到的内容层的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span>(<span class="params">Y_hat, Y</span>):</span></span><br><span class="line">    <span class="comment"># 我们从动态计算梯度的树中分离目标：</span></span><br><span class="line">    <span class="comment"># 这是一个规定的值，而不是一个变量。</span></span><br><span class="line">    <span class="keyword">return</span> torch.square(Y_hat - Y.detach()).mean()</span><br></pre></td></tr></table></figure><p><strong>风格损失</strong></p><p>风格损失与内容损失类似，也通过平方误差函数衡量合成图像与风格图像在风格上的差异。 为了表达风格层输出的风格，我们先通过<code>extract_features</code>函数计算风格层的输出。 假设该输出的样本数为1，通道数为c，高和宽分别为h和w，我们可以将此输出转换为矩阵X，其有c行和hw列。 这个矩阵可以被看作由c个长度为hw的向量x1,…,xc组合而成的。其中向量xi代表了通道i上的风格特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram</span>(<span class="params">X</span>):</span></span><br><span class="line">    num_channels, n = X.shape[<span class="number">1</span>], X.numel() // X.shape[<span class="number">1</span>]</span><br><span class="line">    X = X.reshape((num_channels, n))</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, X.T) / (num_channels * n)</span><br></pre></td></tr></table></figure><p>风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出。这里假设基于风格图像的格拉姆矩阵<code>gram_Y</code>已经预先计算好了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span>(<span class="params">Y_hat, gram_Y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.square(gram(Y_hat) - gram_Y.detach()).mean()</span><br></pre></td></tr></table></figure><p><strong>全变分损失</strong></p><p>有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。 一种常见的去噪方法是<em>全变分去噪</em>（total variation denoising）： 假设xi,j表示坐标(i,j)处的像素值，降低全变分损失<code>∑i,j|xi,j−xi+1,j|+|xi,j−xi,j+1|</code></p><p>能够尽可能使邻近的像素值相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span>(<span class="params">Y_hat</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (torch.<span class="built_in">abs</span>(Y_hat[:, :, <span class="number">1</span>:, :] - Y_hat[:, :, :-<span class="number">1</span>, :]).mean() +torch.<span class="built_in">abs</span>(Y_hat[:, :, :, <span class="number">1</span>:] - Y_hat[:, :, :, :-<span class="number">1</span>]).mean())</span><br></pre></td></tr></table></figure><p><strong>损失函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram</span>):</span></span><br><span class="line">    <span class="comment"># 分别计算内容损失、风格损失和全变分损失</span></span><br><span class="line">    contents_l = [content_loss(Y_hat, Y) * content_weight <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(contents_Y_hat, contents_Y)]</span><br><span class="line">    styles_l = [style_loss(Y_hat, Y) * style_weight <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(styles_Y_hat, styles_Y_gram)]</span><br><span class="line">    tv_l = tv_loss(X) * tv_weight</span><br><span class="line">    <span class="comment"># 对所有损失求和</span></span><br><span class="line">    l = <span class="built_in">sum</span>(<span class="number">10</span> * styles_l + contents_l + [tv_l])</span><br><span class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</span><br></pre></td></tr></table></figure><p>contents_l表示内容损失,</p><h3 id="初始化合成图像"><a href="#初始化合成图像" class="headerlink" title="初始化合成图像"></a>初始化合成图像</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_inits</span>(<span class="params">X, device, lr, styles_Y</span>):</span></span><br><span class="line">    gen_img = SynthesizedImage(X.shape).to(device)</span><br><span class="line">    gen_img.weight.data.copy_(X.data)</span><br><span class="line">    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)</span><br><span class="line">    styles_Y_gram = [gram(Y) <span class="keyword">for</span> Y <span class="keyword">in</span> styles_Y]</span><br><span class="line">    <span class="keyword">return</span> gen_img(), styles_Y_gram, trainer <span class="comment"># 返回与内容图像一样的数据与计算的风格gram特征</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SynthesizedImage</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_shape, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SynthesizedImage, self).__init__(**kwargs)</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(*img_shape))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br></pre></td></tr></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch</span>):</span></span><br><span class="line">    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y) <span class="comment">#生成原本图片 作为变量 并计算风格特征</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, <span class="number">0.8</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">10</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;style&#x27;</span>, <span class="string">&#x27;TV&#x27;</span>],</span><br><span class="line">                            ncols=<span class="number">2</span>, figsize=(<span class="number">7</span>, <span class="number">2.5</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        contents_Y_hat, styles_Y_hat = extract_features(</span><br><span class="line">            X, content_layers, style_layers) <span class="comment"># 获取内容和风格</span></span><br><span class="line">        contents_l, styles_l, tv_l, l = compute_loss(</span><br><span class="line">            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.axes[<span class="number">1</span>].imshow(postprocess(X))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [<span class="built_in">float</span>(<span class="built_in">sum</span>(contents_l)),</span><br><span class="line">                                     <span class="built_in">float</span>(<span class="built_in">sum</span>(styles_l)), <span class="built_in">float</span>(tv_l)])</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device, image_shape = d2l.try_gpu(), (<span class="number">300</span>, <span class="number">450</span>)</span><br><span class="line">net = net.to(device)</span><br><span class="line">content_X, contents_Y = get_contents(image_shape, device)</span><br><span class="line">_, styles_Y = get_styles(image_shape, device) <span class="comment"># 获取content_image和内容特征 内容特征就是传到预训练模型得到的相应层输出  以及style_image的样式特征</span></span><br><span class="line">output = train(content_X, contents_Y, styles_Y, device, <span class="number">0.3</span>, <span class="number">500</span>, <span class="number">50</span>) <span class="comment"># 训练传入X和其内容与样式特征</span></span><br></pre></td></tr></table></figure><p>合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩。例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理</p><h3 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h3><p><img data-src="https://img1.imgtp.com/2023/09/26/fqReGmQh.png" alt="image-20230926225405384"></p><p><img data-src="https://img1.imgtp.com/2023/09/26/spmizDnH.png" alt="image-20230926225430234"></p><p><img data-src="https://img1.imgtp.com/2023/09/26/RlUlfrPS.png" alt="image-20230926225439855"></p><p>可以尝试改动style_weight,看看风格变换.比如style_weight增大,发现style loss太小了,而且会影响content loss</p><p><img data-src="https://img1.imgtp.com/2023/09/26/zHqMS03J.png" alt="image-20230926231717608"></p><ul><li>风格迁移常用的损失函数由3部分组成：（1）<strong>内容损失使合成图像与内容图像在内容特征上接近</strong>；（2）<strong>风格损失令合成图像与风格图像在风格特征上接近</strong>；（3）<strong>全变分损失则有助于减少合成图像中的噪点</strong>。</li><li>我们<strong>可以通过预训练的卷积神经网络来抽取图像的特征</strong>，并通过最小化损失函数来不断更新合成图像来作为模型参数。</li><li>我们使用<strong>gram矩阵表达风格层输出的风格</strong></li></ul><h2 id="使用GAN进行风格迁移"><a href="#使用GAN进行风格迁移" class="headerlink" title="使用GAN进行风格迁移"></a>使用GAN进行风格迁移</h2><p>GANs是生成艺术图像的好方法。另一种有趣的技术是所谓的风格转换，它获取一个内容图像，然后用不同的风格重新绘制，从风格图像中应用过滤器。</p><p>工作方式如下:</p><p>我们从随机噪声图像开始（或从内容图像开始，但为了理解起见，从随机噪声开始更容易）</p><p>我们的目标是创建这样一个图像，<strong>它将接近内容图像和风格图像</strong>。这将由两个损失函数确定：基于CNN在<strong>当前图像和内容图像的某些层提取的特征来计算内容损失</strong>,<strong>使用Gram矩阵巧妙地计算当前图像和风格图像之间的风格损失</strong></p><p>为了使图像更平滑并去除噪声，我们还引入了全变分损失，它计算相邻像素之间的平均距离</p><p>优化方式使用梯度下降（或一些其他优化算法）调整当前图像，以最小化总损失，总损失是所有三个损失的加权和。</p><p>在代码上与之前的差别是使用高斯分布采样得到的噪音作为需要优化的参数,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">img_style = load_image(<span class="string">&#x27;images/style.jpg&#x27;</span>)</span><br><span class="line">img_content = load_image(<span class="string">&#x27;images/image.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img_result = np.random.uniform(size=(img_size,img_size,<span class="number">3</span>))</span><br><span class="line">vgg = tf.keras.applications.VGG16(include_top=<span class="literal">False</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>)</span><br><span class="line">vgg.trainable = <span class="literal">False</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_extractor</span>(<span class="params">layers</span>):</span></span><br><span class="line">    outputs = [vgg.get_layer(x).output <span class="keyword">for</span> x <span class="keyword">in</span> layers]</span><br><span class="line">    model = tf.keras.Model([vgg.<span class="built_in">input</span>],outputs)</span><br><span class="line">    <span class="keyword">return</span> model </span><br><span class="line">content_layers = [<span class="string">&#x27;block4_conv2&#x27;</span>] </span><br><span class="line">content_extractor = layer_extractor(content_layers)</span><br><span class="line"></span><br><span class="line">content_target = content_extractor(preprocess_input(tf.expand_dims(img_content,axis=<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span>(<span class="params">img</span>):</span></span><br><span class="line">    z = content_extractor(preprocess_input(tf.expand_dims(<span class="number">255</span>*img,axis=<span class="number">0</span>))) </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*tf.reduce_sum((z-content_target)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">x</span>):</span></span><br><span class="line">  result = tf.linalg.einsum(<span class="string">&#x27;bijc,bijd-&gt;bcd&#x27;</span>, x, x)</span><br><span class="line">  input_shape = tf.shape(x)</span><br><span class="line">  num_locations = tf.cast(input_shape[<span class="number">1</span>]*input_shape[<span class="number">2</span>], tf.float32)</span><br><span class="line">  <span class="keyword">return</span> result/(num_locations)</span><br><span class="line"></span><br><span class="line">style_layers = [<span class="string">&#x27;block1_conv1&#x27;</span>,<span class="string">&#x27;block2_conv1&#x27;</span>,<span class="string">&#x27;block3_conv1&#x27;</span>,<span class="string">&#x27;block4_conv1&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_extractor</span>(<span class="params">img</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [gram_matrix(x) <span class="keyword">for</span> x <span class="keyword">in</span> layer_extractor(style_layers)(img)]</span><br><span class="line"></span><br><span class="line">style_target = style_extractor(preprocess_input(tf.expand_dims(img_style,axis=<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span>(<span class="params">img</span>):</span></span><br><span class="line">    z = style_extractor(preprocess_input(tf.expand_dims(<span class="number">255</span>*img,axis=<span class="number">0</span>)))</span><br><span class="line">    loss = tf.add_n([tf.reduce_mean((x-target)**<span class="number">2</span>) </span><br><span class="line">                           <span class="keyword">for</span> x,target <span class="keyword">in</span> <span class="built_in">zip</span>(z,style_target)])</span><br><span class="line">    <span class="keyword">return</span> loss / <span class="built_in">len</span>(style_layers)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variation_loss</span>(<span class="params">img</span>):</span></span><br><span class="line">  img = tf.cast(img,tf.float32)</span><br><span class="line">  x_var = img[ :, <span class="number">1</span>:, :] - img[ :, :-<span class="number">1</span>, :]</span><br><span class="line">  y_var = img[ <span class="number">1</span>:, :, :] - img[ :-<span class="number">1</span>, :, :]</span><br><span class="line">  <span class="keyword">return</span> tf.reduce_sum(tf.<span class="built_in">abs</span>(x_var)) + tf.reduce_sum(tf.<span class="built_in">abs</span>(y_var))</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_loss_var</span>(<span class="params">img</span>):</span></span><br><span class="line">    <span class="keyword">return</span> content_loss(img)+<span class="number">150</span>*style_loss(img)+<span class="number">30</span>*variation_loss(img)</span><br><span class="line"></span><br><span class="line">img.assign(clip(np.random.normal(-<span class="number">0.3</span>,<span class="number">0.3</span>,size=img_content.shape)+img_content/<span class="number">255.0</span>))</span><br><span class="line"></span><br><span class="line">train(img,loss_fn=total_loss_var)</span><br></pre></td></tr></table></figure><p>注意,提取风格的层数一般选择每个特征块的前面几层,而提取内容的层数一般选择特征块的后面几块.</p><p>风格迁移与迁移学习存在不可区分的关系,因为我们将一些知识从一个神经网络模型转移到另一个。在迁移学习中，我们通常从预先训练的模型开始，该模型已经在一些大型图像数据集（如ImageNet）上进行了训练。</p><p>预训练模型比如:</p><ul><li>VGG-16/VGG-19，它们是相对简单的模型，仍然提供良好的精度。经常将VGG作为第一次尝试是了解迁移学习如何运作的好选择。</li><li>ResNet是微软研究院于2015年提出的一系列模型。它们有更多的层，因此占用更多的资源。</li><li>MobileNet是一系列尺寸较小的型号，适用于移动设备。如果你缺乏资源，并且可能会牺牲一点准确性，那么就使用它们。</li></ul><p>使用pytorch加载预训练模型.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vgg = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg</span><br></pre></td></tr></table></figure><p>结构如下,可以看见有<code>features</code>,<code>avgpool</code>以及<code>classifier</code></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (3): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (4): MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>)</span><br><span class="line">    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (6): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (8): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (9): MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>)</span><br><span class="line">    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (13): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (15): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (16): MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>)</span><br><span class="line">    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (18): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (20): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (22): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (23): MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>)</span><br><span class="line">    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (25): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (27): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (29): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (30): MaxPool2d(<span class="attribute">kernel_size</span>=2, <span class="attribute">stride</span>=2, <span class="attribute">padding</span>=0, <span class="attribute">dilation</span>=1, <span class="attribute">ceil_mode</span>=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(<span class="attribute">in_features</span>=25088, <span class="attribute">out_features</span>=4096, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">    (1): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (2): Dropout(<span class="attribute">p</span>=0.5, <span class="attribute">inplace</span>=<span class="literal">False</span>)</span><br><span class="line">    (3): Linear(<span class="attribute">in_features</span>=4096, <span class="attribute">out_features</span>=4096, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">    (4): ReLU(<span class="attribute">inplace</span>=<span class="literal">True</span>)</span><br><span class="line">    (5): Dropout(<span class="attribute">p</span>=0.5, <span class="attribute">inplace</span>=<span class="literal">False</span>)</span><br><span class="line">    (6): Linear(<span class="attribute">in_features</span>=4096, <span class="attribute">out_features</span>=1000, <span class="attribute">bias</span>=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>查看每一层后的结果</strong></p><p>利用<code>torchinfo</code>库查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(vgg,input_size=(<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br></pre></td></tr></table></figure><p><img data-src="https://img1.imgtp.com/2023/10/01/MiSavdBT.png" alt="image-20231001161905086" style="zoom:67%;" /></p><p>可以看到输入一张3通道224的图像特征层输出是512通道的宽高为7的特征</p><p><strong>提取图像特征</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">res = vgg.features(sample_image).cpu()</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">plt.imshow(res.detach().view(<span class="number">512</span>,-<span class="number">1</span>).T)</span><br><span class="line"><span class="built_in">print</span>(res.size())</span><br></pre></td></tr></table></figure><p>利用feature层提取特征,然后利用预训练模型提取的特征,直接拿一个简单的Linear层作为分类器进行训练,比如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vgg_dataset = torch.utils.data.TensorDataset(feature_tensor,label_tensor.to(torch.long))</span><br><span class="line">train_ds, test_ds = torch.utils.data.random_split(vgg_dataset,[<span class="number">700</span>,<span class="number">100</span>])</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_ds,batch_size=<span class="number">32</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_ds,batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(torch.nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>,<span class="number">2</span>),torch.nn.LogSoftmax()).to(device)</span><br><span class="line"></span><br><span class="line">history = train(net,train_loader,test_loader)</span><br></pre></td></tr></table></figure><p>net就是简单的线性层加一个激活函数</p><p><strong>常用的加载数据流程</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">std_normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                          std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">trans = transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(), </span><br><span class="line">        std_normalize])</span><br><span class="line">dataset = torchvision.datasets.ImageFolder(<span class="string">&#x27;data/PetImages&#x27;</span>,transform=trans)</span><br><span class="line">trainset, testset = torch.utils.data.random_split(dataset,[<span class="number">20000</span>,<span class="built_in">len</span>(dataset)-<span class="number">20000</span>])</span><br></pre></td></tr></table></figure><p>如果图像在一个文件夹中,利用<code>ImageFolder</code>得到dataset.然后使用<code>dl = torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=True)</code>用于循环每个batch处理.</p><p>可以在训练过程中使用原始VGG-16网络作为一个整体来避免手动预计算特征.如下</p><ul><li><strong>将最终分类器替换为将产生所需数量的类的分类</strong>器。</li><li><strong>冻结卷积特征提取器的权重，使得它们不被训练</strong>。建议最初进行这种冻结，因为否则未经训练的分类器层可能会破坏卷积提取器的原始预训练权重。冻结权重可以通过将所有参数的requires_grad属性设置为False来实现</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> vgg.features.parameters():</span><br><span class="line">    x.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>如果您的对象在视觉上与普通的ImageNet图像不同，则这种功能组合可能无法发挥最佳效果。因此，开始训练卷积层也是有意义的。 为此，我们可以解冻之前冻结的卷积滤波器参数。不过一般都会采用一些微调方法,比如LoRA等.</p><h3 id="其他方向"><a href="#其他方向" class="headerlink" title="其他方向"></a>其他方向</h3><p>domain knowledge,domain adaption或者是transfer learning,本质上都是像提取一些特征,这种特征能在多个domain上使用.我们可以考虑利用这种特征进行可视化或者对抗攻击等.</p><p>比如利用预训练模型作为分类器,尝试从下从正态分布采样得到噪声图,然后作为输入,优化这个输入使其被分类为想要的分类.这样的图像虽然被正确分类了但人眼还是能明显看出差别.</p><p><img data-src="https://img1.imgtp.com/2023/10/01/Mu9sCSMV.png" alt="image-20231001174507317"></p><blockquote><p>这种噪音对我们来说没有多大意义，但很可能它包含了很多需要的类别(比如猫)特有的低级别过滤器。然而，由于有很多方法可以优化输入以获得理想的结果，因此优化算法没有动机找到视觉上可理解的模式.</p><p>为了让它看起来不那么像噪音，我们可以在损失函数中引入一个附加项——变化损失。它测量图像的相邻像素的相似程度。如果我们将这个项添加到损失函数中，它将迫使优化器找到噪声较小的解决方案，从而具有更多可识别的细节</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_loss</span>(<span class="params">target,res</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span>*tf.reduce_mean(keras.metrics.sparse_categorical_crossentropy(target,res)) + \</span><br><span class="line">           <span class="number">0.005</span>*tf.image.total_variation(x,res)</span><br><span class="line"></span><br><span class="line">optimize(x,target,loss_fn=total_loss)</span><br></pre></td></tr></table></figure><p>也就是分类的损失加上全变分损失.全变分损失目的是减小噪声,得到图像图下</p><p><img data-src="https://img1.imgtp.com/2023/10/01/v3ypktNG.png" alt="image-20231001174926326"></p><p>对抗攻击就利用一张本身是狗分类也确实是狗的图片,对这张图片进行优化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(np.expand_dims(img,axis=<span class="number">0</span>).astype(np.float32)/<span class="number">255.0</span>)</span><br><span class="line">optimize(x,target,epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p><img data-src="https://img1.imgtp.com/2023/10/01/xcacfJg1.png" alt="image-20231001175428790"></p><p>在pytorch中使用<code>autograd</code>计算梯度.<a href="https://www.w3cschool.cn/article/4182917.html">pytorch 中autograd.grad()函数的用法说明 | w3cschool笔记</a></p><p>最后推荐一下微软的Ai for beginners的课程,质量比较高,此外还有李沐的d2l,台湾李宏毅老师的深度学习课程以及fast.ai课程,都是比较好的.</p><blockquote><p>我们能够在预先训练的CNN中可视化猫（以及任何其他物体）的理想图像，<strong>使用梯度下降优化来调整输入图像而不是权重</strong>。获得有意义的图像的主要技巧是使用<strong>变化损失作为额外的损失函数</strong>，这会<strong>使图像看起来更平滑</strong>。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://zh.d2l.ai/chapter_computer-vision/neural-style.html">13.12. 风格迁移 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li><a href="https://github.com/microsoft/AI-For-Beginners/tree/main/lessons/4-ComputerVision/10-GANs">AI-For-Beginners/lessons/4-ComputerVision/10-GANs at main · microsoft/AI-For-Beginners (github.com)</a></li><li><a href="https://arxiv.org/abs/1508.06576">[1508.06576] A Neural Algorithm of Artistic Style (arxiv.org)</a></li><li><a href="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/08-TransferLearning/README.md">AI-For-Beginners/lessons/4-ComputerVision/08-TransferLearning/README.md at main · microsoft/AI-For-Beginners (github.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;将一个图像中的风格应用在另一图像之上，即&lt;em&gt;风格迁移&lt;/em&gt;（style transfer）这里我们需要两张输入图像：一张是&lt;em&gt;内容图像&lt;/em&gt;，另一张是&lt;em&gt;风格图像&lt;/em&gt;。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。 &lt;/p&gt;</summary>
    
    
    
    
    <category term="-style transfer" scheme="https://www.sekyoro.top/tags/style-transfer/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_自动驾驶(一)</title>
    <link href="https://www.sekyoro.top/2023/09/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    <id>https://www.sekyoro.top/2023/09/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/</id>
    <published>2023-09-25T07:39:52.000Z</published>
    <updated>2023-10-04T04:26:02.529Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>cvpr 2023 best paper <strong>Planning-oriented Autonomous Driving</strong><a href="https://arxiv.org/abs/2212.10156">[2212.10156] Planning-oriented Autonomous Driving (arxiv.org)</a></p><span id="more"></span><h2 id="Planning-oriented-Autonomous-Driving"><a href="#Planning-oriented-Autonomous-Driving" class="headerlink" title="Planning-oriented Autonomous Driving"></a>Planning-oriented Autonomous Driving</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p><strong>背景介绍</strong></p><p>现代自动驾驶系统的特点是<strong>按顺序执行模块化任务，即感知、预测和规划</strong>.</p><p>为了执行多种多样的任务并实现高级智能，<strong>当代的方法要么是为单个任务部署独立的模型</strong>，要么<strong>是设计一个具有独立头部的多任务范例</strong>。</p><p>但是，他们可能会出现<strong>累积性错误</strong>或<strong>任务协调能力不足</strong>的问题。</p><p><strong>本文工作</strong></p><p>我们将重新审视感知和预测的关键组成部分，并对任务进行优先排序，使所有这些任务都有助于规划。</p><p>我们提出了统一自动驾驶（UniAD）,这是一个将全栈驾驶任务整合到一个网络中的最新综合框架。它设计精巧，充分利用了每个模块的优势，并从全局角度为代理交互提供了互补的功能抽象。任务之间通过统一的查询界面进行交流，以促进彼此的规划工作</p><p><strong>模型有效性(SOTA)</strong></p><p> 我们在具有挑战性的 nuScenes 基准上对 UniAD 进行了实例化。通过广泛的消融，我们证明了使用这种理念的有效性，在所有方面都大大优于以往的先进水平。</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p><img data-src="https://img1.imgtp.com/2023/09/25/wKTdOjZn.png" alt="image-20230925154509561"></p><p>大多数行业解决方案为每项任务独立部署独立模型,只要板载芯片的资源带宽允许.这样的设计虽然<strong>简化了各团队的研发难度</strong>，但由于优化目标的孤立性，它也承担着<strong>跨模块信息丢失、错误积累和功能错位</strong>的风险。</p><p><img data-src="https://img1.imgtp.com/2023/09/25/OAr64WEG.png" alt="image-20230925190048282"></p><p>一种更优雅的设计是将多种任务纳入多任务学习multi-task learning（MTL）范式，方法是将多个任务特定的头(head)插入一个共享的特征提取器中.</p><blockquote><p>相当于保留网络的底部,更换网络的head用于特定任务</p></blockquote><p>MTL在general vision,自动驾驶以及工业产品中使用很多.在MTL中，跨任务的共同训练策略可以<strong>利用特征抽象</strong>,它可以毫不费力地<strong>扩展到其他任务</strong>，并节省板载芯片的计算成本。这样的方案可能会导致不希望的<strong>“负转移”</strong>（迁移学习,微调模型）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>) <span class="comment">#更换输出层</span></span><br><span class="line">nn.init.xavier_uniform_(finetune_net.fc.weight); <span class="comment"># 初始化权重</span></span><br><span class="line"><span class="comment"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                      param_group=<span class="literal">True</span></span>):</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    devices = d2l.try_all_gpus()</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">             <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        trainer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class="line">                                   &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(),</span><br><span class="line">                                    <span class="string">&#x27;lr&#x27;</span>: learning_rate * <span class="number">10</span>&#125;],</span><br><span class="line">                                lr=learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,</span><br><span class="line">                                  weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,</span><br><span class="line">                   devices)</span><br><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/github/d2l-ai/d2l-zh-pytorch-colab/blob/master/chapter_computer-vision/fine-tuning.ipynb#scrollTo=955d840b">fine-tuning.ipynb - Colaboratory (google.com)</a></p><blockquote><p>迁移学习</p><p>Transfer Learning的初衷是节省人工标注样本的时间，让模型可以通过已有的标记数据（source domain data）向未标记数据（target domain data）迁移。</p><p>负迁移指的是，在源域上学习到的知识，对于目标域上的学习产生<strong>负面作用</strong>。</p></blockquote><p><img data-src="https://img1.imgtp.com/2023/09/25/qfzwSSpk.png" alt="image-20230925194500746"></p><p>相比之下，端到端自动驾驶的出现将感知、预测和规划的所有节点统一为一个整体。</p><p><img data-src="https://img1.imgtp.com/2023/09/25/A5zqNJGg.png" alt="image-20230925194951100" style="zoom:80%;" /></p><p>应确定先前任务的选择和优先级，以便于规划,该系统应以规划为导向，精心设计，涉及某些组件，以便很少有<strong>独立模型中的累积误差</strong>或<strong>MTL方案中的负迁移</strong>。</p><p>端到端的做法中,一种“白板(tabula-rasa)”做法是直接预测计划的轨迹，没有任何明确的感知和预测监督。虽然这样的方向值得进一步探索，但在安全保障和可解释性方面是不够的，特别是对于高度动态的城市场景。在本文中，我们倾向于另一个角度，并提出以下问题：<strong>对于可靠且以规划为导向的自动驾驶系统，如何设计有利于规划的管道？前面哪些任务是必需的？</strong></p><p>一个直观的解决方案是感知周围的物体，预测未来的行为并明确计划一个安全的操作。当代方法提供了良好的见解并实现了令人印象深刻的性能。<strong>但是</strong>,以前的工作或多或少没有考虑某些组成部分让人想起以规划为导向的精神</p><p>为此，我们推出了UniAD，这是一个统一的自动驾驶算法框架，利用五个基本任务来实现安全可靠的系统。UniAD的设计理念是面向规划的。我们认为，这不是一堆简单的任务，需要简单的工程工作。一个关键组件是基于查询的设计来连接所有节点。与传统的边界框表示形式相比，查询受益于更大的感受域，以减轻上游预测的复合误差。此外，查询可以灵活地对各种交互进行建模和编码</p><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>UniAD由四个基于transformer decoder-based的感知和预测模块组成，最后由一个规划器组成。查询 Q 扮演连接pipeline的角色，以对驱动方案中实体的不同交互进行建模。</p><p><img data-src="https://img1.imgtp.com/2023/09/26/IfBcHYuX.png" alt="image-20230926094756674" style="zoom: 80%;" /></p><p>具体来说，将一系列多相机图像输入特征提取器，并通过BEVFormer中的现成BEV编码器将生成的透视视图特征转换为统一的鸟瞰图（BEV）特征B.</p><p>在 TrackFormer 中，我们称为跟踪查询(track queries)的可学习嵌入从 B 查询agents的信息，以检测和跟踪agents。learnable embeddings从B查询信息</p><p>MapFormer 将地图查询作为道路元素（例如车道和分隔线）的语义抽象，并对地图进行全景分割.</p><p>通过上述表示代理和映射的查询，MotionFormer 捕获agents之间的交互，并映射和预测每个agents的未来轨迹。</p><p>由于每个代理的操作可以显著影响场景中的其他代理，因此此模块对所有考虑的代理进行联合预测。同时，我们设计了一个自我车辆查询来显式地建模自我车辆，并使其能够在这种以场景为中心的范式中与其他代理进行交互</p><p>OccFormer 使用 BEV 功能 B 作为查询，配备agent-wise知识作为键和值，并在保留agents实体的情况下预测多步骤的未来占用(occupancy)。</p><p>Planner利用来自MotionFormer 的富有表现力的自我车辆查询来预测规划结果,使自己远离OccForer预测的占领区域以避免碰撞</p><p><img data-src="https://img1.imgtp.com/2023/09/25/FN7AyiOX.png" alt="image-20230925230653627"></p><h4 id="Perception-Tracking-and-Mapping"><a href="#Perception-Tracking-and-Mapping" class="headerlink" title="Perception: Tracking and Mapping"></a>Perception: Tracking and Mapping</h4><p>感知：跟踪和映射</p><p><img data-src="https://img1.imgtp.com/2023/09/26/xOis6Bvn.png" alt="image-20230926094812245"></p><p><strong>TrackFormer</strong>:它可以联合执行检测和多目标跟踪 (MOT)，而无需进行无差别的后处理。除了对象检测中使用的传统检测查询  之外，还引入了其他跟踪查询来跨帧跟踪代理.</p><p>具体来说，在每个时间步长内，<strong>初始化的检测查询负责检测首次感知到的新生agents</strong>，而<strong>跟踪查询则对前几帧中检测到的agents进行建模</strong>。</p><p>检测查询和跟踪查询都通过关注 BEV 特征 B 来捕捉agents抽象。TrackFormer 包含 N 层，最终输出状态 Q~A~ 为下游预测任务提供 N~a~ 有效代理的知识。</p><p>除了对自我车辆(ego-vehicle)周围的其他agents进行编码查询外，我们还在查询集中引入了一个特定的自我车辆查询，以明确模拟自动驾驶车辆本身，并进一步用于规划.</p><p><img data-src="https://img1.imgtp.com/2023/09/26/op5MpVnG.png" alt="image-20230926094908119"></p><p><strong>MapFormer</strong>:我们根据二维全景分割方法<code>Panoptic SegFormer</code>设计.我们将道路要素稀疏地表示为地图查询，以帮助下游运动预测，并对位置和结构知识进行编码。</p><p>我们将道路要素稀疏地表示为map queries，以帮助下游运动预测，并对位置和结构知识进行编码。在驾驶场景中，我们将车道、分隔线和交叉路口设置为 “物”，将可驾驶区域设置为 “物”。</p><p>MapFormer 也有 N 个堆叠层，每一层的输出结果都受到监督，只有最后一层的更新查询 QM 才会转发给 MotionFormer，以实现agents与地图之间的交互。</p><h4 id="Prediction-Motion-Forecasting"><a href="#Prediction-Motion-Forecasting" class="headerlink" title="Prediction: Motion Forecasting"></a>Prediction: Motion Forecasting</h4><p>预测运动预测</p><p>这种模式只需一次前向传递就能在帧中生成多代理轨迹，从而大大节省了将整个场景与每个代理的坐标对齐的计算成本。通过分别从 TrackFormer 和 MapFormer 获取的动态代理 QA 和静态地图 QM 的高度抽象查询，MotionFormer 可以以场景为中心预测所有代理的多模态未来运动，即前 k 个可能的轨迹。</p><p>这种范式通过单次前向传递在帧中生成多智能体轨迹，这大大节省了将整个场景与每个智能体的坐标对齐的计算成本.通过分别从 TrackFormer 和 MapFormer 获取的动态agents Q~A~ 和静态map Q~M~ 的高度抽象查询，MotionFormer 可以以场景为中心预测所有agents 的多模态未来运动，即前 k 个可能的轨迹。</p><p>同时，我们将来自TrackPreer的自我车辆查询传递到MotionFormer 以吸引自我车辆与其他代理进行交互，同时考虑未来的动态。形式上，输出运动公式化为</p><script type="math/tex; mode=display">{\{\hat{\mathrm{x}}_{i,k}~\in~\mathbb{R}^{T\times2}|i~=~1,\ldots,{N}_{a};k~=1,...,K}\}</script><p>其中 i 索引agents，k 索引轨迹模态，T 是预测范围的长度。</p><h4 id="MotionFormer"><a href="#MotionFormer" class="headerlink" title="MotionFormer"></a>MotionFormer</h4><p>它由 N 层组成，每层捕获三种类型的交互：agents-agents、agents-map和agents-goal point.</p><p><img data-src="https://img1.imgtp.com/2023/09/26/xGG6gpCp.png" alt="image-20230926105922503"></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;cvpr 2023 best paper &lt;strong&gt;Planning-oriented Autonomous Driving&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.10156&quot;&gt;[2212.10156] Planning-oriented Autonomous Driving (arxiv.org)&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>玩转huggingface</title>
    <link href="https://www.sekyoro.top/2023/09/23/%E7%8E%A9%E8%BD%AChuggingface/"/>
    <id>https://www.sekyoro.top/2023/09/23/%E7%8E%A9%E8%BD%AChuggingface/</id>
    <published>2023-09-23T03:29:45.000Z</published>
    <updated>2023-09-23T04:41:06.489Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Hugging Face这一段时间来特别火,融资拿到了一大笔钱,还跟Cousera这些社区常常联系,推出了<code>diffusers</code>,<code>Gradio</code>,<code>Transformers</code>等等好用的库和框架.也可以作为上传数据集和模型的地方,这里尝试玩玩其常用的一些功能.<br><span id="more"></span></p><h2 id="上传模型或数据集"><a href="#上传模型或数据集" class="headerlink" title="上传模型或数据集"></a>上传模型或数据集</h2><p>使用git上传文件<a href="https://huggingface.co/docs/hub/repositories-getting-started">Getting Started with Repositories (huggingface.co)</a></p><p>数据集项目结构<a href="https://huggingface.co/docs/datasets/repository_structure#define-your-splits-in-yaml">Structure your repository (huggingface.co)</a></p><p>一般来说应该使用<code>git</code>相关功能直接上传下载,但是一旦文件特别多或者大,上传数据集时就比较麻烦.我就遇到了这些问题,而且不止我遇到了.<a href="https://discuss.huggingface.co/t/batch-response-too-many-password-attempts-while-uploading-the-dataset-files-with-lfs/38458">Batch response: Too many password attempts while uploading the dataset files with lfs - 🤗Datasets - Hugging Face Forums</a> 解决办法就是使用官方提供的api写python代码上传,或者通过git尝试分批次上传你的数据集.</p><p><img data-src="https://img1.imgtp.com/2023/09/23/xVFLHv3r.png" alt="image-20230923115145189" style="zoom:80%;" /></p><p>这个问题截至我写时依旧是个Bug</p><p><img data-src="https://img1.imgtp.com/2023/09/23/pyJgakfx.png" alt="image-20230923115624652" style="zoom:67%;" /></p><p>看看这位兄弟的解决方案</p><p><img data-src="https://img1.imgtp.com/2023/09/23/nn8qJpon.png" alt="image-20230923115349963"></p><p>也就是使用<code>upload_folder</code>,知道这点后就方便多了,但是我也建议把git的ssh密钥也加到hugging face里.接下来按照官方教程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br><span class="line"><span class="comment"># or using an environment variable</span></span><br><span class="line">huggingface-cli login --token <span class="variable">$HUGGINGFACE_TOKEN</span></span><br></pre></td></tr></table></figure><p>注意,登录需要token,而创建token时,因为要上传数据,所以需要write权限.</p><p><img data-src="https://img1.imgtp.com/2023/09/23/e2AoQhzn.png" alt="image-20230923122538855"></p><p>登陆之后,如果要上传一个文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> HfApi</span><br><span class="line">api = HfApi()</span><br><span class="line">api.upload_file(</span><br><span class="line">    path_or_fileobj=<span class="string">&quot;D:/anime_face/misaka_mikoto.zip&quot;</span>,</span><br><span class="line">    path_in_repo=<span class="string">&quot;misaka_mikoto.zip&quot;</span>,</span><br><span class="line">    repo_id=<span class="string">&quot;proanimer/anime_face&quot;</span>,</span><br><span class="line">    repo_type=<span class="string">&quot;dataset&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><img data-src="https://img1.imgtp.com/2023/09/23/xaOwvr2o.png" alt="image-20230923121846662" style="zoom:67%;" /></p><p>文件夹也是同理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> HfApi</span><br><span class="line">api = HfApi()</span><br><span class="line">api.upload_folder(</span><br><span class="line">    folder_path=<span class="string">&quot;/path/to/local/space&quot;</span>,</span><br><span class="line">    repo_id=<span class="string">&quot;username/my-cool-space&quot;</span>,</span><br><span class="line">    repo_type=<span class="string">&quot;space&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>当然也可以使用hugging face的cli</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Upload file at root</span></span><br><span class="line">huggingface-cli upload my-cool-model model.safetensors</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload directory at root</span></span><br><span class="line">huggingface-cli upload my-cool-model ./models</span><br><span class="line"></span><br><span class="line"><span class="comment"># Upload `my-cool-model/` directory if it exist, raise otherwise</span></span><br><span class="line">huggingface-cli upload my-cool-model</span><br></pre></td></tr></table></figure><p>然后可以修改<code>DatasetCard</code>让其他人知道这是个什么,还有自动生成的<code>Dataset Viewer</code>.</p><p><img data-src="https://img1.imgtp.com/2023/09/23/ndTBxgfI.png" alt="image-20230923123734196" style="zoom:67%;" /></p><p>此外,官方建议除了CSV, JSON, JSON lines, text 以及Parquet这些格式数据之外,最好添加一个loading脚本用于加载数据集.</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_dataset/</span><br><span class="line">├── <span class="module-access"><span class="module"><span class="identifier">README</span>.</span></span>md</span><br><span class="line">└── my_dataset.py</span><br></pre></td></tr></table></figure><p>目的是</p><ul><li>Add dataset metadata.</li><li>Download data files.</li><li>Generate samples.</li><li>Generate dataset metadata.</li><li>Upload a dataset to the Hub.</li></ul><p>但是这里我就没有使用了,因为你可以直接<code>git clone</code>这个仓库然后解压即可.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Hugging Face这一段时间来特别火,融资拿到了一大笔钱,还跟Cousera这些社区常常联系,推出了&lt;code&gt;diffusers&lt;/code&gt;,&lt;code&gt;Gradio&lt;/code&gt;,&lt;code&gt;Transformers&lt;/code&gt;等等好用的库和框架.也可以作为上传数据集和模型的地方,这里尝试玩玩其常用的一些功能.&lt;br&gt;</summary>
    
    
    
    
    <category term="huggingface" scheme="https://www.sekyoro.top/tags/huggingface/"/>
    
  </entry>
  
  <entry>
    <title>大模型减枝和蒸馏</title>
    <link href="https://www.sekyoro.top/2023/09/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%87%8F%E6%9E%9D%E5%92%8C%E8%92%B8%E9%A6%8F/"/>
    <id>https://www.sekyoro.top/2023/09/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%87%8F%E6%9E%9D%E5%92%8C%E8%92%B8%E9%A6%8F/</id>
    <published>2023-09-21T14:21:00.000Z</published>
    <updated>2023-09-21T14:21:00.885Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>ZTM-pytorchForDL</title>
    <link href="https://www.sekyoro.top/2023/09/16/ZTM-pytorchForDL/"/>
    <id>https://www.sekyoro.top/2023/09/16/ZTM-pytorchForDL/</id>
    <published>2023-09-16T08:35:22.000Z</published>
    <updated>2023-09-26T01:19:14.421Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>ZeroToMastery<a href="https://www.learnpytorch.io/">Zero to Mastery Learn PyTorch for Deep Learning</a>上的课程学习<br><span id="more"></span></p><h2 id="chapter-1"><a href="#chapter-1" class="headerlink" title="chapter 1"></a>chapter 1</h2><p>设置seed,计算tensor的点乘以及tensor数据类型等基础操作,搭配官方文档即可.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random tensor</span></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove single dimensions</span></span><br><span class="line">y = torch.squeeze(x)</span><br><span class="line"><span class="comment"># Print out tensors and their shapes</span></span><br><span class="line"><span class="built_in">print</span>(x,x.shape)</span><br><span class="line"><span class="built_in">print</span>(y,y.shape)</span><br></pre></td></tr></table></figure><p>此外还有一些常用方法,比如<code>torch.squeeze</code>,<code>torch.cat</code>,<code>torch.stack</code>,<code>torch.unsqueeze</code>,<code>torch.Tensor.view</code>,<code>torch.Tensor.reshape</code>,<code>torch.Tensor.transpose</code>,<code>torch.Tensor.permute</code>等等</p><h2 id="chapter-2"><a href="#chapter-2" class="headerlink" title="chapter 2"></a>chapter 2</h2><h3 id="pytorch-workflow"><a href="#pytorch-workflow" class="headerlink" title="pytorch workflow"></a>pytorch workflow</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create PyTorch linear regression model by subclassing nn.Module</span></span><br><span class="line"><span class="comment">## Option 1</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegressionModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.weight = nn.Parameter(data=torch.randn(<span class="number">1</span>, </span><br><span class="line">                                              requires_grad=<span class="literal">True</span>,</span><br><span class="line">                                              dtype=torch.<span class="built_in">float</span></span><br><span class="line">                                              ))</span><br><span class="line">    </span><br><span class="line">    self.bias = nn.Parameter(data=torch.randn(<span class="number">1</span>, </span><br><span class="line">                                              requires_grad=<span class="literal">True</span>,</span><br><span class="line">                                              dtype=torch.<span class="built_in">float</span></span><br><span class="line">                                              ))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> self.weight * x + self.bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## Option 2</span></span><br><span class="line"><span class="comment"># class LinearRegressionModel(nn.Module):</span></span><br><span class="line"><span class="comment">#   def __init__(self):</span></span><br><span class="line"><span class="comment">#     super().__init__()</span></span><br><span class="line"><span class="comment">#     self.linear_layer = nn.Linear(in_features = 1,</span></span><br><span class="line"><span class="comment">#                                   out_features = 1)</span></span><br><span class="line"><span class="comment">#   def forward(self,x : torch.Tensor) -&gt; torch.Tensor:</span></span><br><span class="line"><span class="comment">#     return self.linear_layer(x)</span></span><br><span class="line">  </span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">model_1 = LinearRegressionModel()</span><br><span class="line">model_1,model_1.state_dict()</span><br><span class="line">     </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Create the loss function and optimizer</span></span><br><span class="line">loss_fn = nn.L1Loss()</span><br><span class="line">optimizer = torch.optim.SGD(params = model_1.parameters(),</span><br><span class="line">                            lr = <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="comment"># Train model for 300 epochs</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Send data to target device</span></span><br><span class="line">X_train = X_train.to(device)</span><br><span class="line">X_test = X_test.to(device)</span><br><span class="line">y_train = y_train.to(device)</span><br><span class="line">y_test = y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">  <span class="comment">### Training</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Put model in train mode</span></span><br><span class="line">  model_1.train()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. Forward pass</span></span><br><span class="line">  y_pred = model_1(X_train)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. Calculate loss</span></span><br><span class="line">  loss = loss_fn(y_pred,y_train)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. Zero gradients</span></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. Backpropagation</span></span><br><span class="line">  loss.backward()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 5. Step the optimizer</span></span><br><span class="line">  optimizer.step()</span><br><span class="line"></span><br><span class="line">  <span class="comment">### Perform testing every 20 epochs</span></span><br><span class="line">  <span class="keyword">if</span> epoch % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Put model in evaluation mode and setup inference context </span></span><br><span class="line">    model_1.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass</span></span><br><span class="line">      y_preds = model_1(X_test)</span><br><span class="line">      <span class="comment"># 2. Calculate test loss</span></span><br><span class="line">      test_loss = loss_fn(y_preds,y_test)</span><br><span class="line">      <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss:<span class="number">.3</span>f&#125;</span> | Test loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br><span class="line">     </span><br></pre></td></tr></table></figure><h3 id="save-trained-model"><a href="#save-trained-model" class="headerlink" title="save trained model"></a>save trained model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create models directory </span></span><br><span class="line">MODEL_PATH = Path(<span class="string">&quot;models&quot;</span>)</span><br><span class="line">MODEL_PATH.mkdir(parents = <span class="literal">True</span>,exist_ok = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 2. Create model save path </span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;01_pytorch_model&quot;</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME </span><br><span class="line"><span class="comment"># 3. Save the model state dict</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Saving model to <span class="subst">&#123;MODEL_SAVE_PATH&#125;</span>&quot;</span>)</span><br><span class="line">torch.save(obj = model_1.state_dict(),f = MODEL_SAVE_PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create new instance of model and load saved state dict (make sure to put it on the target device)</span></span><br><span class="line">loaded_model = LinearRegressionModel()</span><br><span class="line">loaded_model.load_state_dict(torch.load(f = MODEL_SAVE_PATH))</span><br><span class="line">loaded_model.to(device)</span><br></pre></td></tr></table></figure><h2 id="chapter-3"><a href="#chapter-3" class="headerlink" title="chapter 3"></a>chapter 3</h2><p>使用<code>torch.inference_mode</code>替代<code>torch.no_grad</code></p><p><a href="https://stackoverflow.com/questions/69543907/pytorch-torch-no-grad-vs-torch-inference-mode">machine learning - PyTorch <code>torch.no_grad</code> vs <code>torch.inference_mode</code> - Stack Overflow</a></p><p>使用<code>torchmetrics</code>作为衡量标准的库方便.使用tensorboard<a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb#scrollTo=fUt3jwrIHCks">tensorboard_with_pytorch.ipynb - Colaboratory (google.com)</a>查看loss,acc等.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">!pip install -q tensorboard </span><br><span class="line"><span class="comment">#  load tensorboard</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line">write = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line"><span class="comment"># do train and evalute</span></span><br><span class="line"></span><br><span class="line">writer.add_scalar() <span class="comment"># 添加变量</span></span><br><span class="line">writer.add_image()  <span class="comment">#添加图像</span></span><br><span class="line">writer.add_graph()</span><br><span class="line"></span><br><span class="line">writer.flush()</span><br><span class="line">writer.close()</span><br><span class="line">!tensorboard --logdir=runs</span><br></pre></td></tr></table></figure><p>使用tensorboard.dev分享到公网.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!tensorboard dev upload --logdir runs \</span><br><span class="line">--name <span class="string">&quot;My latest experiment&quot;</span> \</span><br><span class="line">--description <span class="string">&quot;Simple comparison of several hyperparameters&quot;</span></span><br></pre></td></tr></table></figure><p>在colab上使用魔法命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%tensorboard --logdir logs </span><br></pre></td></tr></table></figure><p><a href="https://medium.com/looka-engineering/how-to-use-tensorboard-with-pytorch-in-google-colab-1f76a938bc34">How to use Tensorboard with PyTorch in Google Colab | by Andrew B. Martin | Looka Engineering | Medium</a></p><p>端口冲突解决</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lsof -i:&lt;port&gt;</span><br><span class="line">kill -<span class="number">9</span> PID</span><br></pre></td></tr></table></figure><p>多GPU训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_parallel</span>(<span class="params">module, <span class="built_in">input</span>, device_ids, output_device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> device_ids:</span><br><span class="line">        <span class="keyword">return</span> module(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output_device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        output_device = device_ids[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    replicas = nn.parallel.replicate(module, device_ids)</span><br><span class="line">    inputs = nn.parallel.scatter(<span class="built_in">input</span>, device_ids)</span><br><span class="line">    replicas = replicas[:<span class="built_in">len</span>(inputs)]</span><br><span class="line">    outputs = nn.parallel.parallel_apply(replicas, inputs)</span><br><span class="line">    <span class="keyword">return</span> nn.parallel.gather(outputs, output_device)</span><br></pre></td></tr></table></figure><p>In general, pytorch’s nn.parallel primitives can be used independently. We have implemented simple MPI-like primitives:</p><ul><li>replicate: replicate a Module on multiple devices</li><li>scatter: distribute the input in the first-dimension</li><li>gather: gather and concatenate the input in the first-dimension</li><li>parallel_apply: apply a set of already-distributed inputs to a set of already-distributed models.</li></ul><h2 id="chapter-4"><a href="#chapter-4" class="headerlink" title="chapter 4"></a>chapter 4</h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;ZeroToMastery&lt;a href=&quot;https://www.learnpytorch.io/&quot;&gt;Zero to Mastery Learn PyTorch for Deep Learning&lt;/a&gt;上的课程学习&lt;br&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://www.sekyoro.top/tags/pytorch/"/>
    
    <category term="DeepLearning" scheme="https://www.sekyoro.top/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习——初探</title>
    <link href="https://www.sekyoro.top/2023/09/12/pytorch%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%9D%E6%8E%A2/"/>
    <id>https://www.sekyoro.top/2023/09/12/pytorch%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%9D%E6%8E%A2/</id>
    <published>2023-09-12T01:04:36.000Z</published>
    <updated>2023-09-15T01:14:05.751Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>我并没有系统地翻阅Pytorch文档,一般都是看别人pytorch实现的网络代码,哪里有不懂的再去看.现在找到一些tutorial并做一些简单的尝试.</p><span id="more"></span><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img data-src="https://img.proanimer.com/imgs/image-20230914101921453.pngundefined" alt="image-20230914101921453"></p><p><img data-src="https://img.proanimer.com/imgs/image-20230912094601559.png" alt="image-20230912094601559" style="zoom:67%;" /></p><p>首先定义网络架构,注意这里也有很多要点,比如像写代码一样,使用模块嵌套,最终形成一个个小模块组成的大模块,模块的超参设置也很重要.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>损失函数需要根据任务去确定,常用交叉熵.优化器基本没有太多改进空间了,常用的Adam或者RMSprop.在训练时,通过梯度更新参数,再进行验证,通过这样来判断是否过拟合等等.</p><p><img data-src="https://img.proanimer.com/imgs/image-20230912095110476.png" alt="image-20230912095110476" style="zoom:67%;" /></p><p>以上是关于网络的训练,对于一般的任务,对于数据集的处理也是非常重要的,导入之前需要做一些transforms,不同任务做的变化不一样,比如图像的话,一般数据集是PIL数据,需要将其转为Tensor类型数据,还可能需要normalize等等.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>上面用的Dataset都是自带的,很多时候需要用我们自己的数据集,那么需要重写Dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, file</span>):</span></span><br><span class="line">      <span class="comment"># read data and preprocess</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">      <span class="comment"># return one sample at a time</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="comment"># return the size of the dataset</span></span><br><span class="line">      <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>然后使用dataloader方便读入batch以及shuffle打乱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = MyDataset(data, <span class="string">&#x27;./data/train.csv&#x27;</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Pytorch中的dim与numpy中的axis一样的,有的时候类似这种术语经常出现混乱.</p><p>常用操作</p><p>transpose 互换维度</p><p>squeezz 去掉指定的长度为1的维度</p><p>unsqueezz 增加一个长度唯一的维度</p><p>cat   将给定维度中的seq张量的给定序列连接起来。所有张量必须具有相同的形状（连接维度除外）或为空</p><p>stack  沿着一个新的维度连接一系列张量。 所有张量的大小都必须相同。</p><h4 id="tensor数据类型"><a href="#tensor数据类型" class="headerlink" title="tensor数据类型"></a>tensor数据类型</h4><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.FloatTensor</code></td><td><code>torch.cuda.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.DoubleTensor</code></td><td><code>torch.cuda.DoubleTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.IntTensor</code></td><td><code>torch.cuda.IntTensor</code></td></tr></tbody></table></div><p>pytorch中的tensor与numpy都有shape和dtype属性</p><p>在方法上,pytorch用于变更维度的有reshape和view(很多时候看别人代码里用view不要忘了其作用).此外numpy也有squeezz,但是没有unsqueezz,而是使用expand_dims</p><p>而torch.Tensor与np.ndarray比较大的差别是tensor可以在GPU上跑,同时也可以设置梯度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available() <span class="comment"># 查看GPU是否可用</span></span><br></pre></td></tr></table></figure><p>通过torch.tensor创造tensor时,会根据输入设置dtype,如果输入是int,那就是int32或者int64,跟os有关,如果是浮点数就是float32.如果使用torch.ones这种来创建tensor,dtype默认是float32.</p><h3 id="torch-nn定义模型"><a href="#torch-nn定义模型" class="headerlink" title="torch.nn定义模型"></a>torch.nn定义模型</h3><h4 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h4><p><img data-src="https://img.proanimer.com/imgs/image-20230914094934618.png" alt="image-20230914094934618"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(). __init__()</span><br><span class="line">    self.net = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">10</span>,<span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = self.net(x)</span><br></pre></td></tr></table></figure><p>对于每一个batch,首先需要使用optimizer.zero_grad()去除gradient,然后使用loss.backward()通过损失函数计算梯度,最后使用optimizer.step更新梯度.</p><p>标准<strong>训练</strong>流程如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(). __init__()</span><br><span class="line">    self.net = nn.Sequential( <span class="comment"># 也可以使用nn.Linear  nn.Sigmoid连续写</span></span><br><span class="line">        nn.Linear(<span class="number">10</span>,<span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = self.net(x)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = MyModel().to(DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> x,y <span class="keyword">in</span> tr_set:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    x,y = x.to(DEVICE),y.to(DEVICE)</span><br><span class="line">    pred = model(x)</span><br><span class="line">    loss = criterion(pred,y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>验证时,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> te_set:</span><br><span class="line">    x,y = x.to(DEVICE),y.to(DEVICE)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">      pred = model(x)</span><br><span class="line">      loss = criterion(pred,y)</span><br><span class="line">      total_loss += loss.cpu().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>: loss = <span class="subst">&#123;total_loss&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>注意</strong> 计算出的loss除了在计算backward时,其他地方需要放在cpu上并移除梯度,应该注意这些细节,也就是将原本在cuda上的数据放在cpu上,并且将tensor数据转为python的数据类型.</p><p><a href="https://zhuanlan.zhihu.com/p/497192910">Pytorch训练过程中，显存（内存）爆炸解决方法 - 知乎 (zhihu.com)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> tt_test:</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pred = model(x)</span><br><span class="line">        preds.append(pred.cpu())</span><br></pre></td></tr></table></figure><p><img data-src="https://img.proanimer.com/imgs/image-20230914224438072.png" alt="image-20230914224438072"></p><p>在训练验证时需要使用model.eval与model.train切换模型中每层的行为,在测试时防止将测试数据放入模型中计算.</p><p>此外要多翻阅Pytorch文档.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.youtube.com/watch?v=6dEp6oRN2NE&amp;ab_channel=Hung-yiLee">【機器學習 2023】 PyTorch Tutorial (introduction + documentation) - YouTube</a></li><li><a href="https://pytorch.org/docs/stable/tensors">torch.Tensor — PyTorch 2.0 documentation</a></li><li><a href="https://github.com/wkentaro/pytorch-for-numpy-users">wkentaro/pytorch-for-numpy-users: PyTorch for Numpy users. https://pytorch-for-numpy-users.wkentaro.com (github.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;我并没有系统地翻阅Pytorch文档,一般都是看别人pytorch实现的网络代码,哪里有不懂的再去看.现在找到一些tutorial并做一些简单的尝试.&lt;/p&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://www.sekyoro.top/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Autoencoder学习</title>
    <link href="https://www.sekyoro.top/2023/09/01/Autoencoder%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/09/01/Autoencoder%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-09-01T12:28:30.000Z</published>
    <updated>2023-09-07T07:22:29.266Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这种encoder-decoder结构很重要,同时也可以作为学习GAN的前置<br><span id="more"></span></p><h2 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h2><p>autoencoders是在深度学习经常听到的词,简单来说是基于latent vector,manifold这种概念上的模型,</p><blockquote><p>利用输入数据  本身作为监督，来指导神经网络尝试学习一个映射关系，从而得到一个重构输出 。在时间序列异常检测场景下，异常对于正常来说是少数，所以我们认为，如果使用自编码器重构出来的输出跟原始输入的差异超出一定阈值（threshold）的话，原始时间序列即存在了异常。</p><p>An autoencoder is a type of algorithm with the primary purpose of learning an “informative” representation of the data that can be used for different applicationsa by learning to reconstruct a set of input observations well enough.</p></blockquote><p>latent feature又可以叫做潜在向量,潜在特征,bottleneck等等,叫法很多,不要听见新的说法发怵.简单来说就是encoder-decoder架构,不过进行自监督,使用损失函数比较输入和输出. 使用,重建误差(Reconsrtuction Error)体现,重建误差（RE）是一个指标，它可以指示自动编码器能够重建输入观测x的好坏。相比于全连接网络和卷积网络,AE并不需要labeled data, 我们可以使用图像同时作为输入和输出.</p><p><strong>The main idea of autoencoder is that we will have an encoder network that converts input image into some latent space (normally it is just a vector of some smaller size), then the decoder network, whose goal would be to reconstruct the original image</strong></p><p><img data-src="https://s2.loli.net/2023/08/31/ABn4DgqX7xWGI6y.png" alt=""></p><p>常用于如下用途</p><ul><li>降低图像的维度以进行可视化或训练图像嵌入。通常，自动编码器比PCA给出更好的结果，因为它考虑了图像的空间性质和层次特征。</li><li>去噪，即从图像中去除噪声。由于噪声携带了大量无用的信息，自动编码器无法将其全部放入相对较小的潜在空间，因此只能捕获图像的重要部分。在训练去噪器时，我们从原始图像开始，并使用带有人工添加噪声的图像作为自动编码器的输入。</li><li>超分辨率，提高图像分辨率。我们从高分辨率图像开始，使用分辨率较低的图像作为自动编码器输入。</li><li>生成模型。一旦我们训练了自动编码器，解码器部分就可以用来从随机潜在向量开始创建新的对象。</li></ul><p>缺点是 传统的AE(autoencoders)潜在向量往往没有太多的语义含义,换句话说，以MNIST数据集为例，弄清楚哪些数字对应于不同的潜在向量并不是一项容易的任务，因为接近的潜在向量不一定对应于同一个数字</p><p>尝试改变潜变量大小,看看效果.</p><p>尝试看看不同图像的潜变量,增加噪声后再查看效果.</p><h3 id="AE常用去噪和超分"><a href="#AE常用去噪和超分" class="headerlink" title="AE常用去噪和超分."></a>AE常用去噪和超分.</h3><p>对于去噪来说,将没有噪声的图片人工加噪,训练时使用噪声图片作为输入,正常无噪图片作为输出.</p><blockquote><p>To train super-resolution network, we will start with high-resolution images, and automatically downscale them to produce network inputs. We will then feed autoencoder with small images as inputs and high-resolution images as outputs.</p></blockquote><p>对于超分将宽高缩小的图片作为输入,将正常图作为输出.</p><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = AutoEncoder().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line">noisy_tensor = torch.FloatTensor(noisify([<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])).to(device)</span><br><span class="line">test_noisy_tensor = torch.FloatTensor(noisify([<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])).to(device)</span><br><span class="line">noisy_tensors = (noisy_tensor, test_noisy_tensor)</span><br><span class="line">train(dataloaders, model, loss_fn, optimizer, <span class="number">100</span>, device, noisy=noisy_tensors)</span><br></pre></td></tr></table></figure><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">predictions = []</span><br><span class="line">noise = []</span><br><span class="line">plots = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_dataset):</span><br><span class="line">    <span class="keyword">if</span> i == plots:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    shapes = data[<span class="number">0</span>].shape</span><br><span class="line">    noisy_data = data[<span class="number">0</span>] + test_noisy_tensor[<span class="number">0</span>].detach().cpu()</span><br><span class="line">    noise.append(noisy_data)</span><br><span class="line">    predictions.append(model(noisy_data.to(device).unsqueeze(<span class="number">0</span>)).detach().cpu())</span><br><span class="line">plotn(plots, noise)</span><br><span class="line">plotn(plots, predictions)</span><br></pre></td></tr></table></figure><p>对于超分,因为输入变化了,考虑潜变量不变,所以encoder需要做一些变化.</p><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>对于图像降维来说影响不大,但要训练生成模型，最好对潜在空间有一些了解。这个想法使我们想到了变分自动编码器(VAE)</p><p>VAE是一种自动编码器，它学习预测潜在参数的统计分布，即所谓的潜在分布。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/images/vae.png" alt="img" style="zoom: 50%;" /></p><p>VAE是一种自动编码器，它学习预测潜在参数的统计分布，即所谓的潜在分布。例如，我们可能希望潜在向量正态分布，具有一些均值z~mean~和标准差z~sigma~（均值和标准差都是一些维度d的向量）。VAE中的编码器学习预测这些参数，然后解码器从这个分布中提取一个随机向量来重建对象。 </p><p>相比于AE简单的损失函数,变分自动编码器使用由两部分组成的复杂损失函数：</p><ul><li>重建损失是显示重建图像离目标有多近的损失函数（它可以是均方误差或MSE）。它与普通自动编码器中的损失函数相同。</li><li>KL损失，确保潜在变量分布保持接近正态分布。它基于Kullback-Leibler散度的概念，这是一个估计两个统计分布相似程度的指标。</li></ul><p>VAE的一个重要优势是，它们使我们能够相对容易地生成新图像，<strong>因为我们知道从哪个分布对潜在向量进行采样</strong>。例如，如果我们在MNIST上用2D潜在向量训练VAE，那么我们可以改变潜在向量的分量以获得不同的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vae_loss</span>(<span class="params">preds, targets, z_vals</span>):</span></span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    reconstruction_loss = mse(preds, targets.view(targets.shape[<span class="number">0</span>], -<span class="number">1</span>)) * <span class="number">784.0</span></span><br><span class="line">    temp = <span class="number">1.0</span> + z_vals[<span class="number">1</span>] - torch.square(z_vals[<span class="number">0</span>]) - torch.exp(z_vals[<span class="number">1</span>]) <span class="comment"># ?尽可能使得潜变量与期望的分布kl相近</span></span><br><span class="line">    <span class="comment"># 期望正态分布 均值0 方差1</span></span><br><span class="line">    kl_loss = -<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(temp, axis=-<span class="number">1</span>)  <span class="comment">#  </span></span><br><span class="line">    <span class="keyword">return</span> torch.mean(reconstruction_loss + kl_loss)</span><br></pre></td></tr></table></figure><p>关键是这里的KL loss,需要使得潜变量与正态分布kl更小,分布更相似. 当我们通过encoder计算出均值和方差的log之后,需要计算其与正态分布的KL,</p><p><img data-src="https://s2.loli.net/2023/09/03/HURDP5TfoBuVjyC.png" alt="image-20230903182226844" style="zoom: 80%;" /></p><p>计算正态分布之间的KL散度公式如上.关于VAE这里的KL 散度比较好的回答<a href="https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes/370048#370048">kullback leibler - Deriving the KL divergence loss for VAEs - Cross Validated (stackexchange.com)</a></p><blockquote><p><strong>KL散度</strong>，是指当某分布q (x)被用于近似p (x)时的信息损失。 KL Divergence 也就是说，q (x)能在多大程度上表达p (x)所包含的信息，KL散度越大，表达效果越差。</p></blockquote><p>所以计算KL时应该是KL(z|n)其中z表示潜变量,n表示正态分布. 目的是利用正态分布描述潜变量的损失.</p><h3 id="Training-a-VAE-with-The-Reparametrization-Trick"><a href="#Training-a-VAE-with-The-Reparametrization-Trick" class="headerlink" title="Training a VAE with The Reparametrization Trick"></a>Training a VAE with The Reparametrization Trick</h3><p>VAE在反向传播时存在一些计算问题.使用了Reparametrization Trick</p><p><a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">mathematical statistics - How does the reparameterization trick for VAEs work and why is it important? - Cross Validated (stackexchange.com)</a></p><h2 id="AAE"><a href="#AAE" class="headerlink" title="AAE"></a>AAE</h2><p>结合GAN和VAE的结构</p><p>对抗性自动编码器是生成对抗性网络和变分自动编码器的组合。编码器将是生成器，鉴别器将学习区分编码器输出的真实图像和生成的图像。编码器的输出是一个分布，从这个输出解码器将尝试解码图像。</p><p><img data-src="https://s2.loli.net/2023/09/03/mbhaVXe7jrO63D8.png" alt="image-20230903230742551" style="zoom: 67%;" /></p><p>众所周知,GAN的生成器在训练时使用噪声作为输入,</p><p><img data-src="https://s2.loli.net/2023/09/04/dcLtUFBSzg41rVl.png" alt="image-20230904105728118" style="zoom:67%;" /></p><p><strong>纠正</strong>:是Adversarial. 注意,上面一层的autoencoder的encoder也是一个generator,相当于共用了GAN的generator和autoencoder的encoder.首先encoder使用一张图像作为输入,生成的潜变量在VAE中需要减小其与正态分布之间的相似度,也就是优化KL散度,但由于<strong>KL散度项的积分除了少数分布之外没有闭合形式的解析解</strong>,所以利用GAN的鉴别器,从正态分布中采样的数据与生成的潜变量作为鉴别器的输入进行鉴别,利用这个鉴别器的损失更新鉴别器</p><script type="math/tex; mode=display">L_D=-\frac1m\sum_{k=1}^mlog(D(z'))+log(1-D(z))</script><p>更新后,再使用encoder(同时也是generator)以原始图像作为输入,生成潜变量,输入给更新后的鉴别器,鉴别器将其判断为真的概率.</p><script type="math/tex; mode=display">L_G=-\frac1m\sum_{k=1}^mlog(D(z))</script><p>以这种方式定义的损失将迫使鉴别器能够识别假样本，同时推动生成器欺骗鉴别器.</p><p>首先，由于编码器的输出必须遵循高斯分布，我们在其最后一层不使用任何非线性。解码器的输出具有S形非线性，这是因为我们使用的输入以其值在0和1之间的方式归一化。鉴别器网络的输出只是0和1之间的一个数字，表示输入来自真实先验分布的概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Encoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Q_net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Q_net, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(X_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3gauss = nn.Linear(N, z_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.droppout(self.lin1(x), p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.droppout(self.lin2(x), p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        xgauss = self.lin3gauss(x)</span><br><span class="line">        <span class="keyword">return</span> xgauss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">P_net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(P_net, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(z_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3 = nn.Linear(N, X_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.lin1(x)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.lin2(x)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = self.lin3(x)</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D_net_gauss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(D_net_gauss, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(z_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3 = nn.Linear(N, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.dropout(self.lin1(x), p=<span class="number">0.2</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.dropout(self.lin2(x), p=<span class="number">0.2</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.lin3(x))</span><br></pre></td></tr></table></figure><p>所以这里的损失函数定义为重建损失(一般为BCEloss或者cross-entropy loss)和GAN的损失,而GAN的训练一般都是G和D一个训练一下,而之前autoencoder训练时也是把encoder-decoder作为整个模型训练的loss.而这里为了在编码器（也是对抗性网络的生成器）的优化过程中具有独立性，我们为网络的这一部分定义了两个优化器,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line">Q, P = Q_net() = Q_net(), P_net(<span class="number">0</span>)     <span class="comment"># Encoder/Decoder</span></span><br><span class="line">D_gauss = D_net_gauss()                <span class="comment"># Discriminator adversarial</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    Q = Q.cuda()</span><br><span class="line">    P = P.cuda()</span><br><span class="line">    D_cat = D_gauss.cuda()</span><br><span class="line">    D_gauss = D_net_gauss().cuda()</span><br><span class="line"><span class="comment"># Set learning rates</span></span><br><span class="line">gen_lr, reg_lr = <span class="number">0.0006</span>, <span class="number">0.0008</span></span><br><span class="line"><span class="comment"># Set optimizators</span></span><br><span class="line">P_decoder = optim.Adam(P.parameters(), lr=gen_lr)</span><br><span class="line">Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)</span><br><span class="line">Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)</span><br><span class="line">D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)</span><br></pre></td></tr></table></figure><p>通过编码器/解码器部分进行正向计算，计算重建损失并更新编码器Q和解码器P网络的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z_sample = Q(X)</span><br><span class="line">X_sample = P(z_sample)</span><br><span class="line">recon_loss = F.binary_cross_entropy(X_sample + TINY, </span><br><span class="line">                                    X.resize(train_batch_size, X_dim) + TINY)</span><br><span class="line">recon_loss.backward()</span><br><span class="line">P_decoder.step()</span><br><span class="line">Q_encoder.step()</span><br></pre></td></tr></table></figure><p>创建一个潜在表示z=Q（x），并从先前的p（z）中提取样本z’，通过鉴别器运行每个样本，并计算分配给每个样本的分数（D（z）和D（z’））</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="module-access"><span class="module"><span class="identifier">Q</span>.</span></span>eval<span class="literal">()</span>    </span><br><span class="line">z_real_gauss = <span class="constructor">Variable(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">train_batch_size</span>, <span class="params">z_dim</span>)</span><span class="operator"> * </span><span class="number">5</span>)   # Sample from <span class="constructor">N(0,5)</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is<span class="constructor">_available()</span>:</span><br><span class="line">    z_real_gauss = z_real_gauss.cuda<span class="literal">()</span></span><br><span class="line">z_fake_gauss = <span class="constructor">Q(X)</span></span><br></pre></td></tr></table></figure><p>训练过程,首先利用encoder-decoder,得到重建后的输出,这里使用二分类交叉熵,计算梯度后更新encoder和decoder的值.然后使用generator(同时也是encoder)使用从高斯分布采样得到的变量作为输入,注意此时需要设置dropout和normalization模式为测试,因为正则目的是为了防止过拟合、加快拟合过程,所以测试、验证时并不需要正则了. 这里Q.eval目的是只需要得到一个生成的潜变量,而不是进行训练.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute discriminator outputs and loss</span></span><br><span class="line">D_real_gauss, D_fake_gauss = D_gauss(z_real_gauss), D_gauss(z_fake_gauss)</span><br><span class="line">D_loss_gauss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(<span class="number">1</span> - D_fake_gauss + TINY))</span><br><span class="line">D_loss_gauss.backward()       <span class="comment"># Backpropagate loss</span></span><br><span class="line">D_gauss_solver.step()   <span class="comment"># Apply optimization step</span></span><br></pre></td></tr></table></figure><p>计算Generator的loss，并相应地更新Q网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator</span></span><br><span class="line">Q.train()   <span class="comment"># Back to use dropout</span></span><br><span class="line">z_fake_gauss = Q(X)</span><br><span class="line">D_fake_gauss = D_gauss(z_fake_gauss)</span><br><span class="line"></span><br><span class="line">G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))</span><br><span class="line">G_loss.backward()</span><br><span class="line">Q_generator.step()</span><br></pre></td></tr></table></figure><p>由于需要更新Generator,恢复训练模式,</p><h3 id="Supervised-approach"><a href="#Supervised-approach" class="headerlink" title="Supervised approach"></a>Supervised approach</h3><blockquote><p>特征学习最稳健的方法是尽可能多地分解因素，尽可能少地丢弃有关数据的信息</p></blockquote><p>通常来说,如果我们能提供更多信息,将其作为一个全监督的模型.</p><p><code>disentangled representations</code>类似于风格迁移中概念,可以将一张图像中的东西分为<strong>内容</strong>和<strong>风格</strong>,进行解耦表示。</p><p>我们可以加上类标签的独热码,这其实就是所谓的Conditional GAN</p><blockquote></blockquote><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/aae_super.png" alt="aae_semi" style="zoom:33%;" /></p><p>这样在代码上就会增加两个损失函数和一个鉴别器用于分辨产生的label的独热码和真实的label的独热码.</p><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/dis_2.png" alt="disentanglement1" style="zoom: 33%;" /></p><p>这是教程<a href="https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/">Adversarial Autoencoders (with Pytorch) (paperspace.com)</a>的图片,使得每一列潜变量第一部分也就是正态分布一样,但类标签不一样,</p><h3 id="Semi-supervised-approach"><a href="#Semi-supervised-approach" class="headerlink" title="Semi-supervised approach"></a>Semi-supervised approach</h3><p>下面这种半监督的方式,我们需要将label加入,而这种加入并不是直接把label作为输入给decoder的,而是类似刚才AAE的方式通过GAN使得潜变量分为两个部分,分别是想要的某种分布和label,将class label的one-hot编码,比如说MNIST数据集中,3这个图像的label就是数字三,one-hot编码是0011(因为一共十个数字,需要4位.</p><p>我们可以修改以前的体系结构，使AAE产生一个由矢量级联组成的潜变量y指示类或标签（使用Softmax）和连续潜在变量z(使用线性层). 使用softmax作为最后一层的激活函数,这样最后一层输出shape就是(Batch_size,4) 每个值在0-1之间,</p><p>通过这种方法还能通过encoder产生的潜变量中的类标签的独热码进行对图像分类,</p><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/aae_semi.png" alt="aae002" style="zoom: 33%;" /></p><p>希望向量y表现为一个独热码，我们通过使用鉴别器Dcat的对抗性网络来强制它遵循类别分布。</p><p>编码器现在是q（z，y|x）。解码器使用类标签和连续潜变量来重建图像</p><h2 id="Conditional-Variational-Autoencoders"><a href="#Conditional-Variational-Autoencoders" class="headerlink" title="Conditional Variational Autoencoders"></a>Conditional Variational Autoencoders</h2><p>条件变分自动编码器对编码器和解码器都有一个额外的输入。</p><p><img data-src="https://cdn.sekyoro.top/imgs/image-20230907105548807.png" alt="image-20230907105548807" style="zoom: 67%;" /></p><p>在训练时，将其图像被馈送的数字提供给编码器和解码器。在这种情况下，它将被表示为一个热向量。</p><p>要生成特定数字的图像，只需将该数字与从标准正态分布采样的潜在空间中的随机点一起输入解码器即可。即使输入同一点来产生两个不同的数字，这个过程也会正确工作，因为系统不再依赖潜在空间来编码你正在处理的数字。相反，潜在空间对其他信息进行编码，如笔划宽度或数字写入的角度。</p><p><img data-src="https://s2.loli.net/2023/09/04/HCPMwute1Z2LBV7.png" alt="image-20230904223354814" style="zoom:50%;" /></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/09-Autoencoders/README.md">AI-For-Beginners/lessons/4-ComputerVision/09-Autoencoders/README.md at main · microsoft/AI-For-Beginners (github.com)</a></li><li><a href="https://arxiv.org/pdf/2201.03898.pdf">*2201.03898.pdf (arxiv.org)</a></li><li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler Divergence Explained — Count Bayesie</a></li><li><a href="https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians">normal distribution - KL divergence between two univariate Gaussians - Cross Validated (stackexchange.com)</a></li><li><a href="https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/">Adversarial Autoencoders (with Pytorch) (paperspace.com)</a></li><li><a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">Conditional Variational Autoencoders (ijdykeman.github.io)</a></li><li><a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">mathematical statistics - How does the reparameterization trick for VAEs work and why is it important? - Cross Validated (stackexchange.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这种encoder-decoder结构很重要,同时也可以作为学习GAN的前置&lt;br&gt;</summary>
    
    
    
    
    <category term="VAE" scheme="https://www.sekyoro.top/tags/VAE/"/>
    
    <category term="autoencoder" scheme="https://www.sekyoro.top/tags/autoencoder/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础知识(二)</title>
    <link href="https://www.sekyoro.top/2023/08/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(%E4%BA%8C)/"/>
    <id>https://www.sekyoro.top/2023/08/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(%E4%BA%8C)/</id>
    <published>2023-08-12T10:16:29.000Z</published>
    <updated>2023-08-18T02:49:56.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>深度学习知识第二部分</p><span id="more"></span><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>解决序列数据</p><p>输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加， 因此需要一个近似方法来使这个计算变得容易处理。 本章后面的大部分内容将围绕着如何有效估计<img data-src="https://s2.loli.net/2023/08/11/c82anIiD6Cmr9OE.png" alt="image-20230811182733764">展开。 简单地说，它归结为以下两种策略。</p><p>第一种策略，假设在现实情况下相当长的序列可能是不必要的， 因此我们只需要满足某个长度为l的时间跨度。 当下获得的最直接的好处就是参数的数量总是不变的， 至少在t&gt;l时如此，这就使我们能够训练一个上面提及的深度网络。 这种模型被称为<em>自回归模型</em>（autoregressive models）， 因为它们是对自己执行回归。</p><p>第二种策略， 是保留一些对过去观测的总结ℎ， 并且同时更新预测和总结ℎ。 这就产生了基于<img data-src="https://s2.loli.net/2023/08/11/RZ8LQh2qyYW4Su6.png" alt="image-20230811182855054">计x， 以及公式<img data-src="https://s2.loli.net/2023/08/11/2mkvsVQXMIpajlY.png" alt="image-20230811182927632">更新的模型。 由于ℎ从未被观测到，这类模型也被称为 <em>隐变量自回归模型</em>（latent autoregressive models</p><p><img data-src="https://zh-v2.d2l.ai/_images/sequence-model.svg" alt="../_images/sequence-model.svg"></p><h4 id="文本预处理方式"><a href="#文本预处理方式" class="headerlink" title="文本预处理方式"></a>文本预处理方式</h4><p>步骤通常包括：</p><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><p><em>词元</em>（token）是文本的基本单位，词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做<em>词表</em>（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。</p><p>将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为<em>语料</em>（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</p><p>自然语言特征:</p><ol><li><p>词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law），</p><p><img data-src="https://s2.loli.net/2023/08/12/5CnHizhOuTlW1wc.png" alt="image-20230812181916062"></p></li><li><p>除了一元语法词，单词序列似乎也遵循齐普夫定律， 尽管公式中的指数α更小 （指数的大小受序列长度的影响）；</p></li><li>词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望；</li><li>很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。</li></ol><h3 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p><img data-src="https://zh.d2l.ai/_images/rnn.svg" alt="../_images/rnn.svg" style="zoom: 67%;" /></p><h4 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h4><p><img data-src="https://s2.loli.net/2023/08/16/zpKICRmb4FvML2g.png" alt="image-20230815235353077"></p><p>隐状态H,有上一个隐状态与本次输入控制.</p><p><img data-src="https://s2.loli.net/2023/08/16/kWAnvRDGEHc2Bxi.png" alt="image-20230816000019295"></p><p>输出O</p><h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><p><img data-src="https://s2.loli.net/2023/08/16/EPUuOp3Gf1Szlxq.png" alt="image-20230816130933516"></p><h4 id="简单的RNN缺点"><a href="#简单的RNN缺点" class="headerlink" title="简单的RNN缺点"></a>简单的RNN缺点</h4><ul><li>我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。</li><li>我们可能会遇到这样的情况：一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。</li><li>我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来<em>重置</em>我们的内部状态表示</li></ul><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><blockquote><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面我们将详细讨论各类门控</p></blockquote><p>引入重置门和更新门. 输入是由当前时间步的输入和前一时间步的隐状态给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给</p><p><img data-src="https://zh-v2.d2l.ai/_images/gru-1.svg" alt="../_images/gru-1.svg" style="zoom: 67%;" /></p><p>利用重置门的输出与常规隐状态集成,得到一个候选隐状态.如如果重置门输出为1,则是普通的隐状态,由本次输入与上次隐状态作为输入,如果重置门输出为0,则候选隐状态只受输入影响,也就是进行了重置.</p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t=\tanh \left(\mathbf{X}_t \mathbf{W}_{x h}+\left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{h h}+\mathbf{b}_h\right),</script><p><img data-src="https://zh-v2.d2l.ai/_images/gru-2.svg" alt="../_images/gru-2.svg"></p><p>结合更新门确定最终隐状态,如果输出为1,不进行更新,保持之前的隐状态,如果是0则将候选隐状态作为新的隐状态.</p><script type="math/tex; mode=display">\mathbf{H}_t=\mathbf{Z}_t \odot \mathbf{H}_{t-1}+\left(1-\mathbf{Z}_t\right) \odot \tilde{\mathbf{H}}_t .</script><p><img data-src="https://zh-v2.d2l.ai/_images/gru-3.svg" alt="../_images/gru-3.svg"></p><blockquote><ul><li>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</li><li>重置门有助于捕获序列中的短期依赖关系。</li><li>更新门有助于捕获序列中的长期依赖关系。</li><li>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</li></ul></blockquote><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>从时间上来说,LSTM比GRU结构要早,结构也更复杂.</p><blockquote><ul><li>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为<em>输出门</em>（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为<em>输入门</em>（input gate）。 我们还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。</li></ul></blockquote><p>引入输入门,忘记门,输出门用于控制隐状态.</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x i}+\mathbf{H}_{t-1} \mathbf{W}_{h i}+\mathbf{b}_i\right), \\\mathbf{F}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x f}+\mathbf{H}_{t-1} \mathbf{W}_{h f}+\mathbf{b}_f\right), \\\mathbf{O}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x o}+\mathbf{H}_{t-1} \mathbf{W}_{h o}+\mathbf{b}_o\right),\end{aligned}</script><p>同时还有候选记忆元,</p><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t=\tanh \left(\mathbf{X}_t \mathbf{W}_{x c}+\mathbf{H}_{t-1} \mathbf{W}_{h c}+\mathbf{b}_c\right)</script><p><img data-src="https://zh-v2.d2l.ai/_images/lstm-1.svg" alt="../_images/lstm-1.svg"></p><p>利用忘记门和输入门控制上一次的记忆元和候选记忆元,隐状态的计算就是根据输出门和记忆元.</p><script type="math/tex; mode=display">\mathbf{H}_t=\mathbf{O}_t \odot \tanh \left(\mathbf{C}_t\right)</script><p>只要输出门接近1，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，我们只保留记忆元内的所有信息，而不需要更新隐状态。</p><p><img data-src="https://zh-v2.d2l.ai/_images/lstm-3.svg" alt="../_images/lstm-3.svg"></p><h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p><img data-src="https://zh-v2.d2l.ai/_images/deep-rnn.svg" alt="../_images/deep-rnn.svg" style="zoom: 80%;" /></p><script type="math/tex; mode=display">\mathbf{H}_t^{(l)}=\phi_l\left(\mathbf{H}_t^{(l-1)} \mathbf{W}_{x h}^{(l)}+\mathbf{H}_{t-1}^{(l)} \mathbf{W}_{h h}^{(l)}+\mathbf{b}_h^{(l)}\right)</script><script type="math/tex; mode=display">\mathbf{H}_t^{(l)}=\phi_l\left(\mathbf{H}_t^{(l-1)} \mathbf{W}_{x h}^{(l)}+\mathbf{H}_{t-1}^{(l)} \mathbf{W}_{h h}^{(l)}+\mathbf{b}_h^{(l)}\right)</script><script type="math/tex; mode=display">\mathbf{O}_t=\mathbf{H}_t^{(L)} \mathbf{W}_{h q}+\mathbf{b}_q</script><p>与多层感知机一样，隐藏层数目L和隐藏单元数目ℎ都是超参数。 也就是说，它们可以由我们调整的。 另外，用门控循环单元或长短期记忆网络的隐状态 来代替隐状态进行计算， 可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。</p><ul><li>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</li><li>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</li><li>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</li></ul><h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>处在序列中间的文字明显可以收到两边的影响.</p><p><img data-src="https://zh-v2.d2l.ai/_images/birnn.svg" alt="../_images/birnn.svg"></p><p>其中ℎ是隐藏单元的数目。 前向和反向隐状态的更新如下</p><script type="math/tex; mode=display">\begin{aligned}& \overrightarrow{\mathbf{H}}_t=\phi\left(\mathbf{X}_t \mathbf{W}_{x h}^{(f)}+\overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{h h}^{(f)}+\mathbf{b}_h^{(f)}\right) \\& \overleftarrow{\mathbf{H}}_t=\phi\left(\mathbf{X}_t \mathbf{W}_{x h}^{(b)}+\overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{h h}^{(b)}+\mathbf{b}_h^{(b)}\right)\end{aligned}</script><script type="math/tex; mode=display">\mathbf{O}_t=\mathbf{H}_t \mathbf{W}_{h q}+\mathbf{b}_q .</script><blockquote><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。 但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。 具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。</p><p>另一个严重问题是，双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链</p></blockquote><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，<strong>填充缺失的单词</strong>、<strong>词元注释</strong>（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤<strong>对序列进行编码</strong></p><h4 id="数据集一般处理流程"><a href="#数据集一般处理流程" class="headerlink" title="数据集一般处理流程"></a>数据集一般处理流程</h4><p>将数据进行预处理(比如替换不间断空格,小写,单词和标点之间插入空格)、词元化后得到词元之后,建立词表.</p><p>由于机器翻译数据集由语言对组成， 因此我们可以分别为源语言和目标语言构建两个词表。 使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。 为了缓解这一问题，这里我们将<strong>出现次数少于2次的低频率词元 视为相同的未知（“<unk>”）词元</strong>。 除此之外，我们还指定了额外的特定词元， 例如在<strong>小批量时用于将序列填充到相同长度的填充词元（“<pad>”）</strong>， 以及<strong>序列的开始词元（“<bos>”）和结束词元（“<eos>”）</strong>。 这些特殊词元在自然语言处理任务中比较常见.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = np.array([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).astype(np.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</li><li>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</li><li>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><h3 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h3><p><img data-src="https://zh-v2.d2l.ai/_images/encoder-decoder.svg" alt="../_images/encoder-decoder.svg"></p><p>前面处理机器翻译时输入和输出长度都是固定的.</p><p>机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个<em>编码器</em>（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是<em>解码器</em>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为<em>编码器-解码器</em>（encoder-decoder）架构.</p><ul><li>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</li><li>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li><li>解码器将具有固定形状的编码状态映射为长度可变的序列。</li></ul><h3 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h3><blockquote><p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq.svg" alt="../_images/seq2seq.svg"></p><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><script type="math/tex; mode=display">\mathbf{h}_{t}=f(\mathbf{x}_{t},\mathbf{h}_{t-1}).</script><p>隐状态根据本次输入和上次的隐状态输出.</p><script type="math/tex; mode=display">\mathbf{c}=q(\mathbf{h}_1,\ldots,\mathbf{h}_T).</script><p>常常会使用一个嵌入层,获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（<code>vocab_size</code>）， 其列数等于特征向量的维度（<code>embed_size</code>）。 对于任意输入词元的索引i， 嵌入层获取权重矩阵的第i行（从0开始）以返回其特征向量</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><script type="math/tex; mode=display">\mathbf{s}_{t^{\prime}}=g(y_{t^{\prime}-1},\mathbf{c},\mathbf{s}_{t^{\prime}-1}).</script><p>在获得解码器的隐状态之后， 我们可以使用输出层和softmax操作 来计算在时间步t′时输出y~t~′的条件概率分布</p><p>损失函数使用交叉熵,在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化,此外应该将填充词元的预测排除在损失函数的计算之外屏蔽不相关项.</p><h4 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h4><p>训练时,特定的序列开始词元（“<bos>”）和 原始的输出序列（不包括序列结束词元“<eos>”） 拼接在一起作为解码器的输入。 这被称为<em>强制教学</em>（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的<em>预测</em>得到的词元作为解码器的当前输入.</p><p>预测时,为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（“<bos>”） 在初始时间步被输入到解码器中.</p><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq-predict.svg" alt="../_images/seq2seq-predict.svg"></p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p><img data-src="https://zh-v2.d2l.ai/_images/qkv.svg" alt="../_images/qkv.svg"></p><p>将注意力简单地分为自主性和非自主性,利用这两种注意力提示,用神经网络来设计注意力机制的框架.</p><p>“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。 给定任何查询，注意力机制通过<em>注意力汇聚</em>（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为<em>值</em>（value）。 更通俗的解释，每个值都与一个<em>键</em>（key）配对， 这可以想象为感官输入的非自主提示。 </p><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。</li><li>注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。</li><li>注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>可视化查询和键之间的注意力权重是可行的。</li></ul><p><img data-src="https://zh-v2.d2l.ai/_images/attention-output.svg" alt="../_images/attention-output.svg"></p><p>高斯核指数部分可以视为<em>注意力评分函数</em>（attention scoring function）， 简称<em>评分函数</em>（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和</p><script type="math/tex; mode=display">f(\mathbf{q},(\mathbf{k}_1,\mathbf{v}_1),\ldots,(\mathbf{k}_m,\mathbf{v}_m))=\sum_{i=1}^m\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\in\mathbb{R}^v,</script><script type="math/tex; mode=display">\alpha(\mathbf{q},\mathbf{k}_i)=\mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i))=\frac{\exp(a(\mathbf{q},\mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q},\mathbf{k}_j))}\in\mathbb{R}.</script><p>注意力机制涉及到q,k,v分别代表查询,键,值. 计算注意力评分有多种方法,常用的有加性注意力和缩放点积注意力.</p><script type="math/tex; mode=display">a(\mathbf{q}, \mathbf{k})=\mathbf{w}_v^{\top} \tanh \left(\mathbf{W}_q \mathbf{q}+\mathbf{W}_k \mathbf{k}\right) \in \mathbb{R}</script><script type="math/tex; mode=display">\operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt d}\right)\mathbf{V}\in\mathbb{R}^{n\times v}.</script><p>有了评分函数后,继续考虑注意力模型问题.循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量， 然后循环神经网络解码器<strong>根据生成的词元和上下文变量</strong> 按词元生成输出（目标）序列词元。</p><blockquote><p>即使并非所有输入（源）词元都对解码某个词元都有用， 在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。 有什么方法能改变上下文变量呢</p></blockquote><h4 id="Bahdanau注意力"><a href="#Bahdanau注意力" class="headerlink" title="Bahdanau注意力"></a>Bahdanau注意力</h4><p>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过<strong>将上下文变量视为注意力集中的输出</strong>来实现的.</p><p>其中解码时间步t~’~都会被c~t’~替换,是作为查询(query)的上一步解码器隐状态和与编码器隐状态</p><script type="math/tex; mode=display">\mathbf{c}_{t^{\prime}}=\sum_{t=1}^T\alpha(\mathbf{s}_{t^{\prime}-1},\mathbf{h}_t)\mathbf{h}_t,</script><p>时间步t′−1时的解码器隐状态s~t~′−1是查询， 编码器隐状态ℎ~t~既是键，也是值. 注意力权重可以使用加性注意力打分.</p><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq-attention-details.svg" alt="../_images/seq2seq-attention-details.svg"></p><p>定义Bahdanau注意力,只需要改变解码器就行了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>首先，初始化解码器的状态，需要下面的输入：</p><p><strong>编码器</strong>在所有时间步的<strong>最终层隐状态，将作为注意力的键和值</strong>；</p><p><strong>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态</strong>；</p><p><strong>编码器有效长度</strong>（排除在注意力池中填充词元）。</p><p>结合了注意力机制与编码器-解码器,使得解码器中每个解码时间步是注意力模型的输出,查询q是上一步的隐状态,键和值都是编码器的最终隐状态.</p><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><blockquote><p>在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 <em>子空间表示</em>（representation subspaces）可能是有益的</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/multi-head-attention.svg" alt="../_images/multi-head-attention.svg"></p><p>可以用独立学习得到的ℎ组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。</p><p>多头注意力机制,对于h个注意力汇聚输出,每一个注意力汇聚都被称作一个”头”.</p><script type="math/tex; mode=display">\mathbf{h}_i=f(\mathbf{W}_i^{(q)}\mathbf{q},\mathbf{W}_i^{(k)}\mathbf{k},\mathbf{W}_i^{(v)}\mathbf{v})\in\mathbb{R}^{p_v},</script><script type="math/tex; mode=display">\mathbf{W}_o\begin{bmatrix}\mathbf{h}_1\\\vdots\\\mathbf{h}_h\end{bmatrix}\in\mathbb{R}^{p_o}.</script><p>其中的W均是可学习的参数,f是注意力汇聚的函数.</p><ul><li>多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li></ul><h3 id="自注意力和位置编码"><a href="#自注意力和位置编码" class="headerlink" title="自注意力和位置编码"></a>自注意力和位置编码</h3><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;深度学习知识第二部分&lt;/p&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://www.sekyoro.top/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>GAN深入学习</title>
    <link href="https://www.sekyoro.top/2023/08/11/GAN%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/08/11/GAN%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-08-11T10:08:06.000Z</published>
    <updated>2023-10-01T04:46:50.194Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>深入GAN学习<br><span id="more"></span></p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/images/gan_architecture.png" alt="img"></p><p>注意,实验复现时最好设置随机种子固定<a href="https://pytorch.org/docs/stable/notes/randomness.html">Reproducibility — PyTorch 2.0 documentation</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># seed setting</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">same_seeds</span>(<span class="params">seed</span>):</span></span><br><span class="line">    <span class="comment"># Python built-in random module</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="comment"># Numpy</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    <span class="comment"># Torch</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">same_seeds(<span class="number">2023</span>)</span><br></pre></td></tr></table></figure><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>标题<strong>Generative Adversarial Nets</strong> 2014年</p><p><strong>摘要</strong></p><p>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples</p><p><img data-src="https://pic4.zhimg.com/80/v2-044f8d58f378b088c9a85a8f19dae363_720w.webp" alt="img" style="zoom:67%;" /></p><p>主要贡献:提出GAN  定义G和D以及损失函数.由于GAN中使用的极小极大（minmax）优化，训练可能非常不稳定。</p><script type="math/tex; mode=display">\min_G\max_DV=E_{x\sim\text{p}_r}[\log D(x)]+E_{x\sim\text{p}_g}[\log(1-D(x))]</script><p>存在问题：梯度不稳定,梯度消失,<strong>模式崩溃</strong>(特别是NS-GAN,使用了the - log D trick),也就是生成器的损失改为-logD(x)<a href="https://zhuanlan.zhihu.com/p/25071913">令人拍案叫绝的Wasserstein GAN - 知乎 (zhihu.com)</a></p><p>首先求得生成器固定,最大化V的D</p><script type="math/tex; mode=display">\mathrm{P}_r(x)\log D(x)+P_g(x)\log[1-D(x)]</script><p>对D(x)求导,让导数为0</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\mathrm{P}_r(x)}{D(x)}-\frac{\mathrm{P}_g(x)}{1-D(x)}=0\\\\&\text{化简上式,得最优的D表达式为}.\\\\&D^*(x)=\frac{\mathrm{P}_r(x)}{\mathrm{P}_r(x)+\mathrm{P}_g(x)}\end{aligned}</script><p>将这个最优的D带入一开始的式子</p><script type="math/tex; mode=display">\begin{aligned}&\operatorname*{min}_{G}V=E_{x\sim\mathrm{p}_{r}}[\operatorname{log}D(x)]+E_{x\sim\mathrm{p}_{g}}[\operatorname{log}(1-D(x))] \\&\text{将最大化的D即式}1\text{的}D^*(x)\text{代入式得}: \\&\operatorname*{min}_{G}V=E_{x\sim\mathrm{p}_{r}}[\operatorname{log}(\frac{\mathrm{p}_{r}(x)}{\mathrm{p}_{r}(x)+\mathrm{p}_{g}(x)})]+E_{x\sim\mathrm{p}_{g}}[\operatorname{log}(\frac{\mathrm{p}_{g}(x)}{\mathrm{p}_{r}(x)+\mathrm{p}_{g}(x)})] \\&\text{再化简一步得式:} \\&\min_GV=E_{x\sim\mathrm{p}_r}[\log(\frac{\mathrm{p}_r(x)}{\frac12(\mathrm{p}_r(x)+\mathrm{p}_g(x))})]+E_{x\sim\mathrm{p}_g}[\log(\frac{\mathrm{p}_g(x)}{\frac12(\mathrm{p}_r(x)+\mathrm{p}_g(x))}]-2\log2\end{aligned}</script><p>将JS散度带入,有</p><script type="math/tex; mode=display">\min_GV=2JS(P_r||P_g)-2\log2</script><p>所以当判别器达到固定G情况下最优时,如果两个分布重叠则为JS则为0,否则JS为log2.梯度一直为0,G得不到更新,所以这种原始GAN会面临<strong>梯度消失问题</strong>,导致训练困难.</p><blockquote><p>上述的推导都是建立在最优判别器的基础上的，但是在我们实操过程中往往一开始判别器性能是不理想的，所以生成器还是有梯度更新的</p></blockquote><p>如果使用logD-trick,</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{x\sim P_{g}}\left[-\log D^{*}(x)\right]& =KL(P_{g}||P_{r})-\mathbb{E}_{x\sim P_{g}}\log[1-D^{*}(x)]  \\&=KL(P_g||P_r)-2JS(P_r||P_g)+2\log2+\mathbb{E}_{x\sim P_r}[\log D^*(x)]\end{aligned}</script><p>所以最后需要最小化前面两项,因为后面两项与G无关. 这个最小化目标需要同时最小化KL散度又要最大化JS散度,直观上荒谬,数值结果上<strong>导致梯度不稳定</strong>,此外第一项的KL散度表示</p><script type="math/tex; mode=display">P_{g}(x)\log\frac{P_{g}(x)}{P_{r}(x)}</script><p>当P~g~(x)趋近于1,P~r~(x)趋近于0这种情况与当P~g~(x)趋近于0,P~r~(x)趋近于1这种情况对于KL散度情况不一致,由于要最小化KL散度,会导致后者这种情况,也就是</p><p>这种情况下,梯度可能不会消失,但会存在梯度不稳定,模式崩溃的问题.</p><p>以上内容部分是WGAN中的,从理论上解释了GAN训练的一些问题.</p><h4 id="使用tensorboard记录损失"><a href="#使用tensorboard记录损失" class="headerlink" title="使用tensorboard记录损失"></a>使用tensorboard记录损失</h4><p>大致流程是首先将损失计入到一个文件,然后使用tensorboard读取,便能使用tensorboard打开一个端口,在网页上查看。<a href="https://pytorch.org/docs/stable/tensorboard.html">torch.utils.tensorboard — PyTorch 2.0 documentation</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorboard</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;mnist_train&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = torchvision.models.resnet50(<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># Have ResNet model take in grayscale rather than RGB</span></span><br><span class="line">model.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(trainloader))</span><br><span class="line"></span><br><span class="line">grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images&#x27;</span>, grid, <span class="number">0</span>)</span><br><span class="line">writer.add_graph(model, images)</span><br><span class="line"><span class="comment"># 关闭writer</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = np.random.random(<span class="number">1000</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&#x27;distribution centers&#x27;</span>, x + i, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img data-src="https://pytorch.org/docs/stable/_images/add_histogram.png" alt="_images/add_histogram.png" style="zoom: 67%;" /></p><p>在google colab使用需要搭配一些magic func</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext tensorboard  <span class="comment">#使用tensorboard 扩展</span></span><br><span class="line">%tensorboard --logdir logs  <span class="comment">#定位tensorboard读取的文件目录</span></span><br></pre></td></tr></table></figure><h4 id="使用visdom可视化"><a href="#使用visdom可视化" class="headerlink" title="使用visdom可视化"></a>使用visdom可视化</h4><p>visdom一般搭配pytorch,毕竟都是meta的.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pip install visdom</span><br><span class="line">python -m visdom.server</span><br><span class="line">iz = Visdom()</span><br><span class="line">  </span><br><span class="line">viz.line([<span class="number">0.</span>],    <span class="comment">#Y的第一个点</span></span><br><span class="line">         [<span class="number">0.</span>],    <span class="comment">#X的第一个点</span></span><br><span class="line">         win=<span class="string">&quot;train loss&quot;</span>,   <span class="comment">#右上角窗口的名称 </span></span><br><span class="line">         opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train_loss&#x27;</span>) <span class="comment">#opt的参数都可以用python字典的格式传入，还有很多其他的类似matplotlib美化图形的参数参考官网</span></span><br><span class="line">        )  </span><br><span class="line">        </span><br><span class="line">viz.line([<span class="number">1</span>,],<span class="comment">#Y的下一个点</span></span><br><span class="line">         [<span class="number">1.</span>],<span class="comment">#X的下一个点</span></span><br><span class="line">         win=<span class="string">&quot;train loss&quot;</span>,</span><br><span class="line">         update=<span class="string">&#x27;append&#x27;</span><span class="comment">#添加到下一个点后面</span></span><br><span class="line">         )</span><br></pre></td></tr></table></figure><p>这里还是推荐选择两者之一即可.</p><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="*DCGAN"></a>*DCGAN</h3><p>标题<strong>WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</strong></p><p><strong>intro</strong></p><p>Learning reusable feature representations from large unlabeled datasets has been an area of active research. In the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classification. We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques. One can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. There has been very limited published research in trying to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs. In this paper, we make the following contributions </p><p>• We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings. We name this class of architectures Deep Convolutional GANs (DCGAN) </p><p>• We use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. </p><p>• We visualize the filters learnt by GANs and empirically show that specific filters have learned to draw specific objects.</p><p>We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated sample</p><p>贡献:提出卷积GAN,卷积层替代全连接,使用训练过的判别器用于分类任务,可视化了生成器中的某层,显示出良好的绘制特定对象的能力.生成器的向量显示出能控制样本的语义质量行为.介绍了一些超参的初始化.</p><p>• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). </p><p>• Use batchnorm in both the generator and the discriminator. • Remove fully connected hidden layers for deeper architectures. </p><p>• Use ReLU activation in generator for all layers except for the output, which uses Tanh. • Use LeakyReLU activation in the discriminator for all layers</p><p>使用了三个数据集</p><ul><li>批量标准化是两个网络中必须的。</li><li>卷积层替代全连接层。</li><li>使用strided卷积(步幅大于1)可以代替池化</li><li>ReLU激活（<em>几乎</em>总是）会有帮助。</li></ul><p><img data-src="https://s2.loli.net/2023/08/29/L1aVuqWoiDHfS2z.png" alt="image-20230829192803486"></p><p>原论文中D判别函数使用的是ReLU,但现在代码中很多其实还是用的LeakyReLU.此外不使用池化,而是使用deconvolution或者叫分数步长卷积(fractionally-strided convolutions).</p><p><img data-src="https://s2.loli.net/2023/08/29/cktnUB7Lj5g6mlI.png" alt="image-20230829214218494"></p><p>pytorch实现中,D判别器使用nn.AvgPool2d平均池化操作.</p><p><img data-src="https://s2.loli.net/2023/08/30/eIhKBaf3t85gVqx.png" alt="image-20230830120457639"></p><p>layer normalization RNN,nlp任务中,每个token的特征数不同,针对每个token</p><p>instance normalization GAN中,针对单个图像不同的通道</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> are very similar, but have some subtle differences. <a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> is applied on each channel of channeled data like RGB images, but <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> is usually applied on entire sample and often in NLP tasks. Additionally, <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> applies elementwise affine transform, while <a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> usually don’t apply affine transform</p><p>ConvTranspose2d</p><p>逆卷积fractionally-strided convolutions,可以利用<code>torchsummary</code>这个库查看模型相关信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">myModel = Discriminator().to(DEVICE)</span><br><span class="line">summary(myModel,(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">================================================================</span><br><span class="line">            Conv2d<span class="string">-1</span>          [<span class="string">-1</span>, 512, 14, 14]           4,608</span><br><span class="line">       BatchNorm2d<span class="string">-2</span>          [<span class="string">-1</span>, 512, 14, 14]           1,024</span><br><span class="line">         LeakyReLU<span class="string">-3</span>          [<span class="string">-1</span>, 512, 14, 14]               0</span><br><span class="line">            Conv2d<span class="string">-4</span>            [<span class="string">-1</span>, 256, 7, 7]       1,179,648</span><br><span class="line">       BatchNorm2d<span class="string">-5</span>            [<span class="string">-1</span>, 256, 7, 7]             512</span><br><span class="line">         LeakyReLU<span class="string">-6</span>            [<span class="string">-1</span>, 256, 7, 7]               0</span><br><span class="line">            Conv2d<span class="string">-7</span>            [<span class="string">-1</span>, 128, 4, 4]         294,912</span><br><span class="line">       BatchNorm2d<span class="string">-8</span>            [<span class="string">-1</span>, 128, 4, 4]             256</span><br><span class="line">         LeakyReLU<span class="string">-9</span>            [<span class="string">-1</span>, 128, 4, 4]               0</span><br><span class="line">        AvgPool2d<span class="string">-10</span>            [<span class="string">-1</span>, 128, 1, 1]               0</span><br><span class="line">           Linear<span class="string">-11</span>                    [<span class="string">-1</span>, 1]             129</span><br><span class="line">          Sigmoid<span class="string">-12</span>                    [<span class="string">-1</span>, 1]               0</span><br><span class="line">================================================================</span><br><span class="line">Total params: 1,481,089</span><br><span class="line">Trainable params: 1,481,089</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.00</span><br><span class="line">Forward/backward pass size (MB): 2.63</span><br><span class="line">Params size (MB): 5.65</span><br><span class="line">Estimated Total Size (MB): 8.28</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>我在测试github上一个DCGAN的代码时,发现其在生成器上除了最后一层使用tanh激活函数,其他层都使用leak激活函数,但是这样生成器会逐渐变大.</p><p><img data-src="https://s2.loli.net/2023/08/31/6QlMEzeH8vsLy7S.png" alt="image-20230831165159320" style="zoom:67%;" /></p><p>因为LeakyReLU照顾到了负数,使得每一线性层输出为负值时也有梯度,这样也许能使得生成器跳出</p><h3 id="WGAN"><a href="#WGAN" class="headerlink" title="*WGAN"></a>*WGAN</h3><p>使用EM距离<a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">GAN — Wasserstein GAN &amp; WGAN-GP. Training GAN is hard. Models may never… | by Jonathan Hui | Medium</a></p><ul><li>判别器最后一层去掉sigmoid</li><li>生成器和判别器的loss不取log</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p><img data-src="https://pic1.zhimg.com/80/v2-b783ce95d8bdf1499fc88994e170a02c_720w.webp" alt="img"></p><p>上面这个公式是基于推图距离的计算</p><p><img data-src="https://img1.imgtp.com/2023/09/15/B6W4SQCF.png" alt="image-20230915205616738" style="zoom: 67%;" /></p><p><img data-src="https://pic2.zhimg.com/80/v2-fe9ef30af6166a5eea47c9006bfc27cd_720w.webp" alt=""></p><p>下面是WGAN论文的intro</p><p><img data-src="https://s2.loli.net/2023/09/05/u4hjRmwN5i3E9oG.png" alt="image-20230905224350055" style="zoom: 80%;" /></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*5jF5gbIDwU6k9m1ILl0Utg.jpeg" alt="img"></p><p>一开始的GAN的损失函数设计被认为有问题,与KL,JS散度有关.</p><script type="math/tex; mode=display">KL(P_1||P_2)=E_{x\sim P_1}log\frac{P_1}{P_2}</script><script type="math/tex; mode=display">KL(P_1||P_2)=\int\limits_xP_1\log\frac{P_1}{P_2}dx\text{或}KL(P_1||P_2)=\sum p_1\log\frac{P_1}{P_2}</script><p>KL散度是熵与交叉熵的差,它不是对称的.</p><p>而JS散度和KL散度是有关联的,可以看出JS散度是对称的,</p><script type="math/tex; mode=display">JS(P_1||P_2)=\frac12KL(P_1||\frac{P_1+P_2}2)+\frac12KL(P_2||\frac{P_1+P_2}2)</script><p>经证明,当两个分布不重叠时,JS散度为log2<a href="https://blog.csdn.net/Invokar/article/details/88917214">GAN：两者分布不重合JS散度为log2的数学证明_为什么深度学习wganjs散度等于log2</a></p><blockquote><p>从理论和经验上来说，真实的数据分布通常是一个<strong>低维流形</strong>，简单地说就是数据不具备高维特性，而是存在一个嵌入在高维度的低维空间内,在实际操作中，我们的维度空间远远不止3维，有可能是上百维，在这样的情况下，数据就更加难于重合.</p></blockquote><p>WGAN打算训练网络得到一个函数,这个函数满足1-Lipschitz,同时也是D辨别器,这样能使得损失函数更有意义,也能解决梯度与模式崩溃问题. </p><p>WGAN贡献:解决GAN训练不稳定与模式崩溃问题,有一个指标(EM距离),这个值越小训练得越好.</p><h4 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h4><p>这里的GP就是gradient penalty的意思.在发了第一篇GAN的文章之后,作者又发了这篇.</p><blockquote><p>The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often <strong>due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior</strong></p></blockquote><p>WGAN以及其衍生主要都是为了满足Lipschitz constraint,包括后面的Spectral Normalizaton<a href="https://arxiv.org/pdf/1802.05957.pdf">1802.05957.pdf (arxiv.org)</a>.</p><p>意思是强制使用梯度裁剪(clamp)到一个范围会导致不想要的行为,因为本身想要的是让critic满足Lipschitz,所以粗暴地使用了梯度裁剪.</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*NVBkG5vDwwz-1ad-zwxddA.jpeg" alt="img"></p><p>需要使得判别器f的梯度范数处处小于1,WGAN-GP证明了需要使得在真实数据和生成的数据之间插值的点对于f应该具有1的梯度范数。</p><p>范数有多种.<img data-src="https://img1.imgtp.com/2023/09/17/ZwMIQPEx.png" alt="image-20230917103610368"></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*TErKpfBkilA-G24FNFg0FA.png" alt="img"></p><p>所以需要使用到梯度,而且是对于输入的梯度,通过限制输入的梯度,而不是WGAN中限制每次模型的weight和bias的值.<a href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html">torch.autograd.grad — PyTorch 2.0 documentation</a>在pytorch中使用autograd.grad计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autograd</span><br><span class="line"><span class="comment"># demo</span></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = torch.<span class="built_in">sum</span>(x**<span class="number">2</span>)</span><br><span class="line">grads = autograd.grad(outputs=y, inputs=x,create_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(grads)</span><br></pre></td></tr></table></figure><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*r8472Sg5fDJ1XKQPUbRC4Q.png" alt=""></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*gi2isFNxtXE-pNiQ_CrZ8w.jpeg" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradient Penalty (e.g. gradients w.r.t x_penalty)</span></span><br><span class="line">eps = torch.rand(batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(DEVICE) <span class="comment"># x shape: (64, 1, 28, 28)</span></span><br><span class="line">x_penalty = eps*x + (<span class="number">1</span>-eps)*x_fake</span><br><span class="line">x_penalty = x_penalty.view(x_penalty.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># n 1 28*28</span></span><br><span class="line">p_outputs = D(x_penalty, y)  <span class="comment"># N,1</span></span><br><span class="line">xp_grad = autograd.grad(outputs=p_outputs, inputs=x_penalty, grad_outputs=D_labels, <span class="comment"># N 1</span></span><br><span class="line">                        create_graph=<span class="literal">True</span>, retain_graph=<span class="literal">True</span>, only_inputs=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(xp_grad)</span><br><span class="line">grad_penalty = p_coeff * torch.mean(torch.<span class="built_in">pow</span>(torch.norm(xp_grad[<span class="number">0</span>], <span class="number">2</span>, <span class="number">1</span>) - <span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>对于辨别器,WGAN一般叫做critic,损失函数,而生成器依旧是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Wasserstein loss</span></span><br><span class="line">x_outputs = D(x, y)</span><br><span class="line">z_outputs = D(x_fake, y)</span><br><span class="line">D_x_loss = torch.mean(x_outputs)</span><br><span class="line">D_z_loss = torch.mean(z_outputs)</span><br><span class="line">D_loss = D_z_loss - D_x_loss + grad_penalty</span><br></pre></td></tr></table></figure><p>此外不使用BN,批次标准化会在同一批次中的样本之间创建相关性。它<strong>影响了梯度惩罚的有效性</strong>，实验证实了这一点。</p><p>一般可以使用Layer Normalization也就是对单个样本进行归一化.</p><h3 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="*Conditional GAN"></a>*Conditional GAN</h3><p>某种程度上里程碑作品,能够控制GAN生成的东西了,通过添加label,也就是condition.</p><p>例如在MNIST数据上,增加数字对应的label的one-hot变量,cat到图像数据上.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">    <span class="keyword">for</span> idx, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        <span class="comment"># Training Discriminator</span></span><br><span class="line">        x = images.to(DEVICE)</span><br><span class="line">        y = labels.view(batch_size, <span class="number">1</span>)</span><br><span class="line">        y = to_onehot(y).to(DEVICE) <span class="comment"># condition</span></span><br><span class="line">        x_outputs = D(x, y)</span><br><span class="line">        D_x_loss = criterion(x_outputs, D_labels)</span><br><span class="line"></span><br><span class="line">        z = torch.randn(batch_size, n_noise).to(DEVICE)</span><br><span class="line">        z_outputs = D(G(z, y), y)</span><br><span class="line">        D_z_loss = criterion(z_outputs, D_fakes)</span><br><span class="line">        D_loss = D_x_loss + D_z_loss</span><br><span class="line">        </span><br><span class="line">        D.zero_grad()</span><br><span class="line">        D_loss.backward()</span><br><span class="line">        D_opt.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % n_critic == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Training Generator</span></span><br><span class="line">            z = torch.randn(batch_size, n_noise).to(DEVICE)</span><br><span class="line">            z_outputs = D(G(z, y), y)</span><br><span class="line">            G_loss = criterion(z_outputs, D_labels)</span><br><span class="line"></span><br><span class="line">            G.zero_grad()</span><br><span class="line">            G_loss.backward()</span><br><span class="line">            G_opt.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;/&#123;&#125;, Step: &#123;&#125;, D Loss: &#123;&#125;, G Loss: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, max_epoch, step, D_loss.item(), G_loss.item()))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            G.<span class="built_in">eval</span>()</span><br><span class="line">            img = get_sample_image(G, n_noise)</span><br><span class="line">            imsave(<span class="string">&#x27;samples/&#123;&#125;_step&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(MODEL_NAME, <span class="built_in">str</span>(step).zfill(<span class="number">3</span>)), img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">            G.train()</span><br><span class="line">        step += <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Simple Generator w/ MLP</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size=<span class="number">100</span>, condition_size=<span class="number">10</span>, num_classes=<span class="number">784</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Linear(input_size+condition_size, <span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">256</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, num_classes),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, c</span>):</span></span><br><span class="line">        x, c = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>), c.view(c.size(<span class="number">0</span>), -<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        v = torch.cat((x, c), <span class="number">1</span>) <span class="comment"># v: [input, label] concatenated vector</span></span><br><span class="line">        y_ = self.layer(v)</span><br><span class="line">        y_ = y_.view(x.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">        <span class="keyword">return</span> y_</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Simple Discriminator w/ MLP</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size=<span class="number">784</span>, condition_size=<span class="number">10</span>, num_classes=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Linear(input_size+condition_size, <span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, num_classes),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, c</span>):</span>        </span><br><span class="line">        x, c = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>), c.view(c.size(<span class="number">0</span>), -<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        v = torch.cat((x, c), <span class="number">1</span>) <span class="comment"># v: [input, label] concatenated vector</span></span><br><span class="line">        y_ = self.layer(v)</span><br><span class="line">        <span class="keyword">return</span> y_</span><br></pre></td></tr></table></figure><h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p>提出利用互信息诱导潜变量.该方法将信息最大化引入到标准GAN网络中。</p><p><a href="https://medium.com/mlearning-ai/infogan-interpretable-representation-learning-to-distangle-data-unsupervised-33a4089d7c09">InfoGAN: Interpretable Representation Learning to Distangle Data Unsupervised | by Renee LIN | MLearning.ai | Medium</a></p><p>期望有良好的特征解耦关系.</p><blockquote><p>GAN公式使用简单的连续输入噪声矢量z，同时对G使用噪声的方式没有限制。因此，噪声可能会被生成器以高度纠缠(entangled)的方式使用，导致 z 的各个维度与数据的语义特征不对应。</p><p>在本文中，将输入噪声向量分解为两部分，而不是使用单个非结构化噪声向量：（i）z，它被视为不可压缩噪声源;（ii） c，我们称之为潜在代码，将针对数据分布的显著结构化语义特征。</p></blockquote><script type="math/tex; mode=display">I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)</script><p>引入互信息,在G输入时加入一个潜变量c,潜在代码 C 和生成器分布 G(z,c) 之间应该有高度的互信息。因此I(c;G(z,c)) 应该很高。给定任何 x ∼ P~G~(x),希望 P~G~（c|x） 有一个较小的熵。换句话说，潜在代码c中的信息不应该在生成过程中丢失。</p><script type="math/tex; mode=display">\operatorname*{min}_{G}\operatorname*{max}_{D}V_{I}(D,G)=V(D,G)-\lambda I(c;G(z,c))</script><p>然而上面互信息的计算涉及后验概率分布P(c|x)，而后者在实际中是很难获取的，所以需要定义一个辅助性的概率分布Q(c|x)，采用Variational Information Maximization对互信息进行下界拟合.</p><script type="math/tex; mode=display">\begin{aligned}I(c;G(z,c))& =H(c)-H(c|G(z,c))  \\&=\mathbb{E}_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log P(c^{\prime}|x)]]+H(c) \\&=\mathbb{E}_{x\sim G(z,c)}[\underbrace{D_{\mathrm{KL}}(P(\cdot|x)\parallel Q(\cdot|x))}_{\geq0}+\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c) \\&\geq\mathbb{E}_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c)\end{aligned}</script><p>这样互信息计算就能确定最小值,继续推导有</p><script type="math/tex; mode=display">\begin{aligned}L_{I}(G,Q)& =E_{c\sim P(c),x\sim G(z,c)}[\log Q(c|x)]+H(c)  \\&=E_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c) \\&\leq I(c;G(z,c))\end{aligned}</script><p>最后目标函数为</p><p><img data-src="https://pic4.zhimg.com/80/v2-ede0624e4acb54575483852435d0ec2b_720w.webp" alt="img"></p><p><img data-src="https://miro.medium.com/v2/resize:fit:543/1*c0wSI0WJR9-yagc0ruFGGg.png" alt="img" style="zoom: 67%;" /></p><p>z,c均为采样得到,z依旧是正态分布采样,c由两部分组成,一部分是离散分布另一部分是连续分布.论文中使用Categorical与Unif分布,是离散均匀分布与连续均匀分布.</p><p><img data-src="https://img1.imgtp.com/2023/09/18/TvDtnXUu.png" alt="image-20230918102341583" style="zoom:67%;" /></p><p>在MNIST数据集上,比如使用c~1~作为离散变量控制生成的数字的类型,其他的c~2~和c~3~作为连续变量控制其他.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_noise</span>(<span class="params">batch_size, n_noise, n_c_discrete, n_c_continuous, label=<span class="literal">None</span>, supervised=<span class="literal">False</span></span>):</span></span><br><span class="line">    z = torch.randn(batch_size, n_noise).to(DEVICE) <span class="comment">#正态分布 潜变量 与VAE的中间变量类似. bottleneck</span></span><br><span class="line">    <span class="comment"># 离散分布 控制数字类型也就是类别 如果supervised 会根据label的值</span></span><br><span class="line">    <span class="keyword">if</span> supervised:</span><br><span class="line">        c_discrete = to_onehot(label).to(DEVICE) <span class="comment"># (B,10)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 否则随机离散均匀生成</span></span><br><span class="line">        c_discrete = to_onehot(torch.LongTensor(batch_size, <span class="number">1</span>).random_(<span class="number">0</span>, n_c_discrete)).to(DEVICE) <span class="comment"># (B,10)</span></span><br><span class="line">    <span class="comment"># 连续分布 控制其他属性 </span></span><br><span class="line">    c_continuous = torch.zeros(batch_size, n_c_continuous).uniform_(-<span class="number">1</span>, <span class="number">1</span>).to(DEVICE) <span class="comment"># (B,2)</span></span><br><span class="line">    c = torch.cat((c_discrete.<span class="built_in">float</span>(), c_continuous), <span class="number">1</span>) <span class="comment">#c (B,12)</span></span><br><span class="line">    <span class="keyword">return</span> z, c</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># Training Discriminator</span></span><br><span class="line">x = images.to(DEVICE)</span><br><span class="line">x_outputs, _, = D(x)</span><br><span class="line">D_x_loss = bce_loss(x_outputs, D_labels)</span><br><span class="line"></span><br><span class="line">z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=<span class="literal">True</span>)</span><br><span class="line">z_outputs, _, = D(G(z, c))</span><br><span class="line">D_z_loss = bce_loss(z_outputs, D_fakes)</span><br><span class="line">D_loss = D_x_loss + D_z_loss</span><br><span class="line"></span><br><span class="line">D_opt.zero_grad()</span><br><span class="line">D_loss.backward()</span><br><span class="line">D_opt.step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_gaussian</span>(<span class="params">c, mu, var</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    criterion for Q(condition classifier)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> -((c - mu)**<span class="number">2</span>)/(<span class="number">2</span>*var+<span class="number">1e-8</span>) - <span class="number">0.5</span>*torch.log(<span class="number">2</span>*np.pi*var+<span class="number">1e-8</span>)</span><br><span class="line">    </span><br><span class="line"> <span class="comment"># Training Generator</span></span><br><span class="line">z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=<span class="literal">True</span>)</span><br><span class="line">c_discrete_label = torch.<span class="built_in">max</span>(c[:, :-<span class="number">2</span>], <span class="number">1</span>)[<span class="number">1</span>].view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">z_outputs, features = D(G(z, c)) <span class="comment"># (B,1), (B,10), (B,4)</span></span><br><span class="line">c_discrete_out, cc_mu, cc_var = Q(features)</span><br><span class="line"></span><br><span class="line">G_loss = bce_loss(z_outputs, D_labels)</span><br><span class="line">Q_loss_discrete = ce_loss(c_discrete_out, c_discrete_label.view(-<span class="number">1</span>))</span><br><span class="line">Q_loss_continuous = -torch.mean(torch.<span class="built_in">sum</span>(log_gaussian(c[:, -<span class="number">2</span>:], cc_mu, cc_var), <span class="number">1</span>)) <span class="comment"># N(x | mu,var) -&gt; (B, 2) -&gt; (,1)</span></span><br><span class="line">mutual_info_loss = Q_loss_discrete + Q_loss_continuous*<span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">GnQ_loss = G_loss + mutual_info_loss</span><br><span class="line"></span><br><span class="line">G_opt.zero_grad()</span><br><span class="line">GnQ_loss.backward()</span><br><span class="line">G_opt.step()</span><br></pre></td></tr></table></figure><p>离散分布的c求损失使用交叉熵,利用一个Q网络,输入是D的倒数第二层输出,得到离散输出与连续输出的均值和logV. 相当于利用Q的输出与D的倒数第二层输出计算损失,判断在生成过程中是否有损失.</p><p>其中log_gaussian是在计算log(q(x)),看来还是要学好数理统计和矩阵论才行.</p><h3 id="BIGGAN"><a href="#BIGGAN" class="headerlink" title="BIGGAN"></a>BIGGAN</h3><h3 id="proGAN"><a href="#proGAN" class="headerlink" title="proGAN"></a>proGAN</h3><h3 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h3><h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><h3 id="SAGAN"><a href="#SAGAN" class="headerlink" title="SAGAN"></a>SAGAN</h3><h3 id="TelDiGAN"><a href="#TelDiGAN" class="headerlink" title="TelDiGAN"></a>TelDiGAN</h3><h3 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p><a href="https://zhuanlan.zhihu.com/p/44926155">盘点各种GAN及资源整理（1） - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://github.com/soumith/ganhacks">soumith/ganhacks: starter from “How to Train a GAN?” at NIPS2016 (github.com)</a></p></li><li><p><a href="https://github.com/Yangyangii/GAN-Tutorial/tree/master">Yangyangii/GAN-Tutorial: Simple Implementation of many GAN models with PyTorch. (github.com)</a> 在一些数据集上的GAN</p></li><li><p><a href="https://github.com/eriklindernoren/PyTorch-GAN">eriklindernoren/PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks. (github.com)</a>pytorch实现的GAN</p></li><li><p><a href="https://github.com/ccc013/GAN_Study">ccc013/GAN_Study: 学习GAN的笔记和代码 (github.com)</a></p></li><li><p><a href="https://github.com/torchgan/torchgan">torchgan/torchgan: Research Framework for easy and efficient training of GANs based on Pytorch (github.com)</a> pytorch实现的库</p></li><li><p><a href="https://github.com/tensorflow/models/tree/master/research/gan">File not found (github.com)</a>tensorflow实现的GAN</p></li><li><p><a href="https://github.com/eriklindernoren/Keras-GAN">eriklindernoren/Keras-GAN: Keras implementations of Generative Adversarial Networks. (github.com)</a>keras实现的GAN</p></li><li><p><a href="https://github.com/zhangqianhui/AdversarialNetsPapers">zhangqianhui/AdversarialNetsPapers: Awesome paper list with code about generative adversarial nets (github.com)</a>GAN论文与代码</p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;深入GAN学习&lt;br&gt;</summary>
    
    
    
    
    <category term="GAN" scheme="https://www.sekyoro.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>tailwind速成</title>
    <link href="https://www.sekyoro.top/2023/08/04/tailwind%E9%80%9F%E6%88%90/"/>
    <id>https://www.sekyoro.top/2023/08/04/tailwind%E9%80%9F%E6%88%90/</id>
    <published>2023-08-04T14:09:28.000Z</published>
    <updated>2023-08-05T12:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>很火的CSS框架<br><span id="more"></span></p><p><a href="https://www.tailwindcss.cn/docs/installation">安装 - TailwindCSS中文文档 | TailwindCSS中文网</a></p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p><img data-src="https://s2.loli.net/2023/08/04/7d4CpYb5QeXHguV.png" alt="image-20230804221132617"></p><p><strong>Mobile first</strong></p><h2 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h2><h2 id="自定义设置值"><a href="#自定义设置值" class="headerlink" title="自定义设置值"></a>自定义设置值</h2><p>在<code>tailwind.config.js</code>文件中,由于 Tailwind 是一个用于构建定制用户界面的框架，因此在设计之初就考虑到了定制化。</p><p>默认情况下，Tailwind会在项目根目录下查找一个可选的 tailwind.config.js 文件，您可以在其中定义任何自定义内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx tailwindcss init</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="attr">theme</span>: &#123;</span><br><span class="line">    <span class="attr">colors</span>: &#123;</span><br><span class="line">      <span class="attr">primary</span>: <span class="string">&#x27;#5c6ac4&#x27;</span>,</span><br><span class="line">      <span class="attr">secondary</span>: <span class="string">&#x27;#ecc94b&#x27;</span>,</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://www.tailwindcss.cn/docs/customizing-colors#naming-your-colors">Customizing Colors - TailwindCSS中文文档 | TailwindCSS中文网</a></p><h2 id="重用"><a href="#重用" class="headerlink" title="重用"></a>重用</h2><p><a href="https://www.tailwindcss.cn/docs/reusing-styles#extracting-classes-with-apply">Reusing Styles - TailwindCSS中文文档 | TailwindCSS中文网</a></p><p>一般使用两种方法,一种使用@layer和@apply,另一种使用react或者vue框架定义component.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;很火的CSS框架&lt;br&gt;</summary>
    
    
    
    
    <category term="css" scheme="https://www.sekyoro.top/tags/css/"/>
    
    <category term="Tailwind" scheme="https://www.sekyoro.top/tags/Tailwind/"/>
    
  </entry>
  
  <entry>
    <title>深度知识基础学习(一)</title>
    <link href="https://www.sekyoro.top/2023/08/02/%E6%B7%B1%E5%BA%A6%E7%9F%A5%E8%AF%86%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0(%E4%B8%80)/"/>
    <id>https://www.sekyoro.top/2023/08/02/%E6%B7%B1%E5%BA%A6%E7%9F%A5%E8%AF%86%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0(%E4%B8%80)/</id>
    <published>2023-08-02T03:59:56.000Z</published>
    <updated>2023-09-05T07:05:44.320Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这方面的知识实在是很多<br><span id="more"></span></p><p>首先写一下计算机视觉方向的,介绍一下各种Net的发展史.</p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>Padding Stride 卷积核大小</p><p>p表示填充padding,k表示卷积核宽高,s表示stride</p><script type="math/tex; mode=display">n_{\mathrm{out}}=\frac{n_{\mathrm{in}}+2p-k}{s}+1</script><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><blockquote><p>LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像 (<a href="https://zh.d2l.ai/chapter_references/zreferences.html#id90">LeCun <em>et al.</em>, 1998</a>)中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果</p></blockquote><p><img data-src="https://s2.loli.net/2023/08/03/wQkVZHUIGzYyvMs.png" alt="image-20230803191703397"></p><p>​    每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>​    为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out = F.relu(self.conv1(x)) <span class="comment"># 3*32*32 -&gt; 6*28*28</span></span><br><span class="line">        out = F.max_pool2d(out,<span class="number">2</span>) <span class="comment"># 6*28*28 -&gt; 6*14*14</span></span><br><span class="line">        out = F.relu(self.conv2(out)) <span class="comment"># 6*14*14 -&gt; 16*10*10</span></span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>) <span class="comment"># 16*10*10 -&gt; 16*5*5</span></span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>) <span class="comment"># 16*5*5 -&gt; 400</span></span><br><span class="line"></span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p><p>AlexNet和LeNet的架构非常相似</p><p><img data-src="https://s2.loli.net/2023/08/03/olDupY69gcaJGH3.png" alt="image-20230803213645408" style="zoom:67%;" /></p><p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。</p><ol><li>AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li><li>AlexNet使用ReLU而不是sigmoid作为其激活函数。</li></ol><p>AlexNet通过<code>DropOut</code>控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。 在 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/image-augmentation.html#sec-image-augmentation">13.1节</a>中更详细地讨论数据扩增。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>), <span class="comment"># 3*224*224 -&gt; 64*55*55</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 64*55*55 -&gt; 64*27*27</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment"># 64*27*27 -&gt; 192*27*27</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 192*27*27 -&gt; 192*13*13</span></span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 192*13*13 -&gt; 384*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 384*13*13 -&gt; 256*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 256*13*13 -&gt; 256*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 256*13*13 -&gt; 256*6*6</span></span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out = self.features(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><blockquote><p>AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/vgg.svg" alt="../_images/vgg.svg" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;VGG11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vgg_name</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        self.features = self._make_layers(cfg[vgg_name])</span><br><span class="line">        self.classifier = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.features(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.classifier(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layers</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        layers = []</span><br><span class="line">        in_channels = <span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> cfg:</span><br><span class="line">            <span class="keyword">if</span> x == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">                layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [nn.Conv2d(in_channels, x, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                           nn.BatchNorm2d(x),</span><br><span class="line">                           nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">                in_channels = x</span><br><span class="line">        layers += [nn.AvgPool2d(kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG16</span>():</span></span><br><span class="line">    <span class="keyword">return</span> VGG(<span class="string">&#x27;VGG16&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG19</span>():</span></span><br><span class="line">    <span class="keyword">return</span> VGG(<span class="string">&#x27;VGG19&#x27;</span>)</span><br></pre></td></tr></table></figure><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。</p><h4 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h4><p><strong>1x1卷积</strong></p><blockquote><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 <em>网络中的网络</em>（<em>NiN</em>）提供了一个非常简单的解决方案：<strong>在每个像素的通道上分别使用多层感知机</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参考AlexNet设计</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NiN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_labels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NiN, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            self.nin_block(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">384</span>, out_channels=num_labels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line">        self.init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weight</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(layer.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                nn.init.constant_(layer.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,</span><br><span class="line">                      padding=padding),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_output_shape</span>(<span class="params">self</span>):</span></span><br><span class="line">        test_img = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">227</span>, <span class="number">227</span>), dtype=torch.float32)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            test_img = layer(test_img)</span><br><span class="line">            <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, test_img.shape)</span><br></pre></td></tr></table></figure><p>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层，或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）</p><h4 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h4><p>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 GoogleNet核心是提出了<code>Inception</code>这种模块.</p><p>这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 </p><p><img data-src="https://zh-v2.d2l.ai/_images/inception.svg" alt="../_images/inception.svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img data-src="https://zh-v2.d2l.ai/_images/inception-full.svg" alt="../_images/inception-full.svg" style="zoom: 67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p>对由多个输入平面组成的输入信号进行二维自适应平均池化处理。</p><p>对于任何输入尺寸，输出的尺寸都是 H x W。输出特征的数量等于输入平面的数量。</p></blockquote><p>nn.AdaptiveAvgPool2d((1,1))，首先这句话的含义是使得池化后的每个通道上的大小是一个1x1的，也就是每个通道上只有一个像素点。（1，1）表示的outputsize。</p><h5 id="Inception-V1"><a href="#Inception-V1" class="headerlink" title="Inception V1"></a>Inception V1</h5><p>使用了全局平均池化</p><h5 id="Inception-V2"><a href="#Inception-V2" class="headerlink" title="Inception V2"></a>Inception V2</h5><p>使用Batch Normalization，加快模型训练速度；<br>使用两个3x3的卷积代替5x5的大卷积，降低了参数数量并减轻了过拟合</p><h5 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception V3"></a>Inception V3</h5><blockquote><p>Inception V3一个最重要的改进是卷积分解（Factorization），将7x7卷积分解成两个一维的卷积串联（1x7和7x1），3x3卷积分解为两个一维的卷积串联（1x3和3x1），这样既可以加速计算，又可使网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU）</p></blockquote><p><img data-src="https://img-blog.csdnimg.cn/20190827133529424.png" alt="img" style="zoom: 67%;" /></p><h5 id="Inception-V4"><a href="#Inception-V4" class="headerlink" title="Inception V4"></a>Inception V4</h5><p>inception v4把原来的inception结构中加入了ResNet中的Residual Blocks结构，把一些层的输出加上前几层的输出，这样中间这几层学习的实际上是残差。</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>假设我们的原始输入为x，而希望学出的理想映射为f(x)（作为上方激活函数的输入）。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。 残差映射在现实中往往更容易优化。 以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)，我们只需将右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 右图是ResNet的基础架构–<em>残差块</em>（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。</p><p><img data-src="https://zh-v2.d2l.ai/_images/residual-block.svg" alt="../_images/residual-block.svg" style="zoom:67%;" /></p><blockquote><p>ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/resnet-block.svg" alt="../_images/resnet-block.svg" style="zoom:80%;" /></p><p><img data-src="https://zh-v2.d2l.ai/_images/resnet18.svg" alt="../_images/resnet18.svg" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差块  通过卷积</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">      self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">      <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">          self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                  kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self.conv3 = <span class="literal">None</span></span><br><span class="line">      self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">      self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">      Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">      Y = self.bn2(self.conv2(Y))</span><br><span class="line">      <span class="keyword">if</span> self.conv3:</span><br><span class="line">          X = self.conv3(X)</span><br><span class="line">      Y += X</span><br><span class="line">      <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#残差模块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm(), nn.Activation(<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">net.add(resnet_block(<span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>),</span><br><span class="line">        resnet_block(<span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">        resnet_block(<span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">        resnet_block(<span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">net.add(nn.GlobalAvgPool2D(), nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p><img data-src="https://zh-v2.d2l.ai/_images/densenet-block.svg" alt="../_images/densenet-block.svg"></p><p>ResNet和DenseNet的关键区别在于，DenseNet输出是<em>连接</em>（用图中的[,]表示）而不是如ResNet的简单相加</p><p><img data-src="https://zh-v2.d2l.ai/_images/densenet.svg" alt="../_images/densenet.svg"></p><p>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂</p><h5 id="denseblock"><a href="#denseblock" class="headerlink" title="denseblock"></a>denseblock</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h5 id="transition-layer"><a href="#transition-layer" class="headerlink" title="transition layer"></a>transition layer</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度.</p><h5 id="DenseNet-1"><a href="#DenseNet-1" class="headerlink" title="DenseNet"></a>DenseNet</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">blks = []</span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    <span class="comment"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p><ul><li>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li><li>DenseNet的主要构建模块是稠密块和过渡层。</li><li>在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量  </li></ul><h2 id="正则化方式"><a href="#正则化方式" class="headerlink" title="正则化方式"></a>正则化方式</h2><h3 id="不同的normalization"><a href="#不同的normalization" class="headerlink" title="不同的normalization"></a>不同的normalization</h3><blockquote><p>为什么需要批量规范化层呢？让我们来回顾一下训练神经网络时出现的一些实际挑战。</p><p>首先，数据预处理的方式通常会对最终结果产生巨大影响。 使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。 直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</p><p>第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：<strong>不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛</strong>。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。</p><p>第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。</p></blockquote><p>Normalization有多种方式,包括BN，IN，GN，LN.</p><h4 id="BN-Batch-Normalization"><a href="#BN-Batch-Normalization" class="headerlink" title="BN Batch Normalization"></a>BN Batch Normalization</h4><p><img data-src="https://s2.loli.net/2023/08/11/YpgyB1C3ZQkTbuh.png" alt="image-20230811102021894" style="zoom:50%;" /></p><p><img data-src="https://s2.loli.net/2023/08/10/GpsxiLJhErfyOTb.png" alt="image-20230810201247075"></p><p>其中N表示样本数,H、W表示高和宽.得到均值和标准差,利用这两个值标准化.</p><p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于<em>批量</em>统计的<em>标准化</em>，才有了<em>批量规范化</em>的名称. 简单来说,就是对于每个batch每个通道计算.得到三对均值和方差,然后对每个通道规范化.</p><h4 id="IN-Instance-Normalization"><a href="#IN-Instance-Normalization" class="headerlink" title="IN Instance Normalization"></a>IN Instance Normalization</h4><p><img data-src="https://s2.loli.net/2023/08/11/PoVaWGy6FLzpfAi.png" alt="image-20230811102050186" style="zoom:67%;" /></p><blockquote><p>Instance Normalization (IN) 最初用于图像的风格迁移。作者发现，在生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，因此可以先把图像在 channel 层面归一化，然后再用目标风格图片对应 channel 的均值和标准差“去归一化”，以期获得目标图片的风格。IN 操作也在单个样本内部进行，不依赖 batch。</p></blockquote><p><img data-src="https://s2.loli.net/2023/08/10/HlRGrK9joUJxkip.png" alt="image-20230810204827706"></p><p>简单点来说,就是对于每个样本,一张彩图三个通道计算,batch=1 用在特定任务比如风格迁移上.</p><h4 id="GN-Group-Normalization"><a href="#GN-Group-Normalization" class="headerlink" title="GN Group Normalization"></a>GN Group Normalization</h4><p>对于特定任务,batch不能过大,否则存在显存占用问题.而一般的BN这时候表现较差.GN 计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。</p><p><img data-src="https://s2.loli.net/2023/08/10/Ryfsc9CJhqEMLv3.png" alt="image-20230810205259430" style="zoom: 67%;" /></p><p>对通道进行分组，统计每个分组通道的高度和宽度，增强对批量大小的稳定性</p><h4 id="LN-Layer-Normalization"><a href="#LN-Layer-Normalization" class="headerlink" title="LN Layer Normalization"></a>LN Layer Normalization</h4><p>在使用BN层时，需要的假设是每个mini batch应该是同分布（或者近似同分布）的，如果不同mini batch的分布差异较大，相当于这个BN层需要学习不同的变换，这便无法解决<strong>Internal Covariate Shift</strong>（ICS,也就是内部偏移）问题。因此，在使用BN层时，batchsize尽可能调大、且数据集彻底打乱，否则BN的效果会显著变差。显而易见，BN也并不适用于需要先后输入数据的RNN模型。</p><p>BN并不适用于序列模型（RNN），对于序列数据，我们其实更加关心独立的数据样本（例如一个句子的特定位置的单词），因此Layer Normalization将每一条数据做归一化。</p><p><img data-src="https://s2.loli.net/2023/08/10/6xSbuFan71pBQYJ.png" alt="image-20230810211021828"></p><p>简单来说,就是计算所有通道上的数据得到均值和标准差.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/u010986753/article/details/99191760">GoogleNet、AleXNet、VGGNet、ResNet等总结_小麦粒的博客-CSDN博客</a></li><li><a href="https://zh-v2.d2l.ai/chapter_convolutional-modern/batch-norm.html">7.5. 批量规范化 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li><a href="https://zhuanlan.zhihu.com/p/91965772">BN、LN、IN、GN的简介 - 知乎 (zhihu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这方面的知识实在是很多&lt;br&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://www.sekyoro.top/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>增加live2d看板娘</title>
    <link href="https://www.sekyoro.top/2023/08/01/%E5%A2%9E%E5%8A%A0live2d%E7%9C%8B%E6%9D%BF%E5%A8%98/"/>
    <id>https://www.sekyoro.top/2023/08/01/%E5%A2%9E%E5%8A%A0live2d%E7%9C%8B%E6%9D%BF%E5%A8%98/</id>
    <published>2023-08-01T13:40:48.000Z</published>
    <updated>2023-08-02T02:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>看到可爱的看板娘了吗?<br><span id="more"></span><br>很多个人博客或者网站都有一个可爱的看板娘,这里主要用的就是一位大佬的代码(详细看参考资料),全都导入后,用于获取以及处理live2d人物以及对话消息.</p><p><img data-src="https://s2.loli.net/2023/08/02/GLDCzqIf9XHAn2Q.png" alt="image-20230802091503678"></p><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>现在的需求就是,后端的api用的是别人的服务,live2d人物的衣服放在github上,如果不翻的话很可能看不了,所以现在主要就是把后端api放到自己的网站上,同时看能不能增加一些模型.</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>如果有自己的服务器,那么上面如果有php的话直接放上去基本就可以了,如果不想这么做,可以使用其他的托管工具,比如<code>Vercel</code>,事实上Vercel是支持php的,利用了serverless  functions.至于增加更多模型,就需要到处找找或者自己使用<code>cubsim</code>制作了.</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://github.com/vercel-community/php/#️-faq">vercel-community/php: 🐘 PHP Runtime for ▲ Vercel Serverless Functions (github.com)</a></li><li><a href="https://github.com/stevenjoezhang/live2d-widget">stevenjoezhang/live2d-widget: 把萌萌哒的看板娘抱回家 (ノ≧∇≦)ノ | Live2D widget for web platform (github.com)</a></li><li><a href="https://github.com/xiaoski/live2d_models_collection/tree/master">xiaoski/live2d_models_collection: Collections of live2d models (github.com)</a></li><li><a href="https://www.live2d.com/zh-CHS/">Live2D - Live2D Cubism【官网】</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;看到可爱的看板娘了吗?&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>浏览器JS</title>
    <link href="https://www.sekyoro.top/2023/07/30/%E6%B5%8F%E8%A7%88%E5%99%A8JS/"/>
    <id>https://www.sekyoro.top/2023/07/30/%E6%B5%8F%E8%A7%88%E5%99%A8JS/</id>
    <published>2023-07-30T07:46:22.000Z</published>
    <updated>2023-08-01T01:44:22.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>浏览器的一些常用js方法.主要是浏览的宽高什么的,我经常弄混.<br><span id="more"></span></p><p>由于经常涉及浏览器里的元素<strong>宽高属性</strong>以及<strong>与其他元素间的距离</strong>,而且这设计到浏览器兼容性问题,相同的获取滚动条距离什么的方法在不同浏览器很可能效果不同,这里就优先考虑谷歌,Edge这些Chromium内核的.</p><p><img data-src="https://www.runoob.com/wp-content/uploads/2021/10/L0hUTUw15byA5Y-R5paH5qGjL2ltYWdlcy9Dc3NCb3hNb2RlbC5wbmc.png" alt="img" style="zoom: 67%;" /></p><p>这个图上,由内到外分别是content,padding,border,margin,然后是其他外层元素.通过这个图可以有一个大致了解.</p><div class="table-container"><table><thead><tr><th>元素尺寸属性</th><th>说明</th></tr></thead><tbody><tr><td>clientWidth</td><td>获取元素可视部分的宽度，即 CSS 的 width 和 padding 属性值之和，元素边框和滚动条不包括在内，也不包含任何可能的滚动区域</td></tr><tr><td>clientHeight</td><td>获取元素可视部分的高度，即 CSS 的 height 和 padding 属性值之和，元素边框和滚动条不包括在内，也不包含任何可能的滚动区域</td></tr><tr><td>offsetWidth</td><td>元素在页面中占据的宽度总和，包括 width、padding、border 以及滚动条的宽度</td></tr><tr><td>offsetHeight</td><td>元素在页面中占据的高度总和，包括 height、padding、border 以及滚动条的宽度</td></tr><tr><td>scrollWidth</td><td>当元素设置了 overflow:visible 样式属性时，元素的总宽度，也称滚动宽度。在默认状态下，如果该属性值大于 clientWidth 属性值，则元素会显示滚动条，以便能够翻阅被隐藏的区域</td></tr><tr><td>scrollHeight</td><td>当元素设置了 overflow:visible 样式属性时，元素的总高度，也称滚动高度。在默认状态下，如果该属性值大于 clientWidth 属性值，则元素会显示滚动条，以便能够翻阅被隐藏的区域</td></tr></tbody></table></div><p><img data-src="https://s2.loli.net/2023/07/31/nP1rODAi6NhLcy5.png" alt="image-20230731223846471"></p><p>比如打开一个有滚动条的网站.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">document</span>.documentElement.offsetWidth</span><br><span class="line">&gt;<span class="number">863</span></span><br><span class="line"><span class="built_in">document</span>.documentElement.clientWidth</span><br><span class="line">&gt;<span class="number">863</span></span><br><span class="line"><span class="built_in">document</span>.documentElement.scrollWidth</span><br><span class="line">&gt;<span class="number">1200</span></span><br></pre></td></tr></table></figure><p>其他的xxxLeft,xxxTop含义类似.比如clienLeft表示border的宽度.如果设置盒模型为border-box,那么clientWidth就等于width-border-滚动条,因为border-box的宽高包括了padding和border,而clientWidth不包括border以及滚动条.</p><p>而offsetLeft表示<code>当前元素左上角相对于offsetParent左边界的偏移</code>,offsetTop同理,计算时会使用元素的offsetParent元素的padding、margin,border以及自己的margin计算.</p><p>最后scrolLeft表示<code>一个元素的内容水平滚动的像素数</code>,比较好理解,就是如果有滚动条的话,就是滚动条滚动的距离</p><h3 id="获取可视区域的宽高"><a href="#获取可视区域的宽高" class="headerlink" title="获取可视区域的宽高"></a>获取可视区域的宽高</h3><ul><li>document.documentElement.clientWidth</li><li>document.documentElement.clientHeight</li></ul><p><img data-src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bd22ffc2355e4de99d1893ffb1f9667e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom: 50%;" /></p><p>注意,这个方法在老的IE浏览器上行不通.可以看到这个宽高不包括margin,border以及可能的滚动条宽高.此外注意这个宽高顾名思义<strong>可视区域</strong>,如果打开发者工具,会发现这个工具也是占宽高的.浏览器上面的任务栏以及滚动条也是占距离的.</p><h3 id="获取滚动条距离"><a href="#获取滚动条距离" class="headerlink" title="获取滚动条距离"></a>获取滚动条距离</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> scrollTop = <span class="built_in">document</span>.body.scrollTop || (<span class="built_in">document</span>.documentElement &amp;&amp; <span class="built_in">document</span>.documentElement.scrollTop);</span><br></pre></td></tr></table></figure><p>在谷歌这类浏览器中通常使用<code>document.documentElement.scrollTop</code></p><p>获取元素</p><p><img data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5e7d4a30e10c421faf01df217a71894b~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp" alt="img" style="zoom:67%;" /></p><h3 id="屏幕分辨率"><a href="#屏幕分辨率" class="headerlink" title="屏幕分辨率"></a>屏幕分辨率</h3><p>屏幕分辨率的高：window.screen.height</p><p>屏幕分辨率的宽：window.screen.width</p><p>屏幕可工作区域的高：window.screen.availHeight</p><p>屏幕可工作区域的宽：window.screen.availWidth</p><p>height和availHeight差别是会受到电脑任务栏的影响.此外这个值跟实际屏幕像素不同是因为有<code>屏幕缩放因子：window.devicePixelRatio</code></p><p>屏幕逻辑分辨率：window.screen.width * window.devicePixelRatio (缩放因子与物理分辨率的乘积)</p><p><img data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2240a67351d6413e8a93d5d7431ea129~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom:50%;" /></p><p>window.screenTop：浏览器相对于屏幕左上角的横向偏移值<br>window.screenLeft：浏览器相对于屏幕左上角的纵向偏移值</p><h3 id="浏览器的宽高"><a href="#浏览器的宽高" class="headerlink" title="浏览器的宽高"></a>浏览器的宽高</h3><p><img data-src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50b6dd61a0a5433fa03ef2c4a17d332f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom:67%;" /></p><p>浏览器总高度：window.outerHeight </p><p>浏览器总宽度：window.outerWidth  </p><p>此外还有不包括工具栏的:window.innerHeight  window.innerWidth </p><p><img data-src="https://s2.loli.net/2023/07/31/2xLB5U1Cwgo9lka.png" alt="image-20230731235947161"></p><p>这两个都包括滚动条,如果没有滚动条</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innerHeight = <span class="built_in">document</span>.documentElement.clientHeight</span><br><span class="line">innerWidth = <span class="built_in">document</span>.documentElement.clientWidth</span><br></pre></td></tr></table></figure><h3 id="元素方法"><a href="#元素方法" class="headerlink" title="元素方法"></a>元素方法</h3><ol><li><p><code>getBoundingClientRect()</code> :</p><p><code>getBoundingClientRect()</code> ： 得到矩形元素的界线，返回的是一个对象，包含top、right、bottom、left四个属性值，大小都是相对于浏览器窗口top、left的距离。返回的内容类似于：<code>&#123;top: 143, right: 1196, bottom: 165, left: 889&#125;</code>；</p><hr></li><li><p><code>scrollIntoView()</code> :</p><p><code>obj.scrollIntoView()</code> 让元素滚动到可视区域（<code>HTML5标准</code>），参数true与浏览器对齐，false元素在窗口居中显示；</p><hr></li><li><p>event.clientX / event.clientY ： 相对于window，为鼠标相对于浏览器窗口的偏移量。</p><p><code>event.clientX</code>鼠标在文档的水平坐标；</p><p><code>even.clientY</code>鼠标在文档的垂直坐标；</p></li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><p><a href="https://github.com/XIANFESchool/FE-problem-collection/issues/31">js获取浏览器中各种宽、高、偏移值 · Issue #31 · XIANFESchool/FE-problem-collection (github.com)</a></p></li><li><p><a href="https://github.com/sophianbj/JavaScript-accumulation/blob/master/前端开发中的各种宽高整理.md">JavaScript-accumulation/前端开发中的各种宽高整理.md at master · sophianbj/JavaScript-accumulation (github.com)</a></p></li><li><p><a href="https://www.runoob.com/jsref/prop-element-clientleft.html">HTML DOM clientLeft 属性 | 菜鸟教程 (runoob.com)</a></p></li><li><p><a href="https://juejin.cn/post/7116306912198524959#heading-3">scrollTop、clientHeight、 scrollHeight…学完真的理解了 - 掘金 (juejin.cn)</a></p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;浏览器的一些常用js方法.主要是浏览的宽高什么的,我经常弄混.&lt;br&gt;</summary>
    
    
    
    
    <category term="js" scheme="https://www.sekyoro.top/tags/js/"/>
    
    <category term="dom" scheme="https://www.sekyoro.top/tags/dom/"/>
    
    <category term="bom" scheme="https://www.sekyoro.top/tags/bom/"/>
    
  </entry>
  
  <entry>
    <title>CSS_Modules介绍</title>
    <link href="https://www.sekyoro.top/2023/07/29/CSS-Modules%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.sekyoro.top/2023/07/29/CSS-Modules%E4%BB%8B%E7%BB%8D/</id>
    <published>2023-07-29T07:53:28.000Z</published>
    <updated>2023-07-30T04:15:06.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前在用Vue框架时,为了便于不同template之间的样式不相互影响,都设置了css的scoped属性.现在在学习React,发现并没有自带这种功能,通常可以使用多种方案解决.</p><span id="more"></span><p><img data-src="https://i.imgur.com/62Mf7KC.png" alt="image-20230721195018708"></p><p>一般在用CSS Modules或者CSS in Js的方法,不过感觉CSS Modules用的比较多,设置也并不复杂,这里写一下简单的教程.</p><blockquote><p>为了让 CSS 也能适用软件工程方法，程序员想了各种办法，让它变得像一门编程语言。从最早的Less、SASS，到后来的 PostCSS，再到最近的 CSS in JS，都是为了解决这个问题。</p></blockquote><p>在React中使用creat-react-app或者使用vite都直接用就行了,一般的脚手架都默认集成了CSS Modules.</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>首先创建css文件,要求命名为xxx.module.css</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* style.module.css */</span></span><br><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: green;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当从JS模块导入CSS模块时，它会导出一个包含所有本地名称到全局名称的映射的对象。</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> styles <span class="keyword">from</span> <span class="string">&quot;./style.module.css&quot;</span>;</span><br><span class="line"><span class="comment">// import &#123; className &#125; from &quot;./style.css&quot;;</span></span><br><span class="line">element.innerHTML = <span class="string">&#x27;&lt;div class=&quot;&#x27;</span> + styles.className + <span class="string">&#x27;&quot;&gt;&#x27;</span>;</span><br></pre></td></tr></table></figure><p>在编译时会给类名前加上一个唯一值用以区分,事实上vue的scoped也是这个原理.</p><p><img data-src="https://i.imgur.com/nCJDs00.png" alt="image-20230721201226474"></p><p>使用<code>import styles from &quot;./xxxx.module.css&quot;;</code>导入后,事实上不管是否使用,都会将这个css文件里的规则用上,比如这个文件</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.nav</span> &#123;</span><br><span class="line">  <span class="attribute">display</span>: flex;</span><br><span class="line">  <span class="attribute">align-items</span>: center;</span><br><span class="line">  <span class="attribute">justify-content</span>: space-between;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">ul</span> &#123;</span><br><span class="line">  <span class="attribute">list-style</span>: none;</span><br><span class="line">  <span class="attribute">display</span>: flex;</span><br><span class="line">  <span class="attribute">align-items</span>: center;</span><br><span class="line">  gap: <span class="number">4rem</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:link</span>,</span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:visited</span> &#123;</span><br><span class="line">  <span class="attribute">text-decoration</span>: none;</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">var</span>(--color-light--<span class="number">2</span>);</span><br><span class="line">  <span class="attribute">text-transform</span>: uppercase;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">1.5rem</span>;</span><br><span class="line">  <span class="attribute">font-weight</span>: <span class="number">600</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* CSS Modules feature */</span></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span>:<span class="built_in">global</span>(.active) &#123;</span><br><span class="line">  color: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">a</span><span class="selector-class">.ctaLink</span><span class="selector-pseudo">:link</span>,</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-class">.ctaLink</span><span class="selector-pseudo">:visited</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">var</span>(--color-dark--<span class="number">0</span>);</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0.8rem</span> <span class="number">2rem</span>;</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">7px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>:red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里<code>div &#123;  background-color:red;&#125;</code>会直接用上,但是如果定义了类并使用的话,这个类就会生成唯一类名.</p><h3 id="命名方式"><a href="#命名方式" class="headerlink" title="命名方式"></a>命名方式</h3><p>对于本地类名，建议使用驼峰命名法，但不强制要求。</p><p>这是因为常见的替代方案，连字符命名法，在尝试访问style.class-name时可能会导致意外行为，因为它作为点符号表示法。您仍然可以使用方括号表示法（例如style[‘class-name’]）来解决连字符命名法，但是style.className更清晰。</p><p>简单来说命名需要驼峰.</p><h3 id="global全局使用css"><a href="#global全局使用css" class="headerlink" title=":global全局使用css"></a>:global全局使用css</h3><p>:global切换到当前选择器的全局作用域，对应的标识符。:global(.xxx) 对应的@keyframes :global(xxx)声明括号中的内容在全局作用域中。</p><p>例如,如果自定义了nav类,下面有类名为active的元素,这时不能使用.nav .active进行选择,因为会给active类名字前生成区分的id,又要在相关元素上加上. 为了避免麻烦,可以使用:global</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span>:<span class="built_in">global</span>(.active) &#123;</span><br><span class="line">  color: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h3><p>使用<code>compose</code>组合其他选择器的规则</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: blue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.title</span> &#123;</span><br><span class="line">  composes: className;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果继承其他文件里的</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: blue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.title</span> &#123;</span><br><span class="line">  composes: className from <span class="string">&#x27;./another.css&#x27;</span>;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://www.ruanyifeng.com/blog/2016/06/css_modules.html">CSS Modules 用法教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://github.com/css-modules/css-modules">css-modules/css-modules: Documentation about css-modules (github.com)</a></li><li><a href="https://juejin.cn/post/7031528538209681444">react 中 css modules-基本使用 - 掘金 (juejin.cn)</a></li><li><a href="https://www.jianshu.com/p/694f9c14ab35">在 React 中使用 CSS Modules - 简书 (jianshu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前在用Vue框架时,为了便于不同template之间的样式不相互影响,都设置了css的scoped属性.现在在学习React,发现并没有自带这种功能,通常可以使用多种方案解决.&lt;/p&gt;</summary>
    
    
    
    
    <category term="css" scheme="https://www.sekyoro.top/tags/css/"/>
    
  </entry>
  
  <entry>
    <title>青龙面板部署项目</title>
    <link href="https://www.sekyoro.top/2023/07/09/%E9%9D%92%E9%BE%99%E9%9D%A2%E6%9D%BF%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/"/>
    <id>https://www.sekyoro.top/2023/07/09/%E9%9D%92%E9%BE%99%E9%9D%A2%E6%9D%BF%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/</id>
    <published>2023-07-09T03:21:18.000Z</published>
    <updated>2023-07-09T04:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>青龙面板主要用于在上面放一些自动化脚本,可以跑一些类似签到的程序.之前我部署过一些,结果服务器出现了一些问题,现在使用docker重装一下并建立一个新的docker image.<br><span id="more"></span></p><p>首先官方仓库<a href="https://github.com/whyour/qinglong">whyour/qinglong: 支持 Python3、JavaScript、Shell、Typescript 的定时任务管理平台（Timed task management platform supporting Python3, JavaScript, Shell, Typescript） (github.com)</a>,推荐使用docker的方式,卸载安装都很方便.</p><p>启动一个容器后,在本地打开设置的端口,这里不推荐使用5700,因为会有机器人扫描端口暴力破解,我之前就遇到过.此外要在防火墙上打开对应的端口.</p><p><img data-src="https://s2.loli.net/2023/07/09/DWxV5MUmoeqikjN.png" alt="image-20230709112516871"></p><p>然后在本地打开对应端口的网页,选择推送方式,我一般九用PushPlus的一对一推送,官方文档<a href="https://www.pushplus.plus/push1.html">一对一消息|pushplus(推送加)-微信消息推送平台</a></p><p>然后设置账号和密码.这样搭建就完成了,这里推荐几个脚本,一般来说越新的越好.</p><h3 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h3><p>有许多库可以使用,我之前就用的faker.详情可以看看文尾参考链接</p><h3 id="阿里云盘签到"><a href="#阿里云盘签到" class="headerlink" title="阿里云盘签到"></a>阿里云盘签到</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ql repo https:<span class="regexp">//gi</span>thub.com<span class="regexp">/mrabit/</span>aliyundriveDailyCheck.git <span class="string">&quot;autoSignin&quot;</span> <span class="string">&quot;&quot;</span> <span class="string">&quot;qlApi&quot;</span></span><br></pre></td></tr></table></figure><p>阿里云盘的自动签到</p><h3 id="美团-饿了吗"><a href="#美团-饿了吗" class="headerlink" title="美团 饿了吗"></a>美团 饿了吗</h3><ul><li>美团</li><li>定时规则：10 8,11,18,21 <em> </em> *</li><li>脚本地址：<a href="https://raw.githubusercontent.com/leafTheFish/DeathNote/main/meituanV3.js">https://raw.githubusercontent.com/leafTheFish/DeathNote/main/meituanV3.js</a></li><li></li><li>饿了么</li><li>定时规则：10 8,11,18,21 <em> </em> *</li><li>脚本地址：<a href="https://raw.githubusercontent.com/leafTheFish/DeathNote/main/elmV3.js">https://raw.githubusercontent.com/leafTheFish/DeathNote/main/elmV3.js</a></li></ul><h3 id="B站"><a href="#B站" class="headerlink" title="B站"></a>B站</h3><p>我fork了一下源项目,把输出限制在5000字以配合PushPlus的免费推送额度</p><p><a href="https://github.com/drowning-in-codes/BiliBiliToolPro/blob/main/qinglong/README.md">BiliBiliToolPro/qinglong/README.md at main · drowning-in-codes/BiliBiliToolPro · GitHub</a></p><p>注意配置相关cookie用于登录</p><p><img data-src="https://s2.loli.net/2023/07/09/czmZdETvkt41i3b.png" alt="image-20230709121439385" style="zoom:67%;" /></p><h2 id="打包镜像"><a href="#打包镜像" class="headerlink" title="打包镜像"></a>打包镜像</h2><p>由于之前装了一堆脚本结果服务器出现了一点问题,这里打算打包镜像,镜像里就带着这些脚本以及一些cookie</p><p>使用<code>docker commit</code></p><p><img data-src="https://s2.loli.net/2023/07/09/iknuaYByJEV7X8z.png" alt="image-20230709122034155"></p><p>查询刚才的新镜像</p><p><img data-src="https://s2.loli.net/2023/07/09/vYIqbO6c2BPuo9Z.png" alt="image-20230709122209545"></p><p>可以使用<code>docker tag [IMAGE ID] [NEW NAME]</code>重命名,同时删除之前的镜像</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li>[2023最新青龙面板京东脚本库（7月2日，持续更新中） - 便利空间 (conveniencespace.com)]</li><li>[<a href="https://www.cnblogs.com/anyview/p/17030881.html">青龙面板自动领京东京豆，做农场浇水、萌宠、种豆、签到等任务 - 视觉书虫 - 博客园 (cnblogs.com)</a>(<a href="https://conveniencespace.com/index.php/2022/05/03/2022最新青龙面板京东脚本库（持续更新中）/">https://conveniencespace.com/index.php/2022/05/03/2022最新青龙面板京东脚本库（持续更新中）/</a>)</li><li><a href="https://www.dujin.org/20920.html">使用青龙面板挂载「阿里云盘账号」自动签到领会员福利-缙哥哥 (dujin.org)</a></li><li><a href="https://blog.renzicu.com/2022/qinglong-meituanelm/index.html">码农大叔博客 - 技术交流 | 「薅羊毛」青龙面板 – 美团&amp;饿了么 (renzicu.com)</a></li><li><a href="https://www.runoob.com/w3cnote/docker-use-container-create-image.html">Docker 使用容器来创建镜像 | 菜鸟教程 (runoob.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;青龙面板主要用于在上面放一些自动化脚本,可以跑一些类似签到的程序.之前我部署过一些,结果服务器出现了一些问题,现在使用docker重装一下并建立一个新的docker image.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Astro+Vercel部署博客</title>
    <link href="https://www.sekyoro.top/2023/07/07/Astro-Vercel%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/"/>
    <id>https://www.sekyoro.top/2023/07/07/Astro-Vercel%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/</id>
    <published>2023-07-07T07:21:56.000Z</published>
    <updated>2023-09-01T13:58:00.121Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>类似于hexo,这里使用Astro部署博客.创建一个Github项目部署在Vercel,主要目的是记录一些琐碎的学习记录,顺便学习一下很热的Astro.</p><span id="more"></span><h2 id="Astro介绍"><a href="#Astro介绍" class="headerlink" title="Astro介绍"></a>Astro介绍</h2><p>Astro 是<strong>集多功能于一体的 Web 框架</strong>，用于构建<strong>快速、以内容为中心</strong>的网站。</p><h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><p><a href="https://docs.astro.build/zh-cn/getting-started/#主要特性">标题部分 主要特性</a></p><ul><li><strong>组件群岛:</strong> 用于构建更快网站的新 web 架构。</li><li><strong>服务器优先的 API 设计:</strong> 从用户设备上去除高成本的 Hydration。</li><li><strong>默认零 JS:</strong> 没有 JavaScript 运行时开销来减慢你的速度。</li><li><strong>边缘就绪:</strong> 在任何地方部署，甚至像 Deno 或 Cloudflare 这样的全球边缘运行时。</li><li><strong>可定制:</strong> Tailwind, MDX 和 100 多个其他集成可供选择。</li><li><strong>不依赖特定 UI:</strong> 支持 React, Preact, Svelte, Vue, Solid, Lit 等等</li></ul><p>可以使用<a href="[CodeSandbox: Code, Review and Deploy in Record Time](https://codesandbox.io/">codesandbox</a>)与<a href="[StackBlitz | Instant Dev Environments | Click. Code. Done.](https://stackblitz.com/">stackblitz</a>)在线写一些代码.</p><p>可以在这个网站<a href="https://astro.new/latest/">Getting Started | astro.new</a>玩玩一些常用模板.</p><p>也有其他用户的例子<a href="https://astro.build/themes/">Themes | Astro</a>，关于主题的.</p><p>另外可以使用astro方便添加其他库,比如添加tailwind.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx astro add tailwind</span><br></pre></td></tr></table></figure><p><img data-src="https://s2.loli.net/2023/07/08/h6JqXOmDHubWRzy.png" alt="image-20230708152345258"></p><p>这里增加了tailwind以及react</p><h3 id="Astro的配置"><a href="#Astro的配置" class="headerlink" title="Astro的配置"></a>Astro的配置</h3><blockquote><p>在 <code>astro.config.mjs</code> 文件中自定义 Astro 的运行方式。它在 Astro 项目中十分常见，所有官方的示例模板和主题都默认附带。</p></blockquote><p>这里我使用了Blog模板,用于记录博客的.</p><blockquote><ul><li>Edit this page in <code>src/pages/index.astro</code></li><li>Edit the site header items in <code>src/components/Header.astro</code></li><li>Add your name to the footer in <code>src/components/Footer.astro</code></li><li>Check out the included blog posts in <code>src/pages/blog/</code></li><li>Customize the blog post page layout in <code>src/layouts/BlogPost.astro</code></li></ul></blockquote><h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h3><blockquote><p>Astro 为你的项目提供了一个有想法的文件夹布局。每个 Astro 项目的根目录下都应该包括以下目录和文件：</p><ul><li><code>src/*</code> - 你的项目源代码（组件、页面、样式等）。</li><li><code>public/*</code> - 你的非代码、未处理的资源（字体、图标等）。</li><li><code>package.json</code> - 项目列表。</li><li><code>astro.config.mjs</code> - Astro 配置文件（可选）。</li><li><code>tsconfig.json</code> - TypeScript 配置文件（可选）。</li></ul></blockquote><h4 id="src-components"><a href="#src-components" class="headerlink" title="src/components"></a><code>src/components</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srccomponents">标题部分 src/components</a></p><p><strong>组件</strong>是你在 HTML 页面中可重复使用的代码单元。它可以是 <a href="https://docs.astro.build/zh-cn/core-concepts/astro-components/">Astro 组件</a> 或是像 React 或 Vue 这样的<a href="https://docs.astro.build/zh-cn/core-concepts/framework-components/">前端组件</a>。通常将你项目中所有组件都分组放在这个文件夹中。</p><p>这在 Astro 项目中是个习惯，但不过你可以自由地根据喜好进行管理。</p><h4 id="src-layouts"><a href="#src-layouts" class="headerlink" title="src/layouts"></a><code>src/layouts</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srclayouts">标题部分 src/layouts</a></p><p><a href="https://docs.astro.build/zh-cn/core-concepts/layouts/">布局</a>是特殊的组件，它将一些内容包裹在一个更大的页面布局中。通常用在 <a href="https://docs.astro.build/zh-cn/core-concepts/astro-pages/">Astro 页面</a>和 <a href="https://docs.astro.build/zh-cn/guides/markdown-content/">Markdown 页面</a>中以定义页面的布局。</p><p>和 <code>src/components</code> 一样，这个目录也只是约定俗成。</p><h4 id="src-pages"><a href="#src-pages" class="headerlink" title="src/pages"></a><code>src/pages</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srcpages">标题部分 src/pages</a></p><p><a href="https://docs.astro.build/zh-cn/core-concepts/astro-pages/">页面</a>是一种用于创建新的页面的特殊组件。一个页面可以是一个 Astro 组件，也可以是一个 Markdown 文件，它代表你网站的一些内容页面。</p><h4 id="src-styles"><a href="#src-styles" class="headerlink" title="src/styles"></a><code>src/styles</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srcstyles">标题部分 src/styles</a></p><p>在 <code>src/styles</code> 目录下存储你的 CSS 或 Sass 文件仍只是个习惯。只要你的样式在 <code>src/</code> 目录下的某个地方，并且正确导入，Astro 就能处理并压缩它们。</p><h4 id="public"><a href="#public" class="headerlink" title="public/"></a><code>public/</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#public">标题部分 public/</a></p><p><code>public/</code> 目录用于文件和资源，它不会在 Astro 构建过程中处理。这些文件将不加修改地被直接复制到构建文件夹。</p><p>这种行为使得 <code>public/</code> 成为存放图片和字体等普通资源或 <code>robots.txt</code> 和 <code>manifest.webmanifest</code> 等特殊文件的理想选择。</p><h3 id="撰写组件"><a href="#撰写组件" class="headerlink" title="撰写组件"></a>撰写组件</h3><blockquote><p>Astro 组件非常灵活的。通常情况下，Astro 组件会包含一些<strong>可在页面中复用的 UI</strong>，如 header 或简介卡。在其他时候，Astro 组件可能包含一个较小的 HTML 片段，像是常见的使 SEO 更好的 <code>&lt;meta&gt;</code> 标签集合。Astro 组件甚至可以包含整个页面布局。</p><p>Astro 组件中最重要的一点是，它们在构建过程中会被<strong>渲染成 HTML</strong>。即使你在组件内运行 JavaScript 代码，它也会抢先一步运行从呈现给用户的最终页面中剥离出来。其最终使得网站变得更快，且默认不用任何 JavaScript。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://docs.astro.build/zh-cn/getting-started/">入门指南 🚀 Astro 文档</a></li><li><a href="https://2022.stateofjs.com/en-US/">State of JavaScript 2022 (stateofjs.com)</a></li><li><a href="https://astro.new/latest/">Getting Started | astro.new</a></li><li><a href="https://www.youtube.com/watch?v=NniT0vKyn-E&amp;ab_channel=developedbyed">(1) Astro Crash Course in 60 Minutes - YouTube</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;类似于hexo,这里使用Astro部署博客.创建一个Github项目部署在Vercel,主要目的是记录一些琐碎的学习记录,顺便学习一下很热的Astro.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Astro" scheme="https://www.sekyoro.top/tags/Astro/"/>
    
    <category term="Vercel" scheme="https://www.sekyoro.top/tags/Vercel/"/>
    
    <category term="Github" scheme="https://www.sekyoro.top/tags/Github/"/>
    
  </entry>
  
</feed>
