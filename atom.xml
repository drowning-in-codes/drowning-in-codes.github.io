<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sekyoro的博客小屋</title>
  
  
  <link href="https://www.sekyoro.top/atom.xml" rel="self"/>
  
  <link href="https://www.sekyoro.top/"/>
  <updated>2023-12-24T07:15:26.734Z</updated>
  <id>https://www.sekyoro.top/</id>
  
  <author>
    <name>Sekyoro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Github_bot创建</title>
    <link href="https://www.sekyoro.top/2023/12/24/Github-bot%E5%88%9B%E5%BB%BA/"/>
    <id>https://www.sekyoro.top/2023/12/24/Github-bot%E5%88%9B%E5%BB%BA/</id>
    <published>2023-12-24T06:50:24.000Z</published>
    <updated>2023-12-24T07:15:26.734Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在看一些开源项目时,会发现一些帮忙处理issue和PR的bot,这些bot都是基于Github的Apps<a href="https://docs.github.com/en/apps/overview">GitHub Apps overview - GitHub Docs</a></p><span id="more"></span><h2 id="GitHub-Apps"><a href="#GitHub-Apps" class="headerlink" title="GitHub Apps"></a>GitHub Apps</h2><blockquote><p>GitHub应用程序是扩展GitHub功能的工具。GitHub应用程序可以在GitHub上做一些事情，比如打开问题、评论拉取请求和管理项目。他们也可以根据GitHub上发生的事件在GitHub之外做事情。例如，当在GitHub上打开问题时，GitHub应用程序可以在Slack上发布。</p></blockquote><p>可以在<a href="https://github.com/marketplace">GitHub Marketplace</a>上查找Github Apps,然后进行安装,有些是需要付费的.</p><p>关于使用直接安装然后看文档进行配置就行了。</p><h3 id="如何开发"><a href="#如何开发" class="headerlink" title="如何开发"></a>如何开发</h3><p><a href="https://github.com/github/github-app-js-sample?tab=readme-ov-file">github/github-app-js-sample: Sample of a GitHub App that comments new pull requests</a></p><p><img data-src="https://i.imgur.com/trK7YAv.png" alt="image-20231224150632938"></p><p>由于本地开发涉及到需要接受github发来的东西,需要涉及到内网穿透啥的,推荐使用smee或者ngrok进行本地开发.建议搭配下面介绍的probot进行开发.<a href="https://probot.github.io/docs/development/#installing-the-app-on-a-repository">probot.github.io/docs/development/#installing-the-app-on-a-repository</a></p><h2 id="Probot"><a href="#Probot" class="headerlink" title="Probot"></a>Probot</h2><blockquote><p>Probot是一个在Node.js中构建GitHub应用程序的框架。它旨在消除所有的繁琐工作，比如接收和验证Webhook，以及进行身份验证倒立，这样你就可以专注于你想要构建的功能。Probet应用程序易于编写、部署和共享。许多最流行的Probet应用程序都是托管的，所以没有什么可供您部署和管理的。</p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = <span class="function">(<span class="params">app</span>) =&gt;</span> &#123;</span><br><span class="line">  app.on(<span class="string">&quot;issues.opened&quot;</span>, <span class="keyword">async</span> (context) =&gt; &#123;</span><br><span class="line">    <span class="keyword">const</span> issueComment = context.issue(&#123;</span><br><span class="line">      <span class="attr">body</span>: <span class="string">&quot;Thanks for opening this issue!&quot;</span>,</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="keyword">return</span> context.octokit.issues.createComment(issueComment);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  app.onAny(<span class="keyword">async</span> (context) =&gt; &#123;</span><br><span class="line">    context.log.info(&#123; <span class="attr">event</span>: context.name, <span class="attr">action</span>: context.payload.action &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  app.onError(<span class="keyword">async</span> (error) =&gt; &#123;</span><br><span class="line">    app.log.error(error);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="Repo-Automation-Bots"><a href="#Repo-Automation-Bots" class="headerlink" title="Repo Automation Bots"></a>Repo Automation Bots</h2><p><a href="https://github.com/googleapis/repo-automation-bots?tab=readme-ov-file">googleapis/repo-automation-bots: A collection of bots, based on probot, for performing common maintenance tasks across the open-source repos managed by Google on GitHub.</a>一组基于probot的机器人,用于谷歌在GitHub上管理的开源转发中执行常见维护任务。下面是一些可用的bot</p><div class="table-container"><table><thead><tr><th><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/auto-approve">auto-approve</a></th><th>Automatically approves and merges PRs matching user-specified configs</th><th><a href="https://github.com/apps/auto-approve-bot">install</a></th></tr></thead><tbody><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/auto-label">auto-label</a></td><td>Automatically labels issues and PRs with product, language, or directory based labels</td><td><a href="https://github.com/apps/product-auto-label">install</a></td></tr><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/blunderbuss">blunderbuss</a></td><td>Assigns issues and PRs randomly to a specific list of users</td><td><a href="https://github.com/apps/blunderbuss-gcf">install</a></td></tr><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/cherry-pick-bot">cherry-pick-bot</a></td><td>Cherry-pick merged PRs between branches</td><td><a href="https://github.com/apps/gcp-cherry-pick-bot">install</a></td></tr></tbody></table></div><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://probot.github.io/docs/">probot.github.io/docs/</a></li><li><a href="https://dev.to/pragativerma18/github-bots-for-every-open-source-project-47hl">GitHub Bots for every open-source project - DEV Community</a></li><li><a href="https://github.com/googleapis/repo-automation-bots?tab=readme-ov-file">googleapis/repo-automation-bots: A collection of bots, based on probot, for performing common maintenance tasks across the open-source repos managed by Google on GitHub.</a></li><li><a href="https://smee.io/dSzRk0AnpSDDOf0T">smee.io | Webhook deliveries</a></li><li><a href="https://ngrok.com/">ngrok | Unified Application Delivery Platform for Developers</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;在看一些开源项目时,会发现一些帮忙处理issue和PR的bot,这些bot都是基于Github的Apps&lt;a href=&quot;https://docs.github.com/en/apps/overview&quot;&gt;GitHub Apps overview - GitHub Docs&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测综述</title>
    <link href="https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/</id>
    <published>2023-12-22T11:38:36.000Z</published>
    <updated>2023-12-23T12:04:08.140Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>2023年的目标检测综述<strong>A comprehensive review of object detection with deep learning</strong>以及<strong>3D Object Detection for Autonomous Driving: A Comprehensive Survey</strong>,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下.</p><span id="more"></span><h2 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h2><p>本综述详细介绍了物体检测及其各个方面。随着用于检测物体的深度学习算法逐渐发展，物体检测模型的性能也有了显著提高。但是，这并不意味着在深度学习出现之前已经发展了几十年的传统物体检测方法已经过时。<strong>在某些情况下，具有全局特征的传统方法是更优的选择</strong>。<strong>本综述论文首先简要概述了物体检测，然后介绍了物体检测框架、骨干卷积神经网络、常见数据集概述以及评估指标</strong>。此外，还详细研究了物体检测问题和应用。还<strong>讨论了设计深度神经网络的一些未来研究挑战</strong>。最后，比较了对象检测模型在 PASCAL VOC 和 MS COCO 数据集上的性能，并得出结论。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>物体检测的主要目的是检测特定类别的视觉物体，如电视/显示器、书籍、猫、人类等，并使用边界框定位它们，然后将它们归入特定物体的类别中。</p><p>通用对象检测还有其他几个术语，例如通用对象类别检测、对象类别检测、类别级对象检测和对象类别检测。它也侧重于识别一些预设类别的实例.</p><p>物体检测的发展通常分为两个历史阶段。<strong>2014 年之前是传统方法阶段，2014 年之后则是基于深度学习的方法阶段</strong>。本文将重点讨论基于深度学习的方法。由于 CNN 在物体检测算法的实施中发挥着重要作用，因此本文将利用 CNN 来获得最佳结果。这两个阶段的架构在精度、速度和硬件资源方面各不相同。将 CNN 与传统技术相比，CNN 具有更好的架构和更强的表现力。</p><p>在讨论基于深度学习的物体检测算法之前，重要的是要了解传统技术的工作原理，并知道为什么基于深度学习的方法要优越得多。这将有助于研究人员更好地理解现代物体检测方法。</p><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>传统目标检测方法分为三个阶段。这些阶段及其各自的缺点如下:</p><p><strong>区域选择</strong> – 由于对象具有不同的大小和纵横比，因此它们可能出现在图像的不同区域。因此，在第一阶段，必须确定物体的区域。因此，使用多尺度滑动窗口方法检查整个图像以检测物体。然而，这种方法的计算成本很高，并且还会导致大量非必要的选择。</p><p><strong>特征提取</strong> – 定位对象后，执行特征提取过程以提供可靠的表示。<strong>HOG、Haar-like 、SIFT</strong> 等方法用于提取特征以进行目标识别，以提供有意义的表示。然而，由于对比鲜明的背景、照明环境和透视差异，手动构建一个能够正确识别各种对象的综合特征描述符是极其困难的。</p><p><strong>分类</strong> – 在这个阶段，使用分类器（如 Adaboost ）来识别目标对象，并构建更有条理、更有意义的视觉感知模型。</p><p>从以上几点可以清楚地看出，<strong>在传统方法中，手工制作的特征并不总是足以正确表示对象</strong>。除此之外，用于生成边界框的滑动窗口方法在计算上成本高昂且效率低下。传统的技术包括HOG、SIFT 、Haar、VJ检测器和其他算法，如。在HOG 中，识别一个物体需要很长时间，因为它采用滑动窗口方法来提取特征。SIFT算法速度极慢，计算成本高，也不擅长光照变化。在VJ检测器中，训练持续时间非常长，仅限于二元分类。因此，深度学习技术正在被用于克服传统方法的问题</p><p>深度学习的出现有可能解决传统技术的一些局限性。最近，深度学习方法在自动从数据中学习特征表示方面变得突出。这些方法显著改善了目标检测。基于深度学习的方法有 <strong>Faster RCNN、SSD、YOLO</strong> 等等。</p><p><img data-src="https://i.imgur.com/lJA5oxG.png" alt="image-20231222203515697"></p><p>由于深度 CNN 具有很高的特征表示能力，因此它们被用于对象检测架构。有两种类型的探测器：两级和一级探测器.</p><p>两阶段目标检测框架<strong>将目标定位和目标分类任务分开</strong>。简单来说，<strong>首先在对象所处的地方生成区域建议，然后根据其特定类别对该区域进行分类</strong>。这就是为什么它被称为两阶段的原因。两级目标探测器的主要优点是检测精度高，缺点是检测速度慢。</p><h4 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h4><p>基于区域的卷积神经网络（RCNN）在使用深度学习方法检测目标方面进行了深入研究。其架构如图所示。RCNN的过程在下面分四个阶段进行解释</p><p><img data-src="https://i.imgur.com/tIKAWn2.png" alt="image-20231222204652846"></p><p>第 1 阶段 :使用<strong>选择性搜索方法提取区域建议</strong>。选择性搜索<strong>根据不同的比例、外壳、纹理和颜色模式来识别这些区域</strong>。它<strong>从每张图像中提取大约 2000 个区域</strong></p><p>第 2 阶段 – 由于全连接层需要固定长度的输入向量，因此<strong>所有这些区域建议都重新缩放为相同的图像大小以匹配 CNN 输入大小</strong>。<strong>使用 CNN 提取每个候选区域的特征</strong>。</p><p>第 3 阶段 – 提取特征后，<strong>使用 SVM 分类器检测对象是否存在于每个区域</strong>中。</p><p>第 4 阶段 – 最后，对于图像中的每个已识别对象，使用线性回归模型在其周围生成更紧密的边界框。尽管RCNN在目标检测方面取得了很大的进步，但仍然存在一些局限性，如目标检测速度慢、多阶段流水线训练和选择性搜索方法的僵化。</p><h4 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h4><p>由于 RCNN 为每张图像生成 2000 个区域建议，因此从这些区域提取 CNN 特征是主要障碍。<strong>固定输入大小的约束只是因为全连接层</strong>。因此，<strong>为了克服这一困难，引入了一种称为空间金字塔池化网络层（SPP-Net）的新技术</strong>。<strong>将 SPP 层添加到最终卷积层的顶部，以生成全连接层的固定长度特征</strong>，无论 RoI（感兴趣区域）的大小如何，并且不会重新缩放它，这可能会导致信息丢失(相当于替代RCNN中的warp操作).</p><p><img data-src="https://i.imgur.com/GyXhKPL.png" alt="image-20231222205054546"></p><p>通过使用SPPNet层，RCNN的速度有了很大的提高，而检测质量没有任何损失。这是因为卷积层<strong>只需要在完整的测试图像上运行一次，就可以为随机大小的区域建议创建固定长度的特征</strong>。这里 SPP 层的输出是 256×M-d 向量。256 是卷积滤波器的数量，M 是bin的数量。全连接层接收固定长度的维向量。</p><h4 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h4><p>尽管SPPNet在效率和准确性方面优于RCNN，但它仍然存在一些问题，例如它大致遵循与RCNN相同的过程，包括网络微调、特征提取和边界框回归。</p><p>Girshick， R. 在 RCNN 和 SPPNet 方面表现出进一步的改进，并提出了一种名为 Fast RCNN 的新探测器 。<strong>它允许对检测器进行端到端训练，同时学习 softmax 分类器和特定于类的边界框回归，同时进行多任务损失，而不是像在 RCNN 和 SPPNet 中那样单独训练它们</strong>。</p><p>在 Fast RCNN 中，它不是对每张图像执行 2000 次 CNN，<strong>而是只运行一次并获取所有感兴趣的区域。然后，在最终卷积层和初始全连接层之间添加RoI池化层，从而提取出所有区域建议的固定长度向量特征</strong>。</p><p>1st – Fast RCNN 获取完整的输入图像并将其传递给 CNN 以生成特征图。</p><p>第 2 个 – 感兴趣区域 （RoI） 是使用选择性搜索方法生成的。</p><p>第三 – 在提取的 RoI 上应用 RoI 池化层以生成固定长度的特征向量。它确保所有区域都具有相同的量级。</p><p>第 4 次 – 然后<strong>将提取的特征发送到全连接层，同时使用 softmax 层和线性回归层进行分类和定位</strong>。</p><p>Fast RCNN消耗的计算时间更少，检测精度更高。然而，<strong>它基于传统的区域建议方法，使用选择性搜索方法，使其非常耗时</strong>。</p><h4 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h4><p>尽管Fast RCNN在速度和准确性方面取得了长足的进步，<strong>但它使用选择性搜索方法生成了2000个region proposals，这是一个非常缓慢的过程</strong>。任，S.等人致力于这个问题，并开发了一种新的检测器，名为Faster RCNN，作为第一个端到端深度学习检测器。<strong>它还通过将传统的region proposal算法（如选择性搜索、多尺度组合分组或边缘框）替换为称为区域建议网络（RPN）的CNN</strong>，提高了Fast RCNN的检测速度。</p><p>a） CNN 将图像作为输入，并提供图像的特征图作为输出。</p><p>b） <strong>RPN 应用于生成的特征图，返回对象建议 （RoI） 及其对象性分数</strong>。</p><p>c） 提取 RoI 后，将 RoI 池化层应用于其，以将所有提案置于固定维度。</p><p>d） <strong>将派生的特征向量提供给连续的全连接层中，顶部有一层 softmax 和回归层，用于对对象的边界框进行分类和输出</strong></p><p><img data-src="https://i.imgur.com/D9dXGsH.png" alt="image-20231222205738677" style="zoom:67%;" /></p><p>RPN 的工作 – 区域提案网络是一个完全卷积网络，它连接到骨干网络的最后一个卷积层 。<strong>它接收特征图，并使用这些特征图上的滑动窗口输出多个对象建议</strong>。<strong>在每个窗口上，网络生成 k 个不同大小和纵横比的锚框</strong>（也称为参考框）。</p><p><strong>只有从锚点框获得的特征是特定于类的，而不是锚点的位置</strong>。</p><p>每个对象提案由 4 个坐标和一个分数组成，用于确定对象是否存在。<strong>每个锚点映射到一个低维向量，并传递给两个全连接层，一个是对象类别分类层，另一个是box回归层</strong></p><h4 id="Feature-pyramid-network"><a href="#Feature-pyramid-network" class="headerlink" title="Feature pyramid network"></a>Feature pyramid network</h4><p>Lin， T. Y. et al. 提出了特征金字塔网络 （FPN）</p><p>DCNN 固有的多尺度金字塔层次结构，以低成本构建特征金字塔。它将任何大小的图像作为输入，并在多个级别输出相同大小的特征图。这种方法在许多应用中显示出相当大的增强。</p><p><img data-src="https://i.imgur.com/or3OwoO.png" alt="image-20231222210741517"></p><p>FPN 不是对象检测器。它是一种特征提取器，与对象检测器结合使用。<strong>FPN的架构使用自上而下的通路和横向连接将语义上较强的低分辨率特征与语义较弱的高分辨率特征相结合。FPN使用CNN架构的序列，通过横向连接构建自下而上的路径和自上而下的路径。在自下而上的路径（红色）中，图像作为输入传递给 CNN，它使用池化层将特征图设置为相同的大小。对于FPN的每个阶段（即每个分辨率级别），定义了一个金字塔级别</strong></p><p>在自上而下的路径（以蓝色显示）中，<strong>通过将特征图上采样回与自下而上部分相同的大小来使用更高分辨率的特征。然后使用横向连接，这些特征通过自下而上途径的特征进行增强</strong>。每个横向连接都从自下而上和自上而下路径组合了相同大小的特征图</p><p>FPN 的过程为生成具有大量语义内容的多尺度特征图提供了广泛的解决方案。F<strong>PN 不依赖于 CNN 的架构，可以强制执行到对象检测的不相同阶段</strong>，例如 RPN、Fast RCNN.尽管DCNN具有强大的表征能力，但<strong>有必要通过金字塔表示来解决多尺度挑战</strong></p><h4 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask RCNN"></a>Mask RCNN</h4><p>He， K. et al.设计了一款名为Mask RCNN的目标检测器，这是对Faster RCNN的增强，用于解决进行目标检测和语义分割作业的实例分割问题。这两个任务是自力更生的过程。Mask RCNN 的目标是执行像素级分割。蒙版RCNN检查每个像素并估计它是否是对象的一部分。</p><p>Mask R-CNN 遵循 Faster R-CNN 的架构;两者都使用相同的RPN，<strong>但区别在于掩码RCNN对每个对象提案有三个输出，即.class标签，边界框偏移量和对象检测掩码</strong>。在 Mask RCNN 中，RoIAlign 层用于将提取的特征与对象的输入位置相关联。RoIAlign 层的目的是修复 RoI 池化层中的错位问题。它无需测量 RoI 阈值，而是使用双线性插值来评估每个采样点的实际特征值。Mask RCNN 在实例分割方面实现了最先进的性能</p><p><strong>基于区域提案的框架由各个阶段组成，这些阶段相互连接并分别进行训练。这些是区域建议生成、使用 CNN 提取特征、分类和边界框回归</strong>。尽管这些方法能够实现高精度，但仍存在一些与实时速度相关的问题。这个问题可以通过统一的阶段检测器来克服，<strong>方法是删除区域建议阶段，并在单个CNN中实现特征提取、建议回归和预测</strong></p><p>损失或成本函数，如Hinge损失、L1 和 L2 损失、对数损失 [52]是预期输出和预测输出之间差异的度量。建议读者参考相应的物体检测器论文以获取更多信息</p><h3 id="One-stage-object-detectors"><a href="#One-stage-object-detectors" class="headerlink" title="One-stage object detectors"></a>One-stage object detectors</h3><p>单阶段对象检测框架使用 DCNN <strong>同时进行定位和分类</strong>，而无需将它们划分为两个部分。</p><p>在这种情况下，只需要通过神经网络进行一次传递。它具有前馈神经网络，可<strong>以一次预测所有边界框。它们将图像像素直接映射到边界框坐标和类概率</strong>。</p><h4 id="DetectorNet"><a href="#DetectorNet" class="headerlink" title="DetectorNet"></a>DetectorNet</h4><p>Szegedy， C.等将DetectorNet框架实现为回归问题。</p><p>它能够学习特征进行分类并获取一些几何信息。它<strong>使用 AlexNet 作为骨干网络，并将 softmax 层替换为回归层</strong>。为了预测前景像素，DetectorNet 将输入图像分割成coarse grid。它的训练过程非常缓慢，因为网络要针对每种对象类型和掩码类型进行训练。此外，DetectorNet 无法处理类似类的多个对象。当它与多尺度从粗到细方法结合使用时，基于 DNN 的对象掩码回归会产生出色的结果</p><h4 id="Overfeat"><a href="#Overfeat" class="headerlink" title="Overfeat"></a>Overfeat</h4><p>Sermanet， P.等提出了一种统一的结构，即<strong>使用卷积网络通过多尺度滑动窗口方法进行定位、分类和检测</strong>。它是最强大的目标检测框架之一，应用于 ImageNet 大规模视觉识别挑战赛 2013 （ILSVRC），在检测和定位方面排名第一 。它是<strong>第一个基于全卷积深度网络的单级检测器，它通过全卷积层使用单次前向通道来检测物体</strong>。</p><p>OverFeat 充当后来出现的算法的基础模型，即 YOLO 及其版本、SSD 等。主要区别在于<strong>分类器和回归器的训练是在 OverFeat 中连续完成的</strong></p><h4 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png" alt="img"></p><p>You Only Look Once（YOLO）是由Redmon， J.等人设计的单级目标检测器，<strong>其中目标检测作为回归问题进行。它预测对象的边界框的坐标，并确定它所关联的类别的可能性</strong>。由于仅使用单个网络，因此可以实现端到端优化。它<strong>使用有限的候选区域选择直接预测检测。与基于区域的方法不同，这些方法使用来自特定区域的特征</strong>，而YOLO广泛使用来自整个图像的特征</p><p>在YOLO目标检测中，图像被划分为S×S网格;每个网格由五个元组（x、y、w、h 和置信度分数）组成。单个对象的置信度分数基于概率。这个分数是给每个类的，无论哪个类的概率很高，该类都会优先。</p><p>边界框的参数宽度 （W） 和高度 （H） 是根据对象的大小来预测的。从重叠的边界框中，选择具有最高 IOU 的框，并删除其余框。</p><p>YOLOv2是YOLOv1的增强版本，由Redmon， J.等人]给出。在这个版本中，应用了不同的思想<strong>，如批量归一化、卷积锚框、高分辨率分类器</strong>、<strong>细粒度特征和多尺度训练</strong>来提高 YOLO 的性能。它使用 Darknet-19 作为包含 19 个卷积层和 5 个最大池化层的骨干分类，这些层需要更少的过程来分析图像，同时实现最佳精度</p><p>YOLOv3 是 YOLOv2  的渐进形式，它使用<strong>逻辑回归来估计每个边界框的客观性分数。边界框中包含多个类，为了预测这些类，使用了多标签分类</strong>。它还使用二进制交叉熵损失、数据增强技术和批量归一化。YOLOv3 使用一个名为 Darknet-53 的健壮特征提取器</p><p>YOLOv4 是一种先进的目标检测器，比以前所有版本的YOLO更准确、更快。它包括一种称为“Bag of freebies”的方法，该方法在不影响推理时间的情况下增加了训练时间。该方法利用<strong>数据增强技术、自对抗训练、交叉小批量归一化 （CmBN）、CIoU 损失 、DropBlock 正则化、余弦退火调度器来改进训练。YOLOv4 还包含了那些只影响推理时间的方法，称为“Bag of specials”;它包括 Mish 激活、多输入加权残差连接 （MiWRC）、SPP 模块 [26]、PAN 路径聚合模块 [58]、跨级部分连接 （CSP）和空间注意力模块模块</strong>。YOLOv4 可以在单个 GPU 上训练，并使用遗传算法来选择超参数</p><p>在 YOLOv4 发布后不久，Ultralytics 公司推出了 YOLOv5 存储库，与以前的 YOLO 模型相比，它有相当大的增强,由于 YOLOv5 不是作为同行评议的研究发表的，因此它引起了许多关于其合法性的争论;但它仍然被用于各种应用，并在产生模型可靠性的同时提供有效的结果。它以 140 fps 的推理速度运行。YOLOv5使用PyTorch，这使得模型的部署更快、更容易、更准确[60]。虽然 YOLOv4 和 YOLOv5 框架相似，因此很难比较它们之间的区别，但后来，YOLOv5 在某些情况下获得了比 YOLOv4 更高的性能。YOLOv5 模型有五种类型：nano、small、medium、large、extralarge。根据数据集选择模型类型。此外，YOLOv5 模型的轻量级模型随 6.0 版本发布;推理速度提高至 1666 fps.</p><h4 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png" alt="img"></p><p>SSD是一种用于多个类别的快速单次多box检测器，由Liu， W.等人实现。它构建了一个统一的检测器框架</p><p>该框架与 YOLO 一样快，与 Faster-RCNN 一样准确。SSD的设计结合了YOLO模型的回归思想和Faster R-CNN算法的锚定过程。通过使用 YOLO 的回归，SSD 降低了神经网络的计算复杂性，以确保实时性能。通过锚点程序，SSD能够提取各种大小和纵横比的特征，以确保检测精度。SSD 使用 VGG-16 作为骨干检测器</p><p>SSD的过程基于前馈CNN，该CNN为这些框中是否存在对象类实例生成固定大小和对象性分数的边界框，然后应用NMS（非最大抑制）进行最终检测。它还使用RPN的概念来获得快速的检测速度，同时保持高检测质量。通过一些辅助数据增强和硬负挖掘方法，SSD 在各种基准数据集上实现了最先进的性能</p><h3 id="Backbone-networks"><a href="#Backbone-networks" class="headerlink" title="Backbone networks"></a>Backbone networks</h3><p>DCNN作为目标检测模型的骨干网络。为了改善特征表示行为，网络的结构变得更加复杂，这意味着网络层会变得更深，其参数也会增加。骨干CNN用于提取基于DCNN的目标检测系统的特征。</p><p><strong>骨干网络作为目标检测方法的主要特征提取器，将图像作为输入，并为每个输入图像生成特征图作为输出</strong>。根据精度和效率的需要;可以使用密集连接的骨干网，如ResNet 、ResNext等。当需要高精度和构建精确的应用程序时，需要复杂的主干网。</p><p>AlexNet是一种重要的CNN架构，由5个卷积层和3个全连接层组成。在为图像提供固定大小（224 × 224）的输入后，网络一遍又一遍地卷积并汇集激活，然后将结果传输到完全连接的层。该网络在ImageNet上进行训练，并结合了多种正则化方法，例如数据增强，dropout等。为了加速数据处理，提高收敛速度，首次使用了ReLu激活函数和GPU。</p><h3 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h3><p>在AlexNet取得成功之后，研究人员想知道卷积层可视化背后的机制，以了解CNN如何学习特征以及如何检查每层图像特征图的差异。</p><p>因此，Zeiler， M. D. et al. 设计了一种<strong>使用反卷积层、解池层和ReLU非线性来可视化特征图的方法。与 AlexNet 一样，第一层的滤波器大小为 11×11，步幅为 4，但在 ZFNet 中，它减少到 7×7，步幅设置为 2 而不是 4</strong>。这样做的原因是第一层的滤波器包含频率信息的变化;它可以是高的，也可以是低的，并且具有非常小的中频百分比。该方法的性能优于AlexNet，并证明了网络的深度会影响深度学习模型的性能</p><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><p>VGG 进一步将AlexNet的深度扩大到16-19层，从而细化了网络的特征表示。VGG16 和 VGG19 是两种流行的 VGG 网络架构。在每一层中，它采用大小为 3×3 的内核，步幅为 1。小内核和步幅更有利于提取图像中物体位置的细节。它的好处是通过合并额外的卷积层来扩展网络的深度。最小化参数可以提高网络的特征表示能力</p><h4 id="GoogLeNet-或-inception-v1"><a href="#GoogLeNet-或-inception-v1" class="headerlink" title="GoogLeNet 或 inception v1"></a>GoogLeNet 或 inception v1</h4><p>GoogleNet  的主要目的Inception v1 架构旨在通过降低计算成本来实现高精度。向网络添加 1×1 卷积层，其深度增加。这种滤波器大小首先用于名为Network-in-Network的技术，主要用作降维以消除计算瓶颈并增加网络的宽度和高度。</p><p>GoogleNet 是一个 22 层的深度架构，是 ILSVRC 2014 竞赛的获胜者。基于这一思路，作者开发了一个具有降维功能的初始模块。通过使用 inception 模块，GoogLeNet 参数的数量减少了。Inception 模块由 1x1、3x3 和 5x5 滤波器大小的卷积层和相互平行组装的最大池化层组成。Inception v2 系列是第一个提出批量归一化的网络 ，从而实现快速训练</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>随着网络深度的增加，可能会出现精度在达到饱和点后下降的情况。这被称为退化问题，为了解决这个问题，提出了一个残差学习（ResNet）模块。与早期设计的架构（如AlexNet 和VGGNet）相比，它的计算复杂度更低。通常使用层数为50和101层的ResNet骨干网络。在 ResNet50 中，使用跳过连接来保留更深层的梯度，并且精度有所提高。在 ResNet101 中，该模块的性能与 VGG 网络相同，但参数数量较少，遵循 GoogLeNet 中的全局平均池化和瓶颈</p><h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p>Huang， G. et al. 提出了由密集块组成的 DenseNet 架构，该架构以前馈方式将每一层与其他层连接起来，从而带来特征重用、参数有效性和隐式深度监督等好处。DenseNet 减少了梯度消失的问题</p><h3 id="Problems-of-object-detection-and-its-solutions"><a href="#Problems-of-object-detection-and-its-solutions" class="headerlink" title="Problems of object detection and its solutions"></a>Problems of object detection and its solutions</h3><h4 id="Small-object-detection"><a href="#Small-object-detection" class="headerlink" title="Small object detection"></a>Small object detection</h4><p>检测小尺寸物体是物体检测中最困难的问题之一。Faster RCNN 和 YOLO等目标检测算法在检测小尺寸物体方面不足。在深度卷积神经网络中，由于独立特征层在实际图像中仅占据很小的像素尺寸，因此缺乏足够的知识。由于低分辨率的小尺寸物体携带有限的上下文细节，因此很难检测到它们。为了克服这个问题，可以<strong>通过增强生成更多的数据，或者可以提高模型的输入分辨率</strong>等</p><h4 id="Multi-scale-object-detection"><a href="#Multi-scale-object-detection" class="headerlink" title="Multi-scale object detection"></a>Multi-scale object detection</h4><p>在目标检测领域，多尺度目标检测是一项具有挑战性的任务。<strong>深度CNN的每一层都会生成特征图，而这些特征图生成的信息是相互独立的。多尺度对象的判别细节可以出现在骨干网络的任一层中，而对于小尺度对象，它出现在初始层中，并在后面的层中消散</strong>。在目标检测算法（一级和两级）中，预测是从最顶层进行的，这给检测多尺度对象（通常是小对象）的方式造成了障碍。为了克服这个困难;该文提出<strong>信息融合与DCNNs分层结构相结合的多层检测和特征融合</strong></p><p>代表性方法包括多尺度深度CNN 、深度监督目标检测（DSOD）和SSD。为了提高多尺度目标检测的可靠性，可以合并多层特征融合和多层检测。这包括特征金字塔网络（FPN）、反卷积单次检测器（DSSD）、尺度可转移检测网络（STDN）、与对象先验网络的反向连接（RON）、自上而下的调制（TDM）等几个具有代表性的框架。</p><h4 id="Intraclass-variation"><a href="#Intraclass-variation" class="headerlink" title="Intraclass variation"></a>Intraclass variation</h4><p>类内variation是指<strong>同一类的不同图像之间发生的variation</strong>。<strong>它们的形状、大小、颜色、材料、质地等各不相同</strong>。对象实例看起来很灵活，可以在缩放和旋转方面轻松转换。这些被称为内在因素。外部因素也会产生一些明显的影响。<strong>它包括照明不当、天气条件、照明、低质量相机等。这种差异可能由多种因素引起</strong>，如遮挡、照明、位置、透视等。这个问题可以通过验证训练数据是否具有良好的多样性（包括上述所有因素）来克服</p><h4 id="Class-imbalance"><a href="#Class-imbalance" class="headerlink" title="Class imbalance"></a>Class imbalance</h4><p>类之间的不规则数据分布称为类不平衡。简单来说，<strong>可以说当类包含不成比例数量的实例时，即在一个数据集中比另一个数据集中的标本多</strong>。从对象检测的角度来看，类不平衡可以分为两种类型：前景-背景不平衡和前景-前景不平衡。前者发生在训练过程中，与数据集中的类别数量无关。后者是指在样本数量范围内批次水平的不平衡，涉及正类。<strong>一般来说，一级目标探测器的精度低于两级目标探测器，其背后的原因之一是类别不平衡。为了解决这个问题，可以对类进行上采样和下采样，或者使用合成少数过采样技术（SMOTE）等生成合成数据</strong></p><h4 id="Generalization-issues"><a href="#Generalization-issues" class="headerlink" title="Generalization issues"></a>Generalization issues</h4><p>当模型欠拟合或过拟合时，就会出现目标检测中的泛化问题。欠拟合可以在训练阶段的初始阶段识别出来，这个问题可以通过增加训练周期的数量或模型的复杂性来解决。对于过拟合，我们可以使用重要的方法，例如<strong>增加训练数据、提前停止、正则化方法（L1、L2）或丢弃层</strong></p><h2 id="3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey"><a href="#3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey" class="headerlink" title="3D Object Detection for Autonomous Driving: A Comprehensive Survey"></a>3D Object Detection for Autonomous Driving: A Comprehensive Survey</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>近年来，自动驾驶因其减轻驾驶员负担和提高驾驶安全性的潜力而受到越来越多的关注。在现代自动驾驶管道中，感知系统是不可或缺的组成部分，<strong>旨在准确估计周围环境的状态，并为预测和规划提供可靠的观测结果。3D 物体检测旨在预测自动驾驶汽车附近 3D 物体的位置、大小和类别</strong>，是感知系统的重要组成部分。本文综述了自动驾驶三维目标检测的研究进展。首先，我们<strong>介绍了3D目标检测的背景，并讨论了该任务的挑战。其次，我们从模型和感官输入方面对3D目标检测的进展进行了全面调查，包括基于激光雷达、基于相机和多模态检测方法。我们还对每类方法的潜力和挑战进行了深入分析</strong>。此外，我们还系统地研究了3D目标检测在驾驶系统中的应用。最后，对三维目标检测方法进行了性能分析，进一步总结了多年来的研究趋势，并展望了该领域的未来发展方向。</p><h3 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h3><p>自动驾驶技术已广泛应用于许多场景，包括自动驾驶卡车、机器人出租车、送货机器人等，能够减少人为错误并增强道路安全性。作为自动驾驶系统的核心组成部分，汽车感知帮助自动驾驶汽车通过感官输入了解周围环境。<strong>感知系统通常以多模态数据（摄像头图像、激光雷达扫描仪点云、高清地图等）为输入，预测道路上关键要素的几何和语义信息。高质量的感知结果可作为目标跟踪、轨迹预测和路径规划等后续步骤的可靠观测</strong></p><p><img data-src="https://i.imgur.com/02X39qA.png" alt="image-20231222223259144"></p><p>3D 对象检测旨在根据感官输入预测驾驶场景中 3D 对象的边界框。3D 目标检测的一般公式可以表示为</p><script type="math/tex; mode=display">\begin{equation}\mathcal{B}=f_{det}(\mathcal{I}_{sensor}),\end{equation}</script><p>f~det~ 是 3D 对象检测模型，I~sensor~ 是一个或多个感官输入,B = {B1， · · · ， BN } 是场景中 N 个 3D 对象的集合。</p><p>如何表示 3D 对象 Bi 是此任务中的一个关键问题，因为它决定了应为以下预测和规划步骤提供哪些 3D 信息。在大多数情况下，3D 对象表示为包含此对象的 3D 长方体，</p><script type="math/tex; mode=display">\begin{equation}B=[x_c,y_c,z_c,l,w,h,\theta,class],\end{equation}</script><p>其中 （x~c~， y~c~， z~c~） 是长方体的 3D 中心坐标，l、w、h 分别是长方体的长度、宽度和高度，θ 是长方体在地平面上的航向角，即偏航角，class 表示 3D 对象的类别，例如汽车、卡车、行人、骑自行车的人。此外也有其他模型使用了更多参数的.</p><p><strong>Sensory inputs</strong></p><p>有许多类型的传感器可以为 3D 物体检测提供原始数据。<strong>在传感器中，雷达、摄像头和LiDAR（光探测和测距）传感器是三种最广泛采用的传感类型</strong>。雷达具有较长的探测范围，并且对不同的天气条件具有鲁棒性。由于多普勒效应，雷达可以提供额外的速度测量。摄像头价格便宜且易于获取，对于理解语义（例如交通标志的类型）至关重要。尽管价格便宜，但相机在用于 3D 物体检测方面存在固有的局限性<strong>。相机只能捕获外观信息，无法直接获取场景的 3D 结构信息</strong>。另一方面，<strong>3D物体检测通常需要在3D空间中进行精确定位，而从图像中估计的3D信息（例如深度）通常具有较大的误差</strong>。图像的变形通常<strong>容易受到极端天气和时间条件的影响</strong>。在<strong>夜间或雾天从图像中检测物体比在晴天检测要困难得多</strong>，这导致了实现自动驾驶的足够鲁棒性的挑战。</p><p><img data-src="https://i.imgur.com/lS0F3vq.png" alt="image-20231222225107201"></p><p>作为替代解决方案，<strong>LiDAR 传感器可以通过发射激光束然后测量其反射信息来获得场景的细粒度 3D 结构。</strong>一个激光雷达传感器发射 m 束并在一个扫描周期内进行 n 次测量，可以产生 I~range~ ∈ R^m×n×3^ 的距离图像,其中,范围图像的<strong>每个像素都包含球面坐标系中的距离 r、方位角α和倾角φ以及反射强度</strong>。</p><p><strong>范围(Range)图像是LiDAR传感器获得的原始数据格式，可以通过将球面坐标转换为笛卡尔坐标来进一步转换为点云。</strong></p><p>点云可以表示为 I~point~ ∈ R^N×3^，其中 N 表示场景中的点数，每个点有 3 个 xyz 坐标通道。<strong>距离图像和点云都包含由LiDAR传感器直接获取的精确3D信息</strong>。</p><p>因此，<strong>与相机相比，LiDAR 传感器更适合检测 3D 空间中的物体，并且 LiDAR 传感器也不太容易受到时间和天气变化的影响</strong>。然而，LiDAR 传感器比摄像头贵得多，这可能会限制在驾驶场景中的应用</p><p>2D 目标检测旨在<strong>在图像上生成 2D 边界框</strong>，是计算机视觉中的一个基本问题。</p><p>3D 目标检测方法借鉴了 2D 对应物的许多设计范式：<strong>region proposals生成和细化、锚点、非最大抑制等</strong>。然而，从很多方面来看，3D目标检测并不是2D目标检测方法对3D空间的简单改变。</p><p>（1）3D目标检测方法必须处理异构数据表示<strong>。点云检测需要新型算子和网络来处理不规则的点数据</strong>，而点云和图像的检测需要特殊的融合机制。</p><p>（2） 3D 目标检测方法通常<strong>利用不同的投影视图来生成对象预测</strong>。</p><p>与从透视图检测对象的 2D 对象检测方法相反，<strong>3D 方法必须考虑不同的视图来检测 3D 对象，例如从鸟瞰图、点视图和圆柱视图</strong>。（3）3D物体检测对物体在3D空间中的精确定位有很高的要求。分米级的定位误差可能导致行人和骑自行车的人等小物体的检测失败，而在二维物体检测中，几个像素的定位误差仍可能在预测边界框和地面实况边界框之间保持较高的交并 （IoU）。因此，精确的 3D 几何信息对于从点云或图像进行 3D 物体检测是必不可少的。</p><p>（1） LiDAR 和 RGB-D 传感器的点云分布不同。在室内场景中，点相对均匀地分布在扫描表面上，大多数 3D 对象在其表面上接收到足够数量的点。然而，在驾驶场景中，大多数点都落在 LiDAR 传感器的附近，而那些远离传感器的 3D 物体只会获得几个点。因此，在驾驶场景中，<strong>特别需要处理各种点云密度的三维物体，并准确检测那些远距离和稀疏的物体。</strong></p><p>（2）驾驶场景下的检测对推理时延有特殊要求。<strong>驾驶场景中的感知必须是实时的</strong>，以避免事故发生。因此，这些方法必须具有计算效率，否则它们将无法应用于实际应用。</p><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p><img data-src="https://i.imgur.com/HiTj4TF.png" alt="image-20231222230257242"></p><h3 id="LiDAR-based-3D-Object-Detection"><a href="#LiDAR-based-3D-Object-Detection" class="headerlink" title="LiDAR-based 3D Object Detection"></a>LiDAR-based 3D Object Detection</h3><p>我们将介绍基于LiDAR数据的3D目标检测方法，即点云或距离图像。回顾和分析了基于不同数据表示的基于 LiDAR 的 3D 目标检测模型，包括<strong>基于点</strong>、<strong>基于网格</strong>、<strong>基于点体素</strong>和<strong>基于距离</strong>的方法。(point-based, grid-based, point-voxel based, and range-based)</p><p><img data-src="https://i.imgur.com/7LLjGj3.png" alt="image-20231222230749845"></p><h4 id="Data-representations-for-3D-object-detection"><a href="#Data-representations-for-3D-object-detection" class="headerlink" title="Data representations for 3D object detection"></a>Data representations for 3D object detection</h4><p><strong>与像素有规律地分布在图像平面上的图像相比，点云是一种稀疏且不规则的 3D 表示</strong>，需要专门设计的模型进行特征提取。<strong>范围图像是一种密集而紧凑的表示形式，但范围像素包含 3D 信息而不是 RGB 值</strong>。因此，在范围图像上直接应用传统的卷积网络可能不是最佳解决方案。另一方面，自动驾驶场景中的检测通常具有实时推理的要求。因此，如何开发一个既<strong>能有效处理点云或范围图像数据又能保持高效率的模型</strong>，仍然是研究界面临的一个公开挑战。</p><h4 id="Point-based-3D-object-detection"><a href="#Point-based-3D-object-detection" class="headerlink" title="Point-based 3D object detection"></a>Point-based 3D object detection</h4><p>基于点的3D目标检测方法通常继承了点云深度学习技术的成功，并<strong>提出了直接从原始点检测3D对象的多种架构</strong>。</p><p>点云首先通过基于<strong>点的骨干网络</strong>，在该网络中，点逐渐采样，点云操作学习特征。然后，根据下采样点和特征预测 3D 边界框。</p><p><img data-src="https://i.imgur.com/Vl1RhDd.png" alt="image-20231222233252147"></p><p>基于点的 3D 对象检测器有两个基本组件：<strong>点云采样</strong>和<strong>特征学习</strong>。</p><p><strong>Point Cloud Sampling</strong>:PointNet++中的<strong>最远点采样（FPS）已被广泛用于基于点的检测器中</strong>，其中最远的点是<strong>从原始点集中依次选择的</strong>。PointRCNN 是一项开创性的工作，<strong>它采用 FPS 逐步对输入点云进行下采样，并从下采样点生成 3D 建议</strong>。类似的设计范式也被用于后续的许多工作，并进行了改进，如分割引导滤波、特征空间采样、随机采样、基于体素的采样(voxel-based sampling)和坐标细化(coordinate refinement)。</p><p><strong>Point Cloud Feature Learning</strong>:具体来说，首先通过球查询(ball query)在预定义的半径内收集上下文点。然后，通过多层感知器和maxpooling对上下文点和特征进行聚合，得到新的特征。还有其他使用不同点云算子的工作，包括图算子，注意力算子和Transformer。</p><p>基于点的检测器的表示能力主要受两个因素的限制：<strong>特征学习中采用的上下文点数和上下文半径</strong>。增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。</p><p><strong>增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗</strong>。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。可并行进行随机均匀采样，效率高。然而，考虑到LiDAR扫描中的点不是均匀分布的，<strong>随机均匀采样可能倾向于对那些高点云密度的区域进行过度采样，而对那些稀疏区域进行采样不足，这通常会导致与最远点采样相比性能较差。</strong></p><p><strong>最远点采样及其变体可以通过从现有点集中依次选择最远的点来获得更均匀的采样结果。然而，最远点采样本质上是一种顺序算法，不能变得高度并行</strong>。因此，最远点采样通常很耗时，并且无法进行实时检测。</p><h4 id="Grid-based-3D-object-detection"><a href="#Grid-based-3D-object-detection" class="headerlink" title="Grid-based 3D object detection"></a>Grid-based 3D object detection</h4><p><img data-src="https://i.imgur.com/O8n1SeP.png" alt="image-20231223135445200"></p><p>基于网格的 3D 对象检测器首先<strong>将点云栅格化为离散的网格表示</strong>，即<strong>体素、柱子和鸟瞰图 （BEV） 特征图</strong>。然后，他们<strong>应用传统的 2D 卷积神经网络或 3D 稀疏神经网络</strong>从网格中提取特征。最后，可以从BEV网格单元中检测到3D物体。基于网格的 3D 目标检测图如图  所示。基于网格的检测器有两个基本组件：<strong>基于网格的表示</strong>和<strong>基于网格的神经网络</strong></p><p><strong>Grid-based representations</strong>:</p><p><strong>voxels</strong>。如果<strong>将检测空间栅格化为规则的 3D 格网</strong>，则体素就是格网像元。<strong>如果点云落入此格网像元中，则体素可以为非空。</strong>由于点云分布稀疏，因此 3D 空间中的大多数体素单元格都是空的，不包含任何点。<strong>在实际应用中，只有那些非空体素才会被存储并用于特征提取。</strong>VoxelNet是一项开创性的工作，它利用稀疏的体素网格，提出了一种新的体素特征编码（VFE）层，从体素单元内的点中提取特征。</p><p>随后的一系列工作采用了类似的体素编码策略。此外，还有两类方法试图改进 3D 目标检测的体素表示：（1） <strong>多视图体素</strong>。一些方法从不同的视角提出了动态体素化和融合方案，例如鸟瞰图和透视图、圆柱面和球面视角、距离视角。（2）<strong>多尺度体素</strong>。一些论文生成不同尺度的体素或使用可重构的体素</p><p><strong>Pillars</strong>:柱子可以看作是特殊的体素，<strong>其中体素大小在垂直方向上是无限的</strong>。<strong>支柱特征可以通过PointNet从点聚合，然后散射回来</strong>，构建2D BEV图像进行特征提取。PointPillars 是一部开创性的著作，它引入了Pillar表示</p><p><strong>BEV feature maps</strong>:鸟瞰图特征图是一种密集的 2D 表示，其中<strong>每个像素对应于一个特定区域</strong>，<strong>并对该区域中的点信息进行编码</strong>。BEV特征图可以通过将3D特征投影到鸟瞰图中，从体素和柱子中获取，也可以通过汇总像素区域内的点统计数据，直接从原始点云中获取。</p><p><strong>Grid-based neural networks</strong>:基于网格的网络主要有两种类型：<strong>用于 BEV 特征图</strong>和<strong>Pillar的 2D 卷积神经网络</strong>，以及<strong>用于体素的 3D 稀疏神经网络</strong>。</p><p>2D convolutional neural networks:传统的 2D 卷积神经网络可以应用于 BEV 特征图，以从鸟瞰图检测 3D 对象。在大多数作品中，2D网络架构通常都是从2D目标检测中的成功设计中改编而来的,比如ResNet，区域建议网络（RPN）和特征金字塔网络（FPN）</p><p>3D 稀疏神经网络。3D稀疏卷积神经网络基于两个专门的3D卷积算子：sparse convolutions和submanifold convolutions，<strong>它们只能在那些非空体素上有效地进行3D卷积。与在整个体素空间上执行标准3D卷积相比，稀疏卷积算子效率更高</strong>，可以获得实时推理速度。</p><p>SECOND是一项开创性的工作，它使用基于 GPU 的哈希表实现了这两个稀疏算子，并构建了一个稀疏卷积网络来提取 3D 体素特征。</p><p>这种网络架构已在许多工程中得到应用，并成为基于体素的探测器中使用最广泛的骨干网络。还有一系列工作试图改进稀疏算子，将扩展为两级检测器，并将Transformer架构引入基于体素的检测。</p><p>与 BEV 特征图和Pillar等 2D 表示相比，体素包含更结构化的 3D 信息。此外，<strong>深度体素特征可以通过 3D 稀疏网络学习</strong>。然而，<strong>3D 神经网络会带来额外的时间和内存成本</strong>。<strong>BEV 特征图是最有效的网格表示，可直接将点云投影到 2D 伪图像中，而无需专门的 3D 运算符</strong>，如稀疏卷积或柱状编码。<strong>2D检测技术也可以无缝应用于BEV特征图，无需太多修改。</strong>基于BEV的检测方法通常可以获得高效率和实时推理速度。但是，<strong>简单地汇总像素区域内的点统计数据会丢失太多的 3D 信息</strong>，与基于体素的检测相比，这会导致检测结果的准确性较低。基于Pillar的检测方法利用 PointNet 对Pillar内的 3D 点信息进行编码，然后将特征分散回 2D 伪图像中以实现高效检测，从而平衡了 3D 目标检测的有效性和效率</p><p><strong>challenges of the grid-based detection methods</strong>:所有基于网格的方法都必须面对的一个关键问题是选择适当大小的网格单元。<strong>网格表示本质上是点云的离散格式</strong>，通过将连续的点坐标转换为离散的网格索引。</p><p>量化过程不可避免地会丢失一些 3D 信息<strong>，其有效性很大程度上取决于网格单元的大小：网格尺寸越小，网格就越高分辨率，因此可以保留更细粒度的细节，这对于准确的 3D 对象检测至关重要</strong>,然而，减小网格单元的大小会导致 2D 网格表示（如 BEV 特征图或支柱）的内存消耗呈二次增加。至于像体素这样的 3D 网格表示，问题可能会变得更加严重。因此，<strong>如何平衡较小网格尺寸带来的功效和内存增加影响的效率仍然是所有基于网格的三维目标检测方法面临的一个悬而未决的挑战</strong>。</p><h4 id="Point-voxel-based-3D-object-detection"><a href="#Point-voxel-based-3D-object-detection" class="headerlink" title="Point-voxel based 3D object detection"></a><strong>Point-voxel based 3D object detection</strong></h4><p>基于点体素的方法采用混合架构，<strong>利用点和体素进行 3D 对象检测</strong>。这些方法可以分为两类：单阶段和两阶段检测框架。</p><p><strong>Single-stage point-voxel detection frameworks.</strong></p><p>基于单级点体素的 3D 目标检测器<strong>试图将点和体素的特征与骨干网络中的点到体素和体素到点变换联系起来</strong></p><p><strong>点包含细粒度的几何信息，体素的计算效率很高，在特征提取阶段将它们组合在一起自然会从这两种表示中受益</strong>。在骨干网中利用点-体素特征融合的思想已被许多著作探索，其贡献包括点-体素卷积，辅助点网络和多尺度特征融合.</p><p><strong>Two-stage point-voxel detection frameworks</strong></p><p>基于点体素的两级 3D 对象检测器<strong>针对不同的检测阶段采用不同的数据表示</strong>。</p><p>具体来说，在第一阶段，采用<strong>基于体素的检测框架来生成一组 3D 对象建议</strong>,在第二阶段，<strong>首先从输入点云中对关键点进行采样，然后通过新的点算子从关键点进一步细化3D建议</strong>。</p><p>基于点体素的方法自然可以受益于从点获得的细粒度 3D 形状和结构信息以及体素带来的计算效率。然而，这些方法仍然存在一些挑战。对于混合点-体素骨干网，点-体素特征的融合一般<strong>依赖于体素-点和点-体素变换机制，可以带来不可忽视的时间成本</strong>。对于两阶段点体素检测框架，一个关键的挑战是<strong>如何有效地聚合 3D 提案的点特征，因为现有的模块和运算符通常非常耗时</strong>。综上所述，与纯基于体素的检测方法相比，基于点体素的检测方法可以获得更好的检测精度，但代价是增加了推理时间。</p><h4 id="Range-based-3D-object-detection"><a href="#Range-based-3D-object-detection" class="headerlink" title="Range-based 3D object detection"></a>Range-based 3D object detection</h4><p>范围图像是一种密集而紧凑的 2D 表示，其中<strong>每个像素都包含 3D 距离信息，而不是 RGB 值</strong>。基于距离的方法从两个方面解决了检测问题：<strong>设计适合距离图像的新模型和算子</strong>，以及<strong>选择合适的视图进行检测</strong>。</p><p><img data-src="https://i.imgur.com/r7H1L1T.png" alt="image-20231223162320834" style="zoom:67%;" /></p><p><strong>Range-based detection models</strong></p><p>由于距离图像是与RGB图像一样的2D表示，因此基于范围的3D对象检测器可以自然地借用2D对象检测中的模型来处理范围图像。</p><p>LaserNet 是一项开创性的工作，它利用深层聚合网络 （DLA-Net）从距离图像中获取多尺度特征并检测 3D 对象。一些论文还采用了其他 2D 目标检测架构</p><p><strong>Range-based operators</strong></p><p>距离图像的像素包含 3D 距离信息而不是颜色值，<strong>因此传统 2D 网络架构中的标准卷积算子对于基于范围的检测来说不是最佳选择</strong>，因为滑动窗口中的像素在 3D 空间中可能彼此相距很远。一些工作采用新颖的算子来有效地从范围像素中提取特征，包括范围扩展卷积、图算子和元核卷积</p><p><strong>Views for range-based detection</strong></p><p>范围图像是从范围视图 （RV） 捕获的，理想情况下，范围视图是点云的球面投影。</p><p>然而，从距离视图进行检测时，<strong>不可避免地会受到球面投影带来的遮挡和尺度变化问题的影响</strong>。</p><p>为了规避这些问题，许多方法都在<strong>利用其他视图来预测3D物体，例如中采用的圆柱视图（CYV），在中采用的距离视图、鸟瞰视图（BEV）和/或点视图（PV）的组合</strong></p><p><strong>Analysis: potentials and challenges of the range-based methods</strong></p><p>范围图像是一种密集而紧凑的 2D 表示，因此<strong>传统或专用的 2D 卷积可以无缝地应用于范围图像</strong>，这使得特征提取过程非常高效.然而，<strong>与鸟瞰图检测相比，距离图检测容易受到遮挡和尺度变化的影响</strong>。因此，<strong>从距离视图中提取特征</strong>，<strong>从鸟瞰图进行目标检测</strong>，成为基于距离的3D目标检测最实用的解决方案。</p><h3 id="Learning-objectives-for-3D-object-detection"><a href="#Learning-objectives-for-3D-object-detection" class="headerlink" title="Learning objectives for 3D object detection"></a>Learning objectives for 3D object detection</h3><p>学习目标在对象检测中至关重要。<strong>由于 3D 物体相对于整个检测范围非常小，因此在 3D 检测中非常需要特殊的机制来增强小物体的定位</strong>。另一方面，<strong>考虑到点云稀疏且物体通常具有不完整的形状</strong>，准确估计 3D 物体的中心和大小是一项长期存在的挑战。</p><h4 id="Anchor-based-3D-object-detection"><a href="#Anchor-based-3D-object-detection" class="headerlink" title="Anchor-based 3D object detection"></a>Anchor-based 3D object detection</h4><p>锚点(anchors)是具有固定形状的预定义长方体,可以放置在 3D 空间中。</p><p><img data-src="https://i.imgur.com/jFknBKH.png" alt="image-20231223174714325"></p><p>3D 对象可以基于与真实值具有高交集 （IoU） 的正锚点进行预测。将从锚点配置和损失函数方面介绍基于锚点的三维目标检测方法。</p><p>Anchor configurations:基于锚点的 3D 对象检测方法通常<strong>从鸟瞰图检测 3D 对象</strong>,其中 3D 锚框放置在 BEV 特征图的每个网格单元上。3D 锚点通常对每个类别具有固定大小，因为同一类别的对象具有相似的大小</p><p>Loss functions:基于锚点的方法利用分类损失Lcls来学习正负锚点，利用回归损失Lreg来学习基于正锚点的物体的大小和位置。此外，L~θ~ 用于学习物体的航向角。</p><script type="math/tex; mode=display">\begin{equation}L_{det}=L_{cls}+L_{reg}+L_\theta.\end{equation}</script><p>回归目标可以进一步应用于这些正锚点,以学习 3D 对象的大小和位置.</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\Delta x&=\frac{x^g-x^a}{d^a},\Delta y=\frac{y^g-y^a}{d^a},\Delta z=\frac{z^g-z^a}{h^a},\\\Delta l&=\log(\frac{l^g}{l^a}),\Delta w=\log(\frac{w^g}{w^a}),\Delta h=\log(\frac{h^g}{h^a}),\end{aligned}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}L_{cls}^{bce}=-[q\cdot\log(p)+(1-q)\cdot\log(1-p)]\end{equation}</script><p>p 是每个锚点的预测概率，如果锚点为正，则目标 q 为 1，否则为 0</p><script type="math/tex; mode=display">\begin{equation}d^a=\sqrt{(l^a)^2+(w^a)^2}\end{equation}</script><p>此外还有使用Focal Loss,</p><script type="math/tex; mode=display">\begin{equation}L_{cls}^{focal}=-\alpha(1-p)^\gamma\log(p),\end{equation}</script><p>使用SmoothL1 loss用于回归</p><script type="math/tex; mode=display">\begin{equation}L_{reg}=\sum_{\begin{array}{c}u\in\{x,y,z,l,w,h\},\\v\in\{\Delta x,\Delta y,\Delta z,\Delta l,\Delta w,\Delta h\}\end{array}}\text{SmoothL1}(u-v).\end{equation}</script><p>为了学习航向角 θ，弧度方向偏移量可以直接用 SmoothL1 损失回归</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\Delta\theta&=\theta^g-\theta^a,\\L_\theta&=\text{SmoothL}1(\theta-\Delta\theta).\end{aligned}\end{equation}</script><p>然而，由于回归范围较大,直接回归弧度偏移通常很困难,另外<strong>,基于bin的航向估计</strong>是学习航向角的较好解，其中<strong>首先将角度空间划分为bin,并采用基于bin的分类L~dir~和残差回归</strong>.</p><script type="math/tex; mode=display">\begin{equation}L_\theta=L_{dir}+\text{SmoothL}1(\theta-\Delta\theta^{\prime}),\end{equation}</script><p>正弦函数也可用于对弧度偏移进行编码</p><script type="math/tex; mode=display">\begin{equation}\Delta\theta=\sin(\theta^g-\theta^a),\end{equation}</script><p>除了分别学习<strong>物体大小</strong>、<strong>位置</strong>和<strong>方向</strong>的损失函数外，将所有物体参数视为一个整体的交并（IoU）损失也可以应用于3D物体检测</p><script type="math/tex; mode=display">\begin{equation}L_{IoU}=1-IoU(b^g,b),\end{equation}</script><p>其中 C^G^ ~i~ 和 C~I~ 分别是地面实况和预测长方体的第 i 个角。</p><p>基于锚点的方法可以从同一类别的 3D 对象应该具有相似形状的先验知识中受益，因此它们可以在 <strong>3D 锚点的帮助下生成准确的对象预测</strong>。然而，<strong>由于3D物体相对于检测范围相对较小，因此需要大量的锚点来确保整个检测范围的完全覆盖</strong>，例如，在KITTI 数据集的中使用了大约70k个锚点。<strong>此外，对于那些非常小的物体，如行人和骑自行车的人，应用基于锚点的分配可能非常具有挑战性</strong>。考虑到锚点通常放置在每个网格单元的中心，如果网格单元较大而单元中的对象较小，则该单元的锚点可能与小对象具有较低的 IoU，这可能会阻碍训练过程。</p><h3 id="Anchor-free-3D-object-detection"><a href="#Anchor-free-3D-object-detection" class="headerlink" title="Anchor-free 3D object detection"></a>Anchor-free 3D object detection</h3><p>无anchor box方法消除了复杂的anchor box设计，可以灵活地应用于不同的视图，例如鸟瞰图、点视图和范围视图。</p><p><img data-src="https://i.imgur.com/6IcyJdX.png" alt="image-20231223190845168"></p><p>基于锚点和无锚点方法之间的<strong>主要区别在于正样本和负样本的选择</strong>。</p><p><strong>Grid-based assignment</strong>:与依赖于带有锚点的 IoU 来确定正负样本的基于锚点的方法相比，无锚点方法<strong>利用各种基于grid的分配策略来评估 BEV 网格单元、Pillar和体素</strong></p><p>PIXOR是一项开创性的工作，它<strong>利用地面实况 3D 物体内部的网格单元作为正例，而其他则作为负例。</strong>Pillar-based object detection for autonomous driving.采用了这种内部对象分配策略，并在中通过选择最接近对象中心的网格单元进一步改进。CenterPoint利用每个对象中心的高斯核来分配正标签。</p><p>损失函数上,分类损失基本不变.但回归损失改变如下</p><script type="math/tex; mode=display">\begin{equation}\Delta=[dx,dy,z^g,\log(l^g),\log(w^g),\log(h^g),\sin(\theta^g),\cos(\theta^g)],\end{equation}</script><p>DX 和 DY 是正grid cells和对象中心之间的偏移量。</p><p><strong>Point-based assignment.</strong></p><p>大多数基于点的检测方法<strong>采用无锚点和基于点的分配策略</strong>，其中<strong>首先对点进行分割，然后选择 3D 对象内部或附近的前景点作为正样本</strong>，最后从这些前景点中学习 3D 边界框。<strong>大多数基于点的检测器都采用了这种前景点分割策略，并进行了改进，例如增加了中心度分数</strong></p><p><strong>Range-based assignment</strong></p><p>无锚点分配也可以用于范围图像。<strong>一种常见的解决方案是选择 3D 对象内部的范围像素作为正样本</strong>。与其他回归目标基于全局三维坐标系的方法不同，<strong>基于范围的方法采用以对象为中心的坐标系进行回归</strong>。</p><p><strong>Set-to-set assignment.</strong>DETR是一种颇具影响力的 2D 检测方法，它引入了<strong>一种集到集的分配策略</strong>，通过匈牙利算法自动将预测分配给相应的地面实况</p><script type="math/tex; mode=display">\begin{equation}\mathcal{M}^*=\underset{\mathcal{M}}{\operatorname*{argmin}}\sum_{(i\to j)\in\mathcal{M}}L_{det}(b_i^g,b_j),\end{equation}</script><p>其中 M 是从每个阳性样本到 3D 对象的一对一映射。</p><h3 id="3D-object-detection-with-auxiliary-tasks"><a href="#3D-object-detection-with-auxiliary-tasks" class="headerlink" title="3D object detection with auxiliary tasks"></a>3D object detection with auxiliary tasks</h3><p>许多方法都采用辅助任务来增强空间特征，<strong>并为精确的 3D 目标检测提供隐式指导。常用的辅助任务包括语义分割、交集并集预测、对象形状补全和对象部分估计</strong>。</p><p><img data-src="https://i.imgur.com/TkWPitq.png" alt="image-20231223195729269"></p><p><strong>Semantic segmentation</strong>:语义分割可以从3个方面帮助3D目标检测：（1）前景分割可以提供对象位置的隐性信息。在大多数基于点的 3D 目标检测器 中，逐点前景分割已被广泛采用，用于生成提案。（2）空间特征可以通过分割来增强。在文献[347]中，利用语义上下文编码器来增强具有语义知识的空间特征。（3）语义分割可以作为预处理步骤，过滤掉背景样本，使3D目标检测更加高效。</p><p><strong>IoU prediction</strong>:交集并集 （IoU） 可以作为纠正对象置信度分数的有用监督信号。Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In: AAAI<strong>提出了一个辅助分支来预测每个检测到的 3D 对象的 IoU 分数 S~IoU~</strong>。</p><script type="math/tex; mode=display">\begin{equation}S_{conf}=S_{cls}\cdot(S_{IoU})^\beta,\end{equation}</script><p>在推理过程中,来自传统分类分支的原始置信度分数 S~con~f = S~cls~ 被 IoU 分数 SIoU 进一步校正.其中，超参数β控制抑制低 IoU 预测和增强高 IoU 预测的程度。<strong>通过 IoU 校正,更容易选择高质量的 3D 对象作为最终预测</strong>。</p><p><strong>Object shape completion</strong></p><p>由于LiDAR传感器的性质，<strong>远处物体通常只在其表面上接收到几个点，因此3D物体通常是稀疏和不完整的</strong>。提高检测性能的一种直接方法是从稀疏点云中完成物体形状。完整的形状可以为准确和稳健的检测提供更多有用的信息<strong>。在3D检测中已经提出了许多形状补全技术，包括形状解码器、形状特征和概率占用网格</strong></p><p><strong>Object part estimation.</strong></p><p>识别对象内部的零件信息有助于 3D 对象检测，因为它可以显示对象的更细粒度的 3D 结构信息。</p><p>3D 对象检测与许多其他 3D 感知和生成任务具有内在关联性。与独立训练 3D 目标检测器相比，3D 检测和分割的多任务学习更有利，形状补全也有助于 3D 目标检测。还有其他任务可以帮助提高 3D 对象检测器的性能。例如，场景流估计可以识别静态和移动对象，在点云序列中跟踪相同的 3D 对象可以更准确地估计该对象。因此，将更多的感知任务集成到现有的3D目标检测管道中将是有希望的。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;2023年的目标检测综述&lt;strong&gt;A comprehensive review of object detection with deep learning&lt;/strong&gt;以及&lt;strong&gt;3D Object Detection for Autonomous Driving: A Comprehensive Survey&lt;/strong&gt;,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下.&lt;/p&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>Python的工程化之路</title>
    <link href="https://www.sekyoro.top/2023/12/04/Python%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8B%E8%B7%AF/"/>
    <id>https://www.sekyoro.top/2023/12/04/Python%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8B%E8%B7%AF/</id>
    <published>2023-12-04T07:54:52.000Z</published>
    <updated>2023-12-23T12:54:53.563Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在工程化上,Python相比于Java,C#这类语言还是差了不少,不过整个生态还是不错的.</p><span id="more"></span><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>一般有两种,一种称为flat另一种为src.</p><ol><li><blockquote><p>├── sample<br>│   ├── AUTHORS.rst<br>│   ├── docs<br>|   |   ├── conf.py<br>│   │   └── index.rst<br>│   ├── HISTORY.rst<br>│   ├── LICENSE<br>│   ├── makefile<br>│   ├── MANIFEST.in<br>│   ├── README.rst<br>│   ├── requirements.txt<br>│   ├── sample<br>|   |   ├── app.py<br>│   │   └── helper.py<br>|   ├── setup.cfg<br>|   ├── setup.py<br>│   └── tests<br><img data-src="https://s2.loli.net/2023/12/04/zSZesqwjb72IaUD.png" alt="image-20231204170021714"></p></blockquote></li></ol><ol><li><img data-src="https://s2.loli.net/2023/12/04/B5I7NEVSCRilscz.png" alt="image-20231204171740258"></li></ol><p>主要是使用poetry等工具打包的时候需要注意一下,因为<code>pyproject.toml</code>字段不完全相同.</p><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ol><li>ImportError: attempted relative import with no known parent package</li></ol><p>包中的模块使用相对导入时不能直接运行该模块,一般是其他包或顶级模块进行调用</p><h2 id="工具链"><a href="#工具链" class="headerlink" title="工具链"></a>工具链</h2><h3 id="版本管理工具"><a href="#版本管理工具" class="headerlink" title="版本管理工具"></a>版本管理工具</h3><p>Anaconda可以同时解决Python版本和包管理的问题,但如果只是想开发个包,没有必要使用conda,在linux上可以考虑pyenv+poetry,windows上有对应的pyenv-windows+poetry.</p><p>pyenv允许您轻松地在多个版本的Python之间切换。它简单、不引人注目，并且遵循了UNIX传统的单用途工具，可以很好地完成一件事。</p><p><a href="https://github.com/pyenv/pyenv">pyenv/pyenv: Simple Python version management (github.com)</a></p><h3 id="包管理工具"><a href="#包管理工具" class="headerlink" title="包管理工具"></a>包管理工具</h3><p>目前开发Python包我推荐Poetry或者PDM,如果是搞数据计算直接Anaconda.</p><h4 id="Poetry"><a href="#Poetry" class="headerlink" title="Poetry"></a>Poetry</h4><p><a href="https://python-poetry.org/docs/">Introduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)</a></p><p>某种程度上告别<code>setup.py</code>,除了一般的虚拟环境和包管理之外,打包和发布到PYPI等都支持,也是现在比较火的工具.</p><p>虚拟环境管理类似conda,会在某个目录下放所有的虚拟环境</p><p><img data-src="https://s2.loli.net/2023/12/04/zg1ChV4DXyoQeEt.png" alt="image-20231204163324338"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poetry init</span><br><span class="line">poetry install </span><br><span class="line">poetry shell</span><br></pre></td></tr></table></figure><p>通过初始化一个新的Poetry项目，这将以交互方式生成一个文件pyproject.toml。该文件将具有所有包依赖项。这与requirements.txt文件类似。</p><p>当参数 <code>virtualenvs.create</code> 为 <code>true</code> 时，执行 <code>poetry install</code> 或 <code>poetry add</code> 时会检测当前项目是否有虚拟环境，没有就自动创建，默认为 <code>true</code>。</p><p>当参数 <code>virtualenvs.in-project</code> 为 <code>true</code> 时，虚拟环境的依赖将会放置于项目的文件夹内，而不是 poetry 默认的 <code>&#123;cache-dir&#125;/virtualenvs</code>，默认为 <code>false</code>。</p><p>我的配置如下:</p><p><img data-src="https://s2.loli.net/2023/12/04/Z1qGakLnrBwdejx.png" alt="image-20231204164441421"></p><blockquote><p>当Poetry完成安装后，它会将所有包及其下载的确切版本写入Poetry.lock文件，从而将项目锁定到这些特定版本。该锁定文件也应包含在您的项目repo中，以便在项目中工作的每个人都被锁定到相同版本的依赖项。</p></blockquote><p><img data-src="https://s2.loli.net/2023/12/04/9rkoJysmI8dFt5b.png" alt="image-20231204163627134"></p><h4 id="PDM"><a href="#PDM" class="headerlink" title="PDM"></a>PDM</h4><p><a href="https://github.com/pdm-project/pdm">pdm-project/pdm: A modern Python package and dependency manager supporting the latest PEP standards (github.com)</a></p><p>支持最新PEP标准的现代Python包和依赖项管理器. Poetry的<code>pyproject.toml</code>与PEP标准不完全符合.</p><p>PDM可以管理项目和集中位置的虚拟环境（venv），类似于Pipenv。它从标准化的pyproject.toml文件中读取项目元数据，并支持锁定文件。用户可以通过插件添加额外的功能，这些功能可以通过将其作为分发版上传来共享。与Poetry和Hatch不同，PDM不局限于特定的构建后端；用户可以自由选择他们喜欢的任何构建后端。</p><h3 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h3><p>代码格式化工具,一般用black就够了.</p><h4 id="Black"><a href="#Black" class="headerlink" title="Black"></a>Black</h4><p>Black是不折不扣的Python代码格式化程序。通过使用它，您同意放弃对手工格式化细节的控制。作为回报，Black为您提供了速度、决定论和自由，使您免受pycode风格对格式的唠叨。你会为更重要的事情节省时间和精力。无论您正在阅读的项目是什么，变黑的代码看起来都是一样的。一段时间后，格式将变得透明，您可以转而关注内容。Black通过产生尽可能小的差异来加快代码审查。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install black</span><br><span class="line">black &#123;source_file_or_directory&#125;</span><br></pre></td></tr></table></figure><h4 id="yapf"><a href="#yapf" class="headerlink" title="yapf"></a>yapf</h4><p>YAPF是一个基于clang格式的Python格式化程序（由Daniel Jasper开发）。本质上，该算法采用代码并计算符合配置样式的最佳格式。它省去了维护代码的许多繁琐工作。最终目标是YAPF生成的代码与程序员在遵循样式指南的情况下编写的代码一样好。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install yapf</span><br></pre></td></tr></table></figure><h4 id="autopep8"><a href="#autopep8" class="headerlink" title="autopep8"></a><strong>autopep8</strong></h4><p>autoep8自动格式化Python代码，以符合PEP8样式指南。它使用pycodestyle实用程序来确定需要格式化代码的哪些部分。autoep8能够修复pycodestyle可能报告的大多数格式问题。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade autopep8</span><br><span class="line">autopep8 --in-place --aggressive --aggressive &lt;filename&gt;</span><br></pre></td></tr></table></figure><h3 id="Linter"><a href="#Linter" class="headerlink" title="Linter"></a>Linter</h3><p>一般用pylint足矣,喜欢尝鲜的可以用用Ruff.</p><h4 id="PyLint"><a href="#PyLint" class="headerlink" title="PyLint"></a>PyLint</h4><p>Pylint是Python 2或3的静态代码分析器。最新版本支持Python 3.8.0及以上版本。Pylint在不实际运行代码的情况下分析代码。它检查错误，强制执行编码标准，寻找代码气味，并可以就如何重构代码提出建议。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pylint</span><br></pre></td></tr></table></figure><h4 id="flake8"><a href="#flake8" class="headerlink" title="flake8"></a>flake8</h4><p>Flake8是这些工具的包装：PyFlakespycode样式Ned Batchelder的McCabe脚本Flake8通过启动单个Flake8命令来运行所有工具。它在每个文件的合并输出中显示警告。<a href="https://github.com/PyCQA/flake8">PyCQA/flake8: flake8 is a python tool that glues together pycodestyle, pyflakes, mccabe, and third-party plugins to check the style and quality of some python code. (github.com)</a></p><h4 id="Ruff"><a href="#Ruff" class="headerlink" title="Ruff"></a>Ruff</h4><p>比较新的工具<a href="https://github.com/astral-sh/ruff">astral-sh/ruff: An extremely fast Python linter and code formatter, written in Rust. (github.com)</a>,不只是语法提示器,也可以用于格式化.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install ruff</span><br><span class="line"></span><br><span class="line">ruff path/to/code/to/check.py</span><br><span class="line">ruff path/to/code/</span><br><span class="line">ruff path/to/code/*.py</span><br></pre></td></tr></table></figure><h3 id="类型检查工具"><a href="#类型检查工具" class="headerlink" title="类型检查工具"></a>类型检查工具</h3><p>在Python中使用typing的检查工具,此外与Pydantic<a href="https://docs.pydantic.dev/latest/">Welcome to Pydantic - Pydantic</a>搭配使用效果更佳</p><blockquote><p>Pydantic是Python中使用最广泛的数据验证库。Pydantic快速且可扩展，可以很好地处理您的linters/IDE/brain。</p><p>定义数据应该如何使用纯规范的Python 3.7+；用Pydantic验证它。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Delivery</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    timestamp: datetime</span><br><span class="line">    dimensions: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = Delivery(timestamp=<span class="string">&#x27;2020-01-02T03:04:05Z&#x27;</span>, dimensions=[<span class="string">&#x27;10&#x27;</span>, <span class="string">&#x27;20&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">repr</span>(m.timestamp))</span><br><span class="line"><span class="comment">#&gt; datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))</span></span><br><span class="line"><span class="built_in">print</span>(m.dimensions)</span><br><span class="line"><span class="comment">#&gt; (10, 20)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Mypy"><a href="#Mypy" class="headerlink" title="Mypy"></a>Mypy</h4><p><a href="https://github.com/python/mypy">python/mypy: Optional static typing for Python (github.com)</a></p><p>Mypy是Python的静态类型检查器。类型检查器有助于确保您在代码中正确使用变量和函数。</p><p>使用mypy，将类型提示（PEP484）添加到Python程序中，当您错误地使用这些类型时，mypy会发出警告。Python是一种动态语言，所以通常只有当你试图运行它时，你才会在代码中看到错误。Mypy是一个静态检查器，所以它甚至不用运行就可以发现程序中的错误！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -U mypy</span><br><span class="line">mypy PROGRAM</span><br></pre></td></tr></table></figure><h4 id="Pyright"><a href="#Pyright" class="headerlink" title="Pyright"></a>Pyright</h4><p><a href="https://github.com/microsoft/pyright">microsoft/pyright: Static Type Checker for Python (github.com)</a></p><p>Pyright是一个功能齐全、基于标准的Python静态类型检查器。它是为高性能而设计的，可以与大型Python源代码库一起使用。</p><h3 id="Git-pre-commit-hook"><a href="#Git-pre-commit-hook" class="headerlink" title="Git pre-commit hook"></a>Git pre-commit hook</h3><p>在<code>git commit</code>之前设置hook进行代码检查</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>测试工具</p><h4 id="Pytest"><a href="#Pytest" class="headerlink" title="Pytest"></a>Pytest</h4><p>pytest框架使编写小型可读测试变得容易，并且可以扩展以支持应用程序和库的复杂功能测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U pytest</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/Dontla/article/details/131538693">python项目结构示例（python代码结构、python目录结构）与python部署结构、python部署目录、flask项目结构、flask目录_python项目结构目录结构-CSDN博客</a></li><li><a href="https://blog.csdn.net/captain5339/article/details/128017400">各类Python项目的项目结构及代码组织最佳实践<em>python项目结构__</em>弯弓__的博客-CSDN博客</a></li><li><a href="https://www.cnblogs.com/cuiyubo/p/11756771.html">Python最佳工程实践，建立一个完美的工程项目 - cuiyubo - 博客园 (cnblogs.com)</a></li><li><a href="https://www.hatica.io/blog/pre-commit-git-hooks/">8 Pre-commit Git Hooks You Must Know for Improved Productivity - Hatica</a></li><li><a href="https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html">结构化您的工程 — The Hitchhiker’s Guide to Python (pythonguidecn.readthedocs.io)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;在工程化上,Python相比于Java,C#这类语言还是差了不少,不过整个生态还是不错的.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>设计模式与重构</title>
    <link href="https://www.sekyoro.top/2023/12/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%87%8D%E6%9E%84/"/>
    <id>https://www.sekyoro.top/2023/12/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%87%8D%E6%9E%84/</id>
    <published>2023-12-03T05:08:27.000Z</published>
    <updated>2023-12-23T14:04:25.837Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>软件开发相关知识,主要是一些设计思想.<br><span id="more"></span></p><h2 id="基本设计思想"><a href="#基本设计思想" class="headerlink" title="基本设计思想"></a>基本设计思想</h2><h3 id="开闭原则"><a href="#开闭原则" class="headerlink" title="开闭原则"></a>开闭原则</h3><p>由Bertrand Meyer提出的开闭原则（Open Closed Principle）是指，软件应该对扩展开放，而对修改关闭。这里的意思是在增加新功能的时候，能不改代码就尽量不要改，如果只增加代码就完成了新功能，那是最好的。</p><h3 id="里氏替换原则"><a href="#里氏替换原则" class="headerlink" title="里氏替换原则"></a>里氏替换原则</h3><p>里氏替换原则是Barbara Liskov提出的，这是一种面向对象的设计原则，即如果我们调用一个父类的方法可以成功，那么替换成子类调用也应该完全可以运行。</p><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h3 id="创建型模式"><a href="#创建型模式" class="headerlink" title="创建型模式"></a>创建型模式</h3><p>这类模式提供创建对象的机制， 能够提升已有代码的灵活性和可复用性。</p><h4 id="工厂方法模式"><a href="#工厂方法模式" class="headerlink" title="工厂方法模式"></a>工厂方法模式</h4><blockquote><p>定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。</p></blockquote><p>工厂方法即Factory Method，是一种对象创建型模式。</p><h4 id="抽象工厂"><a href="#抽象工厂" class="headerlink" title="抽象工厂"></a>抽象工厂</h4><blockquote><p>提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。</p></blockquote><p>抽象工厂模式（Abstract Factory）是一个比较复杂的创建型模式</p><p>抽象工厂模式和工厂方法不太一样，它要解决的问题比较复杂，不但工厂是抽象的，产品是抽象的，而且有多个产品需要创建，因此，这个抽象工厂会对应到多个实际工厂，每个实际工厂负责创建多个实际产品</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><blockquote><p>将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。</p></blockquote><p>生成器模式（Builder）是使用多个“小型”工厂来最终创建出一个完整对象。</p><p>当我们使用Builder的时候，一般来说，是因为创建这个对象的步骤比较多，每个步骤都需要一个零部件，最终组合成一个完整的对象。</p><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><blockquote><p>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p></blockquote><p>原型模式，即Prototype，是指创建新对象的时候，根据现有的一个原型来创建。</p><h4 id="单例"><a href="#单例" class="headerlink" title="单例"></a>单例</h4><blockquote><p>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p></blockquote><p>单例模式（Singleton）的目的是为了保证在一个进程中，某个类有且仅有一个实例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 静态字段引用唯一实例:</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton INSTANCE = <span class="keyword">new</span> Singleton();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// private构造方法保证外部无法实例化:</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结构性模式"><a href="#结构性模式" class="headerlink" title="结构性模式"></a>结构性模式</h3><p>这类模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。</p><h4 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h4><blockquote><p>将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p></blockquote><p>适配器模式是Adapter，也称Wrapper</p><h4 id="桥接"><a href="#桥接" class="headerlink" title="桥接"></a>桥接</h4><blockquote><p>将抽象部分与它的实现部分分离，使它们都可以独立地变化。</p></blockquote><h4 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h4><blockquote><p>将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。</p></blockquote><p>组合模式（Composite）经常用于树形结构，为了简化代码，使用Composite可以把一个叶子节点与一个父节点统一起来处理。</p><p>在XML或HTML中，从根节点开始，每个节点都可能包含任意个其他节点，这些层层嵌套的节点就构成了一颗树。</p><h4 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h4><blockquote><p>动态地给一个对象添加一些额外的职责。就增加功能来说，相比生成子类更为灵活。</p></blockquote><p>装饰器（Decorator）模式，是一种在运行期动态给某个对象的实例增加功能的方法</p><h4 id="外观"><a href="#外观" class="headerlink" title="外观"></a>外观</h4><blockquote><p>为子系统中的一组接口提供一个一致的界面。Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。</p></blockquote><p>外观模式，即Facade，是一个比较简单的模式。它的基本思想如下：</p><p>如果客户端要跟许多子系统打交道，那么客户端需要了解各个子系统的接口，比较麻烦。如果有一个统一的“中介”，让客户端只跟中介打交道，中介再去跟各个子系统打交道，对客户端来说就比较简单。所以Facade就相当于搞了一个中介。</p><h4 id="享元"><a href="#享元" class="headerlink" title="享元"></a>享元</h4><blockquote><p>运用共享技术有效地支持大量细粒度的对象。</p></blockquote><p>享元（Flyweight）的核心思想很简单：如果一个对象实例一经创建就不可变，那么反复创建相同的实例就没有必要，直接向调用方返回一个共享的实例就行，这样即节省内存，又可以减少创建对象的过程，提高运行速度。</p><h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><blockquote><p>为其他对象提供一种代理以控制对这个对象的访问。</p></blockquote><p>代理模式，即Proxy，它和Adapter模式很类似</p><h3 id="行为模式"><a href="#行为模式" class="headerlink" title="行为模式"></a>行为模式</h3><p>这类模式负责对象间的高效沟通和职责委派。</p><h4 id="责任链"><a href="#责任链" class="headerlink" title="责任链"></a>责任链</h4><blockquote><p>使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。</p></blockquote><p>责任链模式（Chain of Responsibility）是一种处理请求的模式，它让多个处理器都有机会处理该请求，直到其中某个处理成功为止</p><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><p>命令模式（Command）是指，把请求封装成一个命令，然后执行该命令。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextEditor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> StringBuilder buffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">paste</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String text = getFromClipBoard();</span><br><span class="line">        add(text);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        buffer.append(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            buffer.deleteCharAt(buffer.length() - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> buffer.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CopyCommand</span> <span class="keyword">implements</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 持有执行者对象:</span></span><br><span class="line">    <span class="keyword">private</span> TextEditor receiver;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CopyCommand</span><span class="params">(TextEditor receiver)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.receiver = receiver;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        receiver.copy();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PasteCommand</span> <span class="keyword">implements</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> TextEditor receiver;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PasteCommand</span><span class="params">(TextEditor receiver)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.receiver = receiver;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        receiver.paste();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="解释器"><a href="#解释器" class="headerlink" title="解释器"></a>解释器</h4><p>给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。</p><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><p>提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。迭代器模式（Iterator）实际上在Java的集合类中已经广泛使用了。我们以<code>List</code>为例，要遍历<code>ArrayList</code>，即使我们知道它的内部存储了一个<code>Object[]</code>数组，也不应该直接使用数组索引去遍历，因为这样需要了解集合内部的存储结构</p><h4 id="中介模式"><a href="#中介模式" class="headerlink" title="中介模式"></a>中介模式</h4><blockquote><p>用一个中介对象来封装一系列的对象交互。中介者使各个对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。</p></blockquote><p>中介模式（Mediator）又称调停者模式，它的目的是把多方会谈变成双方会谈，从而实现多方的松耦合。</p><p>Mediator模式经常用在有众多交互组件的UI上。为了简化UI程序，MVC模式以及MVVM模式都可以看作是Mediator模式的扩展。</p><h4 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h4><blockquote><p>在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。</p></blockquote><p>备忘录模式（Memento），主要用于捕获一个对象的内部状态，以便在将来的某个时候恢复此状态。</p><p>其实我们使用的几乎所有软件都用到了备忘录模式。最简单的备忘录模式就是保存到文件，打开文件。对于文本编辑器来说，保存就是把<code>TextEditor</code>类的字符串存储到文件，打开就是恢复<code>TextEditor</code>类的状态。对于图像编辑器来说，原理是一样的，只是保存和恢复的数据格式比较复杂而已。Java的序列化也可以看作是备忘录模式。</p><h4 id="观察者"><a href="#观察者" class="headerlink" title="观察者"></a>观察者</h4><blockquote><p>定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p></blockquote><p>观察者模式（Observer）又称发布-订阅模式（Publish-Subscribe：Pub/Sub）。它是一种通知机制，让发送通知的一方（被观察方）和接收通知的一方（观察者）能彼此分离，互不影响。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Store</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;ProductObserver&gt; observers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, Product&gt; products = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册观察者:</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addObserver</span><span class="params">(ProductObserver observer)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.observers.add(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取消注册:</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeObserver</span><span class="params">(ProductObserver observer)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.observers.remove(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addNewProduct</span><span class="params">(String name, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">        Product p = <span class="keyword">new</span> Product(name, price);</span><br><span class="line">        products.put(p.getName(), p);</span><br><span class="line">        <span class="comment">// 通知观察者:</span></span><br><span class="line">        observers.forEach(o -&gt; o.onPublished(p));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProductPrice</span><span class="params">(String name, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">        Product p = products.get(name);</span><br><span class="line">        p.setPrice(price);</span><br><span class="line">        <span class="comment">// 通知观察者:</span></span><br><span class="line">        observers.forEach(o -&gt; o.onPriceChanged(p));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><blockquote><p>允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。</p></blockquote><p>状态模式（State）经常用在带有状态的对象中。</p><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><blockquote><p>定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。</p></blockquote><p>策略模式：Strategy，是指，定义一组算法，并把其封装到一个对象中。然后在运行时，可以灵活的使用其中的一个算法</p><h4 id="模板方法"><a href="#模板方法" class="headerlink" title="模板方法"></a>模板方法</h4><blockquote><p>定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。</p></blockquote><p>模板方法（Template Method）是一个比较简单的模式。它的主要思想是，定义一个操作的一系列步骤，对于某些暂时确定不下来的步骤，就留给子类去实现好了，这样不同的子类就可以定义出不同的步骤。</p><h4 id="访问者"><a href="#访问者" class="headerlink" title="访问者"></a>访问者</h4><blockquote><p>表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。</p></blockquote><p>访问者模式（Visitor）是一种操作一组对象的操作，它的目的是不改变对象的定义，但允许新增不同的访问者，来定义新的操作。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1264742167474528">设计模式 - 廖雪峰的官方网站 (liaoxuefeng.com)</a></li><li><a href="https://refactoringguru.cn/design-patterns/catalog">设计模式目录：22种设计模式 (refactoringguru.cn)</a></li><li><a href="https://www.runoob.com/design-pattern/design-pattern-intro.html">设计模式简介 | 菜鸟教程 (runoob.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;软件开发相关知识,主要是一些设计思想.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>图像特征协同感知融合算法</title>
    <link href="https://www.sekyoro.top/2023/11/30/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95/"/>
    <id>https://www.sekyoro.top/2023/11/30/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95/</id>
    <published>2023-11-30T02:58:03.000Z</published>
    <updated>2023-12-23T08:55:21.464Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>除了3D<strong>目标检测</strong>算法外,自动驾驶还需要将获取到的图像数据或者处理后的特征进行<strong>通信</strong>和<strong>融合</strong>,这里介绍相关论文.</p><span id="more"></span><h3 id="Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds"><a href="#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds" class="headerlink" title="Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds"></a>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</h3><h4 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h4><p>自动驾驶汽车可能会因为检测和识别不准确而做出错误的决定。因此，<strong>智能车辆可以将自身数据与其他车辆的数据相结合，增强感知能力，从而提高探测精度和驾驶安全性</strong>。然而，<strong>多车协同感知需要整合真实世界的场景，而原始传感器数据交换的流量远远超过了现有车辆网络的带宽。</strong>据我们所知，我们是<strong>第一个对原始数据层面的合作感知进行研究，以提高自动驾驶系统的检测能力</strong>。</p><p>在这项工作中，我们<strong>以激光雷达三维点云为依托，融合从联网车辆的不同位置和角度收集的传感器数据</strong>。我们提出了一种基于点云的三维物体检测方法，可用于多种对齐点。</p><p>然而，到目前为止，人类驾驶的汽车与自动驾驶汽车之间的大多数比较都是不平衡的，包含各种不公平的因素。自动驾驶汽车不会感到疲劳、情绪衰弱，如愤怒或沮丧。但是，它们无法像细心和经验丰富的人类驾驶员那样，在不确定和模糊的情况下做出熟练的反应或预测</p><h3 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h3><p>为了解决这个问题，我们研究了其中一个基础类别，即原始数据的低级融合。<strong>原始传感数据是自动驾驶汽车上所有传感器的组成部分，因此非常适合在不同制造商的不同汽车之间传输。因此，不同数据处理算法的异质性不会影响车辆之间共享数据的准确性</strong>。由于自动驾驶本身就是一项至关重要的任务，与车辆的集成度如此之高，即使是一个小小的检测错误也可能导致灾难性的事故。因此，我们需要自动驾驶汽车尽可能清晰地感知环境。为了实现这一最终目标，它们需要一个强大而可靠的感知系统。</p><p>在此过程中，我们要解决以下两个主要问题：(1) 我们需要在车辆之间共享的数据类型，以及 (2) 需要传输的数据量与接收车辆实际需要的数据量。</p><p>第一个问题是<strong>汽车数据集中的可共享数据</strong>。第二个问题<strong>是每辆车产生的数据量巨大。由于每辆自动驾驶汽车每天都会收集超过 1000GB 的数据</strong>，因此只收集区域数据变得更加困难。同样，重建附近感知系统从不同位置和角度收集的共享数据也是另一大挑战。</p><p>在不同类型的原始数据中，我们建议使用激光雷达（LiDAR）点云作为解决方案，原因如下：</p><ul><li>与二维图像和视频相比，<strong>激光雷达点云具有空间维度的优势</strong>。</li><li>在保留感知对象的准确模型的同时，对实体或私人数据（如人脸和车牌号码）进行本机混淆。</li><li>由于数据是由点而不是像素组成的，因此在图像和视频融合过程中具有多功能性。<strong>对于图像或视频融合来说，要求有一个清晰的重叠区，而点云数据则不需要重叠区，这使得点云数据成为一种更稳健的选择，尤其是在考虑到汽车的不同视角时</strong>。</li></ul><h5 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h5><p>不准确的物体检测和识别是实现强大而有效的感知系统的主要障碍。自动驾驶汽车最终会屈服于这种能力，无法实现预期结果，这对自动驾驶是不安全的。为了解决这些问题，我们提出了一种解决方案，即<strong>自动驾驶车辆将自身的感知数据与其他联网车辆的感知数据相结合，以帮助增强感知能力</strong>。我们还认为，如前所述，<strong>数据冗余是解决这一问题的方法，我们可以通过自动驾驶车辆之间的数据共享和组合来实现</strong>。我们提出的 Cooper 系统可以提高探测性能和驾驶体验，从而提供保护和安全。具体来说，我们的贡献如下:</p><ul><li>我们提出了稀疏点云物体检测（SPOD）方法，用于<strong>检测低密度点云数据中的物体</strong>。虽然 SPOD 是针对低密度点云设计的，但它也适用于高密度激光雷达数据。</li><li>我们展示了所提出的 Cooper 系统如何通过扩大感知区域和提高检测精度来超越单个感知。</li><li>我们证明，可以利用现有的车载网络技术来促进车辆之间兴趣区域 (ROI) 激光雷达数据的传输，从而实现合作感知。</li></ul><p>鉴于当前自动驾驶汽车数据融合领域的前景和工作，我们需要更进一步，定义我们眼中的合作传感。我们认为，自动驾驶汽车的合作传感将带来一系列挑战和益处，这将是发展过程中不可避免的一部分。</p><h3 id="F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds"><a href="#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds" class="headerlink" title="F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds"></a>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</h3><h4 id="Abs-1"><a href="#Abs-1" class="headerlink" title="Abs"></a>Abs</h4><p>自动驾驶汽车在很大程度上依赖于传感器来完善对周围环境的感知，然而，就目前的技术水平而言，汽车所使用的数据仅限于来自自身传感器的数据.车辆和/或边缘服务器之间的数据共享受到可用网络带宽和自动驾驶应用严格的实时性限制。</p><p>为了解决这些问题，我们为联网自动驾驶汽车提出了<strong>基于点云特征的合作感知框架</strong>（F-Cooper），以实现更高的目标检测精度。</p><p>基于特征的数据不仅足以满足训练过程的需要，我们还利用特征的固有小尺寸来实现实时边缘计算，而不会面临网络拥塞的风险。</p><p>我们的实验结果表明，通过融合特征，我们能够获得更好的物体检测结果，20 米内的检测结果提高了约 10%，更远距离的检测结果提高了 30%，同时还能以较低的通信延迟实现更快的边缘计算，在某些特征选择中只需 71 毫秒。</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>互联自动驾驶汽车（CAV）为改善道路安全提供了一个前景广阔的解决方案。这有赖于车辆能够实时感知路况并精确探测物体。</p><p>然而，准确和实时的感知在现场具有挑战性。它需要处理来自各种传感器的大量连续数据流，并有严格的时间要求。此外，车辆的感知精度往往会受到传感器有限视角和范围的影响。</p><h3 id="Proposed-Solution-1"><a href="#Proposed-Solution-1" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h3><p>我们提出的方法可以提高自动驾驶车辆的检测精度，同时不会带来太多的计算开销。一个有益的启示是，现代自动驾驶车辆的物体检测技术，<strong>无论是基于图像的还是基于三维激光雷达数据的，通常都采用卷积神经网络（CNN）来处理原始数据，并利用区域建议网络（RPN）来检测物体</strong>。我们认为，特征图的能力尚未得到充分挖掘，特别是对于自动驾驶车辆上生成的 3D LiDAR 数据，因为特征图仅用于单个车辆的物体检测。</p><p>为此，我们引入了基于特征的协同感知（FCooper）框架，利用特征级融合实现端到端的三维物体检测，从而提高检测精度。我们的 F-Cooper 框架支持两种不同的融合方案：<strong>体素特征融合和空间特征融合</strong>。</p><p>与原始数据级融合解决方案[3]相比，<strong>前者实现了几乎相同的检测精度提升，而后者则提供了动态调整待传输特征图大小的能力</strong>。F-Cooper 的独特之处在于它可以在车载和路边边缘系统上部署和执行。</p><p>除了能够<strong>提高检测精度</strong>外，<strong>特征融合所需的数据大小仅为原始数据的百分之一</strong>。对于一个典型的激光雷达传感器来说，每个激光雷达帧包含约 100,000 个点，约为 4 MB。对于任何现有的无线网络基础设施来说，如此庞大的数据量都将成为沉重的负担。</p><p>要确认特征对融合的有用性，我们必须回答以下三个基本问题。</p><p>1) 特征是否具备融合的必要手段？<br>2) 我们能否通过特征在自动驾驶车辆之间有效地交流数据？<br>3) 如果特征满足前面两个要求，那么我们从自动驾驶车辆中获取特征图的难度有多大？</p><h4 id="Fusion-Characteristics"><a href="#Fusion-Characteristics" class="headerlink" title="Fusion Characteristics"></a>Fusion Characteristics</h4><p>受致力于融合不同层生成的特征图的研究成果（如特征金字塔网络（FPN） 和级联 R-CNN [2]）的启发，我们发现<strong>在不同的特征图中检测物体是可能的。例如，FPN 采用自上而下的金字塔结构特征图进行检测。这些网络非常善于复合特征融合的效率</strong>。</p><p>从这些著作中汲取灵感，我们假设兼容融合的汽车<strong>将使用相同的检测模型</strong>。这一点非常重要，因为我们看到只有最可靠的检测模型才会被用于自动驾驶。有了这个假设，我们现在来看看融合的特点</p><h4 id="Compression-and-Transmission"><a href="#Compression-and-Transmission" class="headerlink" title="Compression and Transmission"></a>Compression and Transmission</h4><p>与原始数据相比，特征地图的另一个优势在于车辆之间的传输过程。原始数据可能有多种不同的格式，但它们都能达到一个目的，那就是保留所捕获数据的原始状态。例如，从驾驶过程中获取的激光雷达数据将存储驾驶过程中沿途的所有点云。不过，这种存储格式<strong>会将不必要的数据与基本数据一起记录下来</strong>；而特征地图则避免了这一问题.</p><p>在 CNN 网络处理原始数据的过程中，所有无关数据都会被网络过滤掉，只留下可能被网络用于物体检测的信息。<strong>这些特征图存储在稀疏矩阵中，只存储被认为有用的数据，任何被过滤掉的数据在矩阵中都存储为 0。</strong></p><p>通过<strong>无损压缩（如 gzip 压缩方法），数据大小的优势会进一步扩大，如文献[14]所示。再加上稀疏矩阵的特性，我们就能将二者结合起来，实现压缩后的特征数据不超过 1 MB</strong>，使特征数据成为部署 On-Edge 融合的最佳选择。</p><h4 id="Generic-and-Inherent-Properties"><a href="#Generic-and-Inherent-Properties" class="headerlink" title="Generic and Inherent Properties"></a>Generic and Inherent Properties</h4><p>所有自动驾驶车辆都必须根据传感器生成的数据做出决策。<strong>原始数据由车辆上的物理传感器生成，然后传送到车载计算设备。在那里，原始数据通过基于 CNN 的深度学习网络进行处理，最终做出驾驶决策。</strong>在此过程中，<strong>我们可以提取提取的特征进行共享。这样，我们就能有效地获得原始数据的特征图，而无需额外的计算时间或车载计算设备的功率</strong>。迄今为止，几乎所有已知的自动驾驶车辆都使用了基于 CNN 的网络，因此特征提取是通用的，在融合之前无需进一步处理。</p><p>得益于自动驾驶车辆处理数据的方式，我们能够直接从原始激光雷达点云数据中提取处理后的特征图进行融合，因为这本身就提供了位置数据。<strong>只要激光雷达传感器已经按照自动驾驶所需的标准进行了校准，那么我们就能获得能够保留所有物体与车辆相对位置的特征地图</strong>。</p><p>为了融合两辆汽车的三维特征，设计了两种融合范式：体素特征融合和空间特征融合。在范式 I 中，首先融合两组体素特征，然后生成空间特征图。</p><h3 id="Voxel-Features-Fusion"><a href="#Voxel-Features-Fusion" class="headerlink" title="Voxel Features Fusion"></a>Voxel Features Fusion</h3><p>与位图中的像素一样，体素代表三维空间中规则立方体上的一个数值。在一个体素内，可能有零个或多个由激光雷达传感器生成的点云。对于至少包含一个点的任何体素，VoxelNet 的 VFE 层可以生成一个体素特征</p><p>假设原始激光雷达检测区域被划分为一个体素网格。</p><p>在这些体素中，我们将获得绝大多数空体素，而剩余的体素则包含关键信息。所有非空的体素都会通过一系列全连接层进行转换，并转化为长度为 128 的固定大小的矢量。固定大小的向量通常被称为特征图。</p><blockquote><p>为了提高内存/计算效率，我们将非空体素的特征保存到哈希表中，并将体素坐标作为哈希键。由于我们的重点主要是自动驾驶，因此我们只将非空体素存储到哈希表中。</p></blockquote><p>在 VFF 中，我们明确地将来自两个输入的所有体素的特征结合起来。具体来说，来自汽车 1 的体素 3 和来自汽车 2 的体素 5 共享相同的校准位置。</p><blockquote><p>虽然两辆车的物理位置不同，但它们共享同一个配准的三维空间，不同的偏移量表示每辆车在所述三维标定空间中的相对物理位置。为此，我们采用了element-wise maxout来融合体素 3 和体素 5。</p></blockquote><p><img data-src="https://s2.loli.net/2023/11/30/pKu3lxqZ489NEk2.png" alt="image-20231129091921213"></p><p>受卷积神经网络的启发，使用 maxout 进行潜在规模选择，提取明显的特征，同时抑制对三维空间检测无益的特征，从而实现更小的数据量。在我们的实验中，我们使用 maxout 来决定在比较车辆间的数据时哪个特征最突出。</p><h3 id="Spatial-Feature-Fusion"><a href="#Spatial-Feature-Fusion" class="headerlink" title="Spatial Feature Fusion"></a>Spatial Feature Fusion</h3><p>VFF 需要考虑两辆车所有体素的特征，这涉及车辆之间的大量数据交换。<strong>为了进一步减少网络流量，同时保持基于特征融合的优势，我们设计了一种空间特征融合（SFF）方案</strong>。与 VFF 相比，SFF 融合的是空间特征图，与体素特征图相比，空间特征图更为稀疏，因此更容易压缩以进行通信。</p><p>与 VFF 不同，我们对每辆车上的体素特征进行预处理，以获得空间特征。接下来，将两个源空间特征融合在一起，并将融合后的空间特征转发给 RPN，以进行区域建议和目标检测。</p><p><img data-src="https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png" alt="image-20231129210539499"></p><p>特征学习网络的输出是一个稀疏张量，其形状为 128 × 10 × 400 × 352。为了整合所有体素特征，我们采用了三个三维卷积层，依次获得语义信息更多的较小特征图，大小为 64 × 2 × 400 × 352。然而，生成的特征无法满足传统区域建议网络的形状要求。为此，必须将输出重新塑造为 128 × 400 × 352 大小的三维特征图，然后才能将其转发给 RPN。</p><p>对于 SFF，我们生成一个更大的检测范围，大小为 W × H，其中 W &gt; W1，H &gt; H1。接下来，对重叠区域进行融合，同时保留非重叠区域的原始特征。假设 GPS 将汽车 1 的实际位置记录为 (x1，y1)，汽车 2 的实际位置记录为 (x2，y2)，如果 x2 + H1, y2 - W1 2 属于 2 号车的特征图，而左上角代表 1 号车的特征图，那么我们就可以得到左上角的位置。那么我们就很容易确定重叠区域。与 VFF 采用 maxout 策略类似，我们在 SFF 中也采用了 maxout 来融合重叠的空间特征。</p><p><img data-src="https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png" alt="image-20231129215351930" style="zoom:50%;" /></p><p>最后，我们采用区域建议网络在融合特征图上提出潜在区域。</p><blockquote><p>SENet 等最新研究表明，不同的通道具有不同的权重。也就是说，特征图中的某些通道对分类/检测的贡献更大，而其他通道则是多余或不需要的。</p></blockquote><p>选择从全部 128 个通道中选择部分通道进行传输。我们假定自动驾驶汽车装配了与实际应用中相同的训练有素的检测模型。</p><h3 id="使用融合特征进行目标检测"><a href="#使用融合特征进行目标检测" class="headerlink" title="使用融合特征进行目标检测"></a>使用融合特征进行目标检测</h3><p>为了检测车辆，我们将合成特征图输入区域建议网络（RPN）进行对象建议。然后应用损失函数进行网络训练。</p><h4 id="区域建议网络"><a href="#区域建议网络" class="headerlink" title="区域建议网络"></a>区域建议网络</h4><p>RPN:区域建议网络。不管是采用体素融合范式还是空间融合范式，当我们得到空间特征图后，都会将其送入区域提议网络（RPN）。通过 RPN 网络后，我们将得到两个损失函数的输出结果：</p><p>(1) 提议感兴趣区域的概率分数 p∈ [0, 1]；</p><p>(2) 提议区域的位置 P = (Px , Pw , Pz , Pl , Pw , Ph, Pθ ) ，其中 Px , Py , Pz 表示提议区域的中心，(Pl , Pw , Ph, Pθ ) 分别表示长度、宽度、高度和旋转角度。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数由两部分组成：分类损失 Lcls 和回归损失 Lreg。</p><p><img data-src="https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70" alt="img"></p><p>ground-truth bounding box,即gt-box表示为G = Gx , Gy, Gz, Gl , Gw , Gh, Gθ 其中，Gx , Gy , Gz 表示方框的中心点，（Gl , Gw , Gh, Gθ ）分别表示长度、宽度、高度和偏航旋转角</p><p>输出的值包括</p><p><img data-src="https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png" alt="image-20231129221138541"></p><p><img data-src="https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png" alt="image-20231129225123172"></p><p>损失可以表示为</p><p><img data-src="https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png" alt="image-20231129225143741"></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p><strong>KITTI</strong></p><p>由于我们的重点是三维物体检测，因此我们使用了 KITTI 数据集提供的三维 Velodyne 点云数据。</p><p>云点数据<strong>每帧提供 100K 个点</strong>，并存储在二进制浮点矩阵中。<strong>数据包括每个点的三维位置和相关的反射率信息</strong>。<strong>但是，由于 KITTI 数据是由单个车辆记录的，我们必须利用同一记录中的不同时间段来模拟由两辆车生成的数据</strong>。因此，KITTI 数据只适用于某些测试场景。</p><p>为了解决这个问题，我们在两辆名为汤姆和杰瑞（T&amp;J）的车辆上安装了必要的传感器，如激光雷达（Velodyne VLP-16）、摄像头（Allied Vision Mako G-319C）、雷达（Delphi ESR 2.5）、IMU&amp;GPS（Xsens MTi-G-710 套件）和边缘计算设备（Nvidia Drive PX2），以便在我们学校的校园内收集所需的数据。我们的车辆配有 16 波束 Velodyn 激光雷达传感器，以二进制原始以太网数据包的形式存储数据。由于我们的车辆可以相互独立移动，因此我们能够用两辆车在真实环境中测试各种场景。</p><h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>在停车场环境中，我们将距离车辆 20 米以内的物体视为高优先级物体，20 米以外的物体视为低优先级物体。</p><p>由于我们的激光雷达传感器只有 16 个光束，因此与更高端的激光雷达传感器相比，得到的点云数据相对稀疏。为了减轻稀疏数据带来的负面影响，我们将探测范围限制在沿 <strong>X、Y 和 Z 轴[0,70.4]X[-40,40] X [-3,1]</strong> 。我们不使用超出探测范围的数据。除了车辆的检测范围外，我们还<strong>将体素大小设置为 vD = 0.4 米、vH = 0.2 米、vW = 0.2 米，因此 D1 = 10、H1 = 400、W1 = 352</strong>。在我们的实验中，F-Cooper 框架在配备 GeForce GTX 1080 Ti GPU 的计算机上运行。</p><p>为了评估 F-Cooper，我们在实验中收集并测试了 200 多组数据。根据处理激光雷达数据的方法，我们将测试分为四类，方法（1）到（3）均来自相同的检测模型：（1）作为基线的非融合，（2）带有 VFF 的 F-Cooper，（3）带有 SFF 的 F-Cooper，以及（4）原始点云融合方法 - Cooper。特征融合在上述四种情况中随机进行，重点放在繁忙的校园停车场，因为由于遮挡物较多，这是最困难的情况。</p><p>后面结合OpenCood这个项目代码学习学习 <a href="https://github.com/DerrickXuNu/OpenCOOD">ICRA 2022] An opensource framework for cooperative detection. Official implementation for OPV2V. (github.com)</a></p><h2 id="VoxelNet实现以及融合"><a href="#VoxelNet实现以及融合" class="headerlink" title="VoxelNet实现以及融合"></a>VoxelNet实现以及融合</h2><p>TODO:</p><p>协同感知</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;除了3D&lt;strong&gt;目标检测&lt;/strong&gt;算法外,自动驾驶还需要将获取到的图像数据或者处理后的特征进行&lt;strong&gt;通信&lt;/strong&gt;和&lt;strong&gt;融合&lt;/strong&gt;,这里介绍相关论文.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>type hints in Python</title>
    <link href="https://www.sekyoro.top/2023/11/24/tip-types-in-python/"/>
    <id>https://www.sekyoro.top/2023/11/24/tip-types-in-python/</id>
    <published>2023-11-24T13:38:14.000Z</published>
    <updated>2023-11-29T11:47:54.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Python中的类型系统,使用type hints使得整个开发过程更加顺畅.类似typescript的目的.<br><span id="more"></span></p><h2 id="Type-Theory"><a href="#Type-Theory" class="headerlink" title="Type Theory"></a>Type Theory</h2><p>值得一提的是python目前还在蒸蒸日上,所以一些东西后面可能会有些改变,不过答题的东西是不变的,可以使用mypy<a href="https://github.com/python/mypy">python/mypy: Optional static typing for Python (github.com)</a>(或者pyright<a href="https://github.com/microsoft/pyright">microsoft/pyright: Static Type Checker for Python (github.com)</a>)进行检查,可以使用<a href="https://docs.pydantic.dev/latest/">Welcome to Pydantic - Pydantic</a>作为数据验证,大多数IDE本身也对这个默认支持.</p><p><a href="https://www.python.org/dev/peps/pep-0483/">PEP 483</a> 是这一切的起点.</p><h3 id="Subtypes"><a href="#Subtypes" class="headerlink" title="Subtypes"></a>Subtypes</h3><p>一个重要的概念是subtypes(亚型)。</p><p>形式上，如果以下两个条件成立，我们说T型是U的subtypes：</p><ul><li>来自T的每个值也在U类型的值集合中。</li><li>来自U型的每个函数也在T型函数的集合中。</li></ul><p>这两个条件保证了即使类型T与U不同，类型T的变量也可以总是假装为U。</p><blockquote><p>举个具体的例子，考虑T=bool和U=int。bool类型只取两个值。通常这些名称表示为True和False，但这些名称分别只是整数值1和0的别名：</p></blockquote><h3 id="Covariant-Contravariant-and-Invariant"><a href="#Covariant-Contravariant-and-Invariant" class="headerlink" title="Covariant, Contravariant, and Invariant"></a>Covariant, Contravariant, and Invariant</h3><p>在复合类型中使用子类型时会发生什么？例如，Tuple[bool]是Tuple[int]的一个子类型吗？答案取决于复合类型，以及该类型是协变(Covariant)的、反变(Contravariant)的还是不变(Invariant)的。</p><ul><li>元组是协变(Covariant)的。这意味着它保留了其项类型的类型层次结构：Tuple[bool]是Tuple[int]的子类型，因为bool是int的子类型。</li><li>列表是不变(Invariant)的。不变类型不能保证子类型。虽然List[bool]的所有值都是List[int]的值，但您可以将int附加到List[int]，而不是List[bool。换句话说，子类型的第二个条件不成立，并且List[bool]不是List[int]的子类型。</li><li>Callable在其参数中是反变(Contravariant)的。这意味着它颠倒了类型层次结构。若Callable[[T]，…]作为一个函数，它唯一的参数是T类型。Callable的一个例子[[int]，…]是double()函数。反变意味着，如果期望一个在布尔上操作的函数，那么一个在int上操作的功能是可以接受的。</li></ul><h3 id="内置类型"><a href="#内置类型" class="headerlink" title="内置类型"></a>内置类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">x: <span class="built_in">float</span> = <span class="number">1.0</span></span><br><span class="line">x: <span class="built_in">bool</span> = <span class="literal">True</span></span><br><span class="line">x: <span class="built_in">str</span> = <span class="string">&quot;test&quot;</span></span><br><span class="line">x: <span class="built_in">bytes</span> = <span class="string">b&quot;test&quot;</span></span><br></pre></td></tr></table></figure><p>在3.8及之前,使用<code>from typing import List,Dict,Set,Tuple</code>  来使用collections,之后可以直接使用list,dict这种.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">list</span>[<span class="built_in">int</span>] = []</span><br><span class="line">x: <span class="built_in">tuple</span>[<span class="built_in">int</span>,...] = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">x: <span class="built_in">set</span>[<span class="built_in">int</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>&#125;</span><br><span class="line">x: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="built_in">float</span>] = &#123;<span class="string">&quot;field&quot;</span>: <span class="number">2.0</span>, <span class="string">&quot;field2&quot;</span>: <span class="string">&quot;a&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>在3.10+,可以直接使用<code>|</code>代替Union</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">list</span>[<span class="built_in">int</span>|<span class="built_in">str</span>] = [<span class="number">1</span>, <span class="number">2</span>, <span class="string">&quot;a&quot;</span>]</span><br><span class="line">x: <span class="type">Optional</span>[<span class="built_in">str</span>]</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="type">Callable</span>[[<span class="built_in">int</span>], <span class="built_in">str</span>] = stringify</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; Iterator[<span class="built_in">int</span>]:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span>(<span class="params">address: <span class="type">Union</span>[<span class="built_in">str</span>,<span class="built_in">list</span>[<span class="built_in">str</span>],<span class="literal">None</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    ...</span><br><span class="line"><span class="comment"># This says each positional arg and each keyword arg is a &quot;str&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, *args: <span class="built_in">str</span>, **kwargs: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    reveal_type(args)  <span class="comment"># Revealed type is &quot;tuple[str, ...]&quot;</span></span><br><span class="line">    reveal_type(kwargs)  <span class="comment"># Revealed type is &quot;dict[str, str]&quot;</span></span><br><span class="line">    request = make_request(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> self.do_api_query(request)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quux</span>(<span class="params">x: <span class="built_in">int</span>,/, y: <span class="built_in">str</span>, z: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">quux(<span class="number">1</span>, <span class="string">&#x27;2&#x27;</span>, z=<span class="number">3.0</span>)</span><br></pre></td></tr></table></figure><blockquote><p>如果你想要函数的调用者在某个参数位置只能使用位置参数而不能使用关键字参数传参，那么你只需要在所需位置后面放置一个/。</p><p>如果你希望强迫调用者使用某些参数，且必须以关键字参数的形式传参，那么你只需要在所需位置的前一个位置放置一个*。</p></blockquote><h3 id="类"><a href="#类" class="headerlink" title="类"></a>类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> ClassVar</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BankAccount</span>:</span></span><br><span class="line">    account_name: <span class="built_in">str</span></span><br><span class="line">    balance: <span class="built_in">float</span></span><br><span class="line">    </span><br><span class="line">    count: ClassVar</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, account_name: <span class="built_in">str</span>, initial_balance: <span class="built_in">float</span> = <span class="number">0.0</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.account_name = account_name</span><br><span class="line">        self.balance = initial_balance</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deposit</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.balance += amount</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">withdraw</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.balance -= amount</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuditedBankAccount</span>(<span class="params">BankAccount</span>):</span></span><br><span class="line">    audit_log: <span class="built_in">list</span>[<span class="built_in">str</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, account_name: <span class="built_in">str</span>, initial_balance: <span class="built_in">float</span> = <span class="number">0.0</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(account_name, initial_balance)</span><br><span class="line">        self.audit_log = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deposit</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.audit_log.append(<span class="string">f&quot;Deposited <span class="subst">&#123;amount&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">withdraw</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.audit_log.append(<span class="string">f&quot;Withdrew <span class="subst">&#123;amount&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can use the ClassVar annotation to declare a class variable</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span>:</span></span><br><span class="line">    seats: ClassVar[<span class="built_in">int</span>] = <span class="number">4</span></span><br><span class="line">    passengers: ClassVar[<span class="built_in">list</span>[<span class="built_in">str</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Setting&quot;</span>, key, <span class="string">&quot;to&quot;</span>, value)</span><br><span class="line">        self.__dict__[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self, key</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Getting&quot;</span>, key)</span><br><span class="line">        <span class="keyword">return</span> self.__dict__[key]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">A</span>):</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line">    weight: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name: <span class="built_in">str</span>, age: <span class="built_in">int</span>, weight: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = Person(<span class="string">&quot;John&quot;</span>, <span class="number">30</span>, <span class="number">80.0</span>)</span><br><span class="line"><span class="built_in">print</span>(p.name)</span><br></pre></td></tr></table></figure><h3 id="Forward-references"><a href="#Forward-references" class="headerlink" title="Forward references"></a>Forward references</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># You may want to reference a class before it is defined.</span></span><br><span class="line"><span class="comment"># This is known as a &quot;forward reference&quot;.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: A</span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># This will fail at runtime with &#x27;A&#x27; is not defined</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># However, if you add the following special import:</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"><span class="comment"># It will work at runtime and type checking will succeed as long as there</span></span><br><span class="line"><span class="comment"># is a class of that name later on in the file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: A</span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># Ok</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another option is to just put the type in quotes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: <span class="string">&#x27;A&#x27;</span></span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># Also ok</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="comment"># This can also come up if you need to reference a class in a type</span></span><br><span class="line">    <span class="comment"># annotation inside the definition of that class</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span>(<span class="params">cls</span>) -&gt; A:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="Decorators"><a href="#Decorators" class="headerlink" title="Decorators"></a>Decorators</h3><p>decorator通常是将一个函数作为参数并返回另一个函数的函数。</p><p>用类型来描述这种行为可能有点棘手；我们将展示如何使用TypeVar和一种称为参数规范的特殊类型变量来实现这一点。</p><p>假设我们有装饰器，尚未进行类型注释，它保留了原始函数的签名，只打印装饰函数的名称：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwds</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>给这个装饰器类型注释</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Callable</span>, cast, <span class="type">Any</span></span><br><span class="line">F = TypeVar(<span class="string">&quot;F&quot;</span>, bound=<span class="type">Callable</span>[..., <span class="type">Any</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: F</span>) -&gt; F:</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: <span class="type">Any</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> cast(F, wrapper)</span><br></pre></td></tr></table></figure><p>这仍然存在一些不足。首先，我们需要使用不安全的cast()来说服mypy wrapper()与func具有相同的签名。其次，wrapper()函数没有经过严格的类型检查，尽管wrapper函数通常足够小，所以这不是什么大问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, TypeVar</span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> ParamSpec</span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[P, T]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwds: P.kwargs</span>) -&gt; T:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>可以使用参数规范（ParamSpec）来获得更好的类型注释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Callable</span>, <span class="type">Any</span>,ParamSpec</span><br><span class="line">P = ParamSpec(<span class="string">&quot;P&quot;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P,T]</span>) -&gt; <span class="type">Callable</span>[P,T]:</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwargs: P.kwargs</span>) -&gt; <span class="type">Any</span>:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>参数规范还允许描述更改输入函数签名的装饰器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, TypeVar</span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> Concatenate, ParamSpec</span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># We reuse &#x27;P&#x27; in the return type, but replace &#x27;T&#x27; with &#x27;str&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringify</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[P, <span class="built_in">str</span>]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwds: P.kwargs</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(func(*args, **kwds))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta"> @stringify</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">add_forty_two</span>(<span class="params">value: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">     <span class="keyword">return</span> value + <span class="number">42</span></span><br><span class="line"></span><br><span class="line"> a = add_forty_two(<span class="number">3</span>)</span><br><span class="line"> reveal_type(a)      <span class="comment"># Revealed type is &quot;builtins.str&quot;</span></span><br><span class="line"> add_forty_two(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># error: Argument 1 to &quot;add_forty_two&quot; has incompatible type &quot;str&quot;; expected &quot;int&quot;</span></span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[Concatenate[<span class="built_in">str</span>, P], T]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">msg: <span class="built_in">str</span>, /, *args: P.args, **kwds: P.kwargs</span>) -&gt; T:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func, <span class="string">&quot;with&quot;</span>, msg)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@printing_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_forty_two</span>(<span class="params">value: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value + <span class="number">42</span></span><br><span class="line"></span><br><span class="line">a = add_forty_two(<span class="string">&#x27;three&#x27;</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Callable</span>, TypeVar</span><br><span class="line"></span><br><span class="line">F = TypeVar(<span class="string">&#x27;F&#x27;</span>, bound=<span class="type">Callable</span>[..., <span class="type">Any</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bare_decorator</span>(<span class="params">func: F</span>) -&gt; F:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator_args</span>(<span class="params">url: <span class="built_in">str</span></span>) -&gt; <span class="type">Callable</span>[[F], F]:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h3 id="Generics"><a href="#Generics" class="headerlink" title="Generics"></a>Generics</h3><p>内置集合类是泛型类。泛型类型有一个或多个类型参数，这些参数可以是任意类型。例如，dict[int，str]具有类型参数int和str，list[int]具有类型形参int。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Generic</span></span><br><span class="line"></span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>(<span class="params"><span class="type">Generic</span>[T]</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="comment"># Create an empty list with items of type T</span></span><br><span class="line">        self.items: <span class="built_in">list</span>[T] = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span>(<span class="params">self, item: T</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.items.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span>(<span class="params">self</span>) -&gt; T:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.items</span><br></pre></td></tr></table></figure><blockquote><p>类ClassName（Protocol[T]）被允许作为类ClassName的简写class ClassName(Protocol, Generic[T])</p></blockquote><h3 id="TypedDict"><a href="#TypedDict" class="headerlink" title="TypedDict"></a>TypedDict</h3><p>Python程序经常使用带有字符串键的字典来表示对象。TypedDict允许您为表示具有固定架构的对象的字典提供精确的类型，例如｛’id’：1，’items’：〔’x’〕｝。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line">Movie = TypedDict(<span class="string">&#x27;Movie&#x27;</span>, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="built_in">str</span>, <span class="string">&#x27;year&#x27;</span>: <span class="built_in">int</span>&#125;)</span><br><span class="line"></span><br><span class="line">movie: Movie = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Blade Runner&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="number">1982</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Movie</span>(<span class="params">TypedDict</span>):</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    year: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookBasedMovie</span>(<span class="params">Movie</span>):</span></span><br><span class="line">    based_on: <span class="built_in">str</span></span><br></pre></td></tr></table></figure><h3 id="Literal"><a href="#Literal" class="headerlink" title="Literal"></a>Literal</h3><p>Literal类型可以指示表达式等于某个特定的primitive 值。</p><p>例如，如果我们用Literal[“foo”]类型注释一个变量，mypy将理解该变量不仅是str类型的，而且具体地等于字符串“foo”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, <span class="type">Literal</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expects_literal</span>(<span class="params">x: <span class="type">Literal</span>[<span class="number">19</span>]</span>) -&gt; <span class="literal">None</span>:</span> <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">reveal_type(<span class="number">19</span>)</span><br><span class="line">expects_literal(<span class="number">19</span>)</span><br></pre></td></tr></table></figure><h3 id="更多类型"><a href="#更多类型" class="headerlink" title="更多类型"></a>更多类型</h3><ul><li>NoReturn可以告诉mypy函数永远不会正常返回。</li><li>NewType允许您定义类型的变体，该变体被mypy视为单独的类型，但在运行时与原始类型相同。例如，您可以将UserId作为int的一个变体，它在运行时只是一个int。</li><li>@overload允许您定义一个可以接受多个不同签名的函数。如果您需要对难以正常表达的参数和返回类型之间的关系进行编码，这将非常有用。</li><li>Async 类型允许您使用异步和等待来键入检查程序。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> NoReturn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span>() -&gt; NoReturn:</span></span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">&#x27;no way&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> NewType</span><br><span class="line"></span><br><span class="line">UserId = NewType(<span class="string">&#x27;UserId&#x27;</span>, <span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_by_id</span>(<span class="params">user_id: UserId</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">UserId(<span class="string">&#x27;user&#x27;</span>)          <span class="comment"># Fails type check</span></span><br><span class="line"></span><br><span class="line">name_by_id(<span class="number">42</span>)          <span class="comment"># Fails type check</span></span><br><span class="line">name_by_id(UserId(<span class="number">42</span>))  <span class="comment"># OK</span></span><br><span class="line"></span><br><span class="line">num: <span class="built_in">int</span> = UserId(<span class="number">5</span>) + <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://realpython.com/python-type-checking/#type-theory">Python Type Checking (Guide) – Real Python</a></li><li><a href="https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html">Type hints cheat sheet - mypy 1.7.1 documentation</a></li><li><a href="https://python-type-challenges.zeabur.app/">https://python-type-challenges.zeabur.app/</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Python中的类型系统,使用type hints使得整个开发过程更加顺畅.类似typescript的目的.&lt;br&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.sekyoro.top/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>图像读取与显示的问题</title>
    <link href="https://www.sekyoro.top/2023/11/16/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://www.sekyoro.top/2023/11/16/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2023-11-16T11:01:33.000Z</published>
    <updated>2023-11-16T14:26:23.920Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近在用opencv和matplotlib展示图片,但是遇到了一些问题,这里展开说说<br><span id="more"></span></p><p>首先需要明确的是,opencv和matplotlib读取图片都是通道在最后,而前者默认可见光图像是BGR,后者是RGB.此外还有PIL以及imageio等读取图像的工具,这里不一一赘述.</p><h2 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h2><p>对于opencv,使用<code>cv2.imshow</code>,<code>cv2.imread</code>以及<code>cv2.imwrite</code>来读写以及显示.</p><h3 id="imshow"><a href="#imshow" class="headerlink" title="imshow"></a>imshow</h3><blockquote><p>显示图像的缩放取决于图像深度：<br>对 8 位无符号图像，按原样显示；<br>对 16 位无符号或 32 位整数图像，将像素值范围 [0,255*255] 映射到 [0,255] 显示；<br>对 32 位浮点图像，将像素值范围 [0,1] 映射到 [0,255] 显示；</p></blockquote><p>当cv2.imshow()处理图像深度为CV_8U（默认范围为[0,255]）时，按原数据显示；</p><p>当处理图像深度为CV_16U（默认范围为[0,65535]）时，除以256,映射到[0,255]；</p><p>当图像深度为CV_32F和CV_64F时（默认范围为[0,1]），乘以255映射到[0,255],超过255直接饱和；</p><p><strong>当输入负数时，当作0来处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = -<span class="number">1</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = -<span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>由于numpy默认类型float64,浮点数会乘以255,所以只有最上面有一条白线.负值直接黑色</p><p><img data-src="https://i.imgur.com/80ELhGP.png" alt="image-20231116201141606" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 新建numpy数组，注意np.zero()创建的数据类型为float64</span></span><br><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231116201615437.png" alt="image-20231116201615437" style="zoom:50%;" /></p><p>而如果是大于1的浮点数,也是直接饱和.</p><p>如果是uint8,如果超出255,则会被numpy截取,也就是取模</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>),dtype=np.uint8)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">20</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">30</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/tL7jHo0.png" alt="image-20231116201823217" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>),dtype=np.uint8)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] =  <span class="number">512</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>打印img[250:270, 150:350]的值发现是0</p><p><img data-src="https://i.imgur.com/B39iVFz.png" alt="image-20231116202123379" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] =  <span class="number">512</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/QT0dMuS.png" alt="image-20231116202222468" style="zoom:50%;" /></p><p>所以这涉及两个问题,一个是本身numpy的截取另一个是opencv的截取机制.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>), dtype=np.uint16)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">2</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span>*<span class="number">255</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span>*<span class="number">100</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imwrite(<span class="string">&quot;test.png&quot;</span>, img)</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/92aVQEA.png" alt="image-20231116202916344" style="zoom:50%;" /></p><p>如果是16位无符号整数,会除以255.</p><p>最后注意,如果是int32可能会报错</p><h3 id="imwrite"><a href="#imwrite" class="headerlink" title="imwrite"></a>imwrite</h3><p>机制与imshow类似,不过会根据保存文件的后缀进行编码参数.</p><p>cv2.imwrite() 能保存 BGR 3通道图像，或 8 位单通道图像、或 PNG/JPEG/TIFF 16位无符号单通道图像</p><p><strong>注意</strong>:如果保存float32的图像值超过了1,此时会与imshow机制不同,表现为值被归到0-255</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones([<span class="number">255</span>,<span class="number">255</span>,<span class="number">1</span>],dtype=np.float32)</span><br><span class="line">a[<span class="number">0</span>:<span class="number">255</span>,<span class="number">0</span>:<span class="number">255</span>] = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">cv2.imshow(<span class="string">&quot;img&quot;</span>,a)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.imwrite(<span class="string">&quot;test.png&quot;</span>,a)</span><br></pre></td></tr></table></figure><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231116222050856.png" alt="image-20231116222050856"></p><p><img data-src="https://i.imgur.com/kwtCm9Z.png" alt="image-20231116222107874"></p><p>上面有两张图,分别是imwrite的图片与imshow的图片,由于是浮点数,imshow展示时乘了255导致饱和白色.所以会说imwrite对浮点数不友好,不符合imshow的道理,</p><h3 id="imread"><a href="#imread" class="headerlink" title="imread"></a>imread</h3><p>注意如果有通道则通道在最后,可以设置</p><blockquote><p>IMREAD_UNCHANGED            = -1, //如果设置，则返回的数据带有alpha通道（R,G,B,A 四个通道），否则没有alpha通道<br>      IMREAD_GRAYSCALE            = 0,  //如果设置，则将图像转换为单通道灰度图像<br>      IMREAD_COLOR                = 1,  //如果设置，则将图像转换成3通道BGR彩色图像<br>      IMREAD_ANYDEPTH             = 2,  //如果设置，则在输入具有相应深度时返回16位/32位图像，否则将其转换为8位<br>      IMREAD_ANYCOLOR             = 4,  //如果设置，则图像可能以任何颜色格式读取<br>      IMREAD_LOAD_GDAL            = 8,  //如果设置，使用gdal驱动程序加载图像<br>      IMREAD_REDUCED_GRAYSCALE_2  = 16, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/2<br>      IMREAD_REDUCED_COLOR_2      = 17, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/2<br>      IMREAD_REDUCED_GRAYSCALE_4  = 32, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/4<br>      IMREAD_REDUCED_COLOR_4      = 33, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/4<br>      IMREAD_REDUCED_GRAYSCALE_8  = 64, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/8<br>      IMREAD_REDUCED_COLOR_8      = 65, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/8<br>      IMREAD_IGNORE_ORIENTATION   = 128 //如果设置，不会根据EXIF的方向标志旋转图像</p></blockquote><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><h3 id="imshow-1"><a href="#imshow-1" class="headerlink" title="imshow"></a>imshow</h3><p>主要讲讲matplotlib的imshow</p><blockquote><p>matplotlib在imshow时，如果接收到的是二维矩阵，会自动归一化，映射到彩色。如果输入的矩阵里面值都是一样的，归一化会把他们全部变为255，也就是呈现黑色。</p></blockquote><p>用于在使用 cmap 映射到颜色之前将标量数据缩放到 [0, 1] 范围的归一化方法。默认情况下，使用线性缩放，将最低值映射到 0，将最高值映射到 1。</p><p>imshow的输入</p><blockquote><p>图像数据。支持的数组形状有：(M,N)：具有标量数据的图像。使用归一化和颜色图将值映射到颜色。请参阅参数norm、cmap、vmin、vmax。</p><p>(M, N, 3)：具有 RGB 值（0-1 float 或 0-255 int）的图像。</p><p>(M, N, 4)：具有 RGBA 值（0-1 float 或 0-255 int）的图像，即包括透明度。前两个维度（M、N）定义图像的行和列。超出范围的 RGB(A) 值将被剪裁。</p></blockquote><p>所以如果使用单通道的数据,会默认norm,而这种norm是根据输入值的min-max进行norm,并不是norm到0-255</p><p><img data-src="https://i.imgur.com/1zK0cLv.png" alt="image-20231116210309620"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = torch.ones(<span class="number">152</span>,<span class="number">152</span>,<span class="number">1</span>,dtype=torch.uint8)*<span class="number">220</span></span><br><span class="line">img = img.numpy()</span><br><span class="line">plt.imshow(img,cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>解决办法是设置vmin=0,vmax=255,当然使用三通道也可以</p><p><img data-src="https://i.imgur.com/4gWVsnC.png" alt="image-20231116210559535"></p><h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><ol><li><a href="https://blog.csdn.net/zjh12312311/article/details/116209353">matplotlib 可视化图像明明255，结果出来全为黑色的问题<em>plt.imshow 不加vmin和vmax参数是全黑的</em>佳hong的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/weixin_42216109/article/details/89707220">有关函数cv2.imshow()处理不同图像深度时的数据转化问题_cv2.cv_8u图像深度-CSDN博客</a>这篇文章有点问题,目前opencv将负值作为0处理</li><li><a href="https://www.cnblogs.com/siren27/p/12738571.html">opencv中imwrite对float的处理 - siren27 - 博客园 (cnblogs.com)</a></li><li><a href="https://blog.csdn.net/m0_37579176/article/details/105460265">【精选】使用 tiff/png 文件类型对 uint16_t/float 数据类型存取的无聊实验_float存储方式和uint16-CSDN博客</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在用opencv和matplotlib展示图片,但是遇到了一些问题,这里展开说说&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>ppdlm:流行的深度学习模型</title>
    <link href="https://www.sekyoro.top/2023/11/08/ppdlm-%E6%B5%81%E8%A1%8C%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.sekyoro.top/2023/11/08/ppdlm-%E6%B5%81%E8%A1%8C%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</id>
    <published>2023-11-08T03:14:00.000Z</published>
    <updated>2023-11-25T07:00:57.201Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.</p><span id="more"></span><h2 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>主流的序列转换模型基于<strong>复杂的递归或卷积神经网络</strong>，其中<strong>包括一个编码器和一个解码器</strong>。性能最好的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构—“transformer”，它<strong>完全基于注意力机制，无需递归和卷积</strong>。</p><h3 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h3><p>递归神经网络，特别是长短期记忆和门控递归神经网络，已被牢固确立为语言建模和机器翻译等序列建模和转译问题的最先进方法。自此以后，许多人继续努力推动递归语言模型和编码器-解码器架构的发展。</p><p>递归模型通常按照输入和输出序列的符号位置进行计算。将位置与计算时间的步长对齐，它们会生成隐藏状态 h~t~ 的序列，作为前一个隐藏状态 h~t-1~ 和位置 t 的输入的函数。<strong>这种固有的序列性质排除了训练实例内的并行化，而在序列长度较长时，这一点变得至关重要，因为内存约束限制了跨实例的批处理。</strong>最近的研究通过因式分解技巧和条件计算显著提高了计算效率，同时也改善了后者的模型性能。然而，顺序计算的基本限制仍然存在。</p><p>在各种任务中，注意力机制已成为引人注目的序列建模和转导模型的一个组成部分，它可以对依赖关系进行建模，而无需考虑其在输入或输出序列中的距离。然而，除了少数情况，这种注意机制都是与递归网络结合使用的。</p><blockquote><p>比如下图,利用一个双向RNN得到每个token的状态,利用一个简单的ffn聚合这些状态作为输出token的上一个状态</p></blockquote><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png" alt="Image showing an encoder/decoder model with an additive attention layer"></p><p>在这项工作中，我们提出了 Transformer 模型架构，<strong>它摒弃了递归</strong>，<strong>而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong>。Transformer <strong>可以大大提高并行化程度</strong>，在 8 个 P100 GPU 上只需训练 12 个小时，翻译质量就能达到新的水平。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>减少顺序计算的目标也是Extended Neural GPU、ByteNet和 ConvS2S的基础，它们都使用<strong>卷积神经网络</strong>作为基本构建模块<strong>，并行计算</strong>所有输入和输出位置的隐藏表示。在这些模型中，将两个任意输入或输出位置的信号联系起来所需的运算次数随位置间距离的增加而增加，<strong>ConvS2S 是线性增加，ByteNet 是对数增加。这就增加了学习远距离位置之间依赖关系的难度</strong>。在 Transformer 中，这将被减少到一个恒定的操作数(O(1))，尽管<strong>代价是由于平均注意力加权位置而降低了有效分辨率</strong>。</p><p>自我注意（有时也称为内部注意）是一种注意机制，它将单个序列的不同位置联系起来，以计算序列的表征。自我注意已成功应用于多种任务中，包括阅读理解、抽象概括、文本引申和学习与任务无关的句子表征。</p><h3 id="model-architecture"><a href="#model-architecture" class="headerlink" title="model architecture"></a>model architecture</h3><p>大多数转导模型都具有编码器-解码器结构.在这里,编码器将输入的符号表示序列 (x1, …, xn) 映射为连续表示序列 z = (z1, …, zn)。 在给定 z 的情况下，解码器会逐个元素生成一个符号输出序列（y1, …, ym）。在每一步中，模型都是自动回归的，在生成下一步时，会消耗之前生成的符号作为额外输入。</p><p><img data-src="https://i.imgur.com/Y7S2I1W.png" alt="image-20231108115505228" style="zoom: 67%;" /></p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>编码器由 N = 6 层相同的层堆叠组成。每一层都有两个子层。第一个是多头自注意机制，第二个是简单的位置全连接前馈网络。我们在两个子层的每个周围都采用了残差连接，然后进行层归一化。也就是说，每个子层的输出都是 LayerNorm(x + Sublayer(x))，其中 Sublayer(x) 是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生维数为 dmodel = 512 的输出。</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>解码器也由 N = 6 层相同的层堆叠组成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，对编码器堆栈的输出执行多头关注。</p><p>与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。我们还<strong>修改了解码器堆栈中的自我关注子层，以防止位置关注到后续位置</strong>。这种屏蔽，再加上输出嵌入偏移一个位置的事实，确保了对位置 i 的预测只能依赖于小于 i 的位置的已知输出。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>注意力函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。<strong>输出结果以值的加权和的形式计算</strong>，<strong>其中分配给每个值的权重是通过查询与相应密钥的兼容性函数计算得出的</strong>。</p><p><img data-src="https://i.imgur.com/DAhCPHu.png" alt="image-20231108120527812"></p><blockquote><p>上图就是一般用的q与k的计算方式,说白了就是矩阵相乘,而其中的mask是为了把其中用不上的token置为-∞,这样后面做softmax权重就是0了. 因为tensor维度都是相同的,q与k,</p></blockquote><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>输入包括维度为 d~k~的query和key,以及维度为 d~v~的value。我们计算query与所有keys的点积，将每个点积除以 √dk，然后应用软最大函数获得值的权重。</p><script type="math/tex; mode=display">\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>最常用的两种注意力函数是<strong>加法注意力</strong>(additive attention)和<strong>点积</strong>(dot production)注意力。点积注意力与我们的算法相同，只是缩放因子为 1 √dk。</p><p>加法注意使用单隐层前馈网络计算相容函数(相当于用一个全连接网络得到一个输出)。虽然两者的理论复杂度相似，但点积注意力在实际应用中速度更快，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>虽然在 dk 值较小的情况下，这两种机制的表现类似，但在 d~k~ 值较大的情况下，加法注意比点积注意更胜一筹。我们猜测，对于较大的 d~k~ 值，点积的幅度会越来越大，从而将软最大函数推向梯度极小的区域</p><blockquote><p>也就是说除以d~k~原因是使得梯度更大,效果更好</p></blockquote><h4 id="Multi-Head-attention"><a href="#Multi-Head-attention" class="headerlink" title="Multi-Head attention"></a>Multi-Head attention</h4><p>我们发现，将查询、键值和值分别线性投影到 d~k~、d~k~ 和 d~v~ 维度，而不是对 d~model~ 维度的键、值和查询执行单一的注意函数，这样做是有益的。</p><p>多头注意力允许模型<strong>在不同位置共同关注来自不同表征子空间的信息</strong>。而在单注意头的情况下，平均化会抑制这一点。</p><script type="math/tex; mode=display">MultiHead( Q, K, V) = Concat( \mathrm{head}_1, ..., \mathrm{head}_\mathrm{h} ) W^O \\ where \ head¡=Attention( QW_i^Q, KW_i^K, VW_i^V)</script><p>其中，投影是参数矩阵 W^Q^~i~∈R^dmodel×dk^ , W^K^ ~i~∈R^dmodel×dk^ , W^V^~i~∈R^dmodel×dv^ 和 W O∈R^hdv×dmodel^</p><h3 id="Transformer的应用"><a href="#Transformer的应用" class="headerlink" title="Transformer的应用"></a>Transformer的应用</h3><p>在 “encoder-decoder 注意 “层中,query来自前一个decoder层，而memory keys和memory values则来自encoder的输出。这使得decoder中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的encoder-decoder注意机制。</p><p>encoder包含自注意层。在自注意层中，所有的键、值和查询都来自同一个地方，在这种情况下，就是encoder中上一层的输出。encoder中的每个位置都可以关注encoder上一层的所有位置。</p><p>同样，解码器中的自关注层允许解码器中的每个位置关注解码器中包括该位置在内的所有位置。<strong>我们需要防止decoder中的信息向左流动，以保持自动回归特性</strong>。</p><p>通过点乘注意力中的mask实现,也就是在输出序列中,把后面的token得到的value设置为-∞,</p><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>除了注意力子层外，我们的编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别对每个位置进行相同的处理。这包括两个线性变换，中间有一个 ReLU 激活。</p><script type="math/tex; mode=display">\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2</script><p>虽然不同位置的线性变换相同，但各层使用的参数不同。</p><p>另一种描述方法是<strong>两个内核大小为 1 的卷积</strong>(全卷积)。输入和输出的维度为 d~model~ = 512，内层的维度为 d~ff~= 2048。</p><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>与其他序列转换模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度为 d~model~的向量。我们还使用通常学习到的线性变换和softmax，将解码器输出转换为预测的下一个标记词概率，在模型中，我们在两个嵌入层和pre-softmax linear transformation之间共享相同的权重矩阵。</p><p>在嵌入层中，我们将这些权重乘以 √dmodel。</p><h3 id="衍生"><a href="#衍生" class="headerlink" title="衍生"></a>衍生</h3><h4 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h4><p><img data-src="https://i.imgur.com/PcgRBVU.png" alt="image-20231125145939351"></p><p>BERT（来自 Transformers 的双向编码器表示）是一个非常大的多层 Transformer 网络,BERT-base 有 12 层，BERT-large 有 24 层,其旨在通过在所有层中联合调节左右上下文来预训练未标记文本的深度双向表示。因此，预训练的 BERT 模型只需一个额外的输出层即可进行微调，从而为各种任务（例如问答和语言推理）创建最先进的模型，而无需进行大量任务特定的架构修改。</p><h4 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h4><p><img data-src="https://i.imgur.com/bmErBND.png" alt="image-20231125145906882"></p><p>在CV领域,注意力要么与卷积网络结合使用,要么用来替换卷积网络的某些组件,整体结构保持不变。本文证明了CV领域不一定依赖CNN,使用纯粹的Transformer用于图片块序列，也可以很好的完成图像分类任务</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.&lt;/p&gt;</summary>
    
    
    
    <category term="deep learning" scheme="https://www.sekyoro.top/categories/deep-learning/"/>
    
    
    <category term="deep learning" scheme="https://www.sekyoro.top/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>图像融合论文阅读</title>
    <link href="https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2023-11-02T14:24:07.000Z</published>
    <updated>2023-11-18T08:43:46.828Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>课程作业<br><span id="more"></span></p><h1 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h1><p>介绍图像融合概念，回顾sota模型，其中包括数字摄像图像融合，多模态图像融合，</p><p>接着评估一些代表方法</p><p>介绍一些常见应用，比如RGBT目标跟踪，医学图像检查，遥感监测</p><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>动机：</p><p>由于硬件设备的理论和技术限制，单一传感器或单一拍摄设置所拍摄的图像无法有效、全面地描述成像场景</p><p>图像融合：图像融合能够将不同源图像中的有意义信息结合起来，生成单一图像，该图像包含更丰富的信息，更有利于后续应用</p><p>由于融合图像的优异特性，图像融合作为一种图像增强方法已被广泛应用于许多领域，例如摄影可视化</p><h2 id="传统融合方法："><a href="#传统融合方法：" class="headerlink" title="传统融合方法："></a>传统融合方法：</h2><p>在深度学习盛行之前，图像融合已经得到了深入的研究。早期实现图像融合的方法采用相关的数学变换，在<strong>空间域或变换域</strong>人工分析活动水平并设计融合规则，称为传统融合方法。</p><p>典型的传统融合方法包括基于<strong>多尺度变换</strong>的方法、基于<strong>稀疏表示</strong>的方法、<strong>基于子空间</strong>的方法、基于<strong>显著性</strong>的方法、基于total-variance的方法等。</p><p>传统图像融合方法的缺点：</p><ol><li>为了保证后续图像融合的可行性，传统方法会对不同源的图像采用相同变换来提取特征。这种方法没有考虑到源图像的特征差异，可能导致提取的特征表现力较差。</li><li>融合策略粗糙，表现较差。</li></ol><p>引入深度学习的优势：</p><ol><li><p>可以利用不同的网络实现差异化的特征提取</p></li><li><p>在良好设计的损失函数下，融合策略可以学到更合理的特征</p></li></ol><p>现有的深度学习方法致力于解决图像融合中的三个主要问题：“feature extraction, feature fusion and image reconstruction.” (Zhang 等, 2021, p. 323)</p><p>现有方法可以分为 AutoEncoder-based，CNN- based，GAN-based。</p><h3 id="1-AE-based"><a href="#1-AE-based" class="headerlink" title="1.AE-based"></a>1.AE-based</h3><p>AE 方法通常会预先训练一个自动编码器。然后利用训练好的自编码器实现特征提取和图像重建，中间的特征融合则根据传统的融合规则实现。</p><p>DenseFuse</p><h3 id="2-CNN-based"><a href="#2-CNN-based" class="headerlink" title="2.CNN-based"></a>2.CNN-based</h3><p>他们通常以两种不同的形式将卷积神经网络引入图像融合。一种是通过使用精心设计的损失函数和网络结构，实现端到端的特征提取、特征融合和图像重建</p><p>PMGI。它提出了梯度和强度的比例维护损失，引导网络直接生成融合图像。</p><p>另外还有使用CNN得到融合规则，而使用传统的特征提取和重建方法</p><h3 id="3-GAN-baesd"><a href="#3-GAN-baesd" class="headerlink" title="3.GAN-baesd"></a>3.GAN-baesd</h3><p>GAN 方法依靠生成器和判别器之间的对抗博弈来估计目标的概率分布，从而以隐含的方式共同完成特征提取、特征融合和图像重构</p><p>比如FusionGAN 是基于 GAN 的图像融合的先驱，它在融合图像和可见图像之间建立对抗博弈，以进一步丰富融合图像的纹理细节。由于各种图像融合任务存在显著差异，这些方法在不同融合场景中的实现方式也不尽相同。</p><h2 id="图像融合场景"><a href="#图像融合场景" class="headerlink" title="图像融合场景"></a>图像融合场景</h2><p>digital photography image fusion</p><p>由于数字成像设备的性能限制，传感器无法在单一设置完全表征成像场景中的信息</p><p>例如，数码摄影产生的图像只能承受有限的光照变化，并具有预定的景深。</p><p>多曝光度图像融合和多聚焦图像融合</p><p>以产生高动态范围和完全清晰的效果。</p><p>人们使用摄像机拍摄时,希望可以获得同一场景中所有景物都清晰的图像。但是<strong>摄像机镜头受景深的限制,无法同时聚焦所有目标,因此拍摄的照片中部分区域清晰,部分区域模糊。多聚焦图像融合技术可以将多幅同一场景下聚焦区域不同的图像融合成一幅全清晰的图像</strong>,从而有效地解决这个问题,提高图像的信息利用率。</p><p>multi-modal image fusion</p><p>由于成像原理的限制，单个传感器只能捕捉到部分场景信息。多模态图像融合将多个传感器获取的图像中最重要的信息结合起来，从而实现对场景的有效描述。</p><p>典型的多模态图像融合包括红外和可见光图像融合</p><p>sharpening fusion</p><p>在保证信噪比的前提下，光谱/滤镜与瞬时视场（IFOV）之间存在一定的矛盾。</p><p>换句话说，没有任何传感器能同时捕捉高空间分辨率和高光谱分辨率的图像。</p><p>锐化融合专门用于融合不同空间/光谱分辨率的图像，以生成所需的结果，这些结果不仅具有高空间分辨率，还具有高光谱分辨率。</p><p>典型的锐化融合包括多光谱（MS）锐化和高光谱锐化。从源图像成像的角度来看，锐化融合也属于多模态图像融合。不过，就融合目标而言，锐化融合比上述多模态图像融合要求更高的光谱/空间保真度，能直接提高分辨率。因此，锐化融合将作为一个单独的类别进行讨论。</p><p>多光谱锐化是将低空间分辨率（LRMS）的多光谱图像与全色（PAN）图像融合，生成高空间分辨率的多光谱图像。</p><p>与多光谱图像相比，高光谱图像具有更高的光谱分辨率和更低的空间分辨率。</p><blockquote><p>多光谱: 谱段有多个,可以看做是高光谱的一种情况，即成像的波段数量比高光谱图像少，一般只有几个到十几个。由于光谱信息其实也就对应了色彩信息，所以多波段遥感图像可以得到地物的色彩信息，但是空间分辨率较低。更进一步，光谱通道越多，其分辨物体的能力就越强，即光谱分辨率越高。</p><p>高光谱:高光谱由更窄的波段（10-20 nm）组成，具有较高的光谱分辨率，可以检测物体的光谱特效，可提供更多无形的数据,图像可能有数百或数千个波段</p><p>全色图:全色图像是单通道的，其中全色是指全部可见光波段0.38~0.76um，全色图像为这一波段范围的混合图像。因为是单波段，所以在图上显示为灰度图片。全色遥感图像一般空间分辨率高，但无法显示地物色彩，也就是图像的光谱信息少。</p></blockquote><h3 id="digital-photography-image-fusion"><a href="#digital-photography-image-fusion" class="headerlink" title="digital photography image fusion"></a>digital photography image fusion</h3><p>数字成像设备利用光学镜头捕捉反射的可见光然后采用CCD和CMOS等书子模块记录场景信息。另一方面，由于动态范围有限，这些数字模块无法承受过大的成像曝光差异。</p><p>一方面，由于光学镜头受景深限制，通常无法同时聚焦所有物体。</p><h3 id="Infrared-and-visible-image-fusion”"><a href="#Infrared-and-visible-image-fusion”" class="headerlink" title="Infrared and visible image fusion”"></a>Infrared and visible image fusion”</h3><p>红外图像具有<strong>明显的对比度</strong>，即使在恶劣天气下也能从背景中有效地突出目标。可见光图像包含丰富的纹理细节，更符合人类的视觉感知。红外和可见光图像融合就是要将这两种特性结合起来，生成对比度高、纹理丰富的图像。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。</p><p><strong>高对比度，恶劣条件下也能有效突出目标</strong>。</p><p><strong>可见光图像包含丰富的纹理信息</strong>，更符合人类视觉感知。红外和可见光图像融合就是要<strong>将这两种特性结合起来，生成对比度高、纹理丰富的图像</strong>。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。</p><p>AE方法</p><p>首先使用数据集训练一个autoencoder，训练好的自动编码器自然可以用来解决图像融合中的两个子问题：特征提取和图像重建</p><p>图像融合的关键在于<strong>特征融合策略</strong>的设计。目前，在红外和可见光图像融合中，特征融合的策略仍然是手工计算的，无法学习，如加法、l1-norm [19]、注意力加权等。这种手工计算的融合策略比较粗糙，限制了红外图像和可见光图像融合的进一步改进。</p><p>一种用于红外和可见光图像融合的 CNN 方法是端对端地实现三个子问题。这种CNN结构通常需要残差连接，全连接以及双端结构。</p><p>由于红外图像和可见光图像融合没有ground truth，因此损失函数的设计在于确定<strong>融合结果和源图像之间对比度和纹理的相似性</strong>。</p><p>参与红外和可见光图像融合的另一种 CNN 形式是使用预先训练好的网络（如 VGGNet）从源图像中提取特征，并根据这些特征生成融合权重图。</p><p>从这个角度看，卷积神经网络只实现了融合，而不考虑特征提取和图像重建，带来的融合性能非常有限。</p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>GAN 方法是目前红外和可见光图像融合领域最流行的方法，它能够以<strong>隐含的方式完成特征提取、特征融合和图像重建</strong>。</p><p>一般来说，GAN 方法依赖于两种损失函数，即内容损失和对抗损失。内容损失与 CNN 方法类似，用于初步融合源图像，而对抗损失则进一步限制信息融合的趋势。</p><p>早期GAN方法 fusionGAN，只是在融合后的图像和可见光图像之间建立对抗博弈，以进一步增强对可见光图像丰富细节的保留。</p><p>为了更好地平衡红外信息和可见光信息，随后的方法 [25,66-69] 开始使用具有<strong>多个分类约束条件的单一判别器或双判别器来同时估计源图像的两种概率分布。</strong></p><p>一般来说，GAN 方法可以产生很好的融合结果。然而，在训练过程中保持生成器和判别器之间的平衡并非易事。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>评估指标包括:EN,SSIM,PSNR,SF,SD,CC,SF,VIF以及融合运行时间等等. </p><h4 id="EN"><a href="#EN" class="headerlink" title="EN"></a>EN</h4><p>熵值</p><p><img data-src="https://i.imgur.com/czqa4Qz.png" alt="image-20231103165404222"></p><p>p~l~是融合图像中相应灰度级的归一化直方图</p><p>熵越大，融合图像包含的信息就越多，融合方法的性能就越好。</p><h4 id="SD"><a href="#SD" class="headerlink" title="SD"></a>SD</h4><p>标准差</p><p><img data-src="https://i.imgur.com/nP1CzPO.png" alt="image-20231103165557350"></p><p>对比度高的区域总是能吸引人的注意力，而对比度高的融合图像往往能产生较大的标清值，这意味着融合图像能达到更好的视觉效果。</p><h4 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h4><p>结构相似性,取值[-1,1],<strong>数值越接近1表示结构相似性越高</strong></p><p>SSIM 用于建立图像损失和失真的模型，<strong>衡量源图像和融合图像之间的结构相似性</strong>。SSIM 主要由三部分组成：<strong>相关性损失、亮度失真和对比度失真</strong>。</p><p><img data-src="https://pic4.zhimg.com/80/v2-13bb3c60b27920c3ec834e045ec8756f_720w.webp" alt="img"></p><p>在融合任务中,计算两张源图与融合后图像的SSIM和</p><p><img data-src="https://i.imgur.com/xSMcHqm.png" alt="image-20231103171546346"></p><h4 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h4><p>峰值信噪比,衡量图像有效信息与噪声之间的比率,能够反映图像是否失真.<strong>数值越大表示失真越小</strong></p><p><img data-src="https://i.imgur.com/5OAM3nA.png" alt="image-20231103164139843"></p><p>Z表示理想参考图像灰度最大值与最小值之差，通常为255。PSNR的值越大，表示融合图像的质量越好。</p><blockquote><p>PSNR值的范围通常在<strong>0到100之间</strong>，单位为分贝（dB）。 通常情况下，PSNR值越高，表示原始图像与重建图像之间的差异越小，图像质量越接近原始图像。 一般来说，PSNR值在30到40dB之间被认为是可以接受的</p></blockquote><h4 id="CC"><a href="#CC" class="headerlink" title="CC"></a>CC</h4><p>CC 衡量融合图像与源图像的线性相关程度,CC 越大，融合后的图像与源图像越相似，融合效果越好</p><p><img data-src="https://i.imgur.com/EnD6Jys.png" alt="image-20231103172617252"></p><p><img data-src="https://i.imgur.com/FW1COsX.png" alt="image-20231103172623697"></p><h4 id="SF-空间频率"><a href="#SF-空间频率" class="headerlink" title="SF 空间频率"></a>SF 空间频率</h4><p><img data-src="https://i.imgur.com/qczz1P9.png" alt="image-20231103173521300"></p><p>测量图像的梯度分布</p><p><img data-src="https://i.imgur.com/Hqdxy5j.png" alt="image-20231103173745980"></p><p><img data-src="https://i.imgur.com/DmKSSiy.png" alt="image-20231103173758017"></p><p>SF 越大，融合图像的边缘和纹理就越丰富</p><p>​    </p><h4 id="VIF-空间信息保真度"><a href="#VIF-空间信息保真度" class="headerlink" title="VIF 空间信息保真度"></a>VIF 空间信息保真度</h4><p>VIF 衡量融合图像的信息保真度，其计算方法分为四个步骤：首先，将源图像和融合图像划分为不同的区块；然后，评估每个区块在失真和未失真情况下的视觉信息；接着，评估每个子波段的 VIF；最后，根据 VIF 计算总体指标。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>TNO TNO影像融合数据集包含不同军事相关场景的单光谱（增强视觉、近红外和长波红外或热）夜间影像，在不同的多波段camnera系统中注册。</p><p> INO RoadScene MSRS LLVIP MFD</p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>主要关注红外图像与可见光图像融合以及多焦点图像融合,从这些出发,最后到一个统一的图像融合框架.</p><h3 id="FusionGAN"><a href="#FusionGAN" class="headerlink" title="FusionGAN"></a>FusionGAN</h3><p>2019年较早的使用GAN作为图像融合算法融合红外和可见光</p><p><img data-src="https://i.imgur.com/OP6inkz.png" alt="image-20231104161851282"></p><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p><img data-src="https://i.imgur.com/gcNlg22.png" alt="image-20231104161928602"></p><p>注意损失函数设计</p><script type="math/tex; mode=display">\mathcal{L}_G=V_\text{FusionGAN}(G)+\lambda\mathcal{L}_{\mathrm{content}},</script><p>使用了一个对于GAN对抗的融合损失以及一个内容损失,对抗损失,这种想法来源LSGAN,a 和 b 分别表示虚假数据和真实数据的标签，c表示生成器希望鉴别器相信的虚假数据值。</p><script type="math/tex; mode=display">\begin{aligned}\min_DV_{\mathrm{LSGAN}}(D)&=~\frac12\mathbb{E}_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-a)^2],\\\min_GV_{\mathrm{LSGAN}}(G)&=~\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-c)^2],\end{aligned}</script><p>有两种方法可以确定公式中的 a、b 和 c 值。第一种是设置 b - c = 1 和 b - a = 2，从而最小化公式 ，使 P~data~ +P~g~ 与 P~g~ 之间的 Pearson χ2 最小化</p><p>第二种是设置 c = b，使生成器生成的样本尽可能真实。上述两种方法通常能获得相似的性能。</p><script type="math/tex; mode=display">V_{\text{FusionGAN}} ( G ) = \frac 1 N \sum _ { n = 1 }^{N}\left(D_{\theta_D}(I_f^n)-c\right)^2,</script><p>第二项 L~content~代表内容损失，λ 用于在 V~FusionGAN~(G) 和 L~content~之间取得平衡。由于红外图像的热辐射信息由其像素强度表征，而可见光图像的纹理细节信息可部分由其梯度表征. 当然可以有其他用于表征图片的某些特性的指标,比如上面介绍的SSIM等.</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{content}}=\frac1{HW}(\|I_f-I_r\|_F^2+\xi\|\nabla I_f-\nabla I_v\|_F^2),</script><p>实际上，如果没有 D~θ~，我们也可以得到融合图像，它可以保留红外图像中的热辐射信息和可见光图像中的梯度信息。</p><p>但这往往还不够，因为仅使用梯度信息无法完全表现可见图像中的纹理细节。因此，我们在生成器 G~θG~和判别器 D~θD~ 之间建立了一个对抗博弈，以调整基于可见光图像 IIv 的融合图像 If。</p><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>从第一层到第四层，我们在卷积层中使用 3 × 3 滤波器，并将stride设为 2，不带填充。这与生成器网络不同。其根本原因在于，鉴别器是一个分类器，它首先从输入图像中提取特征图，然后进行分类。因此，它的工作方式与池化层相同，将stride设置为 2。</p><p><img data-src="https://i.imgur.com/XZhUCg2.png" alt="image-20231104163811787"></p><script type="math/tex; mode=display">\mathcal{L}_D=\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_v)-b\right)^2+\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_f)-a\right)^2,</script><p>我使用了这个模型</p><h3 id="TarDAL"><a href="#TarDAL" class="headerlink" title="TarDAL"></a>TarDAL</h3><p>面向检测的融合</p><p>我们采用双层优化公式同时进行图像融合和物体检测，不仅检测精度高，而且融合后的图像视觉效果更好。</p><p>我们设计了一种参数较少的目标感知双对抗学习网络（TarDAL），用于面向检测的融合。这种 “求同存异 “的单生成器双判别器网络可保留红外目标信息和可见光纹理细节。</p><p>我们从双层表述中推导出一种合作训练方案，为快速推理（融合和检测）提供最佳网络参数。</p><p>与以往追求高视觉质量的方法不同，我们认为，IVIF 必须生成既有利于视觉检测又有利于计算机感知的图像，即面向检测的融合。</p><h3 id="问题建模"><a href="#问题建模" class="headerlink" title="问题建模"></a>问题建模</h3><p><img data-src="https://i.imgur.com/Qwko06N.png" alt="image-20231104153245210"></p><p>假设红外图像、可见光图像和融合图像都是大小为 m×n 的灰度图像，分别用列向量 x、y 和 u∈R^mn×1^ 表示。</p><p>L~d~ 是目标检测相关的训练损失,Ψ是一个目标检测网络,f () 是一个基于能量的保真度项，包含融合图像 u 以及源图像 x 和 y，而 g~T~ () 和 g~D~ () 则是两个可行性约束条件,分别定义在红外图像和可见光图像上。</p><p><img data-src="https://i.imgur.com/lv9Mg1o.png" alt="image-20231104154941474"></p><p>引入一个带有学习参数 ω~f~的融合网络 Φ，并将双级优化转换为单级优化. </p><p>因此，我们将优化分解为两个学习网络 Φ 和 Ψ。我们采用 YOLOv53 作为检测网络 Ψ 的主干，其中 L~d~也沿用其设置，并精心设计了融合网络 Φ 。</p><blockquote><p>典型的深度融合方法致力于学习两种不同成像模式的共同特征。相反，我们的融合网络在<strong>学习这两种成像方式互补特征的差异的同时，也在寻求共性</strong>。通常情况下<strong>，红外图像能突出显示目标的独特结构，而可见光图像则能提供背景的纹理细节</strong>。</p></blockquote><h4 id="Target-aware-dual-adversarial-network"><a href="#Target-aware-dual-adversarial-network" class="headerlink" title="Target-aware dual adversarial network"></a>Target-aware dual adversarial network</h4><p><img data-src="https://i.imgur.com/vuIYIRj.png" alt="image-20231107114627598"></p><p>生成器G生成一张逼真的融合图像,目标判别器D~T~使用强度一致性评估红外图像中的目标与G提供的融合图像中被mask的目标.细节判别器 D~D~判别的是可见光梯度分布与融合图像的梯度分布</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p>生成器的作用是生成能保留整体结构并保持与源图像相似强度分布的融合图像。常用的结构相似性指数(SSIM)作为损失函数.</p><p>为了平衡源图像的像素强度分布，引入了基于突出度权重（SDW）的像素损失。</p><p>另外提出了一个基于显著性pixel loss</p><script type="math/tex; mode=display">S_{\mathbf{x}(k)}=\sum_{i=0}^{2\text{55}} \boldsymbol { H _ { \mathbf{x}}}(i)|\mathbf{x}(k)-i|,</script><p>其中H~x~(i)表示直方图中i的值,x(k)表示第k个值,因为x为一个大小为mn的vector</p><script type="math/tex; mode=display">\mathscr{L}_{\mathrm{pixe}1}=\|\mathrm{u}-\omega_1\mathrm{x}\|_1+\|\mathrm{u}-\omega_2\mathrm{y}\|_1,</script><p>pixel loss如上,其中</p><script type="math/tex; mode=display">\boldsymbol{\omega}_1=S_\mathbf{x}(k)/[S_\mathbf{x}(k)-S_\mathbf{y}(k)],\boldsymbol{\omega}_\mathbf{2}=1-\boldsymbol{\omega}_\mathbf{1}.</script><p>使用 5 层密集块作为 G 来提取共同特征，然后使用包含三个卷积层的合并块进行特征聚合。每个卷积层由一个卷积运算、批处理归一化和 ReLU 激活函数组成。生成的融合图像 u 与源图像大小相同。</p><h4 id="目标鉴别器与细节鉴别器"><a href="#目标鉴别器与细节鉴别器" class="headerlink" title="目标鉴别器与细节鉴别器"></a>目标鉴别器与细节鉴别器</h4><p>目标判别器 D~T~ 用于将融合结果的前景热目标与红外目标区分开来。而细节判别器 D~D~ 的作用是将融合结果的背景细节与可见光图像的细节区分开来。</p><p>采用了预训练的显著性检测网络从红外图像中计算出目标掩码 m，这样两个判别器就能在各自的区域（目标和背景）进行判别(也就是将图像中的目标与背景分割)</p><p>对抗损失如下,R(x)表示红外图像中的目标,R(u)表示融合后图像中的目标,R^^^则表示背景</p><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}_{D_T}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\mathcal{R}(\mathbf{x}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\mathcal{R}(\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{D_D}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{y}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{\mathbf{f}}^{\mathrm{adv}}=\mathcal{L}_{D_T}^\mathbf{f}+\mathcal{L}_{D_D}^\mathbf{f},\end{gathered}</script><p>R = x*m ,R^^^= 1 − R. m表示使用预训练模型得到mask.</p><p>对于鉴别器,损失分别是</p><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}_{D_T}=\mathcal{L}_{D_T}^\mathbf{f}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\mathcal{R}(\mathbf{x}))}[(\|\nabla D_T(\tilde{x})\|)^p], \\\mathcal{L}_{D_{D}}=\mathcal{L}_{D_{D}}^{\mathbf{f}}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\hat{\mathcal{R}}(\nabla\mathbf{x}))}[(\|\nabla D_{D}(\tilde{x})\|)^{p}], \end{gathered}</script><p>两个鉴别器 D~T~ 和 D~D~ 具有相同的网络结构，即四个卷积层和一个全连接层。</p><p><img data-src="https://i.imgur.com/A3gJRD2.png" alt="image-20231107131708716"></p><h4 id="合作训练策略"><a href="#合作训练策略" class="headerlink" title="合作训练策略"></a>合作训练策略</h4><script type="math/tex; mode=display">\begin{aligned}\min_{\boldsymbol{\omega}_{\mathbf{d}},\boldsymbol{\omega}_{\mathbf{f}}}\mathcal{L}^{\mathbf{d}}(\Psi(\mathbf{u}^*;\boldsymbol{\omega}_{\mathbf{d}}))+\lambda\mathcal{L}^{\mathbf{f}}\big(\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}})\big),\\s.t.\mathbf{~u}^*=\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}}),\end{aligned}</script><p>双层优化自然会衍生出一种合作训练策略，以获得最佳网络参数 ω = (ω~d~, ω~f~)</p><p>引入了一个融合正则因子 L^f^，将受融合约束的检测优化转换为相互优化</p><p>损失函数包含目标检测的损失函数以及融合的损失函数.</p><h3 id="红外与可见光图像的融合结果"><a href="#红外与可见光图像的融合结果" class="headerlink" title="红外与可见光图像的融合结果"></a>红外与可见光图像的融合结果</h3><h4 id="定性比较"><a href="#定性比较" class="headerlink" title="定性比较"></a>定性比较</h4><p>首先，可以很好地保留红外图像中的分辨目标。如图 6（第二组的绿色缠结）所示，我们的方法中的人表现出高对比度和独特的突出轮廓，因此有利于视觉观察.</p><p>其次，我们的结果可以保留可见光图像中丰富的纹理细节（第一组和第三组的绿色缠结），这更符合人类的视觉系统。</p><h4 id="定量比较"><a href="#定量比较" class="headerlink" title="定量比较"></a>定量比较</h4><p>在 400 个图像对（20 个来自 TNO 的图像对、40 个来自 RoadScene 的图像对和 340 个来自 M3FD 的图像对）上对我们的 TarDAL 和上述竞争对手进行了定量比较。</p><p>使用了MI,EN和SD作为评估指标.</p><h3 id="红外与可见光目标检测结果"><a href="#红外与可见光目标检测结果" class="headerlink" title="红外与可见光目标检测结果"></a>红外与可见光目标检测结果</h3><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h3 id="多聚焦图像融合"><a href="#多聚焦图像融合" class="headerlink" title="多聚焦图像融合"></a>多聚焦图像融合</h3><h3 id="MFIF-GAN"><a href="#MFIF-GAN" class="headerlink" title="MFIF-GAN"></a>MFIF-GAN</h3><p>针对多焦点图像融合</p><p>在数码摄影领域，有限的景深（DOF）导致单一场景中可能存在多种图像焦点区域，并产生散焦效应（DSE）[1]。作为一种图像增强技术，多焦点图像融合（MFIF）被用来融合图 1(a) 和图 1(b) 所示的多焦点图像，使融合结果（如图 1(c) 所示）能够清晰地保留来源信息。这一操作是各类计算机视觉（CV）任务的前提条件，例如物体检测和定位、识别和分割</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>MFIF-GAN 中的生成器将源彩色图像 IA 和 IB 作为输入，旨在生成焦点图 ̂ F。判别器的输入是 IA、IB 和（真实或生成的）焦点图的连接。生成器的目的是尽可能精确地重建焦点图，而鉴别器的目的是将生成的焦点图与真实的焦点图区分开来。</p><p>G 包括一个编码器、一个张量连接模块和一个解码器。<strong>为了有效处理彩色图像，编码器被设计成六个并行子网络分支，共享源图像每个通道的参数</strong>。</p><h3 id="FuseGAN"><a href="#FuseGAN" class="headerlink" title="FuseGAN"></a>FuseGAN</h3><p>我们的目标是通过构建基于 cGAN 的网络 FuseGAN，学习从源图像到置信度图的映射，从而为融合任务提供重点信息。我们首先详细介绍了该架构，然后分析了其目标函数；最后阐述了融合方案.</p><p>生成器 G：如图所示，生成器 G 由三个部分组成：编码器、张量并合器和解码器。<strong>它将一对多焦点图像作为输入，并输出置信度图</strong>。具体来说，编码器有两个分支，每个分支包含 12 个块。为简单起见，我们将卷积层、批规范层和转置卷积层分别称为 Conv、BN 和 Decov。其中，第一块是 Conv-BN-ReLu，滤波器尺寸较大，为 7×7，步长为 1，目的是粗略提取特征。</p><p><img data-src="https://i.imgur.com/Fgpy1VL.png" alt="image-20231108184311430" style="zoom: 80%;" /></p><p>我们利用 中的 PatchGAN 作为判别器 D。从概念上讲，它试图辨别图像中每个大小为 K×K 的patch是真是假。鉴别器对图像中的所有响应进行卷积平均，最后生成输出。。</p><p><img data-src="https://i.imgur.com/oI8nYfO.png" alt="image-20231108190809183"></p><p>因此，我们利用自适应权重块设计的特定内容损失函数可以自适应地引导融合图像在像素级逼近源图像中重点区域的强度分布和梯度分布</p><p>此外,由于我们的优化目标是<strong>基于每个像素</strong>,<strong>为了避免融合后的图像出现色差,保证整体的自然度,我们增加了 SSIM 损失项</strong>。根据统计学原理,计算每个源图像片段中较大分数的平均值,作为相应 SSIM 损失项的权重。</p><h3 id="MFFGAN"><a href="#MFFGAN" class="headerlink" title="MFFGAN"></a>MFFGAN</h3><p>图像融合的理念是从源图像中提取并组合最有意义的信息。<strong>对于多焦点图像融合来说，最有意义的信息是源图像中的锐利区域，这些区域反映在强度分布和纹理细节上</strong>。自然，在信息提取过程中，应保留锐利区域的这些信息，而摒弃模糊区域的这些信息。</p><p>当然，在信息提取过程中，尖锐区域的信息应该保留，模糊区域的信息应该舍弃。因此，有必要在优化过程中<strong>引入损失函数的调整机制</strong>，以约束网络有选择地提取和重构信息。</p><p>首先，我们设计了一个自适应决策块，它可以根据重复模糊原理评估每个像素的清晰度,也就是说，清晰度较高的图像，经过模糊处理后，像素值变化较大。根据这一观察结果，生成screening map来描述有效信息的位置。screening map作用于我们构建的特定内容损失函数，从而在像素尺度上调整优化目标。</p><p>判定块可以自适应地引导融合结果在像素尺度上逼近清晰源图像的强度分布和梯度分布</p><p>.我们的具体方法是选择分数较大的像素（放弃较小的分数）作为两个源图像相应像素位置的优化目标。在决策块和内容损失的共同作用下，生成器可以得到相对清晰自然的结果。</p><p>我们将联合梯度图定义为真实数据，将融合图像的梯度图定义为假数据。持续的对抗性学习可以引导生成器更专注于纹理的保留。因此，我们可以获得更高质量的融合结果，其中包含更丰富的纹理细节。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损耗函数由生成器损耗L~G~和鉴别器损耗L~D~组成。</p><h4 id="生成器-1"><a href="#生成器-1" class="headerlink" title="生成器"></a>生成器</h4><p>生成器的损失有两部分，即用于提取和重构信息的内容损失L~Gcon~，以及用于增强纹理细节的对抗性损失L~Gadv~。</p><script type="math/tex; mode=display">\mathcal{L}_{G}=\mathcal{L}_{G_{\mathrm{adv}}}+\alpha L_{G_{\mathrm{con}}}</script><script type="math/tex; mode=display">\mathcal{L}_{G_{\mathrm{adv}}}=\frac{1}{N}\sum_{n=1}^{N}(D(\nabla(I_{\mathrm{fused}}^{n}))-a)^2</script><p>其中 N 是训练期间批次中融合图像的数量，a 是生成器期望判别器确定融合图像的概率标签</p><script type="math/tex; mode=display">L_{G_{\mathrm{con}}}=\beta_{1}\mathcal{L}_{\mathrm{int}}+\beta_{2}\mathcal{L}_{\mathrm{grad}}</script><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{int}}=\frac{1}{HW}\sum_{\cdot}\sum_{\cdot}S_{1_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{1_{i,j}})^{2}+S_{2_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{2_{i,j}})^{2}</script><script type="math/tex; mode=display">\begin{aligned}S_{1_{i,j}}&=\operatorname{sign}(RB(I_{1_{i,j}})-\min(RB(I_{1_{i,j}}),RB(I_{2_{i,j}}))),\\S_{2_{i,j}}&=1-S_{1_{i,j}},\end{aligned}</script><p>重复模糊函数</p><script type="math/tex; mode=display">RB(\cdot)~=~abs(I_{i,j}-LP(I_{i,j}))</script><p>LP （⋅） 表示低通滤波器函数。值得注意的是，S（⋅）的大小也是H × W。</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\mathrm{grad}}&=\frac1{HW}\sum_i\sum_jS_{\mathbf{1}_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{\mathbf{1}_{i,j}})^2\\&+S_{2_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{2_{i,j}})^2.\end{aligned}</script><h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><p>判别器的损失功能使判别器能够准确识别真假数据。在我们的方法中，假数据是融合图像的梯度图。真实数据是我们构建的联合梯度图。</p><script type="math/tex; mode=display">Grad_{\mathrm{fused}}=\mathrm{abs}(\nabla I_{\mathrm{fused}}) \\Grad_{\mathrm{joint}}=\max(\mathrm{abs}(\nabla I_1),\mathrm{abs}(\nabla I_2)),</script><script type="math/tex; mode=display">\mathcal{L}_{D_{\mathrm{adv}}}=\frac1N\sum_{n=1}^{N}[D(Grad_{\mathrm{fused}}^{n})-b]^{2}+[D(Grad_{\mathrm{joint}}^{n})-c]^{2}</script><p>其中 b 是融合图像梯度图的标签，应设置为 0。c 是联合梯度图的标签，应设置为 1。</p><p>也就是说，判别器期望准确地将联合梯度图识别为真实数据，将融合图像的梯度图识别为假数据。在这种约束下，判别器可以引导生成器在信息维护方面的倾向，即有利于强纹理保存.</p><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img data-src="https://i.imgur.com/WPHLLvv.png" alt="image-20231108222112489"></p><h4 id="生成器架构"><a href="#生成器架构" class="headerlink" title="生成器架构"></a>生成器架构</h4><p>我们将生成器拆分为两条路径来提取信息，对应于两个源图像。生成器网络的设计灵感来自pseudo-Siamese，它擅长处理两种相对不同的输入。由于多焦点图像对在相应的像素位置清晰或模糊，因此pseudo-Siamese网络适用于此类图像</p><p><img data-src="https://i.imgur.com/l2ONRey.png" alt="image-20231108215808309"></p><p>在这两条路径中，都有四个卷积层来提取特征。第一个卷积层使用 5 × 5 卷积核，其余三个卷积层使用 3 × 3 卷积核。它们都使用 Leaky ReLU 作为激活函数。为了防止卷积过程中的信息丢失，我们根据 DenseNet 的思想重用了这些特征.</p><p>同时，为了提取更充分的信息，我们在两条路径之间交换信息。具体来说，交换的信息是通过连接和卷积的方法生成的。然后，交换的信息与所有前一个卷积层的输出连接在一起，作为下一个卷积层的输入。</p><p>最后，我们将两条路径中所有卷积层的输出串联起来，然后通过卷积层生成融合图像。卷积层的核大小为 1 × 1，激活函数为 tanh。值得注意的是，在所有卷积层中，填充模式设置为“SAME”，即特征图的大小在整个卷积过程中没有变化，这与源图像的大小相同。</p><h4 id="判别器架构"><a href="#判别器架构" class="headerlink" title="判别器架构"></a>判别器架构</h4><p><img data-src="https://i.imgur.com/veuN5Um.png" alt="image-20231108222051637"></p><p>判别器中的输入有两种类型，一种是<strong>基于源图像的联合梯度图和融合图像的梯度图</strong>。鉴别器由四个卷积层和一个线性层组成。四个卷积层的卷积核大小为 3 × 3，它们都使用了LeakyReLU 激活函数。这些卷积层的步幅设置为 2。最后一层是用于查找分类概率的线性层。</p><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>我们的实验是在两个数据集上进行的，比如Lytro数据集[34]和我们基于公共数据库构建的MFI-WHU数据集。</p><p>在 Lytro 数据集和 MFI-WHU 数据集上，用于测试的图像对数分别为 10 和 30。对于训练，为了获得更多的训练数据，我们采用了剪裁分解的扩展策略。具体来说，对于 Lytro 数据集，我们将其余图像裁剪为 22,090 个大小为 60 × 60 的图像图块对进行训练;对于 MFI-WHU 数据集，我们将其余图像裁剪为 202,246 个大小为 60 × 60 的图像patch对进行训练。</p><p>batch_size=32,epochs=20,训练一个epoch需要m步数,将一张图片分为多个patch,m设置为所有patch数除以b. 一般考虑训练更多的判别器,训练判别器次数是生成器的p倍.</p><p><img data-src="https://i.imgur.com/TYR6p1y.png" alt="image-20231109104901606"></p><p>我们将图像从 RGB 转换为 YCbCr 色彩空间。由于 Y 通道（亮度通道）可以表示结构细节和亮度变化，因此我们只致力于融合 Y 通道值。对于 Cb 和 Cr 通道（色度通道），我们以传统方式融合它们。然后，将这些通道的融合分量转移到RGB以获得最终结果。</p><h3 id="一些结果"><a href="#一些结果" class="headerlink" title="一些结果"></a>一些结果</h3><h4 id="多焦图像融合"><a href="#多焦图像融合" class="headerlink" title="多焦图像融合"></a>多焦图像融合</h4><p><img data-src="https://i.imgur.com/YrdNz1J.png" alt="image-20231118163720106"></p><p><img data-src="https://i.imgur.com/8y3MF5I.png" alt="image-20231118164233372"></p><h4 id="红外可见光图像融合"><a href="#红外可见光图像融合" class="headerlink" title="红外可见光图像融合"></a>红外可见光图像融合</h4><p><img data-src="https://i.imgur.com/6H8J7CW.png" alt="image-20231118164258767"></p><h3 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h3><p><a href="https://github.com/drowning-in-codes/UFGAN">drowning-in-codes/UFGAN: GAN for Image Fusion which is inspired by FusionGAN and U-net (github.com)</a></p><p><a href="https://github.com/drowning-in-codes/MFF-GAN">drowning-in-codes/MFF-GAN: Code of MFF-GAN: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion. (github.com)</a></p><p>colab 链接<a href="https://colab.research.google.com/drive/1wcb28gzgF62GphVdx42XkoZ68GxDaRpo#scrollTo=K8FjqBFQxmnR">UFGAN.ipynb - Colaboratory (google.com)</a></p><h3 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h3><p>利用预训练模型提供内容和风格 transfer learning?</p><p>利用cGAN思想? 此外损失函数的设计有必要换成神经网络而不是人工设计的一些值了.可以看看一篇CVPR的TARDAL<a href="http://arxiv.org/abs/2203.16220">http://arxiv.org/abs/2203.16220</a></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><p><a href="https://blog.csdn.net/Chaolei3/article/details/79404806">详细理解RGB图像、全色图像、多光谱图像、高光谱图像-CSDN博客</a></p></li><li><p><a href="https://github.com/Linfeng-Tang/Image-Fusion">Linfeng-Tang/Image-Fusion: Deep Learning-based Image Fusion: A Survey (github.com)</a></p><p><strong>综述</strong></p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253521001342">Image fusion meets deep learning: A survey and perspective - ScienceDirect</a></p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522001518">Current advances and future perspectives of image fusion: A comprehensive review - ScienceDirect</a></p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;课程作业&lt;br&gt;</summary>
    
    
    
    
    <category term="image fusion" scheme="https://www.sekyoro.top/tags/image-fusion/"/>
    
  </entry>
  
  <entry>
    <title>目标检测学习_P3</title>
    <link href="https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/"/>
    <id>https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/</id>
    <published>2023-11-01T13:29:52.000Z</published>
    <updated>2023-11-30T02:45:02.311Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO.<br><span id="more"></span></p><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>YOLO模型是构建快速实时物体探测器的第一次尝试。因为YOLO不经历区域建议步骤，并且只在有限数量的边界框上进行预测，所以它能够超快速地进行推理。</p><ol><li><p>残差块</p><p>首先，将图像划分为不同的网格。每个网格的尺寸为S x S。将输入图像转换为网格的过程如下图所示。每个网格单元将检测其中出现的对象。</p></li><li><p>边界框线性回归</p></li></ol><p>边界框是高亮显示图像中具有某些属性（如宽度（bw）、高度（bh）和类别（如人、汽车、红绿灯等）的对象的轮廓，由字母C表示。边界框的中心（bx）。YOLO使用单边界框回归来预测对象的高度、宽度、中心和类别。</p><ol><li><p>IOU</p><p>并集交集（IOU）是一种用于对象检测的工具，用于解释方框如何重叠。YOLO使用IOU完美地围绕对象的完美输出框。网格中的每个单元负责预测边界框及其置信度得分。如果预测的边界框与实际框相同，则IOU等于1。此技术可以消除与实际框不相等的边界框。</p></li></ol><p>YOLOv2:YOLOv2于2017年发布，其架构对YOLO进行了几次迭代改进，包括BatchNorm、更高分辨率和锚盒。</p><p>YOLOv3：于2018年发布，YOLOv3在以前的模型的基础上，为边界框预测添加客观性分数，为主干层添加连接性，并在三个不同的级别进行预测，以提高对较小对象的性能。</p><p>YOLOv4:YOLOv4由Alexey Bochkovskiy于2020年4月发布，其中引入了改进的功能聚合、“免费包”（带增强）、漏洞激活等改进。</p><p>YOLOv5:由Glenn Jocher于2020年6月发布，YOLOv5与之前的所有版本不同，因为它是PyTorch实现，而不是原始暗网的分支。与YOLO v4一样，YOLO v5具有CSP脊椎和PA-NET颈部。主要改进包括马赛克数据扩展和自动学习边界框锚定。</p><p>PP-YOLO：百度基于YOLO v3于2020年8月发布。PP-YOLO的主要目标是实现一种具有相对平衡的效率和有效性的对象检测器，该检测器可以直接用于当前的应用场景，而不是设计新的检测模型。</p><p>Scaled YOLOv4:发布于2020年11月，作者：王、博奇科夫斯基和廖。该模型使用跨阶段部分网络来增加网络大小，同时保持YOLOv4的准确性和速度。</p><p>PP-YOLOv2：再次由百度团队撰写并于2021年4月发布，它对PP-YOLO进行了小修改，以获得更好的性能，包括添加错误激活功能和路径聚合网络。</p><p>流程:</p><ol><li>预训练一个CNN用于图像分类任务</li><li>将输入图像分为SxS的块,如果一个物体的中心落入一个块cell中，该块“负责”检测该物体的存在.包括预测<strong>每个块预测碰撞盒的位置</strong>,<strong>置信度</strong>以及<strong>包含物体的概率</strong></li><li>位置就是(x,y,w,h),x,y是相对于cell的offset,w,h被归一化</li><li>置信度是<code>Pr(containing an object) x IoU(pred, truth)</code>; 其中<code>Pr</code> = 概率</li><li>如果一个cell包含物体,它会预测一个概率,表示这个物体属于每一类的概率Pr(the object belongs to the class C_i | containing an object),在该阶段模型仅预测每个cell的一组类概率,而与bbox无关</li><li>最终,一张图像包含SXSXB个bbox,每个bbox包含四个预测位置以及置信度和K个条件概率.所以预测的值shape是SXSX(5B+K)</li></ol><h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo.png" alt="img">Network Architecture</h3><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png" alt="img"></p><blockquote><p>作为一个单级对象检测器，YOLO速度极快，但由于候选边界框的数量有限，它不善于识别形状不规则的对象或一组小对象。</p></blockquote><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img data-src="https://s2.loli.net/2023/11/29/63TqhgSLFPbIKki.png" alt="image-20231129233153863"></p><p>损失由两部分组成，边界框偏移预测的定位损失和条件类概率的分类损失。这两部分都计算为误差平方和。</p><p><img data-src="https://s2.loli.net/2023/11/30/AGzmK4HVNhW95rq.png" alt="image-20231130101219411"></p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png" alt="img"></p><h3 id="YOLOV2改进"><a href="#YOLOV2改进" class="headerlink" title="YOLOV2改进"></a>YOLOV2改进</h3><p>应用了多种修改以使YOLO预测更准确、更快，包括：</p><p>1.BatchNorm有助于：在所有卷积层上添加批次范数，从而显著提高收敛性。</p><p>2.图像分辨率很重要：用高分辨率图像微调基本模型可以提高检测性能。</p><p>3.卷积锚盒检测：YOLOv2不是在整个特征图上预测具有完全连接层的边界盒位置，而是使用卷积层来预测锚盒的位置，就像在更快的R-CNN中一样。空间位置的预测和类概率是解耦的。总体而言，这一变化导致mAP略有下降，但召回率有所上升。</p><p>4.框维度的K-均值聚类：与使用手工挑选的锚框大小的更快的R-CNN不同，YOLOv2对训练数据进行K-均值集群，以在锚框维度上找到良好的先验。距离度量是根据IoU分数设计的：</p><p><img data-src="https://s2.loli.net/2023/11/30/nKCZulO46oeVdJb.png" alt="image-20231130103908962"></p><p>通过聚类生成的锚框在固定数量的框的条件下提供更好的平均IoU。</p><p>5.直接位置预测：YOLOv2以一种不会与中心位置偏离太多的方式来制定边界框预测。如果盒子位置预测可以将盒子放置在图像的任何部分，就像在区域提案网络中一样，那么模型训练可能会变得不稳定。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png" alt="img" style="zoom:50%;" /></p><p>6.添加细粒度特性：YOLOv2添加了一个直通层，将细粒度特性从早期层带到最后一个输出层。该穿透层的机制类似于ResNet中的身份映射，以从以前的层中提取更高维度的特征。这将使性能提高1%。</p><p>7.多尺度训练：为了训练模型对不同大小的输入图像具有鲁棒性，每10个批次随机采样一个新大小的输入维度。由于YOLOv2的conv层将输入维度下采样因子为32，因此新采样的大小是32的倍数。</p><p>8.轻量级基础模型：为了更快地进行预测，YOLOv2采用了轻量级基础模型DarkNet-19，该模型有19个conv层和5个最大池化层。关键是在3x3 conv层之间插入平均池和1x1 conv滤波器。</p><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p>Single Shot Detector（SSD；Liu等人，2016）是<strong>首次尝试使用卷积神经网络的金字塔特征层次来有效检测各种大小的对象之一</strong>。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png" alt="img"></p><p>该模型以图像作为输入，<strong>该图像通过具有不同大小滤波器（10x10、5x5和3x3）的多个卷积层。使用来自网络不同位置的卷积层的特征图来预测边界框</strong>。它们由具有3x3滤波器的特定卷积层处理，称为额外特征层，以产生一组类似于快速R-CNN的锚框的边界框。</p><p>与需要对象建议的方法相比，SSD 非常简单，因为它<strong>完全省去了建议生成和随后的像素或特征重采样阶段</strong>，并将所有计算封装在一个网络中。</p><p><img data-src="https://miro.medium.com/v2/resize:fit:770/1*f0p4it3vSVV_qeTJq5Jv1Q.png" alt="img"></p><p>此模型<strong>主要由基础网络组成，其后是几个多尺度特征块</strong>。 <strong>基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络</strong>。</p><p> 单发多框检测论文中选用了在分类层之前截断的VGG (<a href="http://zh.d2l.ai/chapter_references/zreferences.html#id98">Liu <em>et al.</em>, 2016</a>)，现在也常用ResNet替代。 我们可以设计基础网络，使它输出的高和宽较大。 这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。</p><p><strong>接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小</strong>（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。</p><p>通过深度神经网络分层表示图像的多尺度目标检测的设计。 由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。 简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。</p><h4 id="default-box的生成"><a href="#default-box的生成" class="headerlink" title="default box的生成"></a>default box的生成</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-box-scales.png" alt="img"></p><script type="math/tex; mode=display">\begin{aligned}\text{level index: } &\ell = 1, \dots, L \\\text{scale of boxes: } &s_\ell = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{L - 1} (\ell - 1) \\\text{aspect ratio: } &r \in \{1, 2, 3, 1/2, 1/3\}\\\text{additional scale: } & s'_\ell = \sqrt{s_\ell s_{\ell + 1}} \text{ when } r = 1 \text{thus, 6 boxes in total.}\\\text{width: } &w_\ell^r = s_\ell \sqrt{r} \\\text{height: } &h_\ell^r = s_\ell / \sqrt{r} \\\text{center location: } & (x^i_\ell, y^j_\ell) = (\frac{i+0.5}{m}, \frac{j+0.5}{n})\end{aligned}</script><script type="math/tex; mode=display">\mathcal{L}_\text{cls} = -\sum_{i \in \text{pos}} \mathbb{1}_{ij}^k \log(\hat{c}_i^k) - \sum_{i \in \text{neg}} \log(\hat{c}_i^0)\text{, where }\hat{c}_i^k = \text{softmax}(c_i^k)</script><p>其中1表示对于k类bbox与gt-box是否match</p><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_\text{loc} &= \sum_{i,j} \sum_{m\in\{x, y, w, h\}} \mathbb{1}_{ij}^\text{match} L_1^\text{smooth}(d_m^i - t_m^j)^2\\L_1^\text{smooth}(x) &= \begin{cases}    0.5 x^2             & \text{if } \vert x \vert < 1\\    \vert x \vert - 0.5 & \text{otherwise}\end{cases} \\t^j_x &= (g^j_x - p^i_x) / p^i_w \\t^j_y &= (g^j_y - p^i_y) / p^i_h \\t^j_w &= \log(g^j_w / p^i_w) \\t^j_h &= \log(g^j_h / p^i_h)\end{aligned}</script><p>此外SSD使用了NMS和HHM优化训练过程.</p><blockquote><p>NMS:非最大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。跳过具有高IoU（即大于0.5）的剩余框，使用之前选择的框</strong>。</p><p>HNM:有些负类很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。</p></blockquote><h3 id="连结多尺度的预测"><a href="#连结多尺度的预测" class="headerlink" title="连结多尺度的预测"></a>连结多尺度的预测</h3><p><strong>单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量</strong>。</p><p>在不同的尺度下,特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。 因此，不同尺度下预测输出的形状可能会有所不同。</p><p>除了批量大小这一维度外，其他三个维度都具有不同的尺寸。 为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cls_predictor</span>(<span class="params">num_inputs, num_anchors, num_classes</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class="number">1</span>),</span><br><span class="line">                     kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x, block</span>):</span></span><br><span class="line">    <span class="keyword">return</span> block(x)</span><br><span class="line"></span><br><span class="line">Y1 = forward(torch.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>)), cls_predictor(<span class="number">8</span>, <span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">Y2 = forward(torch.zeros((<span class="number">2</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>)), cls_predictor(<span class="number">16</span>, <span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_pred</span>(<span class="params">pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.flatten(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_preds</span>(<span class="params">preds</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat([flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br><span class="line">concat_preds([Y1, Y2]).shape</span><br></pre></td></tr></table></figure><h4 id="高和宽减半块"><a href="#高和宽减半块" class="headerlink" title="高和宽减半块"></a>高和宽减半块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">down_sample_blk</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        blk.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                             kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    blk.append(nn.MaxPool2d(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure><h4 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h4><p>Feature Pyramid Networks for Object Detection</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/featurized-image-pyramid.png" alt="img"></p><p>在看多尺度特征的时候注意到了这篇文章.提出了一个利用深度卷积神经网络固有的多尺度金字塔结构来以极小的计算量构建特征金字塔的网络结构</p><p><img data-src="https://s2.loli.net/2023/11/29/SaJdTtXjVy5qx7f.png" alt="image-20231129231413439"></p><p><img data-src="https://upload-images.jianshu.io/upload_images/18299912-aa79ebef839e6772.png?imageMogr2/auto-orient/strip|imageView2/2/w/611/format/webp" alt="img"></p><ul><li>自下而上的路径是正常的前馈计算。</li><li>自上而下的路径朝着相反的方向发展，通过横向连接将粗糙但语义更强的特征图添加回更大尺寸的先前金字塔级别。</li></ul><p>首先，更高级别的特征在空间上更粗糙地上采样，使其大2倍。对于图像放大，本文使用了最近邻上采样。虽然有许多图像放大算法，例如使用deconv，但<strong>采用另一种图像缩放方法可能会也可能不会提高RetinaNet的性能</strong>。</p><p>较大的特征图<strong>经过1x1 conv层以减小通道尺寸</strong>。</p><p>最后，通过<strong>元素相加</strong>将这两个特征图合并。</p><p>根据消融研究，特征化图像金字塔设计的组件的重要性等级如下：1x1横向连接&gt;跨多层检测对象&gt;自上而下的富集&gt;金字塔表示（与仅使用最底层相比）。</p><p>与SSD中一样，<strong>通过对每个合并的特征图进行预测，可以在所有金字塔级别中进行检测</strong>。因为预测共享相同的分类器和框回归器，所以它们都形成为具有相同的通道维度d=256。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/retina-net.png" alt="img"></p><h4 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h4><p>[<a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">overfeat</a>]</p><p>Overfeat是将目标检测、定位和分类任务集成到一个卷积神经网络中的先驱模型。主要思想是（i）<strong>以滑动窗口的方式在图像的多个尺度的区域上的不同位置进行图像分类</strong>，以及（ii）使用<strong>在相同卷积层上训练的回归器来预测边界框位置</strong>。</p><p>（1）用一个共享的CNN（ConvNet）来同时处理图像分类，定位，检测三个任务，可以提升三个任务的表现。</p><p>（2）用CNN有效地实现了一个多尺度的，滑动窗口的方法，来处理任务。</p><p>（3）提出了一种方法，通过累积预测来求bounding boxes（而不是传统的非极大值抑制）</p><p><a href="https://blog.csdn.net/Gentleman_Qin/article/details/84836122">OverFeat——全卷积首次用于检测问题 (目标检测)(深度学习)(ICLR 2014）_overfeat是做什么的-CSDN博客</a></p><p><img data-src="https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/overfeat-training.png" alt="img"></p><h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><h3 id="Focal-Loss-for-Dense-Object-Detection"><a href="#Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Focal Loss for Dense Object Detection"></a>Focal Loss for Dense Object Detection</h3><p>在损失函数上进行改进.对象检测模型训练的一<strong>个问题是不包含对象的背景和包含感兴趣对象的前景之间的极端不平衡。焦点损失被设计为在硬的、容易被错误分类的例子（即具有噪声纹理或部分对象的背景）上分配更多的权重，并对容易被加权的例子（例如明显为空的背景）进行加权。</strong></p><h3 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h3><p>迄今为止，准确率最高的物体检测器都是基于 R-CNN 推广的两阶段方法，即对稀疏的候选物体位置集进行分类。<strong>相比之下，应用于对可能的物体位置进行规则、密集采样的单阶段检测器有可能更快、更简单，但迄今为止，其准确性仍落后于两阶段检测器。在本文中，我们将探讨出现这种情况的原因。</strong></p><p>我们发现，dense detectors训练过程中遇到的前景-背景类别极度不平衡是主要原因。我们建议通过重塑标准交叉熵损失来解决这种类别不平衡问题，从而降低分类良好示例的损失权重。</p><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>焦点损失（Focal Loss）的设计目<strong>的是解决在训练过程中前景类和背景类之间极度不平衡（例如 1:1000）的单阶段物体检测问题</strong>。我们从用于二元分类的交叉熵（CE）损失开始引入焦点损失</p><p><img data-src="https://s2.loli.net/2023/11/29/nHdKf2JCcxV6sry.png" alt="image-20231129231823763"></p><p>在上述公式中，y∈{±1} 表示地面实况类别，p∈[0, 1]是模型对标签 y = 1 的类别的估计概率。</p><p>我们定义 p~t~</p><p><img data-src="https://s2.loli.net/2023/11/29/b1BWorKMJTxf2R4.png" alt="image-20231129231934979"></p><p>重写 CE(p, y) = CE(p~t~) = - log(pt)。</p><p>我们建议在交叉熵损失中加入一个调制因子 (1 - p~t~)γ ，可调聚焦参数 γ ≥ 0。</p><p><img data-src="https://s2.loli.net/2023/11/29/3DqjYcTbewiCrE6.png" alt="image-20231129231711234"></p><p>我们注意到焦点损失的两个特性。(1) 当一个例子被错误分类且 p~t~ 较小时，调制因子接近 1，损失不受影响。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/focal-loss.png" alt="img" style="zoom:67%;" /></p><h4 id="BackBone"><a href="#BackBone" class="headerlink" title="BackBone"></a>BackBone</h4><p>我们采用特征金字塔网络（FPN）作为 RetinaNet 的骨干网络。</p><p>简而言之，FPN 利用自上而下的路径和横向连接增强了标准卷积网络，因此该网络能从单一分辨率的输入图像中有效构建丰富的多尺度特征金字塔。金字塔的每一层都可用于检测不同尺度的物体。<strong>FPN 可以改进全卷积网络 (FCN) [23] 的多尺度预测，这体现在它对 RPN [28] 和 DeepMask 式提案 [24] 以及快速 R-CNN [10] 或 Mask R-CNN [14] 等两阶段检测器的增益上</strong>。继 [20] 之后，我们在 ResNet 架构 [16] 的基础上构建了 FPN。我们构建了一个 P3 到 P7 级的金字塔，其中 l 表示金字塔级别（Pl 的分辨率比输入低 2l）。与文献 [20] 一样，所有金字塔层级都有 C = 256 个通道。虽然许多设计选择并不重要，但我们<strong>强调使用 FPN 主干网才是关键；使用仅来自最后 ResNet 层的特征进行的初步实验得出的 AP 值较低。</strong></p><h4 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h4><p>我们使用了与中 RPN 变体类似的平移不变锚点框。锚点在金字塔 P3 到 P7 层的面积分别为 32^2^ 到 512^2^。与文献[20]一样，我们在每个金字塔层使用了三种纵横比的锚点{1:2, 1:1, 2:1}。为了获得比[20]更密集的比例覆盖，我们在每个层级添加了尺寸为{2^0^, 2^1/3^, 2^2/3^}的锚点，这些锚点是原始的 3 种宽高比锚点的集合。这改进了我们的 AP 设置。每个级别总共有 A = 9 个锚点，相对于网络的输入图像，这些锚点覆盖了 32-813 个像素的范围。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO.&lt;br&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>3D Object Detection Learning</title>
    <link href="https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/"/>
    <id>https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/</id>
    <published>2023-10-30T08:19:04.000Z</published>
    <updated>2023-12-23T08:57:23.757Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知.</p><span id="more"></span><p>先看几篇论文.</p><p>anchor-free detection,脱离了SSD,RetinaNet以及YOLO的anchor-based的工作。</p><h2 id="Objects-as-Points-2019"><a href="#Objects-as-Points-2019" class="headerlink" title="Objects as Points  2019"></a>Objects as Points  2019</h2><h3 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h3><p>检测是将图像中的物体识别为轴对齐的方框。大多数成功的物体检测器都会枚举几乎所有潜在的物体位置，并对每个位置进行分类。这不仅浪费资源、效率低下，还需要额外的后期处理。在本文中，我们采用了一种不同的方法。我们将物体建模为一个点—其边界框的中心点。</p><p>我们的检测器<strong>使用关键点估算来寻找中心点</strong>，并<strong>对所有其他物体属性进行回归，如大小、三维位置、方向甚至姿态</strong>。与相应的基于边界框的检测器相比，我们基于中心点的方法 CenterNet 是端到端可微分的，更简单、更快速、更准确。</p><h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><p><img data-src="https://i.imgur.com/JWpC9qY.png" alt="image-20231222110245995" style="zoom: 67%;" /></p><p>使用中心点作为预测结果,输出是一个热力图.假设 I∈ R^W×H×3^ 是宽度为 W、高度为 H 的输入图像，我们的目标是生成一个关键点热图 ˆ Y∈ [0, 1] ^W/R×H/R×C^,R 是输出跨度，C 是关键点类型的数量(就是类别).使用R = 4 的默认输出跨度。输出步长对输出预测进行下采样.预测值 ˆ Y = 1 对应于检测到的关键点，而 ˆ Y= 0 则是背景。</p><blockquote><p>论文中使用几种不同的全卷积编码器-解码器网络来预测图像 I 中的ˆY：堆叠沙漏网络、上卷积残差网络（ResNet）和深层聚合（DLA）。</p></blockquote><p>对于gtbox中的每个中心(也就是keypoint)会计算出一个低分辨率等效点<img data-src="https://s2.loli.net/2023/12/01/OZL5Fz6Vi7mEcvK.png" alt="image-20231201151345184"></p><p>因为预测的输出坐标是经过四倍下采样的,然后利用这个真值通过一个高斯核函数拼接到热图上.我们知道预测的输出是在0-1之间的,而且大小是W/R×H/R×C,利用这个核函数计算每个下采样后的关键点在热力图上的值.其中，σ 是与物体大小相适应的标准偏差，如果同一类别的两个高斯重叠，我们取元素最大值</p><p><img data-src="https://s2.loli.net/2023/12/01/t5BC7l1IqMNKDbf.png" alt="image-20231201151511596"></p><p>损失使用RetinaNet提出的Focal损失变型.主要是得到预测的中心位置,ground truth没有直接使用,而是使用一个高斯核将不是中心的点的值设置为(0-1),相当于更好地优化了.从简单的0-1到离散值.</p><p><img data-src="https://s2.loli.net/2023/12/01/q59OprywgN2Ehsk.png" alt="image-20231201151714656"></p><p>此外,因为需要恢复输出跨距造成的离散化误差,还添加了损失.</p><p>为每个中心点预测一个局部偏移量 ˆ O∈ R^W/R×H/RX2^。所有类别 c 共享相同的偏移预测,由于输入是一张图像,通过backbone(论文中的是ResNet和DLA)得到downsampling之后的feature map(原文叫heat map)</p><p><img data-src="https://s2.loli.net/2023/12/01/GTviaIdYJ9gFA6Z.png" alt="image-20231201152014024"></p><p>由此得到了物体的中心点,接下来需要回归得到尺寸.</p><blockquote><p>我们使用关键点估计器 ˆ Y 来预测所有中心点。此外，我们对每个对象 k 的对象尺寸 sk = (x(k) 2 - x(k) 1 , y(k) 2 - y(k) 1 ) 进行回归。</p></blockquote><script type="math/tex; mode=display">\begin{equation}L_{size}=\frac1N\sum_{k=1}^N\left|\hat{S}_{p_k}-s_k\right|.\end{equation}</script><script type="math/tex; mode=display">L_{det}=L_k+\lambda_{size}L_{size}+\lambda_{off}L_{off}.</script><p>对于3D目标检测,还需要得到深度、三维空间和方向。会为每个输出添加一个单独的头部。</p><p>深度:深度 d 是每个中心点的<strong>单一标量</strong>。然而，深度很难直接回归。我们使用 Eigen 等人的输出变换和 d = 1/σ( ˆ d) - 1，其中 σ 是 sigmoid 函数。我们将深度作为关键点估计器的附加输出通道 ˆ D∈[0, 1] W R ×H R 来计算。</p><p>物体的三维尺寸是三个标量。使用单独的头 </p><script type="math/tex; mode=display">\begin{equation}\hat{\Gamma}\in\mathcal{R}^{\frac WR\times\frac HR\times3}\end{equation}</script><p> 和 L1 损失直接回归到它们的绝对值（以米为单位）。</p><p>默认情况下，方向是一个单一标量。但是，很难对其进行回归。效仿 Mousavian 等人的研究，将方向表示为两个bins，并进行bins内回归。具体来说，bin使用 8 个标量编码，每个bin有 4 个标量。对于一个bins，两个标量用于softmax，其余两个标量在每个分区内回归到一个angle。</p><h2 id="Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation"><a href="#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation" class="headerlink" title="Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation"></a>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</h2><p><a href="http://arxiv.org/abs/2111.09515">http://arxiv.org/abs/2111.09515</a></p><h3 id="Abs-1"><a href="#Abs-1" class="headerlink" title="Abs"></a>Abs</h3><p>近年来，用于自动驾驶的激光雷达数据三维物体检测技术取得了长足进步,在最先进的方法中，将<strong>点云编码成鸟瞰图</strong>（BEV,bird’s eye view）已被证明是既有效又高效的方法。<strong>与透视图(perspective views)不同，鸟瞰图保留了物体之间丰富的空间和距离信息</strong>。然而,在 BEV 中,虽然<strong>同类型的远距离物体看起来并不更小</strong>,但它们<strong>包含的点云特征却更稀疏</strong>。这一事实<strong>削弱了使用共享权重卷积神经网络（CNN）提取 BEV 特征的能力</strong>.</p><p>为了应对这一挑战,我们提出了范围感知注意力网络 (RAANet),它能提取有效的 BEV 特征并生成出色的 3D  object detection 输出.</p><p>范围感知注意力（RAA）卷积显著改善了<strong>对远近物体的特征提取</strong>。</p><p>此外，我们还提出了一<strong>种用于点密度估计</strong>(point density estimation)的新型辅助损失，以进一步<strong>提高 RAANet 对遮挡物体的检测精</strong>度。值得注意的是，我们提出的 RAA 卷积是轻量级的,可以集成到任何用于检测 BEV 的 CNN 架构中.</p><p>在 <strong>nuScenes 和 KITTI 数据集上</strong>进行的大量实验表明，在基于激光雷达(LiDAR-based 3D object detection)的三维物体检测方面，我们提出的方法优于最先进的方法，在 nuScenes 激光雷达帧上进行的测试中，完整版的实时推理速度为 16 Hz，精简版为 22 Hz。</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>随着处理单元的快速改进，得益于深度神经网络的成功，自动驾驶的感知能力近年来得到了蓬勃发展。通过<strong>激光雷达传感器进行 3D 物体检测</strong>是自动驾驶的重要功能之一。</p><p>早期的研究采用了三维卷积神经网络（CNN），这种网络处理速度慢，内存需求大。</p><p>为了降低内存要求并提供实时处理，最近的方法利用了体素化(voxelization)和鸟瞰投影（BEV）。</p><p>体素化(Voxelization)作为三维点云(3D point clouds)的一种预处理方法得到了广泛应用，因为<strong>结构更合理的数据可提高计算效率和性能精度</strong>。</p><p>一般来说，体素化将点云划分为均匀分布的体素网格，然后将三维激光雷达点分配到各自的体素上。输出空间保留了物体之间的欧氏距离，并避免了边界框的重叠。</p><p>这些特点使得<strong>无论物体与激光雷达的距离如何，都能将物体的尺寸变化控制在一个相对较小的范围内</strong>，从而有<strong>利于在训练过程中进行形状回归</strong>。</p><p>在本文中，我们提出了距离感知注意力网络（RAANet），其中包含新型的范围感知注意力卷积层（RAAConv），设计<strong>用于LiDAR BEV的目标检测</strong>。RAAConv 由两个独立的卷积分支和注意力图组成,对输入特征图的位置信息敏感.</p><p>我们的方法受到BEV图像特性的启发，<strong>随着物体和自我车辆之间距离的增加，点变得越来越稀疏</strong>。<strong>理想情况下，对于BEV特征图，不同位置的元素应由不同的卷积核处理</strong>。但是，应用不同的内核会显着增加计算费用。</p><p>为了在BEV特征提取过程中<strong>利用位置信息，在避免繁重计算的同时，将BEV特征图视为稀疏特征和密集特征的组合</strong>。我们应用两个不同的卷积核来同时提取稀疏和密集特征。</p><p>每个提取的特征图的通道大小都是最终输出的一半。同时，根据输入形状生成范围和位置编码。然后，根<strong>据相应的特征图以及范围和位置编码计算每个范围感知注意力热图</strong>。最后，将<strong>注意力热图应用于特征图以增强特征表示</strong>。从两个分支生成的特征图按通道concat为 RAAConv 输出。</p><p>此外,遮挡的影响也不容忽视,因为同一物体在不同的遮挡量下可能具有不同的点分布。因此，我们提出了一个高效的辅助分支，称为<strong>辅助密度水平估计模块</strong>（ADLE），允许RAANet考虑遮挡。由于注释各种遮挡是一项耗时且昂贵的任务，因此我们<strong>设计了ADLE来估计每个对象的点密度水平。如果没有遮挡，则近处物体的点密度水平高于远处物体的点密度水平</strong>。</p><p>但是，<strong>如果附近的物体被遮挡，则其点密度水平会降低。因此，通过结合距离信息和密度水平信息，我们能够估计遮挡信息的存在</strong>。ADLE仅用于训练阶段，用于提供密度信息指导，在推理状态下可以删除，以提高计算效率。</p><p>主要贡献:</p><ol><li>我们提出了RAAConv层，它允许基于LiDAR的探测器提取更具代表性的BEV特征。此外，RAAConv 层可以集成到任何用于 LiDAR BEV 的 CNN 架构中。</li><li>我们提出了一种新的用于点密度估计的辅助损失，以帮助主网络学习与遮挡相关的特征。该密度水平估计器进一步提高了RAANet对被遮挡物体的检测精度。</li><li>我们提出了范围感知注意力网络（RAANet），它集成了前面提到的RAA和ADLE模块。RAANet通过基于ground truth生成各向异性(anistropic)高斯热图，进一步优化，</li></ol><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>大多数目标检测工作可以分为两大类：有锚点和无锚点的目标检测。此外，在早期阶段存在对点云数据进行编码的工作]，但它们超出了目标检测网络重构的范围。</p><h4 id="Object-detection-with-anchors"><a href="#Object-detection-with-anchors" class="headerlink" title="Object detection with anchors"></a>Object detection with anchors</h4><p>固定形状的锚回归方法，以便可以提取中间特征</p><p>two-stage:RCNN家族</p><p>one-stage:YOLO,Retinanet,SSD</p><p>YOLO:将目标检测重新定义为单一回归问题，该问题采用端到端神经网络进行单次前向传播来检测目标</p><p>SSD:Liu等开发了一种多分辨率锚点技术，用于检测尺度混合物的物体，并在一定程度上学习偏移量，而不是学习锚点。</p><p>RetinaNet:Lin等提出了一种焦点损失，以解决密集和小目标检测问题，同时处理类不平衡和不一致。</p><p>Zhou和Tuzel(VoxelNet)以及Lang等(PointPillars)提出了用于点云的神经网络，这为3D检测任务开辟了新的可能性。</p><h4 id="Object-detection-without-anchors"><a href="#Object-detection-without-anchors" class="headerlink" title="Object detection without anchors"></a>Object detection without anchors</h4><p>为了解决锚点回归带来的计算开销和超参数冗余问题，并有效地处理点云编码，无锚点目标检测已在许多工作中得到应用。无锚点目标检测可分为两大类，即<strong>基于中心的方法和基于关键点的方法</strong>。</p><p>基于中心的方法:在这种方法中，对象的中心点用于定义正样本和负样本，而不是IoU。该方法通过预测从正样本到物体边界的四个距离来生成边界框，从而大大降低了计算成本。</p><p>基于关键点的方法:通过几个预定义的方法或自学习模型定位关键点，然后生成边界框来对对象进行分类。</p><p>为了提取具有代表性的特征，我们重点关注两个主要组成部分：<strong>范围感知特征提取</strong>和<strong>遮挡监督</strong>。</p><p>我们提出的范围感知注意力网络（RAANet）的主要架构如图所示</p><p><img data-src="https://i.imgur.com/GERZSZ7.png" alt="image-20231119161935219"></p><p>我们结合了CenterNet的思想来构建一个无锚探测器，并引入了两个新颖的模块：距离感知注意力卷积层（RAAConv）和辅助密度级估计模块（ADLE）。</p><p>区域建议网络 （RPN） 将该 BEV 特征图作为输入，并使用多个下采样和上采样模块来生成高维特征图。</p><p>除了主要任务中的检测头外，我们还提出了一个辅助任务，用于点密度水平估计，以实现更好的检测性能。</p><p>RAAConv 首先利用两组卷积核来提取每个分支的中间特征图。</p><p>然后，将热图 fa 和 fb 分别乘以可学习标量 γa 和 γb。γa 和 γb 初始化为 1.0，并在训练过程中逐渐学习它们的值</p><h2 id="VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017"><a href="#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017" class="headerlink" title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017"></a>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>准确检测三维点云中的物体是自主导航、看家机器人和增强/虚拟现实等许多应用中的核心问题。点云数据 高度稀疏</p><p>为了将高度稀疏的激光雷达点云与区域建议网络（RPN）连接起来，现有的大部分工作都集中在手工制作的特征表示上，例如鸟瞰投影。</p><p>在这项工作中，我们<strong>不再需要对三维点云进行人工特征工程，而是提出了一种通用的三维检测网络—VoxelNet，它将特征提取和边界框预测统一为一个单一阶段、端到端可训练的深度网络</strong>。</p><h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><p>3D传感器技术的快速发展促使研究人员开发有效的表示来<strong>检测和定位点云中的物体</strong>,当有丰富而详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。</p><p>然而，它们<strong>无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性</strong>，导致自主导航等不受控制的场景的成功有限。</p><p>鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框,然而，基于图像的三维检测方法的精度受深度估计精度的限制。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>所提出的VoxelNet由三个功能块组成：（1）特征学习网络(Feature learning network)，（2）卷积中间层(Convolutional middle layers)，（3）区域建议网络(Region proposal network)。</p><h3 id="Feature-learning-network"><a href="#Feature-learning-network" class="headerlink" title="Feature learning network"></a>Feature learning network</h3><p>Voxel Partition</p><p><img data-src="https://i.imgur.com/dhgI1JD.png" alt="image-20231119112120958"></p><p>Stacked Voxel Feature Encoding</p><p>用V表示一个体素(Voxel),</p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p><img data-src="https://i.imgur.com/wvQa3jb.png" alt="image-20231119112203016"></p><p>RPN层有两个分支，一个用来输出类别的概率分布（通常叫做Score Map），一个用来输出Anchor到真实框的变化过程（通常叫做 Regression Map）</p><blockquote><p>注意这里论文是直接输出预测的anchor box的坐标而不是修正值.</p></blockquote><h4 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h4><p>我们初始化一个 K × T × 7 维张量结构来<strong>存储体素输入特征缓冲区</strong>，其中 <strong>K 是非空体素的最大数量，T 是每个体素的最大点数，7 是每个点的输入编码维度</strong>。</p><p>这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img data-src="https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png" alt="image-20231129221227758"></p><p>da = √(la)2 + (wa)2 是anchor box的对角线。</p><p><img data-src="https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png" alt="image-20231129221345941"></p><p>ui ∈ R^7^ 和 u∗ i ∈ R^7^ 分别是正锚点 a^pos^ ~i~ 的回归输出和地面实况。</p><h2 id="Center-based-3D-Object-Detection-and-Tracking"><a href="#Center-based-3D-Object-Detection-and-Tracking" class="headerlink" title="Center-based 3D Object Detection and Tracking"></a>Center-based 3D Object Detection and Tracking</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>与研究透彻的二维检测问题相比，点云上的三维检测提出了一系列有趣的挑战.点云稀疏，<strong>三维空间的大部分区域都没有测量值</strong>,其次，<strong>输出结果是一个三维方框，通常无法与任何全局坐标框架很好地对齐</strong>。第三，<strong>三维物体有多种尺寸、形状和长宽比</strong>，例如，在交通领域，自行车接近平面，公共汽车和豪华轿车细长，行人高大。</p><p><strong>二维和三维检测之间的这些显著差异使得这两个领域之间的理念转换变得更加困难。问题的关键在于，轴对齐的二维方框  并不能代表自由形态的三维物体</strong></p><p>一种解决方案可能是为每个物体方向分类不同的模板（锚，<strong>但这不必要地增加了计算负担，并可能带来大量潜在的假阳性检测</strong>。我们认为，<strong>将二维和三维领域连接起来的主要挑战在于物体的这种表现形式</strong>。</p><p>然后，它将这一表示法<strong>扁平化为俯视地图视图，并使用标准的基于图像的关键点检测器来查找对象中心</strong>，对于每个检测到的中心点，它会<strong>根据中心点位置的点特征回归到所有其他物体属性，如三维尺寸、方向和速度</strong>。</p><p>​        基于中心的表示法有几个主要优点：首先，<strong>与边界框不同，点没有固有方向。这大大缩小了物体检测器的搜索空间</strong>，同时<strong>允许骨干学习物体的旋转不变性和相对旋转的旋转等差性</strong>。其次，基于中心的表示法<strong>简化了追踪等下游任务</strong>。如果物体是点，小轨迹就是空间和时间中的路径。中<strong>心点可以预测连续帧之间物体的相对偏移（速度），然后将其贪婪地连接起来</strong>。第三，基于<strong>点的特征提取使我们能够设计一个有效的两阶段细化模块，其速度比以往的方法快得多</strong></p><p><img data-src="https://i.imgur.com/tS9TBGi.png" alt="image-20231222175607203"></p><p>CenterPoint 的第一阶段预测特定类别的热图、物体大小、子象素位置细化、旋转和速度。所有输出均为密集预测。</p><h3 id="Center-heatmap-head"><a href="#Center-heatmap-head" class="headerlink" title="Center heatmap head."></a>Center heatmap head.</h3><p>中心头的目标是在检测到的任何物体的中心位置生成一个热图峰值。该头会生成 K 个通道的热图 ˆ Y，K 个类别中的每个类别都有一个通道。</p><p>在训练过程中，它的目标是将注释边界框的三维中心投影到地图视图中产生的二维高斯。我们使用focal损耗</p><blockquote><p>自上而下地图视图中的物体比图像中的要稀疏。在地图视图中，距离是绝对的，而在图像视图中，距离会因透视而扭曲。以道路场景为例，在地图视图中，车辆所占的面积很小，但在图像视图中，几个大物体可能占据了屏幕的大部分区域</p></blockquote><p>采用 CenterNet的标准监督方式会导致监督信号非常稀疏，大多数位置都被视为背景。为了解决这个问题，我们通过<strong>扩大每个地面实况对象中心的高斯峰值，来增加目标热图 Y 的正向监督</strong>。</p><h3 id="Regression-heads"><a href="#Regression-heads" class="headerlink" title="Regression heads"></a>Regression heads</h3><p>在物体的中心特征处存储了几个物体属性：<strong>子象素位置细化</strong>(sub-voxel) o∈R2、<strong>离地高度</strong> hg∈R、<strong>三维尺寸</strong>(3D dimension)s∈R3，<strong>以及偏航旋转角度</strong>（sin(α), cos(α)）∈R2。</p><p>子体素位置细化 o 可减少主干网络体素化和跨距造成的量化误差</p><p>地面高度 hg 可帮助定位三维物体，并补充地图视图投影中缺失的高程信息。</p><p>方位预测使用偏航角的正弦和余弦作为连续回归目标。</p><p>结合方框大小，这些回归头可提供三维边界框的全部状态信息。每个输出都使用自己的回归头。在训练时，只使用 L1 回归损失对地面实况中心进行监督。</p><h3 id="Two-Stage-CenterPoint"><a href="#Two-Stage-CenterPoint" class="headerlink" title="Two-Stage CenterPoint"></a>Two-Stage CenterPoint</h3><p>第二阶段从骨干网的输出中提取额外的点特征。</p><p>我们从预测边界框的每个面的三维中心提取一个点特征。请注意，边界框中心、顶面和底面中心在地图视图中都投影到同一个点。</p><p>因此，我们只考虑四个朝外的方框面和预测的物体中心。对于每个点，我们使用双线性插值法从骨干地图视图输出 M 中提取特征。</p><p>第二阶段在单阶段 CenterPoint 预测结果的基础上，预测与类别无关的置信度得分和box refinement。</p><p>对于不区分类别的置信度得分预测，遵循的方法，使用得分目标 I，该目标由方框的 3D IoU 和相应的地面实况边界方框引导</p><script type="math/tex; mode=display">\begin{equation}I=\min(1,\max(0,2\times IoU_t-0.5))\end{equation}</script><p>IoUt 是第 t 个建议框与gt bbox之间的 IoU</p><script type="math/tex; mode=display">\begin{equation}L_{score}=-I_t\log(\hat{I}_t)-(1-I_t)\log(1-\hat{I}_t)\end{equation}</script><p>在推理过程中，<strong>直接使用单阶段中心点的类别预测，并以两个分数的几何平均值计算最终置信度分数</strong></p><script type="math/tex; mode=display">\begin{equation}\hat{Q_t}=\sqrt{\hat{Y_t}*\hat{I_t}}\end{equation}</script><p>其中 ˆ Qt 是对象 t 的最终预测置信度，ˆ Yt = max0≤k≤K ˆ Yp,k 和 ˆ It 分别是对象 t 的第一阶段和第二阶段置信度。</p><p>对于<strong>bbox回归</strong>，模型<strong>在第一阶段建议的基础上预测细化，用 L1 损失来训练模</strong>型。</p><h2 id="SECOND-Sparsely-Embedded-Convolutional-Detection-2018"><a href="#SECOND-Sparsely-Embedded-Convolutional-Detection-2018" class="headerlink" title="SECOND: Sparsely Embedded Convolutional Detection 2018"></a>SECOND: Sparsely Embedded Convolutional Detection 2018</h2><h1 id="PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018"><a href="#PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018" class="headerlink" title="PointPillars: Fast Encoders for Object Detection from Point Clouds 2018"></a>PointPillars: Fast Encoders for Object Detection from Point Clouds 2018</h1><h1 id="PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019"><a href="#PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019" class="headerlink" title="PIXOR: Real-time 3D Object Detection from Point Clouds 2019"></a>PIXOR: Real-time 3D Object Detection from Point Clouds 2019</h1><h2 id="Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021"><a href="#Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021" class="headerlink" title="Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021"></a>Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021</h2><h2 id="CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021"><a href="#CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021" class="headerlink" title="CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021"></a>CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021</h2><h2 id="协同感知-3D检测任务"><a href="#协同感知-3D检测任务" class="headerlink" title="协同感知 3D检测任务"></a>协同感知 3D检测任务</h2><p>综述</p><h3 id="Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges"><a href="#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges" class="headerlink" title="Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges"></a>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</h3><p>协作感知对于解决自动驾驶中的遮挡和传感器故障问题至关重要。</p><p>自动驾驶感知可分为<strong>个体感知和协作感知</strong>。虽然个体感知随着深度学习的发展取得了长足的进步，但一些问题也限制了其发展。首先，<strong>个体感知在感知相对全面的环境时经常会遇到遮挡</strong>。其次，<strong>车载传感器在感知远处物体时存在物理限制</strong>。此外，<strong>传感器噪音也会降低感知系统的性能</strong>。为了弥补个体感知的不足，协作或合作感知利用了多个代理之间的互动，受到了广泛关注。</p><p>协同感知是一种多agent系统，其中agent共享感知信息，以克服自我视听的视觉局限。在单个感知场景中，自我视听只能检测到附近物体的部分遮挡和远处稀疏的点云。在协作感知场景中，ego AV通过接收其他agent的信息来扩大视野。<strong>通过这种协作方式，ego AV不仅能检测到远处和被遮挡的物体，还能提高在密集区域的检测精度</strong>。</p><p><img data-src="https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png" alt="image-20231122203934187"></p><p>长期以来，协作感知一直是人们关注的焦点。之前的工作专注于构建协作感知系统，以评估该技术的可行性。然而，由<strong>于缺乏大型公共数据集，它没有得到有效的推进</strong>。近年来，随着深度学习的发展和大规模协作感知数据集的公众关注和研究激增。</p><p><strong>考虑到通信中的带宽限制，大多数研究人员致力于设计新颖的协作模块，以实现精度和带宽之间的权衡</strong>。</p><p>在协作感知场景中，自我 AV 通过接收来自其他智能体的信息来扩展视野。通过这种协作方式，自我AV不仅可以检测远处和被遮挡的物体，还可以提高密集区域的检测精度。</p><p>为了总结这些技术和问题，我们回顾了自动驾驶中的协同感知方法，并从方法、数据集和挑战方面对近年来的进展进行了全面综述。我们还注意到近年来发表了一些关于协作感知的综述。</p><h3 id="Collaboration-scheme"><a href="#Collaboration-scheme" class="headerlink" title="Collaboration scheme"></a>Collaboration scheme</h3><h4 id="早期融合"><a href="#早期融合" class="headerlink" title="早期融合"></a>早期融合</h4><p>早期协作在网络输入端采用原始数据融合，也称为数据级或低级融合</p><p>因此，早期协作可以从根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效。</p><p>在自动驾驶场景中，自我车辆接收并转换来自其他智能体的原始传感器数据，然后聚合车载转换后的数据。原始数据包含最全面的信息和实质性的代理描述。因此，早期协作可以从<strong>根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效</strong>。</p><p>考虑到早期协作的高带宽，一些工作提出了中间协作感知方法来平衡性能-带宽的权衡。在中间协作中，其他智能体通常会将深层语义特征转移到自我载体。自我车辆融合特征以做出最终预测。中间协作已成为最流行的多智能体协作感知灵活性选择。然而，特征提取往往会造成信息丢失和不必要的信息冗余，这促使人们探索合适的特征选择和融合策略。</p><h4 id="中期"><a href="#中期" class="headerlink" title="中期"></a>中期</h4><p>考虑到早期协作的高带宽，一些研究提出了中间协作感知方法，以平衡性能与带宽之间的权衡。在中间协作中，其他代理通常会将深层语义特征传输给自我车辆。</p><h3 id="晚期"><a href="#晚期" class="headerlink" title="晚期"></a>晚期</h3><p>后期或对象级协作在网络输出端采用预测融合。每个代理单独训练网络并相互共享输出。自我车辆在空间上转换输出，并在后处理后合并所有输出。后期协作比早期和中期协作更节省带宽，也更简单。然而，后期的合作也有局限性。由于<strong>单个输出可能是嘈杂和不完整的，因此后期协作总是具有最差的感知性能</strong>。</p><h3 id="原始数据融合-Raw-Data-Fusion"><a href="#原始数据融合-Raw-Data-Fusion" class="headerlink" title="原始数据融合(Raw Data Fusion)"></a>原始数据融合(Raw Data Fusion)</h3><p>早期协作在<strong>输入阶段采用原始数据融合。由于点云是不规则的，可以直接汇总</strong>，因此早期的协同工作通常采用<strong>点云融合策略</strong>。</p><p>第一个早期的协同感知系统 Cooper<strong>选择激光雷达数据</strong>作为融合目标。只需提取位置坐标和反射值，就能将点云压缩成较小的尺寸。在代理之间进行交互后，Cooper 利用变换矩阵重构接收到的点云，然后将自我点云集concat起来，进行最终预测。</p><h3 id="customized-communication-mechanism"><a href="#customized-communication-mechanism" class="headerlink" title="customized communication mechanism"></a>customized communication mechanism</h3><p>早期协作中的原始数据融合拓宽了自我飞行器的视野，也<strong>造成了高带宽压力</strong>。为了缓解上述问题，<strong>越来越多的工作 发展了中间协作</strong>。</p><p>最初的中间协作方法<strong>遵循一种贪婪的通信机制，以获取尽可能多的信息。一般来说，它们会与通信范围内的所有代理共享信息，并将压缩后的完整特征图放入集体感知信息（CPM,collective perception message）中</strong>。然而，由于特征稀疏和代理冗余，贪婪通信可能会极大地浪费带宽。</p><p>Who2com 建立了首个带宽限制下的通信机制，通过三阶段握手实现。具体来说，<strong>Who2com 使用一般注意力函数计算代理之间的匹配分数，并选择最需要的代理，从而有效减少带宽</strong>。</p><p>在 Who2com 的基础上，When2com<strong>引入了缩放一般注意力来决定何时与他人交流</strong>。这样，自我代理只有在信息不足时才会与他人交流，从而有效地节省了协作资源。</p><p>除了选择合适的通信代理外，<strong>通信内容对于减少带宽压力也很重要</strong>。FPVRCNN 中提出了初始特征选择策略.具体来说，FPV-RCNN 采用检测头生成proposals，并只选择proposals中的特征点。</p><p><strong>关键点选择模块减少了共享深度特征的冗余，为初始proposals提供了有价值的补充信息。</strong></p><p>Where2comm 也提出了一种新颖的空间信心感知通信机制。其核心思想是<strong>利用空间置信度图来决定共享特征和通信目标</strong>。<strong>在特征选择阶段</strong>，<strong>Where2comm 选择并传输满足高置信度和其他agent请求的空间元素</strong>。在<strong>agent选择阶段，自我代理只与能提供所需特征的代理通信。通过发送和接收感知关键区域的特征，Where2comm 节省了大量带宽，并显著提高了协作效率</strong>。</p><h3 id="Feature-Fusion"><a href="#Feature-Fusion" class="headerlink" title="Feature Fusion"></a>Feature Fusion</h3><blockquote><p>Feature fusion module is crucial in intermediate collaboration. After receiving CPMs from other agents, the ego vehicle can <strong>leverage different strategies to aggregate these features</strong>.</p></blockquote><p>可行的融合策略能够捕捉特征之间的潜在关系，提高感知网络的性能。根据基于特征融合的思想，我们将现有的特征融合方法分为传统融合、基于图的融合和基于注意力的融合。</p><h4 id="传统融合"><a href="#传统融合" class="headerlink" title="传统融合"></a>传统融合</h4><p>在协同感知研究的早期阶段，研究人员倾向于使用传统的策略来融合特征，如concat、求和和线性加权。中级协作将这些不变的置换操作应用于深度特征，因其简单性而实现了快速推理。</p><p>第一个中间协同感知框架 FCooper<strong>提取了低级体素和深度空间特征</strong>。基于这两级特征，F-Cooper 提出了两种特征融合策略：<strong>体素特征融合</strong>（VFF）和<strong>空间特征融合</strong>（SFF）。</p><p>这两种方法都采用<strong>元素最大值（element-wise maxout）来融合重叠区域的特征</strong>。由于<strong>体素特征更接近原始数据，因此 VFF 与原始数据融合方法一样能够进行近距离物体检测</strong>。同时，SFF 也有其优势。</p><p>受 SENet的启发，SFF 选择选择部分信道来减少传输时间消耗，同时保持可比的检测精度</p><p>考虑到 F-Coope<strong>r忽略了低置信度特征的重要性</strong>，Guo 等人提出了 CoFF 来改进 F-Cooper。<strong>CoFF 通过测量重叠特征的相似度和重叠面积对其进行加权。相似度越小，距离越大，邻近特征提供的补充信息就越直观。</strong></p><p>此外，还添加了一个增强参数，以提高弱特征的值。</p><p>实验表明，简单而高效的设计使 CoFF 大大提高了 F-Cooper 的性能。</p><p>传统的融合方法虽然简单，但并没有被最近的方法所抛弃。Hu 等人提出了协作式纯相机三维检测（CoCa3D），证明了协作在增强基于相机的三维检测方面的潜力。</p><h4 id="图融合"><a href="#图融合" class="headerlink" title="图融合"></a>图融合</h4><p>基于图的融合：尽管传统的中间融合很简单，但它们忽略了多方agent之间的潜在关系，无法推理从发送方到接收方的信息。图神经网络（GNN）能够传播和聚合来自邻居的信息，最近的研究表明，图神经网络在感知和自动驾驶方面非常有效。</p><p>V2VNet 首先利用空间感知图神经网络（GNN）对代理之间的通信进行建模,在 GNN 信息传递阶段，V2VNet 利用变分图像压缩算法来压缩特征。在跨车辆聚合阶段，V2VNet 首先补偿时间延迟，为每个节点创建初始状态，然后对从邻近代理到自我车辆的压缩特征进行扭曲和空间变换，所有这些操作都在重叠视场中(overlapping fields of view)进行。在特征融合阶段，V2VNet 采用平均运算来聚合特征，并利用卷积门控递归单元（ConvGRU）更新节点状态。虽然 V2VNet与 GNN 相比性能有所提高，但标量值协作权重无法反映不同空间区域的重要性。受此启发，DiscoNet 提出使用矩阵值边缘权重来捕捉高分辨率的代理间注意力。在信息传递过程中，DiscoNet 将特征串联起来，并为特征图中的每个元素应用矩阵值边缘权重。此外，DiscoNet 还将早期融合和中期融合结合在一起，通过对特征图中的每个元素应用矩阵值边缘权重。zhou 等人提出了另一种基于 GNN 的广义感知框架 MP-Pose。在信息传递阶段，MP-Pose 利用空间编码网络编码相对空间关系，而不是直接扭曲特征。受图形注意网络（GAT）的启发，MP-Pose 进一步使用动态交叉注意编码网络来捕捉代理之间的关系，并像 GAT 一样聚合多个特征。</p><h4 id="Attention-based"><a href="#Attention-based" class="headerlink" title="Attention-based"></a>Attention-based</h4><p>除了图形学习，注意力机制也已成为探索特征关系的有力工具.注意机制可根据数据域分为<strong>通道注意、空间注意和通道与空间注意</strong></p><p>在过去的十年中，<strong>注意力机制在计算机视觉领域发挥了越来越重要的作用 ，并激发了协作感知研究</strong>。</p><p>为了捕捉特征图中特定区域之间的相互作用，Xu 等人提出了 AttFusion，并首先在准确的空间位置采用自注意操作。具体来说，<strong>AttFusion 引入了单头自注意融合模块，与传统方法 F-Cooper和基于图的方法 DiscoNet相比，实现了性能和推理速度之间的平衡</strong>。</p><p>除了传统的基于注意力的方法，基于transformer的方法也能激发协作感知。Cui 等人提出了基于点transformer的 COOPERNAUT，这是一种用于点云处理的自注意力网络。</p><p>接收到信息后，ego agent会使用下采样块和点transformer block来聚合点特征。这两种操作<strong>都保持了信息的排列不变性</strong>。更重要的是，COOPERNAUT <strong>将协同感知与控制决策相结合，这对自动驾驶的模块联动具有重要意义</strong></p><p>与 V2V 协作相比，<strong>V2I 可以利用大量基础设施提供更稳定的协作信息，但目前很少有研究关注这一场景</strong>。</p><p>Xu 等人提出了首个统一转换器架构（V2X-ViT），它同时涵盖了 V2V 和 V2I。为了在不同类型的agent之间建立互动模块，V2X-ViT 提出了一个新颖的异构多代理关注模块（HMSA）来学习 V2V 和 V2I 之间的不同关系。此外，还引入了多尺度窗口注意模块（MSwin），以捕捉高分辨率检测中的长距离空间交互。</p><p>定制损失函数：虽然 V2V 通信为自我车辆提供了相对丰富的感知视野，但共享信息的冗余性和不确定性带来了新的挑战。</p><p>以往的协作感知研究大多侧重于<strong>协作效率和感知性能</strong>，但所有这些方法都假设了完美的条件。在现实世界的自动驾驶场景中，通信系统可能存在以下问题</p><p>1) 定位错误；2) 通信延迟和中断；3) 模型或任务差异；4) 隐私和安全问题</p><h4 id="协同感知数据集"><a href="#协同感知数据集" class="headerlink" title="协同感知数据集"></a>协同感知数据集</h4><ul><li><p>V2X-Sim是一个<strong>全面的模拟多代理感知数据集</strong>。它<strong>由交通模拟 SUMO  和 CARLA 模拟器生成</strong>，数据格式遵循 nuScenes 。V2X-Sim 配备了 RGB 摄像头、激光雷达、GPS 和 IMU，收集了 100 个场景共 10,000 个帧，每个场景包含 2-5 辆车。V2X-Sim 中的帧分为 8,000/1,000/1,000 帧，用于训练/验证/测试。V2X-Sim 的基准支持三个关键的感知任务：检测、跟踪和分割，需要注意的是，所有任务都采用鸟瞰（BEV）表示法，并以二维 BEV 生成结果。</p></li><li><p>OPV2V：O<strong>PV2V是另一个针对V2V通信的模拟协同感知数据集</strong>，它是<strong>通过协同模拟框架OpenCDA和CARLA模拟器收集的</strong>。整个数据集可通过提供的配置文件进行重现。OPV2V 包含 11,464 帧激光雷达点和 RGB 摄像机。OPV2V 的一个显著特点是提供了一个名为 “卡尔弗城 “的仿真测试集，可用于评估模型的泛化能力。其基准支持三维物体检测和 BEV 语义分割，目前只包含一种类型的物体（车辆）。</p></li><li><strong>V2XSet 是一个大规模的 V2X 感知开放模拟数据集</strong>。该数据集格式与 OPV2V类似，共有 11,447 个帧。与 V2X 协作数据集 V2X-Sim和 V2I 协作数据集 DAIR-V2X相比，V2XSet 包含更多场景，并且该基准考虑了不完美的真实世界条件。该基准支持 3D 物体检测和 BEV 分割，有两种测试设置（完美和嘈杂）供评估。</li><li>DAIR-V2X：作为<strong>第一个来自真实场景的大规模 V2I 协同感知数据集</strong>，DAIR-V2X [对自动驾驶的协同感知意义重大。DAIR-V2X-C 集可用于研究 V2I 协作，VIC3D 基准可用于探索 V2I 物体检测任务。与主要关注激光雷达点的 V2X-Sim和 V2XSet不同，VIC3D 物体检测基准同时提供了基于图像和基于激光雷达点的协作方法。</li><li>V2V4Real：V2V4Real 是<strong>首个大规模真实世界多模式 V2V 感知数据集</strong>，由俄亥俄州哥伦布市的一辆特斯拉汽车和一辆福特 Fusion 汽车收集而成，覆盖 410 公里的道路。该数据集包含 20,000 个 LiDAR 帧和超过 240,000 个三维边界框注释，涉及五个不同的车辆类别。此外，V2V4Real 还提供了三个合作感知任务的基准，包括三维物体检测、物体跟踪和域适应。</li></ul><p>TODO:</p><p>3D目标检测</p><ul><li>PointNet,PointNet++</li><li>MV3D,AVOD</li><li>SECOND,PointPillars,PIXOR</li></ul><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h4 id="KITTI"><a href="#KITTI" class="headerlink" title="KITTI"></a>KITTI</h4><h4 id="nuScenes"><a href="#nuScenes" class="headerlink" title="nuScenes"></a>nuScenes</h4><p>nuScenes数据集（发音为/nuõsiõnz/）是Motional（前身为nuTonomy）团队开发的一个用于<strong>自动驾驶的公共大规模数据集</strong>。Motional正在使无人驾驶汽车成为一种安全、可靠和可访问的现实。通过向公众发布我们的一部分数据，Motional旨在支持公众对计算机视觉和自动驾驶的研究。</p><p>为此，我们收集了波士顿和新加坡的1000个驾驶场景，这两个城市以交通密集和极具挑战性的驾驶环境而闻名。20秒长的场景是手动选择的，以展示一组多样而有趣的驾驶动作、交通状况和意外行为。nuScenes的丰富复杂性将鼓励开发能够在每个场景有几十个物体的城市地区安全驾驶的方法。收集不同大陆的数据进一步使我们能够研究计算机视觉算法在不同地点、天气条件、车辆类型、植被、道路标记以及左右交通中的通用性。</p><p>所有检测结果均按照平均精度 (mAP)、平均平移误差 (mATE)、平均比例误差 (mASE)、平均方向误差 (mAOE)、平均速度误差 (AVE)、平均属性误差 (AAE) 和 nuScenes 检测得分 (NDS) 进行评估。</p><h4 id="The-Waymo-opendataset"><a href="#The-Waymo-opendataset" class="headerlink" title="The Waymo opendataset"></a>The Waymo opendataset</h4><h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><ol><li><a href="https://paperswithcode.com/task/3d-object-detection">3D Object Detection | Papers With Code</a></li><li><a href="https://github.com/patrick-llgc/Learning-Deep-Learning">patrick-llgc/Learning-Deep-Learning: Paper reading notes on Deep Learning and Machine Learning (github.com)</a></li><li><a href="https://www.stereolabs.com/docs/object-detection/">3D Object Detection Overview | Stereolabs</a></li><li><a href="https://zhuanlan.zhihu.com/p/591349104">系列二：3D Detection目标检测系列论文总结（2023年更） - 知乎 (zhihu.com)</a></li><li>3D点云<a href="https://github.com/HuangCongQing/3D-Point-Clouds">HuangCongQing/3D-Point-Clouds: 🔥3D点云目标检测&amp;语义分割(深度学习)-SOTA方法,代码,论文,数据集等 (github.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Deep Learning" scheme="https://www.sekyoro.top/tags/Deep-Learning/"/>
    
    <category term="3D Object Detection" scheme="https://www.sekyoro.top/tags/3D-Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>后台执行:从nohup到tmux</title>
    <link href="https://www.sekyoro.top/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/"/>
    <id>https://www.sekyoro.top/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/</id>
    <published>2023-10-26T10:18:34.000Z</published>
    <updated>2023-10-26T12:36:29.012Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>远程连接到Linux服务器运行命令后,后面如果直接关掉终端会影响程序运行.这时可以使用nohup等命令用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行.<br><span id="more"></span></p><h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h3><blockquote><p><strong>nohup</strong> 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。</p><p><strong>nohup</strong> 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 <strong>$HOME/nohup.out</strong> 文件中</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python myprogramm.py &gt; output.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep <span class="string">&quot;python&quot;</span> </span><br><span class="line"><span class="comment"># 查看相关进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 &lt;pid&gt;</span><br><span class="line"><span class="comment"># 如果要关闭该进程</span></span><br></pre></td></tr></table></figure><blockquote><p>使用nohup启动的程序会忽略hangup信号，hangup只是终止信号的一种，但是在关闭终端时，还会有其他的终止的信号，所以这时候往往需要配合 &amp; 一起使用，这样就可以做到不管是我们主动或者意外断开终端，程序依然能够继续运行。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python test.py &gt; output.txt &amp;</span><br></pre></td></tr></table></figure><h3 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h3><p>光是nohup功能太少了</p><blockquote><p>screen命令用于多重视窗管理程序。</p><p>screen为多重视窗管理程序。此处所谓的视窗，是指一个全屏幕的文字模式画面。通常只有在使用telnet登入主机或是使用老式的终端机时，才有可能用到screen程序</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install screen </span><br></pre></td></tr></table></figure><p>常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">screen -S <span class="comment">#创建新会话</span></span><br><span class="line">screen -ls <span class="comment">#显示所有会话</span></span><br><span class="line">screen -r <span class="comment">#进入该会话</span></span><br><span class="line">screen -d <span class="comment">#退出该会话 或者ctrl+a d</span></span><br><span class="line">screen -S screen_name -X quit <span class="comment">#删除会话</span></span><br><span class="line">screen -R 　<span class="comment">#先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业</span></span><br></pre></td></tr></table></figure><h3 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h3><p>tmux应该是用得比较多的工具了,不过操作还要复杂一些,支持分屏,分窗口以及类似screen得分session.</p><blockquote><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次”会话”（session）</strong> </p><p>会话的一个重要特点是，窗口与其中启动的进程是<a href="https://www.ruanyifeng.com/blog/2016/02/linux-daemon.html">连在一起</a>的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p><p>一个典型的例子就是，<a href="https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html">SSH 登录</a>远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了</p></blockquote><p>tmux是一个 terminal multiplexer（终端复用器），它可以启动一系列终端会话。它解绑了会话和终端窗口。关闭终端窗口再打开，会话并不终止，而是继续运行在执行。将会话与终端窗后彻底分离。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure><h4 id="会话管理"><a href="#会话管理" class="headerlink" title="会话管理"></a>会话管理</h4><p>第一个启动的 Tmux 窗口，默认编号是<code>0</code>，第二个窗口的编号是<code>1</code>，以此类推。这些窗口对应的会话，就是 0 号会话、1 号会话。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s &lt;session-name&gt;</span><br><span class="line">tmux detach <span class="comment"># 分离会话 或者ctrl+b d</span></span><br><span class="line">tmux ls<span class="comment">#查看会话</span></span><br><span class="line">tmux attach -t [0|&lt;session_name&gt;] <span class="comment">#使用会话</span></span><br><span class="line">tmux kill-session -t [0|&lt;session_name&gt;] <span class="comment"># 删除会话</span></span><br><span class="line">tmux switch -t [0|&lt;session_name&gt;] <span class="comment"># 切换会话</span></span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b d</code>：分离当前会话。</li><li><code>Ctrl+b s</code>：列出所有会话。</li><li><code>Ctrl+b $</code>：重命名当前会话。</li></ul><h4 id="窗口管理"><a href="#窗口管理" class="headerlink" title="窗口管理"></a>窗口管理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tmux new-window -n &lt;window-name&gt; <span class="comment">#新建一个指定名称的窗口</span></span><br><span class="line">tmux select-window -t [&lt;window-name&gt;|&lt;window-number&gt;] <span class="comment">#选择窗口</span></span><br><span class="line">tmux rename-window &lt;new-name&gt; <span class="comment">#重命名当前window</span></span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b c</code>：创建一个新窗口，状态栏会显示多个窗口的信息。</li><li><code>Ctrl+b p</code>：切换到上一个窗口（按照状态栏上的顺序）。</li><li><code>Ctrl+b n</code>：切换到下一个窗口。</li><li><code>Ctrl+b &lt;number&gt;</code>：切换到指定编号的窗口，其中的<code>&lt;number&gt;</code>是状态栏上的窗口编号。</li><li><code>Ctrl+b w</code>：从列表中选择窗口。</li><li><code>Ctrl+b ,</code>：窗口重命名。</li></ul><h4 id="窗格管理"><a href="#窗格管理" class="headerlink" title="窗格管理"></a>窗格管理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分上下两个窗格</span></span><br><span class="line">tmux split-window</span><br><span class="line"><span class="comment"># 划分左右两个窗格</span></span><br><span class="line">tmux split-window -h</span><br><span class="line"><span class="comment"># 当前窗格上移</span></span><br><span class="line">tmux swap-pane -U</span><br><span class="line"><span class="comment"># 当前窗格下移</span></span><br><span class="line">tmux swap-pane -D</span><br><span class="line"><span class="comment"># 光标切换到上方窗格</span></span><br><span class="line">tmux select-pane -U</span><br><span class="line"><span class="comment"># 光标切换到下方窗格</span></span><br><span class="line">tmux select-pane -D</span><br><span class="line"><span class="comment"># 光标切换到左边窗格</span></span><br><span class="line">tmux select-pane -L</span><br><span class="line"><span class="comment"># 光标切换到右边窗格</span></span><br><span class="line">tmux select-pane -R</span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b %</code>：划分左右两个窗格。</li><li><code>Ctrl+b &quot;</code>：划分上下两个窗格。</li><li><code>Ctrl+b &lt;arrow key&gt;</code>：光标切换到其他窗格。<code>&lt;arrow key&gt;</code>是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键<code>↓</code>。</li><li><code>Ctrl+b ;</code>：光标切换到上一个窗格。</li><li><code>Ctrl+b o</code>：光标切换到下一个窗格。</li><li><code>Ctrl+b &#123;</code>：当前窗格与上一个窗格交换位置。</li><li><code>Ctrl+b &#125;</code>：当前窗格与下一个窗格交换位置。</li><li><code>Ctrl+b Ctrl+o</code>：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。</li><li><code>Ctrl+b Alt+o</code>：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。</li><li><code>Ctrl+b x</code>：关闭当前窗格。</li><li><code>Ctrl+b !</code>：将当前窗格拆分为一个独立窗口。</li><li><code>Ctrl+b z</code>：当前窗格全屏显示，再使用一次会变回原来大小。</li><li><code>Ctrl+b Ctrl+&lt;arrow key&gt;</code>：按箭头方向调整窗格大小。</li><li><code>Ctrl+b q</code>：显示窗格编号。</li></ul><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有快捷键，及其对应的 Tmux 命令</span></span><br><span class="line">$ tmux list-keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有 Tmux 命令及其参数</span></span><br><span class="line">$ tmux list-commands</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出当前所有 Tmux 会话的信息</span></span><br><span class="line">$ tmux info</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载当前的 Tmux 配置</span></span><br><span class="line">$ tmux source-file ~/.tmux.conf</span><br></pre></td></tr></table></figure><p>我是推荐用screen或者tmux的,有条件用tmux.</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://blog.csdn.net/afterlake/article/details/105826424">nohup · VS · screen_screen nohup-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/107802400">使用 screen 代替 nohup - 知乎 (zhihu.com)</a></li><li><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">Tmux 使用教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/98384704">tmux使用教程 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/NSJim/article/details/127754413">Tmux使用教程_tmux 新建窗口-CSDN博客</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;远程连接到Linux服务器运行命令后,后面如果直接关掉终端会影响程序运行.这时可以使用nohup等命令用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>DDNLP:深入NLP</title>
    <link href="https://www.sekyoro.top/2023/10/23/DDNLP-%E6%B7%B1%E5%85%A5NLP/"/>
    <id>https://www.sekyoro.top/2023/10/23/DDNLP-%E6%B7%B1%E5%85%A5NLP/</id>
    <published>2023-10-23T02:31:33.000Z</published>
    <updated>2023-11-19T05:49:44.922Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前学过一段时间NLP,因为其中涉及到一些深度学习常用的知识或者框架,但苦于不系统以及没有任务focus不能长久.这里借助微软的教程写点东西.<br><span id="more"></span></p><h2 id="tokenization-amp-amp-representation"><a href="#tokenization-amp-amp-representation" class="headerlink" title="tokenization&amp;&amp;representation"></a>tokenization&amp;&amp;representation</h2><p>将一句话中的单词分割就是分词(tokenization),英文分词比较简单.中文就比较麻烦了.需要把握分词的粒度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = torchtext.data.utils.get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">tokenizer(<span class="string">&#x27;He said: hello&#x27;</span>)</span><br></pre></td></tr></table></figure><p>分词之后就需要表示每个分词的含义了,<strong>需要某种方式将文本表示为张量</strong>.可以分为</p><ul><li>字符级表示(<strong>Character-level representation</strong>),当我们通过将每个字符视为一个数字来表示文本时。鉴于我们的文本语料库中有 C (如果是英语也就26个字符)不同的字符，单词 Hello 将由 5xC 张量表示。每个字母将对应于一个独热编码中的张量列。</li><li>单词级表示(<strong>Word-level representation</strong>),其中我们创建文本中所有单词的词汇表(<strong>vocabulary</strong> )，然后使用独热编码表示单词。这种方法在某种程度上更好，因为每个字母本身没有太多意义，因此通过使用更高层次的语义概念 - 单词 - 我们简化了神经网络的任务。但是，鉴于字典大小较大，我们需要处理高维稀疏张量。</li></ul><blockquote><p>无论表示方式如何，我们首先需要将文本转换为一系列标记(<strong>tokens</strong>)，一个标记是字符、单词，有时甚至是单词的一部分(也即是上面说的分词)</p><p>然后，我们将token转换为一个数字，通常使用词汇表(<strong>vocabulary</strong>)(也就是使用单词级表示)，并且可以使用独热编码(one-hot encoding)将这个数字输入神经网络。</p></blockquote><p>常用的方法包括BOW或者N-Grams</p><h4 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag-of-Words"></a>Bag-of-Words</h4><p>在解决文本分类等任务时，我们需要能够通过一个固定大小的向量来表示文本，我们将将其用作最终分类器的输入。</p><blockquote><p>最简单的方法之一是组合所有单独的单词表示，例如。通过添加它们。如果我们为每个单词添加独热编码，我们最终会得到一个频率向量，显示每个单词在文本中出现的次数。文本的这种表示称为词袋（BoW）</p><p>BoW 本质上表示文本中出现的单词和数量，这确实可以很好地指示文本的内容</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    counter.update(tokenizer(line))</span><br><span class="line">vocab = torchtext.vocab.vocab(counter, min_freq=<span class="number">1</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Vocab size if <span class="subst">&#123;vocab_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">stoi = vocab.get_stoi() <span class="comment"># dict to convert tokens to indices</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [stoi[s] <span class="keyword">for</span> s <span class="keyword">in</span> tokenizer(x)]</span><br><span class="line"></span><br><span class="line">encode(<span class="string">&#x27;I love to play with my words&#x27;</span>)</span><br><span class="line"></span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_bow</span>(<span class="params">text,bow_vocab_size=vocab_size</span>):</span></span><br><span class="line">    res = torch.zeros(bow_vocab_size,dtype=torch.float32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> encode(text):</span><br><span class="line">        <span class="keyword">if</span> i&lt;bow_vocab_size:</span><br><span class="line">            res[i] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(to_bow(train_dataset[<span class="number">0</span>][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>简单来说就是根据原本的语义资料,统计词频先建立一个counter,类似于一个字典,key是词,value是频次.根据counter(或者OrderDict)建立一个vocab. vocab建立一个词汇到index的一个字典,然后根据这个字典获得一个词的index,但是并直接使用index作为词的表示,而是使用类似one-hot encoding,出现了一个词,获取其index,再在一个大小为vocab_size的tensor上的index处加1,这样一个句子的BOW就有了.</p><p><img data-src="https://i.imgur.com/MXAQdkP.png" alt="image-20231023115954650"></p><p><img data-src="https://i.imgur.com/G7ll59u.png" alt="image-20231023115905862"></p><p>BoW 的问题在于某些常用词，例如 and、is 等出现在大多数文本中，并且它们的频率最高，掩盖了真正重要的单词。我们可以通过考虑单词在整个文档集合中出现的频率来降低这些单词的重要性。</p><h4 id="N-Grams"><a href="#N-Grams" class="headerlink" title="N-Grams"></a>N-Grams</h4><p>在自然语言中，单词的精确含义只能在上下文中确定。例如，神经网和钓鱼网.</p><p>一种解决办法是使用单词对(pairs of words)(也就是不使用单个单词而是多个单词,因为单个单词在不同语境下含义由差异),然后将单词对(pairs of words)视为单独的词汇标记。</p><p>这样相当于把一个句子的表示变多了,除了所有单个单词,还有单词对.</p><p>这种方法的问题在于字典大小显着增长，并且像go fishing和go shopping这样的组合由不同的标记呈现，尽管动词相同，但它们没有任何语义相似性。</p><blockquote><p>在某些情况下，我们也可以考虑使用三元语法 - 三个单词的组合。因此，这种方法通常被称为n-grams。此外，使用具有<strong>字符级表示的 n 元语法</strong>是有意义的，在这种情况下，n-gram 将大致对应于不同的音节。</p></blockquote><p>可以使用sklearn或者pytorch库,均能实现.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram_vectorizer = CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), token_pattern=<span class="string">r&#x27;\b\w+\b&#x27;</span>, min_df=<span class="number">1</span>)</span><br><span class="line">corpus = [</span><br><span class="line">        <span class="string">&#x27;I like hot dogs.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The dog ran fast.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Its hot outside.&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">bigram_vectorizer.fit_transform(corpus)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vocabulary:\n&quot;</span>,bigram_vectorizer.vocabulary_)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(bigram_vectorizer.vocabulary_))</span><br><span class="line">bigram_vectorizer.transform([<span class="string">&#x27;My dog likes hot dogs on a hot day.&#x27;</span>]).toarray()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/ES1UYjS.png" alt="image-20231023121801392"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    l = tokenizer(line)</span><br><span class="line">    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">bi_vocab = torchtext.vocab.vocab(counter, min_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bigram vocabulary length = &quot;</span>,<span class="built_in">len</span>(bi_vocab))</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/LvNDH2u.png" alt="image-20231023120608981"></p><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>在 BoW 表示中，无论单词本身如何，单词出现次数都是均匀加权的。但是，很明显，与专业术语相比，常用词（例如a，in等）对于分类的重要性要低得多。事实上，在大多数NLP任务中，有些单词比其他单词更相关。</p><p>TF-IDF 代表术语频率 – 反向文档频率。它是BOW的变体，其中使用浮点值而不是指示单词在文档中出现的二进制 0/1 值，这与语料库中单词出现的频率有关。</p><p>主要引入了document文档概念,如果一个词在多个文档中出现,那么其权重会降低.</p><p><img data-src="https://i.imgur.com/BLTg4Tp.png" alt="image-20231023120917811"></p><p>其中tf~ij~表示在j文档中i词出现的次数,N表示总文档数,df~i~表示出现i这个词的文档数.</p><p>这样就计算出了单个文档中词i的权重,这里的文档也可以是单个句子.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">corpus = [</span><br><span class="line">        <span class="string">&#x27;I like hot dogs.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The dog ran fast.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Its hot outside.&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">vectorizer = TfidfVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">vectorizer.fit_transform(corpus)</span><br><span class="line">vectorizer.transform([<span class="string">&#x27;My dog likes hot dogs on a hot day.&#x27;</span>]).toarray()</span><br></pre></td></tr></table></figure><p>这里结合了N-gram和TF-IDF. 由于其中使用了TfidfVectorizer,默认参数如下</p><p><img data-src="https://i.imgur.com/W998L4F.png" alt="image-20231023122145845"></p><p>将其中的<code>I,I like</code>去掉了,所以词汇表少了两个.此外sklearn库中的算法与上面的公式也不同.默认为log [ n / df(t) ] + 1(设置<code>smooth_idf=False</code>)</p><p>上面的方法对于句子中词的语义理解能力有限,而且通常维度是整个训练资料的vocab大小,维度高且稀疏.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>嵌入的想法是通过<strong>低维密集向量</strong>来表示单词，这在某种程度上<strong>反映了单词的语义</strong>含义。</p><p>也就是从上面简单的text representation中的vocab_size变为embedding_size,输出不是one hot encoding的高维向量了。</p><p>训练方式与BOW类似,但是需要填充.比如一个batch中有多个句子,每个句子长度不同,需要padding成这个batch中最大的句子的encode(就是计算BOW)长度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.fc = torch.nn.Linear(embed_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;after embedding&quot;</span>,x.shape)</span><br><span class="line">        x = torch.mean(x,dim=<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padify</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># b is the list of tuples of length batch_size</span></span><br><span class="line">    <span class="comment">#   - first element of a tuple = label,</span></span><br><span class="line">    <span class="comment">#   - second = feature (text sequence)</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># first, compute max length of a sequence in this minibatch</span></span><br><span class="line">    l = <span class="built_in">max</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of two tensors - labels and features</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=padify, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><blockquote><p>需要将所有序列填充到相同的长度，以便将它们放入小批量中。这不是表示可变长度序列的最有效方法.</p><p>另一种选择是使用偏移向量，这将保留存储在一个大向量中的所有序列的偏移量。</p></blockquote><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="constructor">EmbedClassifier(<span class="params">torch</span>.<span class="params">nn</span>.Module)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>, <span class="params">vocab_size</span>, <span class="params">embed_dim</span>, <span class="params">num_class</span>)</span>:</span><br><span class="line">        super<span class="literal">()</span>.<span class="constructor">__init__()</span></span><br><span class="line">        self.embedding = torch.nn.<span class="constructor">EmbeddingBag(<span class="params">vocab_size</span>, <span class="params">embed_dim</span>)</span></span><br><span class="line">        self.fc = torch.nn.<span class="constructor">Linear(<span class="params">embed_dim</span>, <span class="params">num_class</span>)</span></span><br><span class="line"></span><br><span class="line">    def forward(self, text, off):</span><br><span class="line">        x = self.embedding(text, off) <span class="comment">//它以内容向量和偏移向量为输入</span></span><br><span class="line">        return self.fc(x)</span><br><span class="line">        </span><br><span class="line">        def offsetify(b):</span><br><span class="line">    # first, compute data tensor from all sequences</span><br><span class="line">    x = <span class="literal">[<span class="identifier">torch</span>.<span class="identifier">tensor</span>(<span class="identifier">encode</span>(<span class="identifier">t</span>[<span class="number">1</span>]</span>)) <span class="keyword">for</span> t <span class="keyword">in</span> b]</span><br><span class="line">    # now, compute the offsets by accumulating the tensor <span class="keyword">of</span> sequence lengths</span><br><span class="line">    o = <span class="literal">[<span class="number">0</span>]</span> + <span class="literal">[<span class="identifier">len</span>(<span class="identifier">t</span>) <span class="identifier">for</span> <span class="identifier">t</span> <span class="identifier">in</span> <span class="identifier">x</span>]</span></span><br><span class="line">    o = torch.tensor(o<span class="literal">[:-<span class="number">1</span>]</span>).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    return (</span><br><span class="line">        torch.<span class="constructor">LongTensor([<span class="params">t</span>[0]-1 <span class="params">for</span> <span class="params">t</span> <span class="params">in</span> <span class="params">b</span>])</span>, # labels</span><br><span class="line">        torch.cat(x), # text</span><br><span class="line">        o</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.<span class="constructor">DataLoader(<span class="params">train_dataset</span>, <span class="params">batch_size</span>=16, <span class="params">collate_fn</span>=<span class="params">offsetify</span>, <span class="params">shuffle</span>=True)</span></span><br></pre></td></tr></table></figure><p>可以看到数据集多了一个数据,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">net = EmbedClassifier(vocab_size,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_emb</span>(<span class="params">net,dataloader,lr=<span class="number">0.01</span>,optimizer=<span class="literal">None</span>,loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>),epoch_size=<span class="literal">None</span>, report_freq=<span class="number">200</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(),lr=lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line">    total_loss,acc,count,i = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> labels,text,off <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        labels,text,off = labels.to(device), text.to(device), off.to(device)</span><br><span class="line">        out = net(text, off)</span><br><span class="line">        loss = loss_fn(out,labels) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss+=loss</span><br><span class="line">        _,predicted = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        acc+=(predicted==labels).<span class="built_in">sum</span>()</span><br><span class="line">        count+=<span class="built_in">len</span>(labels)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i%report_freq==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;count&#125;</span>: acc=<span class="subst">&#123;acc.item()/count&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch_size <span class="keyword">and</span> count&gt;epoch_size:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/count, acc.item()/count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_epoch_emb(net,train_loader, lr=<span class="number">4</span>, epoch_size=<span class="number">25000</span>)</span><br></pre></td></tr></table></figure><p>在前面的示例中，模型嵌入层学习将单词映射到向量表示，但是这种表示没有太多的语义意义。应该学到的是:相似的单词或同义词将对应于在某些向量距离（例如欧几里得距离）方面彼此接近的向量</p><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>为此，我们需要以特定方式在大量文本上预训练我们的嵌入模型。</p><p>训练语义嵌入的第一种方法称为Word2Vec。它基于两个主要体系结构,用于生成单词的分布式表示,包括COW和Skip-Ngram.</p><p><img data-src="https://i.imgur.com/pOIIEEj.png" alt="image-20231023164434940"></p><p>在CBOW，我们训练模型从周围上下文中预测单词。给定 ngram （W−2，W−1，W0，W1，W2），模型的目标是从 （W−2，W−1，W1，W2） 预测 W0。</p><h4 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h4><p>通过学习每个单词的向量表示以及每个单词中的字符 n 元语法来构建 Word2Vec。然后在每个训练步骤中将表示值平均为一个向量。虽然这为预训练增加了大量额外的计算，但它使词嵌入能够对子词信息进行编码。</p><h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p>GloVe利用分解共现矩阵( co-occurrence matrix)的思想，使用神经方法将共现矩阵分解为更具表现力和非线性的词向量。</p><p><img data-src="https://i.imgur.com/zKMz0Hk.png" alt="image-20231023203859837"></p><p>传统的预训练嵌入表示（如 Word2Vec）的一个关键限制是词义消歧问题。虽然预训练嵌入可以在上下文中捕获单词的某些含义，但单词的每个可能含义都编码到相同的嵌入中。这可能会导致下游模型中出现问题，因为许多单词（例如“play”）具有不同的含义，具体取决于它们使用的上下文。</p><p>为了克服这个限制，我们需要基于语言模型构建嵌入，该语言模型在大量文本语料库上进行训练，并且知道如何在不同上下文中将单词组合在一起(我的理解是相当于自己训练一个专注于自己下游任务的embedding)</p><h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>语言建模背后的主要思想是以<strong>无监督的方式在未标记的数据集上训练它们</strong>。这很重要，因为我们有大量的未标记文本可用，而标记文本的数量始终受到我们可以在标记上花费的工作量的限制。</p><blockquote><p>大多数情况下，我们可以构建可以<strong>预测文本中缺失单词的语言模型</strong>，因为很容易屏蔽文本中的随机单词并将其用作训练样本.</p></blockquote><p>为了建立一个网络来预测下一个单词，我们需要提供相邻单词作为输入，并获取单词编号作为输出。</p><p>CBoW网络的架构如下：</p><p>输入单词通过嵌入层传递。这个嵌入层将是我们的 Word2Vec 嵌入，因此我们将它单独定义为嵌入变量。在这个例子中，我们将使用嵌入大小 = 30，即使你可能想尝试更高的维度（真正的 word2vec 有 300）</p><p>然后，嵌入向量将被传递到预测输出字的线性层。因此它具有vocab_size神经</p><p><img data-src="https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png" alt="word2vec_skipgrams" style="zoom:67%;" /></p><p><img data-src="https://i.imgur.com/GgvayYN.png" alt="image-20231023195917251"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">ngrams = <span class="number">1</span>, min_freq = <span class="number">1</span>, vocab_size = <span class="number">5000</span> , lines_cnt = <span class="number">500</span></span>):</span></span><br><span class="line">    tokenizer = torchtext.data.utils.get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading dataset...&quot;</span>)</span><br><span class="line">    test_dataset, train_dataset  = torchtext.datasets.AG_NEWS(root=<span class="string">&#x27;./data&#x27;</span>)</span><br><span class="line">    train_dataset = <span class="built_in">list</span>(train_dataset)</span><br><span class="line">    test_dataset = <span class="built_in">list</span>(test_dataset)</span><br><span class="line">    classes = [<span class="string">&#x27;World&#x27;</span>, <span class="string">&#x27;Sports&#x27;</span>, <span class="string">&#x27;Business&#x27;</span>, <span class="string">&#x27;Sci/Tech&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Building vocab...&#x27;</span>)</span><br><span class="line">    counter = collections.Counter()</span><br><span class="line">    <span class="keyword">for</span> i, (_, line) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=ngrams))</span><br><span class="line">        <span class="keyword">if</span> i == lines_cnt:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    vocab = torchtext.vocab.Vocab(collections.Counter(<span class="built_in">dict</span>(counter.most_common(vocab_size))))</span><br><span class="line">    <span class="keyword">return</span> train_dataset, test_dataset, classes, vocab, tokenizer</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">x, vocabulary, tokenizer = tokenizer</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [vocabulary[s] <span class="keyword">for</span> s <span class="keyword">in</span> tokenizer(x)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_cbow</span>(<span class="params">sent,window_size=<span class="number">2</span></span>):</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(sent):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>,i-window_size),<span class="built_in">min</span>(i+window_size+<span class="number">1</span>,<span class="built_in">len</span>(sent))):</span><br><span class="line">            <span class="keyword">if</span> i!=j:</span><br><span class="line">                res.append([sent[j],x])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(to_cbow([<span class="string">&#x27;I&#x27;</span>,<span class="string">&#x27;like&#x27;</span>,<span class="string">&#x27;to&#x27;</span>,<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;networks&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(to_cbow(encode(<span class="string">&#x27;I like to train networks&#x27;</span>, vocab)))</span><br></pre></td></tr></table></figure><p>在设计数据集的时候,得到的就是例如[2,3],[4,3],其中3是预测的词,2,4是其周围的词,这样也不需要padding了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleIterableDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleIterableDataset).__init__()</span><br><span class="line">        self.data = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            self.data.append( (Y[i], X[i]) )</span><br><span class="line">        random.shuffle(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(self.data)</span><br><span class="line"></span><br><span class="line">ds = SimpleIterableDataset(X, Y)</span><br><span class="line">dl = torch.utils.data.DataLoader(ds, batch_size = <span class="number">256</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span>(<span class="params">net, dataloader, lr = <span class="number">0.01</span>, optimizer = <span class="literal">None</span>, loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>), epochs = <span class="literal">None</span>, report_freq = <span class="number">1</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(), lr = lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        total_loss, j = <span class="number">0</span>, <span class="number">0</span>, </span><br><span class="line">        <span class="keyword">for</span> labels, features <span class="keyword">in</span> dataloader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            features, labels = features.to(device), labels.to(device)</span><br><span class="line">            out = net(features)</span><br><span class="line">            loss = loss_fn(out, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % report_freq == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: loss=<span class="subst">&#123;total_loss.item()/j&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/j</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/0xvQ6IJ.png" alt="image-20231023201214794"></p><p>关键是生成了一堆数据,句子中的某个单词由周围N个单词生成(CBOW).模型是简单的embedding层加一个全连接,输出特征大小是vocab_size,用softmax损失,最后就能无监督训练得到一个embedding层.</p><h2 id="RNN-Recurrent-Neural-Networks"><a href="#RNN-Recurrent-Neural-Networks" class="headerlink" title="RNN(Recurrent Neural Networks)"></a>RNN(Recurrent Neural Networks)</h2><blockquote><p>之前直接使用的是全连接层,这种架构的作用是捕获句子中单词的聚合含义，但它没有考虑单词的顺序，因为嵌入之上的聚合操作从原始文本中删除了此信息。由于这些模型无法对单词排序进行建模，因此它们无法解决更复杂或模糊的任务，例如文本生成或问答。</p></blockquote><p>给定标记 X~0~,…,X~n~ 的输入序列，RNN 创建一个神经网络块序列，并使用反向传播端到端地训练该序列。每个网络块将一对（X~i~，S~i~）作为输入，并产生S~i+1~。最终状态 S~n~ 或（输出 Y~n~）进入线性分类器以产生结果。所有网络块共享相同的权重，并使用一个反向传播通道进行端到端训练。</p><p>为了捕捉文本序列的含义，我们需要使用另一种神经网络架构，称为递归神经网络或RNN。在 RNN 中，我们通过<strong>网络一次传递一个符号，网络产生一些状态，然后我们用下一个符号再次传递给网络</strong>。</p><p><img data-src="https://i.imgur.com/FwxIpWX.png" alt="image-20231023222833797"></p><p>pytorch中普通RNN隐状态通过了tanh激活,每一层的隐状态与输出是一样.</p><p>RNN循环网络是每次拿每个batch中的一个sequence中的一个,大小是embed_size(或者直接是one-hot编码的vacab_size,同时可以输入一个初始状态,shape是hidden_size,然后两个矩阵分别是(embed_size,hidden_size),(hidden_size,hidden_size),其实就是连个全连接然后直接concat通过激活函数,这就是简单的RNN,),</p><p><img data-src="https://i.imgur.com/5uUeVVj.png" alt="image-20231023224359316"></p><p>对于一个句子的数据,X是(seq_length,embedding_size),权重W是(embedding_size,hidden_dim),H是(hidden_dim,hidden_dim),S是(seq_length,hidden_dim),S是上一层的输出,也就是W×X~i~+H×S~i-1~+b.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/images/rnn.png" alt="RNN"></p><p>由于状态向量 S0,…,Sn 通过网络传递，因此它能够学习单词之间的顺序依赖关系。例如，当单词没有出现在序列中的某个地方时，它可以学习否定状态向量中的某些元素，从而导致否定.</p><p><strong>RNN内部结构</strong></p><p><img data-src="https://i.imgur.com/dk3vOfq.png" alt="image-20231023212717744"></p><p>简单的RNN接受先前的状态 S~i-1~和当前符号 X~i~作为输入，并且必须产生输出状态 S~i~（有时，我们也对其他一些输出 Y~i~ 感兴趣，例如生成网络的情况）</p><p><img data-src="https://i.imgur.com/YwIbQc4.jpg" alt="img"></p><blockquote><p>注意,上面的seq_length是输入的长度,但并不是每一句的长度,因为每一句长度很可能不一样,这样RNN无法计算,是一个batch中vocab最大的长度,也就是在一个batch中padding到最大长度</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padify</span>(<span class="params">b,voc=<span class="literal">None</span>,tokenizer=tokenizer</span>):</span></span><br><span class="line">    <span class="comment"># b is the list of tuples of length batch_size</span></span><br><span class="line">    <span class="comment">#   - first element of a tuple = label, </span></span><br><span class="line">    <span class="comment">#   - second = feature (text sequence)</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>],voc=voc,tokenizer=tokenizer) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch</span></span><br><span class="line">    l = <span class="built_in">max</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of two tensors - labels and features</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v])</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=padify, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在许多情况下，输入token在进入 RNN 之前通过嵌入层以降低维度。每一层输出是σ(W×X~i~+H×S~i-1~+b)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">100</span>   <span class="comment"># 输入数据编码的维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>   <span class="comment"># 隐含层维度</span></span><br><span class="line">num_layers = <span class="number">4</span>     <span class="comment"># 隐含层层数</span></span><br><span class="line"></span><br><span class="line">rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rnn:&quot;</span>,rnn)</span><br><span class="line"></span><br><span class="line">seq_len = <span class="number">10</span>        <span class="comment"># 句子长度</span></span><br><span class="line">batch_size = <span class="number">1</span>      </span><br><span class="line">x = torch.randn(seq_len,batch_size,input_size)        <span class="comment"># 输入数据</span></span><br><span class="line">h0 = torch.zeros(num_layers,batch_size,hidden_size)   <span class="comment"># 输入数据</span></span><br><span class="line"></span><br><span class="line">out, h = rnn(x, h0)  <span class="comment"># 输出数据</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;out.shape:&quot;</span>,out.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;h.shape:&quot;</span>,h.shape)</span><br></pre></td></tr></table></figure><p>注意,pytorch RNN默认输入数据是(seq_length,batch_size,embedding_size),除非设置<code>batch_first=True</code></p><h4 id="LSTM-amp-amp-GRU"><a href="#LSTM-amp-amp-GRU" class="headerlink" title="LSTM&amp;&amp;GRU"></a>LSTM&amp;&amp;GRU</h4><p><img data-src="https://i.imgur.com/VYCgrZW.png" alt="image-20231023214516563"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-<span class="number">0.5</span></span><br><span class="line">        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x,(h,c) = self.rnn(x)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">net = LSTMClassifier(vocab_size,<span class="number">64</span>,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">train_epoch(net,train_loader, lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><p>LSTM增加了三个门用来控制隐状态,输入.</p><ul><li>忘记门采用隐藏的向量并确定我们需要忘记向量 c 的哪些分量，以及要通过哪些分量。</li><li>输入门从输入和隐藏向量中获取一些信息，并将其插入状态。</li><li>输出门通过具有tanh激活的某个线性层转换状态，然后使用隐藏向量H~i~选择其部分组件以产生新的状态c~i+1~。</li></ul><p>而GRU结构要简单一些,支持隐状态的门控. 重置门允许我们控制“可能还想记住”的过去状态的数量, 更新门将允许我们控制新状态中有多少个是旧状态的副本。</p><p><img data-src="https://i.imgur.com/v83gWCu.png" alt="image-20231023214736240"></p><p><img data-src="https://i.imgur.com/eMlaE2s.png" alt="image-20231023214808081"></p><p><img data-src="https://i.imgur.com/ARG5B52.png" alt="image-20231026095926569"></p><p><img data-src="https://i.imgur.com/r4xeKjo.png" alt="image-20231023214906219"></p><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231023214919424.png" alt="image-20231023214919424"></p><p><img data-src="https://i.imgur.com/8r65rwW.png" alt="image-20231023214931931"></p><h4 id="PACKED-SEQUENCE"><a href="#PACKED-SEQUENCE" class="headerlink" title="PACKED SEQUENCE"></a>PACKED SEQUENCE</h4><p>填充一批可变长度序列</p><p>我们必须用零向量填充小批量中的所有序列。虽然这会导致一些内存浪费，但对于 RNN,为填充的输入项创建额外的 RNN 单元更为重要，这些输入项参与训练，但不携带任何重要的输入信息。<strong>仅将 RNN 训练到实际序列大小会好得多</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line">seq = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">lens = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">packed = pack_padded_sequence(seq, lens, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">packed</span><br><span class="line">seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=<span class="literal">True</span>)</span><br><span class="line">seq_unpacked</span><br><span class="line">lens_unpacked</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_length</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch and length sequence itself</span></span><br><span class="line">    len_seq = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    l = <span class="built_in">max</span>(len_seq)</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of three tensors - labels, padded features, length sequence</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v]),</span><br><span class="line">        torch.tensor(len_seq)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=pad_length, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMPackClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-<span class="number">0.5</span></span><br><span class="line">        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, lengths</span>):</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=<span class="literal">True</span>,enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        pad_x,(h,c) = self.rnn(pad_x)</span><br><span class="line">        x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x,batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">net = LSTMPackClassifier(vocab_size,<span class="number">64</span>,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">train_epoch_emb(net,train_loader_len, lr=<span class="number">0.001</span>,use_pack_sequence=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>要生成打包序列，我们可以使用<code>torch.nn.utils.rnn.pack_padded_sequence</code>函数。所有循环层，包括RNN，LSTM和GRU，都支持打包序列作为输入，并产生可以使用<code>torch.nn.utils.rnn.pad_packed_sequence</code>解码打包输出。</p><p>训练时,传入<code>len_seq = list(map(len,v))</code>,使用<code>torch.nn.utils.rnn.pack_padded_sequence</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=<span class="literal">True</span>,enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">pad_x,(h,c) = self.rnn(pad_x)</span><br></pre></td></tr></table></figure><p>再使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x,batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>可以解码打包的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_emb</span>(<span class="params">net,dataloader,lr=<span class="number">0.01</span>,optimizer=<span class="literal">None</span>,loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>),epoch_size=<span class="literal">None</span>, report_freq=<span class="number">200</span>,use_pack_sequence=<span class="literal">False</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(),lr=lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line">    total_loss,acc,count,i = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> labels,text,off <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        labels,text = labels.to(device), text.to(device)</span><br><span class="line">        <span class="keyword">if</span> use_pack_sequence:</span><br><span class="line">            off = off.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            off = off.to(device)</span><br><span class="line">        out = net(text, off)</span><br><span class="line">        loss = loss_fn(out,labels) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss+=loss</span><br><span class="line">        _,predicted = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        acc+=(predicted==labels).<span class="built_in">sum</span>()</span><br><span class="line">        count+=<span class="built_in">len</span>(labels)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i%report_freq==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;count&#125;</span>: acc=<span class="subst">&#123;acc.item()/count&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch_size <span class="keyword">and</span> count&gt;epoch_size:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/count, acc.item()/count</span><br></pre></td></tr></table></figure><blockquote><p>目前，pack_padded_sequence函数要求长度序列张量位于CPU设备上，因此训练函数在训练时需要避免将长度序列数据移动到GPU。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> use_pack_sequence:</span><br><span class="line">            off = off.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            off = off.to(device)</span><br></pre></td></tr></table></figure><h4 id="Bidirectional-and-Multilayer-RNNs"><a href="#Bidirectional-and-Multilayer-RNNs" class="headerlink" title="Bidirectional and Multilayer RNNs"></a>Bidirectional and Multilayer RNNs</h4><p>由于在许多实际情况下，我们可以随机访问输入序列，因此在两个方向上运行循环计算可能是有意义的。这样的网络被称为双向RNN。在处理双向网络时，我们需要两个隐藏状态向量，每个方向一个。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg" alt="Image showing a Multilayer long-short-term-memory- RNN"></p><p>与卷积网络一样，可以在第一层之上构建另一个循环层，以捕获更高级别的模式，并从第一层提取的低级模式进行构建。这导致我们得出多层RNN的概念，它由两个或多个循环网络组成，其中前一层的输出作为输入传递到下一层。</p><h2 id="GRN-Generative-Recurrent-Networks"><a href="#GRN-Generative-Recurrent-Networks" class="headerlink" title="GRN(Generative Recurrent Networks)"></a>GRN(Generative Recurrent Networks)</h2><p>递归神经网络（RNN）及其门控细胞变体，如长短期记忆细胞（LSTM）和门控循环单元（GRU）为语言建模提供了一种机制，因为它们可以学习单词顺序并为序列中的下一个单词提供预测。这使我们能够将 RNN 用于生成任务，例如<strong>普通文本生成、机器翻译，甚至图像字幕</strong>。</p><p>每个 RNN 单元产生下一个隐藏状态作为输出。但是，我们也可以为每个循环单元添加另一个输出，这将允许我们输出一个序列（长度等于原始序列）。此外，我们可以使用在每一步都不接受输入的 RNN 单元，只需获取一些初始状态向量，然后生成一系列输出。分别对应多对多与一对多.</p><p><img data-src="https://i.imgur.com/srjJVXN.png" alt="image-20231023223320804"></p><p><img data-src="https://i.imgur.com/SeTNrFy.png" alt="image-20231023223412819"></p><ul><li>一对一是一个输入和一个输出的传统神经网络</li><li>一对多是一种生成式体系结构，它接受一个输入值，并生成一系列输出值。例如，如果我们想训练一个图像字幕网络来生成图片的文本描述，我们可以将图片作为输入，通过CNN传递以获得其隐藏状态，然后让循环链逐字生成标题。</li><li>多对一对应于我们在上一个单元中描述的 RNN 架构，例如文本分类</li><li>多对多或<strong>序列到序列</strong>对应于机器翻译等任务，其中我们首先让 RNN 将所有信息从输入序列收集到隐藏状态，另一个 RNN 链将此状态展开到输出序列中。</li></ul><p>对于生成序列任务而言,在每一步中，我们将获取长度为 nchars 的字符序列，并要求网络为每个输入字符生成下一个输出字符</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png" alt="Image showing an example RNN generation of the word &#39;HELLO&#39;."></p><p>在生成文本时（在推理过程中），从一些提示开始，该提示通过 RNN 单元格生成其中间状态，然后从该状态开始生成。我们一次生成一个字符，并将状态和生成的字符传递给另一个 RNN 单元以生成下一个，直到我们生成足够的字符。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate-inf.png" alt="img"></p><p>这样需要添加一些特殊字符表明开始与结尾,比如\<eos>(在训练数据中添加).</p><p>如果只需要无穷的生成字符,只需要固定序列大小,比如为nchars,在l长的序列中就有l-nchars这么多个数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">char_tokenizer</span>(<span class="params">words</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(words) <span class="comment">#[word for word in words]</span></span><br><span class="line"></span><br><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    counter.update(char_tokenizer(line))</span><br><span class="line">vocab = torchtext.vocab.vocab(counter)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enc</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(encode(x,voc=vocab,tokenizer=char_tokenizer))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span>(<span class="params">s,nchars=nchars</span>):</span></span><br><span class="line">    ins = torch.zeros(<span class="built_in">len</span>(s)-nchars,nchars,dtype=torch.long,device=device)</span><br><span class="line">    outs = torch.zeros(<span class="built_in">len</span>(s)-nchars,nchars,dtype=torch.long,device=device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)-nchars):</span><br><span class="line">        ins[i] = enc(s[i:i+nchars])</span><br><span class="line">        outs[i] = enc(s[i+<span class="number">1</span>:i+nchars+<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> ins,outs <span class="comment"># 获得成对的数据 每个数据长度nchars</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">net,size=<span class="number">100</span>,start=<span class="string">&#x27;today &#x27;</span></span>):</span></span><br><span class="line">        chars = <span class="built_in">list</span>(start)</span><br><span class="line">        out, s = net(enc(chars).view(<span class="number">1</span>,-<span class="number">1</span>).to(device))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            nc = torch.argmax(out[<span class="number">0</span>][-<span class="number">1</span>])</span><br><span class="line">            chars.append(vocab.get_itos()[nc])</span><br><span class="line">            out, s = net(nc.view(<span class="number">1</span>,-<span class="number">1</span>),s)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br></pre></td></tr></table></figure><p>因为网络以字符作为输入，词汇量很小，我们不需要嵌入层，独热编码输入可以直接进入LSTM单元。</p><p>但是，由于我们将字符号作为输入传递，因此我们需要在传递给 LSTM 之前对它们进行独热编码。输出编码器将是一个线性层，它将隐藏状态转换为独热编码输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMGenerator</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, s=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)</span><br><span class="line">        x,s = self.rnn(x,s)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x),s</span><br></pre></td></tr></table></figure><blockquote><p>在训练期间希望能够对生成的文本进行采样。定义 generate 函数，该函数将从初始字符串开始生成长度大小的输出字符串。</p></blockquote><p>首先将通过传递整个起始字符串，并取出输出状态 s 和下一个预测字符。由于 out 是独热编码的，我们采用 argmax 来获取词汇表中字符 nc 的索引，并使用 itos 找出实际字符并将其附加到生成的字符字符列表中。生成一个字符的过程是重复<code>size</code>次数以生成所需数量的字符。</p><p>说人话就是,搭建的模型输出shape是vacab_size(就是RNN或者LSTM的输出),其中最大值的index就是vocab的index.使用交叉熵损失,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">net = LSTMGenerator(vocab_size,<span class="number">64</span>).to(device)</span><br><span class="line"></span><br><span class="line">samples_to_train = <span class="number">10000</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(),<span class="number">0.01</span>)</span><br><span class="line">loss_fn = torch.nn.CrossEntropyLoss()</span><br><span class="line">net.train()</span><br><span class="line"><span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">    <span class="comment"># x[0] is class label, x[1] is text</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(x[<span class="number">1</span>])-nchars&lt;<span class="number">10</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    samples_to_train-=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> samples_to_train: <span class="keyword">break</span></span><br><span class="line">    text_in, text_out = get_batch(x[<span class="number">1</span>])</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out,s = net(text_in)</span><br><span class="line">    loss = torch.nn.functional.cross_entropy(out.view(-<span class="number">1</span>,vocab_size),text_out.flatten()) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Current loss = <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(generate(net))</span><br></pre></td></tr></table></figure><p>可以改进的地方</p><ol><li>我们准备训练数据的方式是从一个样本生成一个小批量。这并不理想，因为<strong>小批量的大小都不同，其中一些甚至无法生成</strong>，因为文本小于 nchars。此外，<strong>小批量不能充分利用GPU</strong>。更明智的做法是从<strong>所有样本中获取一大块文本，然后生成所有输入输出对，打乱它们，并生成大小相等的小批量</strong>。</li><li><strong>多层 LSTM</strong>。尝试 2 或 3 层 LSTM 单元是有意义的。正如我们在上一个单元中提到的，LSTM 的每一层都从文本中提取某些模式，在字符级生成器的情况下，我们可以期望较低的 LSTM 级别负责提取音节，而较高的级别负责提取单词和单词组合。这可以通过将层数参数传递给 LSTM 构造函数来简单地实现。</li></ol><h4 id="soft-text-generation-and-temperature"><a href="#soft-text-generation-and-temperature" class="headerlink" title="soft text generation and temperature"></a>soft text generation and temperature</h4><p>在前面的 generate 定义中，我们始终将概率最高的字符作为生成文本中的下一个字符。这导致文本经常一次又一次地在相同的字符序列之间“循环”(来回就是那那几个字符,类似石头剪刀布,石头经常赢剪刀,剪刀经常赢布)</p><p>但是，如果我们看一下下一个字符的概率分布，可能是几个最高概率之间的差异并不大，例如一个字符的概率为 0.2，另一个字符的概率为 0.19，等等。例如，当在序列“play”中查找下一个字符时，下一个字符同样可以是空格或e。</p><p>所以选择概率较高的字符并不总是“公平的”，因为选择第二高的字符仍可能使我们获得有意义的文本。从网络输出给出的概率分布中对字符进行采样更为明智。可以使用实现所谓多项式分布的多项式函数( <strong>multinomial distribution</strong>)进行此采样。实现此软文本生成的函数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_soft</span>(<span class="params">net,size=<span class="number">100</span>,start=<span class="string">&#x27;today &#x27;</span>,temperature=<span class="number">1.0</span></span>):</span></span><br><span class="line">        chars = <span class="built_in">list</span>(start)</span><br><span class="line">        out, s = net(enc(chars).view(<span class="number">1</span>,-<span class="number">1</span>).to(device))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            <span class="comment">#nc = torch.argmax(out[0][-1])</span></span><br><span class="line">            out_dist = out[<span class="number">0</span>][-<span class="number">1</span>].div(temperature).exp()</span><br><span class="line">            nc = torch.multinomial(out_dist,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            chars.append(vocab.get_itos()[nc])</span><br><span class="line">            out, s = net(nc.view(<span class="number">1</span>,-<span class="number">1</span>),s)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0.3</span>,<span class="number">0.8</span>,<span class="number">1.0</span>,<span class="number">1.3</span>,<span class="number">1.8</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;--- Temperature = <span class="subst">&#123;i&#125;</span>\n<span class="subst">&#123;generate_soft(net,size=<span class="number">300</span>,start=<span class="string">&#x27;Today &#x27;</span>,temperature=i)&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure><p>引入了一个称为温度的参数，用于指示应该坚持最高概率的力度(温度越低越严格).</p><p>如果温度为 1.0，我们进行公平的多项式采样，当温度变为无穷大时.</p><p>所有概率都变得相等，我们随机选择下一个字符。当我们过度升高温度时，文本变得毫无意义，当它接近 0 时，它类似于“循环”的硬生成文本。</p><p>核心是下面代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out_dist = out[<span class="number">0</span>][-<span class="number">1</span>].div(temperature).exp()</span><br><span class="line">nc = torch.multinomial(out_dist,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">chars.append(vocab.get_itos()[nc])</span><br></pre></td></tr></table></figure><h3 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h3><p>NLP领域最重要的问题之一是机器翻译,这是谷歌翻译等工具的基本任务,或者更一般地说，任何序列到序列的任务.</p><p>循环网络的一个主要缺点是序列中的<strong>所有单词对结果都有相同的影响</strong>。这会导致标准 LSTM 编码器-解码器模型在序列到序列任务（如命名实体识别和机器翻译）中性能欠佳。实际上，输入序列中的特定单词通常比其他单词对顺序输出的影响更大。</p><blockquote><p>GRN,LSTM等引入遗忘门,更新门这种机制试图解决长序列遗忘问题,但不能解决不同单词权重的问题</p></blockquote><p>注意力机制提供了一种<strong>加权每个输入向量对RNN的每个输出预测的上下文影响的方法</strong>。它的实现方式是在输入 RNN 的中间状态和输出 RNN 之间创建快捷方式。</p><h4 id="Positional-Encoding-Embedding"><a href="#Positional-Encoding-Embedding" class="headerlink" title="Positional Encoding/Embedding"></a>Positional Encoding/Embedding</h4><p>使用 RNN 时，token的相对位置由步数表示(因为RNN不是并行的,由第一个token开始累积状态)，因此不需要显式表示。然而,如果使用注意力层，就需要知道token在序列中的相对位置(因为将整个sequence作为整体).</p><p>为了获得位置编码,使用序列中的标记位置序列（即数字序列 0,1 等）与token本身相加.</p><p>要将位置（整数）转换为向量，我们可以使用不同的方法：</p><ul><li>可训练嵌入，类似于token嵌入。这就是我们在这里考虑的方法。我们在标记及其位置之上应用嵌入层，从而产生相同维度的嵌入向量，然后将它们相加。</li><li>固定位置编码(比如使用一个余弦函数,使用0~len作为输入).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TokenAndPositionEmbedding</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, maxlen, vocab_size, embed_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TokenAndPositionEmbedding, self).__init__()</span><br><span class="line">        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)</span><br><span class="line">        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)</span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        maxlen = self.maxlen</span><br><span class="line">        positions = tf.<span class="built_in">range</span>(start=<span class="number">0</span>, limit=maxlen, delta=<span class="number">1</span>)</span><br><span class="line">        positions = self.pos_emb(positions)</span><br><span class="line">        x = self.token_emb(x)</span><br><span class="line">        <span class="keyword">return</span> x+positions</span><br></pre></td></tr></table></figure><p>这里使用两个embedding,分别处理token和position.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/images/pos-embedding.png?raw=1" alt="img" style="zoom: 25%;" /></p><p>transformer层如图,主要使用了multi-head attn,然后使用了resnet中的思想添加了输入x,也就是x+f(x),normalization使用layernorm,对一个sample中的所有维进行规范化.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/images/transformer-layer.png?raw=1" alt="img" style="zoom:33%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, num_heads, ff_dim, rate=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerBlock, self).__init__()</span><br><span class="line">        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=<span class="string">&#x27;attn&#x27;</span>)</span><br><span class="line">        self.ffn = keras.Sequential(</span><br><span class="line">            [keras.layers.Dense(ff_dim, activation=<span class="string">&quot;relu&quot;</span>), keras.layers.Dense(embed_dim),]</span><br><span class="line">        )</span><br><span class="line">        self.layernorm1 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout1 = keras.layers.Dropout(rate)</span><br><span class="line">        self.dropout2 = keras.layers.Dropout(rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training</span>):</span></span><br><span class="line">        attn_output = self.att(inputs, inputs)</span><br><span class="line">        attn_output = self.dropout1(attn_output, training=training)</span><br><span class="line">        out1 = self.layernorm1(inputs + attn_output)</span><br><span class="line">        ffn_output = self.ffn(out1)</span><br><span class="line">        ffn_output = self.dropout2(ffn_output, training=training)</span><br><span class="line">        <span class="keyword">return</span> self.layernorm2(out1 + ffn_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">embed_dim = <span class="number">32</span>  <span class="comment"># Embedding size for each token</span></span><br><span class="line">num_heads = <span class="number">2</span>  <span class="comment"># Number of attention heads</span></span><br><span class="line">ff_dim = <span class="number">32</span>  <span class="comment"># Hidden layer size in feed forward network inside transformer</span></span><br><span class="line">maxlen = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">20000</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential([keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(<span class="number">1</span>,)),</span><br><span class="line">    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),</span><br><span class="line">    TransformerBlock(embed_dim, num_heads, ff_dim),</span><br><span class="line">    keras.layers.GlobalAveragePooling1D(),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.1</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">20</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.1</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">4</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training tokenizer&#x27;</span>)</span><br><span class="line">model.layers[<span class="number">0</span>].adapt(ds_train.<span class="built_in">map</span>(extract_text))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;acc&#x27;</span>], optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">model.fit(ds_train.<span class="built_in">map</span>(tupelize).batch(<span class="number">128</span>),validation_data=ds_test.<span class="built_in">map</span>(tupelize).batch(<span class="number">128</span>))</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/sFOcUVC.png" alt="image-20231106225205449"></p><p>网络结构如上.</p><p>可以看看这篇文章<a href="https://zhuanlan.zhihu.com/p/366592542">注意力,多头注意力,自注意力及Pytorch实现 - 知乎 (zhihu.com)</a></p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT是一个非常大的多层transformer网络，其中 12 层用于 BERT 基础，24 层用于 BERT-large。该模型首先使用无监督训练（预测句子中的掩饰词）在大型文本数据语料库（WikiPedia + 书籍）上进行预训练。</p><p>在预训练期间，模型吸收了大量语言理解，然后可以通过微调将其与其他数据集一起使用。这个过程称为迁移学习。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png" alt="picture from http://jalammar.github.io/illustrated-bert/"></p><h4 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h4><p><img data-src="https://i.imgur.com/bCtBKQC.png" alt="img"></p><h2 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h2><blockquote><p>到目前为止，我们主要关注一项 NLP 任务——分类。然而，还有其他 NLP 任务可以通过神经网络来完成。其中一项任务是命名实体识别 (NER)，它处理识别文本中的特定实体，例如地点、人名、日期时间间隔、化学式等。</p></blockquote><p>假设您想开发一个自然语言聊天机器人，类似于 Amazon Alexa 或 Google Assistant。智能聊天机器人的工作方式是通过对输入句子进行文本分类来了解用户想要什么。这种分类的结果就是所谓的意图(<strong>intent</strong>)，它决定了聊天机器人应该做什么。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/19-NER/images/bot-ner.png" alt="Bot NER" style="zoom:67%;" /></p><p>然而，用户可以提供一些参数作为短语的一部分。例如，当询问天气时，她可能会指定地点或日期。机器人应该能够理解这些实体，并在执行操作之前相应地填充参数槽。这正是 NER 发挥作用的地方。</p><p>也就是说,从原本的一句话分类变成对一个单词的分类和理解。</p><p>NER 模型本质上是 token 分类模型,因为对于每个输入 token,我们需要决定它是否属于一个实体,如果属于,则属于哪个实体类。</p><p>由于 NER 模型本质上是一个 token 分类模型，因此我们可以使用我们已经熟悉的 RNN 来完成此任务。在这种情况下，循环网络的每个块都会返回token ID。</p><p>也就是说每个token会给一个tag,这个tag包含这个entity是否是第一个,以及所属得类别.类似下面的tag.</p><div class="table-container"><table><thead><tr><th>Token</th><th>Tag</th></tr></thead><tbody><tr><td>Tricuspid</td><td>B-DIS</td></tr><tr><td>valve</td><td>I-DIS</td></tr><tr><td>regurgitation</td><td>I-DIS</td></tr><tr><td>and</td><td>O</td></tr><tr><td>lithium</td><td>B-CHEM</td></tr><tr><td>carbonate</td><td>I-CHEM</td></tr><tr><td>toxicity</td><td>B-DIS</td></tr><tr><td>in</td><td>O</td></tr><tr><td>a</td><td>O</td></tr><tr><td>newborn</td><td>O</td></tr><tr><td>infant</td><td>O</td></tr><tr><td>.</td><td>O</td></tr></tbody></table></div><h2 id="Pre-Trained-Large-Language-Models"><a href="#Pre-Trained-Large-Language-Models" class="headerlink" title="Pre-Trained Large Language Models"></a>Pre-Trained Large Language Models</h2><p>在我们之前的所有任务中，我们都在使用标记数据集训练神经网络来执行特定任务。对于大型转换器模型，如BERT，我们以自监督的方式使用语言建模来构建语言模型，然后通过进一步的领域特定训练将其专门用于特定的下游任务。</p><p><strong>然而，已经证明，大型语言模型也可以在没有任何特定领域训练的情况下解决许多任务。一个能够做到这一点的模型家族被称为GPT： Generative Pre-Trained Transformer。</strong></p><p><img data-src="https://i.imgur.com/6rtsvGL.png" alt="image-20231118230348795"></p><p>因为GPT已经根据大量数据进行了训练，以理解语言和代码，所以它们会根据输入（提示）提供输出。提示是GPT输入或查询，用于向模型提供下一次完成任务的指令。为了获得想要的结果，你需要最有效的提示，包括选择正确的单词、格式、短语甚至符号.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前学过一段时间NLP,因为其中涉及到一些深度学习常用的知识或者框架,但苦于不系统以及没有任务focus不能长久.这里借助微软的教程写点东西.&lt;br&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://www.sekyoro.top/tags/NLP/"/>
    
    <category term="Deep Learning" scheme="https://www.sekyoro.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>目标检测_初识</title>
    <link href="https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/"/>
    <id>https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/</id>
    <published>2023-10-21T13:22:20.000Z</published>
    <updated>2023-11-30T15:05:40.572Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>需要一些基本的cv知识<br><span id="more"></span></p><h3 id="图像分辨率"><a href="#图像分辨率" class="headerlink" title="图像分辨率"></a>图像分辨率</h3><blockquote><p>图像分辨率可以定义为图像中存在的像素数。当像素数量增加时，图像的质量会增加。我们已经在前面看到了图像的形状，它给出了行和列的数量。这可以说是该图像的分辨率。几乎所有人都知道的一些标准分辨率是320 x 240像素（主要适用于小屏幕设备）、1024 x 768像素（适用于在标准计算机显示器上观看）、720 x 576像素（适合在宽高比为4:3的标准清晰度电视机上观看），1280 x 1024像素（适用于在宽高比为5:4的液晶显示器上全屏观看）、1920 x 1080像素（用于在高清电视上观看），现在我们甚至有4K、5K和8K分辨率，超高清显示器和电视分别支持3840 x 2160像素、5120 x 2880像素和7680 x 4320像素</p></blockquote><p>图像像素的高位包含的信息比低位更多,我们可以将图像划分为不同级别的位平面。例如，将图像划分为8位（0-7）平面，其中最后几个平面包含图像的大部分信息。</p><p><img data-src="https://editor.analyticsvidhya.com/uploads/61607page%2015.gif" alt="bit plans "></p><p><img data-src="https://s2.loli.net/2023/11/23/li2FYa5bBWDy6uv.png" alt="image-20231123110414030"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img  = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line">c1 = np.mod(img,<span class="number">2</span>)</span><br><span class="line">c2 = np.mod(np.floor(img/<span class="number">2</span>),<span class="number">2</span>)</span><br><span class="line">c3 = np.mod(np.floor(img/<span class="number">4</span>),<span class="number">2</span>)</span><br><span class="line">c4 = np.mod(np.floor(img/<span class="number">8</span>),<span class="number">2</span>)</span><br><span class="line">c5 = np.mod(np.floor(img/<span class="number">16</span>),<span class="number">2</span>)</span><br><span class="line">c6 = np.mod(np.floor(img/<span class="number">32</span>),<span class="number">2</span>)</span><br><span class="line">c7 = np.mod(np.floor(img/<span class="number">64</span>),<span class="number">2</span>)</span><br><span class="line">c8 = np.mod(np.floor(img/<span class="number">128</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">cc = <span class="number">2</span>*(<span class="number">2</span>*(<span class="number">2</span>*c8+c7)+c6)</span><br><span class="line">to_plot = [img,c1,c2,c3,c4,c5,c6,c7,c8,cc]</span><br><span class="line">fig,axes = plt.subplots(<span class="number">2</span>,<span class="number">5</span>, subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: [], <span class="string">&#x27;yticks&#x27;</span>: []&#125;)</span><br><span class="line">fig.subplots_adjust(hspace=<span class="number">0.05</span>, wspace=<span class="number">0.05</span>)</span><br><span class="line"><span class="keyword">for</span> ax,i <span class="keyword">in</span> <span class="built_in">zip</span>(axes.flat, to_plot):</span><br><span class="line">    ax.imshow(i, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>可以使用像素的高位重建图像</p><h4 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a>图像金字塔</h4><p><img data-src="https://docs.opencv.org/3.4/Pyramids_Tutorial_Pyramid_Theory.png" alt="Pyramids_Tutorial_Pyramid_Theory.png"></p><p><strong>图像金字塔是一组图像，所有图像都来自一张原始图像，这些图像被连续下采样，直到达到某个期望的停止点</strong>。</p><p>有两种常见的图像金字塔：</p><ul><li>高斯金字塔：用于对图像进行下采样</li><li>拉普拉斯金字塔：用于从金字塔中较低的图像重建上采样图像（分辨率较低）</li></ul><h3 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a>图像直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">his</span>(<span class="params">img_gray</span>):</span></span><br><span class="line">    hist = cv2.calcHist([img_gray], [<span class="number">0</span>], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&quot;Grayscale Histogram&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;bins&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;pixels&quot;</span>)</span><br><span class="line">    plt.plot(hist)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/OzzSilQ.png" alt="image-20231123134730139"></p><h3 id="图像深度"><a href="#图像深度" class="headerlink" title="图像深度"></a>图像深度</h3><p>数字化图像的每个像素是用一组二进制数进行描述，像素的色彩由RGB通道决定，其中包含表示图像颜色的位数称为图像深度。如灰度图像，每个像素颜色占用1个字节8位，则称图像深度为8位，而RGB的彩色图像占用3字节，图像深度为24位。</p><p>图像深度又称为色深（Color Depth），它确定了一幅图像中最多能使用的颜色数，即彩色图像的每个像素最大的颜色数，或者确定灰度图像的每个像素最大的灰度级数。<br>使用opencv的imread读取模式有</p><blockquote><p>IMREAD_UNCHANGED = -1, //返回原始图像。alpha通道不会被忽略，如果有的话。加载给定格式的图像，包括alpha通道。Alpha通道存储透明度信息——Alpha通道的值越高，像素就越不透明<br>IMREAD_GRAYSCALE = 0, //返回灰度图像<br>IMREAD_COLOR = 1, //返回通道顺序为BGR的彩色图像<br>IMREAD_ANYDEPTH = 2, //当输入具有相应的深度时返回16位/ 32位图像，否则将其转换为8位。.<br>IMREAD_ANYCOLOR = 4, //则以任何可能的颜色格式读取图像。</p></blockquote><h3 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h3><p>颜色空间是一种协议(protocol)，用于以易于再现的方式表示颜色。我们知道，灰度图像具有单个像素值，彩色图像每个像素包含3个值——红色、绿色和蓝色通道的强度。</p><p>大多数计算机视觉用例处理RGB格式的图像。然而,<strong>视频压缩和设备独立存储等应用程序在很大程度上依赖于其他颜色空间，如色相(Hue即色相，就是我们平时所说的红、绿，如果你分的更细的话可能还会有洋红、草绿等等)、饱和度(色彩的深浅度(0-100%，对于一种颜色比如红色，我们可以用浅红——大红——深红——红得发紫等等语言来描述它)、色调(纯度，色彩的亮度(0-100%) ，这个在调整屏幕亮度的时候比较常见)即HSV颜色空间</strong>。</p><p><strong>RGB图像由不同颜色通道的颜色强度组成，即强度和颜色信息在RGB颜色空间中混合</strong>，但<strong>在HSV颜色空间中,颜色和强度信息彼此分离。这将使HSV颜色空间对光源更改更加稳健</strong>。</p><p><img data-src="https://i.imgur.com/gkNZfIa.png" alt="image-20231123115359582"></p><h3 id="图像resize"><a href="#图像resize" class="headerlink" title="图像resize"></a>图像resize</h3><p><strong>机器学习模型使用固定大小的输入</strong>。同样的想法也适用于计算机视觉模型。<strong>我们用于训练模型的图像必须具有相同的大小</strong>。现在，<strong>如果我们通过从各种来源抓取图像来创建自己的数据集，这可能会成为问题。这就是调整图像大小的功能凸显出来的地方</strong>。</p><p><img data-src="https://i.imgur.com/Agk6m0B.jpg" alt=""></p><blockquote><p>INTER_NEAREST:最近邻插值</p><p>INTER_LINEAR:双线性插值</p><p>INTER_AREA：使用像素面积关系重新采样</p><p>INTER_CUBIC:4×4像素邻域上的双三次插值</p><p>INTER_LANCZOS4:8邻域上的Lanczos插值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> npimg = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;img&quot;</span>,img)</span><br><span class="line">smaller_img = cv2.resize(img,(<span class="number">200</span>,<span class="number">200</span>),interpolation=cv2.INTER_LINEAR)</span><br><span class="line">cv2.imshow(<span class="string">&quot;smaller_img&quot;</span>,smaller_img)</span><br></pre></td></tr></table></figure><h3 id="图像旋转以及平移"><a href="#图像旋转以及平移" class="headerlink" title="图像旋转以及平移"></a>图像旋转以及平移</h3><p>可以用作图像增强的技术</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line">rows,cols = img.shape[:<span class="number">2</span>]</span><br><span class="line">M = cv2.getRotationMatrix2D((cols/<span class="number">2</span>,rows/<span class="number">2</span>),<span class="number">45</span>,<span class="number">1</span>)</span><br><span class="line">dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv2.imshow(<span class="string">&quot;dst&quot;</span>,dst)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>会使用到cv2.getRotationMatrix2D与cv2.warpAffine.</p><p>cv2.getRotationMatrix2D参数分别是中心,旋转角度以及缩放系数.</p><p>cv2.warpAffine是做仿射变换,</p><p><img data-src="https://img-blog.csdnimg.cn/ab91284739724a70a016342a38f84baa.png#pic_center" alt="在这里插入图片描述"></p><p>图像平移可以用于为模型添加平移不变性<strong>，因为通过平移，我们可以改变对象在图像中的位置，为模型提供更多的多样性，从而获得更好的可推广性</strong>，这在困难的条件下有效，即当对象没有完全对准图像中心时。这种增强技术还可以帮助模型正确地对具有部分可见对象的图像进行分类。以下图为例。即使图像中没有完整的鞋子，模型也应该能够将其分类为鞋子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">M = np.float32([[<span class="number">1</span>,<span class="number">0</span>,-<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,-<span class="number">100</span>]])</span><br><span class="line">dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class="line">plt.imshow(dst)</span><br><span class="line">cv2.imshow(<span class="string">&quot;dst&quot;</span>,dst)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Nsds6wk.png" alt="image-20231123123137312"></p><h3 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a>图像阈值</h3><p>阈值分割是一种图像分割方法。它将像素值与阈值进行比较，并相应地进行更新。图像阈值的一个简单应用可以将图像划分为前景和背景,阈值只能应用于灰度图像。</p><p><img data-src="https://i.imgur.com/GY8rrIF.png" alt="image-20231123123310090"></p><p>上面是简单阈值,此外还有自适应阈值</p><p>简单阈值是全局的，对于在不同区域具有不同照明条件的图像可能不太适用，此时可以使用自适应阈值处理。<strong>算法计算图像的局部阈值，在同一图像的不同区域获得不同的阈值，并为具有不同照明的图像提供了更好的结果</strong>。在自适应阈值的情况下，对图像的不同部分使用不同的阈值。该函数可为具有不同照明条件的图像提供更好的结果，因此被称为“自适应”。<strong>Otsu的二值化方法为整个图像找到一个最佳阈值。它适用于双峰图像</strong>（直方图中有2个峰值的图像）。</p><p><code>cv2.ADAPTIVE_THRESH_MEAN_C</code>：阈值是邻域的<strong>平均值</strong>。<br><code>cv2.ADAPTIVE_THRESH_GAUSSIAN_C</code>：阈值是邻域值的<strong>加权和</strong>，其中权重是高斯窗口。<br><code>Block Size</code> 决定邻域的大小。<br><code>C</code> 从计算的平均值或加权平均值中减去常数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#import the libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ADAPTIVE THRESHOLDING</span></span><br><span class="line">gray_image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ret,thresh_global = cv2.threshold(gray_image,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line"><span class="comment">#here 11 is the pixel neighbourhood that is used to calculate the threshold value</span></span><br><span class="line">thresh_mean = cv2.adaptiveThreshold(gray_image,<span class="number">255</span>,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">thresh_gaussian = cv2.adaptiveThreshold(gray_image,<span class="number">255</span>,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">names = [<span class="string">&#x27;Original Image&#x27;</span>,<span class="string">&#x27;Global Thresholding&#x27;</span>,<span class="string">&#x27;Adaptive Mean Threshold&#x27;</span>,<span class="string">&#x27;Adaptive Gaussian Thresholding&#x27;</span>]</span><br><span class="line">images = [gray_image,thresh_global,thresh_mean,thresh_gaussian]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(names[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Gb1mmoJ.png" alt="image-20231123135259991"></p><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><p><strong>图像分割是将图像中的每个像素分类到某个类别的任务</strong>。例如，将每个像素分类为前景或背景。图像分割对于从图像中提取相关部分是重要的。</p><p>分水岭算法是一种经典的图像分割算法。它将图像中的像素值视为地形。为了找到对象边界，它将初始标记作为输入。然后，该算法开始从标记淹没盆地(flooding the basin from the markers)，直到标记在对象边界相遇。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/watershed.gif" alt="watershed algorithm"></p><h3 id="位操作"><a href="#位操作" class="headerlink" title="位操作"></a>位操作</h3><p>按位运算包括AND、OR、NOT和XOR。在计算机视觉中，当我们<strong>有一个遮罩图像并想将该遮罩应用于另一个图像以提取感兴趣的区域时</strong>，这些操作非常有用。</p><p><img data-src="https://i.imgur.com/O85ljV8.png" alt="image-20231123161116735"></p><p><img data-src="https://i.imgur.com/Tzrz5xe.png" alt="image-20231123161520256"></p><h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p><strong>边缘是图像中图像亮度急剧变化或具有不连续性的点</strong>。这种不连续性通常对应于：</p><ul><li>深度不连续</li><li>表面方向的不连续性</li><li>材料特性的变化</li><li>场景照明的变化</li></ul><p>边缘是图像的非常有用的特征，可以用于不同的应用，如图像中对象的分类和定位。甚至深度学习模型也会计算边缘特征，以提取图像中存在的对象的信息。</p><p><img data-src="https://i.imgur.com/NTrwEOY.png" alt="image-20231123161625316"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">edges = cv2.Canny(img, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">&quot;edges&quot;</span>, edges)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><h3 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h3><p>在图像滤波中，使用像素值的相邻值来更新像素值。但是，这些值最初是如何更新的呢？，有多种更新像素值的方法，例如从邻居中选择最大值，使用邻居的平均值等。每种方法都有自己的用途。例如，对邻域中的像素值取平均值用于图像模糊。</p><p>高斯滤波也用于图像模糊，其根据相邻像素与所考虑像素的距离为相邻像素赋予不同的权重。对于图像过滤，我们使用内核。核是不同形状的数字矩阵，如3 x 3、5 x 5等。核用于计算图像一部分的点积。当计算像素的新值时，内核中心与像素重叠。相邻像素值与内核中的相应值相乘。计算出的值被分配给与内核中心重合的像素。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/Image-Filtering.png" alt="img"></p><p><img data-src="https://i.imgur.com/0EahLmk.png" alt="image-20231123161852790"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;../imgs/test.png&#x27;</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.float32)/<span class="number">25</span></span><br><span class="line"><span class="comment">#using the averaging kernel for image smoothening</span></span><br><span class="line">averaging_kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.float32)/<span class="number">9</span></span><br><span class="line">filtered_image = cv2.filter2D(image,-<span class="number">1</span>,kernel)</span><br><span class="line">cv2.imshow(<span class="string">&quot;avg_filtered_image&quot;</span>,filtered_image)</span><br><span class="line"><span class="comment">#get a one dimensional Gaussian Kernel</span></span><br><span class="line">gaussian_kernel_x = cv2.getGaussianKernel(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">gaussian_kernel_y = cv2.getGaussianKernel(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#converting to two dimensional kernel using matrix multiplication</span></span><br><span class="line">gaussian_kernel = gaussian_kernel_x * gaussian_kernel_y.T</span><br><span class="line"><span class="comment">#you can also use cv2.GaussianBLurring(image,(shape of kernel),standard deviation) instead of cv2.filter2D</span></span><br><span class="line">filtered_image = cv2.filter2D(image,-<span class="number">1</span>,gaussian_kernel)</span><br><span class="line">cv2.imshow(<span class="string">&quot;filtered_image&quot;</span>,filtered_image)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Wtr5Pab.png" alt="image-20231123170248409"></p><p>上图的两个卷积核分别是</p><script type="math/tex; mode=display">\left.\left[\begin{matrix}0&-1&0\\-1&5&-1\\0&-1&0\\\end{matrix}\right.\right]</script><script type="math/tex; mode=display">\left.\left[\begin{matrix}0&-1&0\\-1&4&-1\\0&-1&0\\\end{matrix}\right.\right]</script><h3 id="图像轮廓-contours"><a href="#图像轮廓-contours" class="headerlink" title="图像轮廓(contours)"></a>图像轮廓(contours)</h3><p>轮廓是表示图像中对象边界的点或线段的闭合曲线。<strong>轮廓本质上是图像中对象的形状。与边缘不同，轮廓不是图像的一部分。相反，它们是与图像中对象的形状相对应的点和线段的抽象集合</strong>。我们可以使用轮廓来计算图像中对象的数量，根据对象的形状对其进行分类，或者从图像中选择特定形状的对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = cv2.imread(<span class="string">&#x27;../imgs/test.png&#x27;</span>)</span><br><span class="line"><span class="comment">#converting RGB image to Binary</span></span><br><span class="line">gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh = cv2.threshold(gray_image,<span class="number">127</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#calculate the contours from binary image</span></span><br><span class="line">contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">with_contours = cv2.drawContours(image,contours,-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">plt.imshow(with_contours[...,::-<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/ttcJEOh.png" alt="image-20231123163932600"></p><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>使用SIFT或SURF从不同图像中提取的特征可以进行匹配，以找到存在于不同图像中的相似对象/模式。OpenCV库支持多种特征匹配算法，如brute force 匹配、knn特征匹配等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#reading images in grayscale format</span></span><br><span class="line">image1 = cv2.imread(<span class="string">&#x27;../imgs/00000.png&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">image2 = cv2.warpAffine(image1, np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">100</span>], [<span class="number">0</span>, <span class="number">1</span>, -<span class="number">100</span>]]), (image1.shape[<span class="number">1</span>], image1.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"><span class="comment">#finding out the keypoints and their descriptors</span></span><br><span class="line">keypoints1,descriptors1 = sift.detectAndCompute(image1,<span class="literal">None</span>)</span><br><span class="line">keypoints2,descriptors2 = sift.detectAndCompute(image2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#matching the descriptors from both the images</span></span><br><span class="line">bf = cv2.BFMatcher()</span><br><span class="line">matches = bf.knnMatch(descriptors1,descriptors2,k = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#selecting only the good features</span></span><br><span class="line">good_matches = []</span><br><span class="line"><span class="keyword">for</span> m,n <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="keyword">if</span> m.distance &lt; <span class="number">0.75</span>*n.distance:</span><br><span class="line">        good_matches.append([m])</span><br><span class="line">image3 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good_matches,<span class="literal">None</span>,flags=<span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;image3&quot;</span>,image3)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/BBs3dlN.jpg" alt="image-20231123165033904"></p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/02/keypoint_matching.png" alt="img"></p><p>在上面的图像中，我们可以看到从原始图像（左侧）中提取的关键点与其旋转版本的关键点相匹配。这是因为特征是使用SIFT提取的，SIFT对这种变换是不变的。</p><h3 id="人脸检测"><a href="#人脸检测" class="headerlink" title="人脸检测"></a>人脸检测</h3><p>OpenCV支持基于haar级联的对象检测。<strong>Haar级联是基于机器学习的分类器，用于计算图像中的不同特征，如边缘、线条等</strong>。然后，这些分类器使用多个正样本和负样本进行训练。OpenCV Github仓库中提供了针对人脸、眼睛等不同对象的经过训练的分类器，也可以针对任何对象训练自己的haar级联。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#load the classifiers downloaded </span></span><br><span class="line">face_cascade = cv2.CascadeClassifier(<span class="string">&#x27;haarcascade_frontalface_default.xml&#x27;</span>)</span><br><span class="line">eye_cascade = cv2.CascadeClassifier(<span class="string">&#x27;haarcascade_eye.xml&#x27;</span>)</span><br><span class="line"><span class="comment">#read the image and convert to grayscale format</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;rotated_face.jpg&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#calculate coordinates </span></span><br><span class="line">faces = face_cascade.detectMultiScale(gray, <span class="number">1.1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    roi_gray = gray[y:y+h, x:x+w]</span><br><span class="line">    roi_color = img[y:y+h, x:x+w]</span><br><span class="line">    eyes = eye_cascade.detectMultiScale(roi_gray)</span><br><span class="line">    <span class="comment">#draw bounding boxes around detected features</span></span><br><span class="line">    <span class="keyword">for</span> (ex,ey,ew,eh) <span class="keyword">in</span> eyes:</span><br><span class="line">        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line"><span class="comment">#plot the image</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment">#write image </span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;face_detection.jpg&#x27;</span>,img)</span><br></pre></td></tr></table></figure><p>很多级联器的xml文件都是直接拿别人的,网上也有训练的教程.</p><h3 id="图像梯度向量"><a href="#图像梯度向量" class="headerlink" title="图像梯度向量"></a>图像梯度向量</h3><p>image gradient vector</p><p>图像梯度矢量被定义为每个单独像素的度量,包含x轴和y轴上的像素颜色变化。该定义与连续多变量函数的梯度一致，该函数是所有变量的偏导数的向量。</p><h3 id="边缘检测算子"><a href="#边缘检测算子" class="headerlink" title="边缘检测算子"></a>边缘检测算子</h3><p><img data-src="https://editor.analyticsvidhya.com/uploads/81269Capture.PNG" alt="Sharpening An Image  2"></p><h4 id="prewitt"><a href="#prewitt" class="headerlink" title="prewitt"></a>prewitt</h4><p>Prewitt算子不是只依赖于四个直接相邻的邻居，而是利用八个周围的像素来获得更平滑的结果。</p><p><img data-src="https://s2.loli.net/2023/11/23/8LM4JgTNA2ZDVpy.png" alt="image-20231123095504365"></p><h4 id="sobel"><a href="#sobel" class="headerlink" title="sobel"></a>sobel</h4><p>为了更加强调直接相邻像素的影响，它们被分配了更高的权重。</p><p><img data-src="https://i.imgur.com/XXcPkBM.png" alt="image-20231123172141630"></p><h3 id="角点检测"><a href="#角点检测" class="headerlink" title="角点检测"></a>角点检测</h3><blockquote><p>角点检测(Corner Detection)是计算机视觉系统中用来获得图像特征的一种方法，广泛应用于运动检测、图像匹配、视频跟踪、三维建模和目标识别等领域中。也称为特征点检测。 <strong>角点通常被定义为两条边的交点，更严格的说，角点的局部邻域应该具有两个不同区域的不同方向的边界。</strong>而实际应用中，大多数所谓的角点检测方法检测的是拥有特定特征的图像点，而不仅仅是“角点”。这些特征点在图像中有具体的坐标，并具有某些数学特征，如局部最大或最小灰度、某些梯度特征等</p></blockquote><h4 id="Harris"><a href="#Harris" class="headerlink" title="Harris"></a>Harris</h4><ul><li>计算窗口中各像素点在x和y方向的梯度；</li><li>计算两个方向梯度的乘积,即Ix ^ 2 , Iy ^ 2 , IxIy(可以用一些一阶梯度算子求得图像梯度)</li><li>使用滤波核对窗口中的每一像素进行加权，生成矩阵M和元素A，B，C</li><li>计算每个像素的Harris响应值R，并对小于某阈值T的R置0；</li><li>由于角点所在区域的一定邻域内都有可能被检测为角点，所以为了防止角点聚集，最后在3×3或5×5的邻域内进行非极大值抑制，局部最大值点即为图像中的角点。</li></ul><p><a href="https://blog.csdn.net/SESESssss/article/details/106774854">【理解】经典角点检测算法—Harris角点-CSDN博客</a></p><h3 id="常用特征"><a href="#常用特征" class="headerlink" title="常用特征"></a>常用特征</h3><h4 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h4><p>关键点是处理图像时应该注意的一个概念。这些基本上是图像中的兴趣点。关键点类似于给定图像的特征。它们是定义图像中有趣内容的位置。关键点很重要，因为无论图像如何修改（旋转、收缩、扩展、失真），我们都会为图像找到相同的关键点。</p><p>尺度不变特征变换（SIFT）是一种非常流行的关键点检测算法。它包括以下步骤：</p><ul><li>Scale-space extrema detection</li><li>Keypoint localization</li><li>Orientation assignment</li><li>Keypoint descriptor</li><li>Keypoint matching</li></ul><p>从SIFT提取的特征可用于图像拼接、对象检测等应用。下面的代码和输出显示了使用SIFT计算的关键点及其方向。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#show OpenCV version</span></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="comment">#read the iamge and convert to grayscale</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#create sift object</span></span><br><span class="line">sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"><span class="comment">#calculate keypoints and their orientation</span></span><br><span class="line">keypoints,descriptors = sift.detectAndCompute(gray,<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#plot keypoints on the image</span></span><br><span class="line">with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class="line"><span class="comment">#plot the image</span></span><br><span class="line">plt.imshow(with_keypoints)</span><br></pre></td></tr></table></figure><blockquote><p>SIFT算法有助于定位图像中的局部特征，通常称为图像的“关键点”。这些关键点是尺度和旋转不变量，可用于各种计算机视觉应用，如图像匹配、对象检测、场景检测等。我们还可以在模型训练期间使用使用SIFT生成的关键点作为图像的特征。SIFT特征、边缘特征或弓形特征的主要优点是它们不受图像的大小或方向的影响。</p></blockquote><p>整个过程可以分为4个部分：</p><p><strong>Constructing a Scale Space</strong>：确保要素与scale无关 </p><p><strong>Keypoint Localisation</strong>：识别合适的特征或关键点 </p><p><strong>Orientation Assignment</strong>：确保关键点旋转不变 </p><p><strong>Keypoint Descriptor:</strong>：为每个关键点分配一个唯一的id</p><h5 id="Constructing-the-Scale-Space"><a href="#Constructing-the-Scale-Space" class="headerlink" title="Constructing the Scale Space"></a>Constructing the Scale Space</h5><p>我们需要识别给定输入图像中最明显的特征，同时忽略任何噪声。此外，我们需要确保这些功能不依赖于scale。</p><p>对于图像中的每个像素，高斯模糊会基于其具有特定σ值的相邻像素来计算一个值。</p><p><strong>纹理和次要细节将从图像中删除，只保留相关信息，如形状和边缘</strong></p><blockquote><p>比例空间是从单个图像生成的具有不同比例的图像的集合。</p></blockquote><p>因此，这些模糊图像是为多个比例创建的。为了创建一组不同比例的新图像，将拍摄原始图像并将比例缩小一半。对于每个新图像，我们将创建模糊版本。</p><p>理想的缩放次数是四次，对于每次缩放，模糊图像的数量应该是五个。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-24-18-27-46.png" alt="sift octave | SIFT algorithm"></p><blockquote><p>高斯差分是一种特征增强算法，它涉及<strong>将原始图像的一个模糊版本与另一个模糊程度较低的原始图像版本相减</strong>。</p></blockquote><p>DoG为每个octave创建另一组图像，方法是从相同比例的前一个图像中减去每个图像。</p><p>到目前为止，我们已经创建了<strong>多个尺度的图像</strong>（通常用σ表示），并对<strong>每个尺度使用高斯模糊来减少图像中的噪声</strong>。接下来将尝试使用一种名为<strong>高斯差分</strong>（DoG）的技术来增强这些特征。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-12-48-03-300x205.png" alt="difference of gaussian"></p><p>如下图,在左边，我们有5个图像，都来自第一个octave(我的理解就是不同scale的图像)。通过在前一图像上应用高斯模糊来创建每个后续图像。在右边，我们有四个通过减去连续的高斯而生成的图像。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-14-18-26.png" alt="difference of gaussian | SIFT algorithm"></p><p>我们为这些图像中的每一个都增强了功能。现在我们有了一组新的图像，我们将使用它来找到重要的关键点</p><h5 id="Keypoint-Localization"><a href="#Keypoint-Localization" class="headerlink" title="Keypoint Localization"></a>Keypoint Localization</h5><p>一旦创建了图像，下一步就是从图像中<strong>找到可用于特征匹配的重要关键点</strong>。其思想是<strong>找到图像的局部最大值和最小值</strong>。</p><p>本部分分为两个步骤：1)求局部最大值和最小值 2)删除低对比度关键点（关键点选择）</p><blockquote><p>为了定位局部最大值和最小值，我们遍历图像中的每个像素，并将其与相邻像素进行比较。</p></blockquote><p>当说“相邻”时，这不仅包括该图像的周围像素（像素所在），还包括octave中上一个和下一个图像的九个像素。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-16-50-01-300x207.png" alt="Scale invariant"></p><p>这意味着将每个像素值与其他26个像素值进行比较，以确定它是否是称为极值的局部最大值/最小值。例如，有三个来自第一个octave的图像。标记为x的像素与相邻像素（绿色）进行比较，如果它是相邻像素中最高或最低的，则选择它作为关键点或兴趣点</p><h5 id="Keypoint-Selection"><a href="#Keypoint-Selection" class="headerlink" title="Keypoint Selection"></a>Keypoint Selection</h5><p>已经成功地生成了<strong>尺度不变的关键点</strong>。但是<strong>这些关键点中的一些可能对噪声不具有鲁棒性</strong>。我们需要进行最终检查，以确保我们有最准确的关键点来表示图像特征</p><p>因此，我们<strong>将消除对比度低或非常靠近边缘的关键点</strong>。为了处理低对比度关键点，为每个关键点计算二阶泰勒展开。如果结果值小于0.03（以大小计），我们将拒绝关键点。</p><h5 id="Keypoint-Descriptor"><a href="#Keypoint-Descriptor" class="headerlink" title="Keypoint Descriptor"></a>Keypoint Descriptor</h5><p>到目前为止已经有了尺度不变和旋转不变的稳定关键点。</p><p>最后将使用相邻的像素、它们的方向和大小来为这个关键点生成一个独特的特征，称为“描述符”。</p><p>首先在关键点周围取一个16×16的邻域。这个16×16的块被进一步划分为4×4个子块，对于这些子块中的每一个子块，我们使用幅度和方向来生成直方图。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-26-20-10-52.png" alt="sift feature"></p><p>在这个阶段，bin的尺寸增加了，我们只取了8个bins。这些箭头中的每一个表示8bins，箭头的长度定义了大小。因此，我们将为每个关键点总共有128个bin值。</p><h5 id="Feature-Matching"><a href="#Feature-Matching" class="headerlink" title="Feature Matching"></a>Feature Matching</h5><p>现在将使用SIFT特征进行特征匹配。为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># read images</span></span><br><span class="line">img1 = cv2.imread(<span class="string">&#x27;eiffel_2.jpeg&#x27;</span>)  </span><br><span class="line">img2 = cv2.imread(<span class="string">&#x27;eiffel_1.jpg&#x27;</span>) </span><br><span class="line"></span><br><span class="line">img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class="line">img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sift</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"></span><br><span class="line">keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class="literal">None</span>)</span><br><span class="line">keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read images</span></span><br><span class="line">img1 = cv2.imread(<span class="string">&#x27;eiffel_2.jpeg&#x27;</span>)  </span><br><span class="line">img2 = cv2.imread(<span class="string">&#x27;eiffel_1.jpg&#x27;</span>) </span><br><span class="line"></span><br><span class="line">img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class="line">img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sift</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"></span><br><span class="line">keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class="literal">None</span>)</span><br><span class="line">keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#feature matching</span></span><br><span class="line">bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">matches = bf.match(descriptors_1,descriptors_2)</span><br><span class="line">matches = <span class="built_in">sorted</span>(matches, key = <span class="keyword">lambda</span> x:x.distance)</span><br><span class="line"></span><br><span class="line">img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:<span class="number">50</span>], img2, flags=<span class="number">2</span>)</span><br><span class="line">plt.imshow(img3),plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/index_71.png" alt="SIFT algorithm"></p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/index_61.png" alt="feature matching | SIFT algorithm"></p><ul><li>SIFT（Scale Invariant Feature Transform，尺度不变特征变换）是一种强大的图像匹配技术，它可以识别和匹配图像中对<strong>缩放、旋转和仿射失真不变的特征</strong>。</li><li>它被广泛应用于计算机视觉应用，包括图像匹配、物体识别和三维重建。SIFT技术包括生成具有不同尺度的图像的尺度空间，然后使用高斯差分（DoG）方法来识别图像中的关键点。</li><li>它还涉及为每个关键点计算描述符，这些描述符可用于特征匹配和对象识别。</li><li>它可以使用Python和OpenCV库来实现，OpenCV库提供了一组用于检测关键点、计算描述符和匹配特征的函数。</li></ul><h4 id="SURF"><a href="#SURF" class="headerlink" title="SURF"></a>SURF</h4><p>Speeded Up Robust Features（SURF）是SIFT的增强版。它的工作速度要快得多，并且对图像转换更健壮。</p><p>在SIFT中，使用高斯拉普拉斯算子来近似尺度空间。拉普拉斯算子是用于计算图像边缘的核。拉普拉斯核通过近似图像的二阶导数来工作。因此，它对噪声非常敏感。<strong>我们通常将高斯核应用于拉普拉斯核之前的图像，因此将其命名为高斯拉普拉斯</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#show OpenCV version</span></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="comment">#read image and convert to grayscale</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#instantiate surf object</span></span><br><span class="line">surf  = cv2.xfeatures2d.SURF_create(<span class="number">400</span>)</span><br><span class="line"><span class="comment">#calculate keypoints and their orientation</span></span><br><span class="line">keypoints,descriptors = surf.detectAndCompute(gray,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class="line"></span><br><span class="line">plt.imshow(with_keypoints)</span><br></pre></td></tr></table></figure><h4 id="HOG"><a href="#HOG" class="headerlink" title="HOG"></a>HOG</h4><p>面向梯度直方图（HOG）是一种从像素颜色中提取特征的有效方法，用于构建对象识别分类器。</p><ol><li>预处理图像，包括调整大小和颜色标准化。</li><li>计算每个像素的梯度矢量，以及其大小和方向。</li></ol><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/image-gradient-vector-pixel-location.png" alt="img" style="zoom: 33%;" /></p><ol><li>将图像划分为许多8x8像素的单元格。在每个单元中，这64个单元的幅度值被装箱，并累积添加到9个无符号方向的bucket中（没有符号，因此0-180度而不是0-360度；这是基于经验实验的实际选择）。</li></ol><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/HOG-histogram-creation.png" alt="img"></p><p>4.然后，我们在图像上滑动一个2x2个单元格（因此是16x16像素）的块。在每个块区域中，<strong>4个单元的4个直方图被连接成36个值的一维向量，然后被归一化为具有单位权重。最终的HOG特征向量是所有块向量的级联</strong>。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-19-18-24-37-300x87.png" alt="img"></p><p>它可以被输入到像SVM这样的分类器中，用于学习对象识别任务。</p><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/block_histogram.png" alt="img"></p><h3 id="Camera-Calibration"><a href="#Camera-Calibration" class="headerlink" title="Camera Calibration"></a>Camera Calibration</h3><p>相机是一种将3D世界转换为2D图像的设备。相机在捕捉三维图像并将其存储在二维图像中起着非常重要的作用。</p><p>相机校准是图像处理或计算机视觉领域中常用的词。<strong>相机校准方法旨在识别图像创建过程的几何特征</strong>。这是在许多计算机视觉应用中执行的重要步骤，尤其是当需要场景上的度量信息时。在这些应用中，<strong>相机通常根据一组固有参数进行分类，如轴的偏斜、焦距和主点，其方向由旋转和平移等外部参数表示。线性或非线性算法用于实时利用已知点及其在图像平面中的投影来估计内在和外在参数</strong>。</p><p><img data-src="https://editor.analyticsvidhya.com/uploads/887441.png" alt="Overview of Camera Calibration"></p><p>1.内在或内部参数它允许在图像帧中的像素坐标和相机坐标之间进行映射。例如，透镜的光学中心、焦距和径向失真系数。</p><p>2.外部或外部参数它描述了相机的方向和位置。这是指相机相对于某个世界坐标系的旋转和平移。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>综述</p><ol><li><a href="https://viso.ai/deep-learning/object-detection/">Object Detection in 2023: The Definitive Guide - viso.ai</a></li><li><a href="https://arxiv.org/abs/1905.05055">[1905.05055] Object Detection in 20 Years: A Survey (arxiv.org)</a></li><li><a href="https://arxiv.org/abs/2104.11892">[2104.11892] A Survey of Modern Deep Learning based Object Detection Models (arxiv.org)</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S1051200422004298">A comprehensive review of object detection with deep learning - ScienceDirect</a></li></ol><p>博客</p><ol><li><a href="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/">Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/?utm_source=reading_list&amp;utm_medium=https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/">A Beginner’s Guide to Image Processing With OpenCV and Python (analyticsvidhya.com)</a></li><li>也许还不错的学习网站<a href="https://pyimagesearch.com/">PyImageSearch - You can master Computer Vision, Deep Learning, and OpenCV.</a></li><li>维基百科 卷积核<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing">Kernel (image processing) - Wikipedia</a>)</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;需要一些基本的cv知识&lt;br&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
    <category term="cv" scheme="https://www.sekyoro.top/tags/cv/"/>
    
  </entry>
  
  <entry>
    <title>Python并行计算</title>
    <link href="https://www.sekyoro.top/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"/>
    <id>https://www.sekyoro.top/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/</id>
    <published>2023-10-20T12:20:56.000Z</published>
    <updated>2023-10-21T08:52:53.594Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>主要是因为Python库的设计很不错,通过这门语言进一步学习并行,涉及到进程线程以及异步编程等.建议是对性能有要求的利用其他语言实现,但是基本的思想、方法是一样的.<br><span id="more"></span></p><h2 id="创建进程-amp-amp-线程"><a href="#创建进程-amp-amp-线程" class="headerlink" title="创建进程&amp;&amp;线程"></a>创建进程&amp;&amp;线程</h2><ul><li>进程可以包含多个并行运行的线程。</li><li>通常，操作系统创建和管理线程比进程更能节省CPU的资源。线程用于一些小任务，进程用于繁重的任务——运行应用程序。</li><li>同一个进程下的线程共享地址空间和其他资源，进程之间相互独立</li></ul><p>进程有自己的地址空间，数据栈和其他的辅助数据来追踪执行过程；系统会管理所有进程的执行，通过调度程序来分配计算资源等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## The following modules must be imported</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">## this is the code to execute</span></span><br><span class="line">program = <span class="string">&quot;python&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process calling&quot;</span>)</span><br><span class="line">arguments = [<span class="string">&quot;called_Process.py&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">## we call the called_Process.py script</span></span><br><span class="line">os.execvp(program, (program,) + <span class="built_in">tuple</span>(arguments))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Good Bye!!&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello Python Parallel Cookbook!!&quot;</span>)</span><br><span class="line">closeInput = <span class="built_in">input</span>(<span class="string">&quot;Press ENTER to exit&quot;</span>)</span><br><span class="line"><span class="built_in">print</span><span class="string">&quot;Closing calledProcess&quot;</span></span><br></pre></td></tr></table></figure><p>线程创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To use threads you need import Thread using the following code:</span></span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"><span class="comment"># Also we use the sleep function to make the thread &quot;sleep&quot;</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># To create a thread in Python you&#x27;ll want to make your class work as a thread.</span></span><br><span class="line"><span class="comment"># For this, you should subclass your class from the Thread class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookBook</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.message = <span class="string">&quot;Hello Parallel Python CookBook!!\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># this method prints only the message</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_message</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(self.message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The run method prints ten times the message</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Thread Starting\n&quot;</span>)</span><br><span class="line">        x = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (x &lt; <span class="number">10</span>):</span><br><span class="line">            self.print_message()</span><br><span class="line">            sleep(<span class="number">2</span>)</span><br><span class="line">            x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Thread Ended\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the main process</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process Started&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an instance of the HelloWorld class</span></span><br><span class="line">hello_Python = CookBook()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the message...starting the thread</span></span><br><span class="line">hello_Python.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># end the main process</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process Ended&quot;</span>)</span><br></pre></td></tr></table></figure><p>Python解释器并不完全是线程安全的。为了支持多线程的Python程序，CPython使用了一个叫做全局解释器锁（Global Interpreter Lock， GIL）的技术。这意味着同一时间只有一个线程可以执行Python代码；执行某一个线程一小段时间之后，Python会自动切换到下一个线程。GIL并没有完全解决线程安全的问题，如果多个线程试图使用共享数据，还是可能导致未确定的行为。</p><h2 id="线程的并行"><a href="#线程的并行" class="headerlink" title="线程的并行"></a>线程的并行</h2><blockquote><p>在软件应用中使用最广泛的并发编程范例是多线程。通常，一个应用有一个进程，分成多个独立的线程，并行运行、互相配合，执行不同类型的任务。</p><p>线程是独立的处理流程，可以和系统的其他线程并行或并发地执行。多线程可以共享数据和资源，利用所谓的共享内存空间。线程和进程的具体实现取决于你要运行的操作系统，但是总体来讲，我们可以说线程是包含在进程中的，同一进程的多个不同的线程可以共享相同的资源。相比而言，进程之间不会共享资源。</p><p>每一个线程基本上包含3个元素：程序计数器，寄存器和栈。与同一进程的其他线程共享的资源基本上包括数据和系统资源。每一个线程也有自己的运行状态，可以和其他线程同步，这点和进程一样。线程的状态大体上可以分为ready,running,blocked。线程的典型应用是应用软件的并行化——为了充分利用现代的多核处理器，使每个核心可以运行单个线程。相比于进程，使用线程的优势主要是性能。相比之下，在进程之间切换上下文要比在统一进程的多线程之间切换上下文要重的多。</p></blockquote><p>多线程编程一般使用共享内容空间进行线程间的通讯。这就使管理内容空间成为多线程编程的重点和难点。</p><p>使用Python的<code>threading</code>模块管理多线程.</p><p>线程被创建之后并不会马上运行，需要手动调用 <code>start()</code> ， <code>join()</code> 让调用它的线程一直等待直到执行结束</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function called by thread %i&quot;</span> % i)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slowProcess</span>():</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;slow process done&quot;</span>)</span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=slowProcess, args=())</span><br><span class="line">t.start()</span><br><span class="line">t.join()</span><br><span class="line">t = threading.Thread(target=func, args=(<span class="number">1</span>,))</span><br><span class="line">t.start()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process quit&quot;</span>)</span><br></pre></td></tr></table></figure><p>设置<code>t = threading.Thread(target=slowProcess, args=(),name=&quot;slowp&quot;)</code>线程名称</p><p>线程被创建之后并不会马上运行，需要手动调用 <code>start()</code>.此外<code>join()</code> 让<strong>调用它的线程一直等待直到执行结束</strong>（即阻塞调用它的主线程， <code>t</code> 线程执行结束，主线程才会继续执行）</p><p><code>threading.current_thread().name</code>访问执行当前代码的线程的名称.</p><p>主线程是<code>MainThread</code>. 实现多线程可以选择继承threading.Thread类或者直接使用<code>threading.Thread</code>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">exitFlag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myThread</span>(<span class="params">threading.Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threadID, name, counter</span>):</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.threadID = threadID</span><br><span class="line">        self.name = name</span><br><span class="line">        self.counter = counter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;starting &quot;</span> + self.name)</span><br><span class="line">        print_time(self.name, self.counter, <span class="number">5</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;exiting &quot;</span> + self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_time</span>(<span class="params">threadName, delay, counter</span>):</span></span><br><span class="line">    <span class="keyword">while</span> counter:</span><br><span class="line">        <span class="keyword">if</span> exitFlag:</span><br><span class="line">            _thread.exit()</span><br><span class="line">        time.sleep(delay)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s:%s&quot;</span> % (threadName, time.ctime(time.time())))</span><br><span class="line">        counter -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">thread1 = myThread(<span class="number">1</span>, <span class="string">&quot;thread-1&quot;</span>, <span class="number">1</span>)</span><br><span class="line">thread2 = myThread(<span class="number">2</span>, <span class="string">&quot;thread-2&quot;</span>, <span class="number">2</span>)</span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line">thread1.join()</span><br><span class="line">thread2.join()</span><br></pre></td></tr></table></figure><p><code>threading</code> 模块是创建和管理线程的首选形式。每一个线程都通过一个继承 <code>Thread</code> 类，重写 <code>run()</code> 方法来实现逻辑，这个方法是线程的入口。在主程序中，我们创建了多个 <code>myThread</code> 的类型实例，然后执行 <code>start()</code> 方法启动它们。调用 <code>Thread.__init__</code> 构造器方法是必须的，通过它我们可以给线程定义一些名字或分组之类的属性。调用 <code>start()</code> 之后线程变为活跃状态，并且持续直到 <code>run()</code> 结束，或者中间出现异常。所有的线程都执行完成之后，程序结束。</p><h3 id="线程的同步"><a href="#线程的同步" class="headerlink" title="线程的同步"></a>线程的同步</h3><p>当两个或以上对共享内存的操作发生在并发线程中，并且至少有一个可以改变数据，又没有同步机制的条件下，就会产生竞争条件，可能会导致执行无效代码、bug、或异常行为。</p><h4 id="Lock锁同步"><a href="#Lock锁同步" class="headerlink" title="Lock锁同步"></a>Lock锁同步</h4><p>竞争条件最简单的解决方法是使用锁。锁的操作非常简单，当一个线程需要访问部分共享内存时，它必须先获得锁才能访问。此线程对这部分共享资源使用完成之后，该线程必须释放锁，然后其他线程就可以拿到这个锁并访问这部分资源了。</p><p>使用lock同步线程,通过它我们可以将共享资源某一时刻的访问限制在单一线程或单一类型的线程上，线程必须得到锁才能使用资源，并且之后必须允许其他线程使用相同的资源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">shared_resource_with_lock = <span class="number">0</span></span><br><span class="line">shared_resource_with_no_lock = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">COUNT = <span class="number">1000000</span></span><br><span class="line"></span><br><span class="line">shared_resource_lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_with_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_lock.acquire()</span><br><span class="line">        shared_resource_with_lock += <span class="number">1</span></span><br><span class="line">        shared_resource_lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrement_with_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_lock.acquire()</span><br><span class="line">        shared_resource_with_lock  -=<span class="number">1</span></span><br><span class="line">        shared_resource_lock.release()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_without_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_no_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_with_no_lock +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrement_without_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_no_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_with_no_lock -=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    t1 = threading.Thread(target=increment_with_lock)</span><br><span class="line">    t2 = threading.Thread(target=decrement_with_lock)</span><br><span class="line">    t3 = threading.Thread(target=increment_without_lock)</span><br><span class="line">    t4 = threading.Thread(target=decrement_without_lock)</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t3.start()</span><br><span class="line">    t4.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    t3.join()</span><br><span class="line">    t4.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the value of shared variable with lock management is %s&quot;</span> % shared_resource_with_lock)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the value of shared variable with race condition is %s&quot;</span> % shared_resource_with_no_lock)</span><br></pre></td></tr></table></figure><ul><li>锁有两种状态： locked（被某一线程拿到）和unlocked（可用状态）</li><li><p>我们有两个方法来操作锁： <code>acquire()</code> 和 <code>release()</code></p></li><li><p>如果状态是unlocked， 可以调用 <code>acquire()</code> 将状态改为locked</p></li><li>如果状态是locked， <code>acquire()</code> 会被block直到另一线程调用 <code>release()</code> 释放锁</li><li>如果状态是unlocked， 调用 <code>release()</code> 将导致 <code>RuntimError</code> 异常</li><li>如果状态是locked， 可以调用 <code>release()</code> 将状态改为unlocked</li></ul><blockquote><p>尽管理论上行得通，但是锁的策略不仅会导致有害的僵持局面。还会对应用程序的其他方面产生负面影响。这是一种保守的方法，经常会引起不必要的开销，也会限制程序的可扩展性和可读性。更重要的是，有时候需要对多进程共享的内存分配优先级，使用锁可能和这种优先级冲突。最后，从实践的经验来看，使用锁的应用将对debug带来不小的麻烦。所以，最好使用其他可选的方法确保同步读取共享内存，避免竞争条件。</p></blockquote><p>事实上我执行这段代码时跟线程是否join有关,基本上上面代码是否加锁都没有出问题</p><h4 id="RLock锁同步"><a href="#RLock锁同步" class="headerlink" title="RLock锁同步"></a>RLock锁同步</h4><p>如果你想让只有拿到锁的线程才能释放该锁，那么应该使用 <code>RLock()</code> 对象。和 <code>Lock()</code> 对象一样， <code>RLock()</code> 对象有两个方法： <code>acquire()</code> 和 <code>release()</code> 。当你需要在类外面保证线程安全，又要在类内使用同样方法的时候 <code>RLock()</code> 就很实用了</p><blockquote><p>RLock其实叫做“Reentrant Lock”，就是可以重复进入的锁，也叫做“递归锁”。这种锁对比Lock有是三个特点：1. 谁拿到谁释放。如果线程A拿到锁，线程B无法释放这个锁，只有A可以释放；2. 同一线程可以多次拿到该锁，即可以acquire多次；3. acquire多少次就必须release多少次，只有最后一次release才能改变RLock的状态为unlocked</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Box</span>:</span></span><br><span class="line">    lock = threading.RLock()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.total_items = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute</span>(<span class="params">self,n</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.total_items += n</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.execute(<span class="number">1</span>)</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span>(<span class="params">self</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.execute(-<span class="number">1</span>)</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span>(<span class="params">box, items</span>):</span></span><br><span class="line">    <span class="keyword">while</span> items &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;adding 1 item in the box&quot;</span>)</span><br><span class="line">        box.add()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        items -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remover</span>(<span class="params">box, items</span>):</span></span><br><span class="line">    <span class="keyword">while</span> items &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;removing 1 item in the box&quot;</span>)</span><br><span class="line">        box.remove()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        items -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    items = <span class="number">5</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;putting %s items in the box &quot;</span> % items)</span><br><span class="line">    box = Box()</span><br><span class="line">    t1 = threading.Thread(target=adder, args=(box, items))</span><br><span class="line">    t2 = threading.Thread(target=remover, args=(box, items))</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s items still remain in the box &quot;</span> % Box().total_items)</span><br></pre></td></tr></table></figure><p>相比于Lock有一些更稳定的设定.</p><h4 id="信号量同步"><a href="#信号量同步" class="headerlink" title="信号量同步"></a>信号量同步</h4><p>信号量是一个内部数据，用于标明当前的共享资源可以有多少并发读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">semaphore = threading.Semaphore(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;consumer is waiting.&quot;</span>)</span><br><span class="line">    semaphore.acquire()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed item number %s &quot;</span> % item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>():</span></span><br><span class="line">    <span class="keyword">global</span> item</span><br><span class="line">    time.sleep(<span class="number">10</span>)</span><br><span class="line">    item = random.randint(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;producer notify:produced item number %s&quot;</span> % item)</span><br><span class="line">    semaphore.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">        t1 = threading.Thread(target=producer)</span><br><span class="line">        t2 = threading.Thread(target=consumer)</span><br><span class="line">        t1.start()</span><br><span class="line">        t2.start()</span><br><span class="line">        t1.join()</span><br><span class="line">        t2.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;program terminated&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>信号量的一个特殊用法是互斥量。互斥量是初始值为1的信号量，可以实现数据、资源的互斥访问。</p><p>信号量在支持多线程的编程语言中依然应用很广，然而这可能导致死锁的情况。例如，现在有一个线程t1先等待信号量s1，然后等待信号量s2，而线程t2会先等待信号量s2，然后再等待信号量s1，这样就可能会发生死锁，导致t1等待s2，但是t2在等待s1。</p></blockquote><h4 id="条件进行同步"><a href="#条件进行同步" class="headerlink" title="条件进行同步"></a>条件进行同步</h4><p>条件指的是应用程序状态的改变。这是另一种同步机制，其中某些线程在等待某一条件发生，其他的线程会在该条件发生的时候进行通知。一旦条件发生，线程会拿到共享资源的唯一权限</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread, Condition</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">items = []</span><br><span class="line">condition = Condition()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">consumer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">consume</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> condition</span><br><span class="line">        <span class="keyword">global</span> items</span><br><span class="line">        condition.acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(items) == <span class="number">0</span>:</span><br><span class="line">            condition.wait()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;consumer notify: no item to consume&quot;</span>)</span><br><span class="line">        items.pop()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed 1 item&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;consumer notify: items to consume are &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">20</span>):</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            self.consume()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">producer</span>(<span class="params">Thread</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produce</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> condition</span><br><span class="line">        <span class="keyword">global</span> items</span><br><span class="line">        condition.acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(items) == <span class="number">10</span>:</span><br><span class="line">            condition.wait()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Producer notify : items producted are &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Producer notify : stop the production!!&quot;</span>)</span><br><span class="line">        items.append(<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Producer notify : total items producted &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line">        condition.notify()</span><br><span class="line">        condition.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">20</span>):</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            self.produce()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    producer = producer()</span><br><span class="line">    consumer = consumer()</span><br><span class="line">    producer.start()</span><br><span class="line">    consumer.start()</span><br><span class="line">    producer.join()</span><br><span class="line">    consumer.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="事件同步"><a href="#事件同步" class="headerlink" title="事件同步"></a>事件同步</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread,Event</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">items = []</span><br><span class="line">event = Event()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">consumer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,items,event</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.items = items</span><br><span class="line">        self.event = event</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            self.event.wait()</span><br><span class="line">            item = self.items.pop()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed 1 item&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">producer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, items, event</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.items = items</span><br><span class="line">        self.event = event</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> item</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            item = random.randint(<span class="number">0</span>, <span class="number">256</span>)</span><br><span class="line">            self.items.append(item)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Producer notify : item N° %d appended to list by %s&#x27;</span> % (item, self.name))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Producer notify : event set by %s&#x27;</span> % self.name)</span><br><span class="line">            self.event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Produce notify : event cleared by %s &#x27;</span>% self.name)</span><br><span class="line">            self.event.clear()</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = producer(items, event)</span><br><span class="line">    t2 = consumer(items, event)</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br></pre></td></tr></table></figure><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/event.png" alt="../_images/event.png" style="zoom:67%;" /></p><h4 id="使用with简化"><a href="#使用with简化" class="headerlink" title="使用with简化"></a>使用with简化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.DEBUG, <span class="built_in">format</span>=<span class="string">&#x27;(%(threadName)-10s) %(message)s&#x27;</span>,)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">threading_with</span>(<span class="params">statement</span>):</span></span><br><span class="line">    <span class="keyword">with</span> statement:</span><br><span class="line">        logging.debug(<span class="string">&#x27;%s acquired via with&#x27;</span> % statement)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">threading_not_with</span>(<span class="params">statement</span>):</span></span><br><span class="line">    statement.acquire()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logging.debug(<span class="string">&#x27;%s acquired directly&#x27;</span> % statement )</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        statement.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># let&#x27;s create a test battery</span></span><br><span class="line">    lock = threading.Lock()</span><br><span class="line">    rlock = threading.RLock()</span><br><span class="line">    condition = threading.Condition()</span><br><span class="line">    mutex = threading.Semaphore(<span class="number">1</span>)</span><br><span class="line">    threading_synchronization_list = [lock, rlock, condition, mutex]</span><br><span class="line">    <span class="comment"># in the for cycle we call the threading_with e threading_no_with function</span></span><br><span class="line">    <span class="keyword">for</span> statement <span class="keyword">in</span> threading_synchronization_list :</span><br><span class="line">       t1 = threading.Thread(target=threading_with, args=(statement,))</span><br><span class="line">       t2 = threading.Thread(target=threading_not_with, args=(statement,))</span><br><span class="line">       t1.start()</span><br><span class="line">       t2.start()</span><br><span class="line">       t1.join()</span><br><span class="line">       t2.join()</span><br></pre></td></tr></table></figure><p>此外,当线程之间共享资源时,可以利用上面的原语,也可以使用queue.队列操作起来更容易，也使多线程编程更安全，因为队列可以将资源的使用通过单线程进行完全控制，并且允许使用更加整洁和可读性更高的设计模式。</p><p>Queue常用的方法有以下四个：</p><ul><li><code>put()</code>: 往queue中放一个item</li><li><code>get()</code>: 从queue删除一个item，并返回删除的这个item</li><li><code>task_done()</code>: 每次item被处理的时候需要调用这个方法</li><li><code>join()</code>: 所有item都被处理之前一直阻塞</li></ul><h3 id="进程的并行"><a href="#进程的并行" class="headerlink" title="进程的并行"></a>进程的并行</h3><p>由父进程创建子进程。父进程既可以在产生子进程之后继续异步执行，也可以暂停等待子进程创建完成之后再继续执行.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;called function in process:%s&#x27;</span> % i)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Process_jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=foo, args=(i,))</span><br><span class="line">        Process_jobs.append(p)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure><p>使用进程对象调用 <code>join()</code> 方法。如果没有 <code>join()</code> ，主进程退出之后子进程会留在idle中，必须手动杀死它们。</p><p>进程名字与获取与线程类似.</p><h4 id="后台运行进程"><a href="#后台运行进程" class="headerlink" title="后台运行进程"></a>后台运行进程</h4><blockquote><p>如果需要处理比较巨大的任务，又不需要人为干预，将其作为后台进程执行是个非常常用的编程模型。此进程又可以和其他进程并发执行。通过Python的multiprocessing模块的后台进程选项，我们可以让进程在后台运行</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting %s \n&quot;</span> % name)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exiting %s \n&quot;</span> % name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    background_process = multiprocessing.Process(name=<span class="string">&quot;background_process&quot;</span>, target=foo)</span><br><span class="line">    background_process.daemon = <span class="literal">True</span></span><br><span class="line">    no_background_process = multiprocessing.Process(name=<span class="string">&quot;no_background_process&quot;</span>, target=foo)</span><br><span class="line">    no_background_process.daemon = <span class="literal">False</span></span><br><span class="line">    background_process.start()</span><br><span class="line">    no_background_process.start()</span><br></pre></td></tr></table></figure><p>为了在后台运行进程，我们设置 <code>daemon</code> 参数为 <code>True</code></p><p>在非后台运行的进程会看到一个输出，后台运行的没有输出，<strong>后台运行进程在主进程结束之后会自动结束</strong></p><blockquote><p>注意，后台进程不允许创建子进程。否则，当后台进程跟随父进程退出的时候，子进程会变成孤儿进程。另外，它们并不是Unix的守护进程或服务（daemons or services），所以当非后台进程退出，它们会被终结。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;i=&#123;&#125;,foo thread daemon is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, threading.current_thread().daemon))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=foo, name=<span class="string">&#x27;foo_thread&#x27;</span>, daemon=<span class="literal">True</span>) <span class="comment"># set daemon to True to make it a daemon thread which will exit when the main thread exits</span></span><br><span class="line">t.start()</span><br><span class="line"><span class="comment"># t.join() important otherwise the main thread will exit before the foo thread</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Main thread daemon is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(threading.current_thread().daemon))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Main Thread Exit.&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h4><p>可以使用 <code>terminate()</code> 方法立即杀死一个进程。另外，我们可以使用 <code>is_alive()</code> 方法来判断一个进程是否还存活。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting function&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished function&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    p = multiprocessing.Process(target=foo)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process before execution:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process running:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.terminate()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process terminated:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process joined:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process exit code:&#x27;</span>, p.exitcode)</span><br></pre></td></tr></table></figure><p>进程的 <code>ExitCode</code> 状态码（status code）验证进程已经结束， <code>ExitCode</code> 可能的值如下：</p><ul><li>== 0: 没有错误正常退出</li><li>> 0: 进程有错误，并以此状态码退出</li><li>&lt; 0: 进程被 <code>-1 *</code> 的信号杀死并以此作为 ExitCode 退出</li></ul><h4 id="子类中使用进程"><a href="#子类中使用进程" class="headerlink" title="子类中使用进程"></a>子类中使用进程</h4><p>实现一个自定义的进程子类，需要以下三步：</p><ul><li>定义 <code>Process</code> 的子类</li><li>覆盖 <code>__init__(self [,args])</code> 方法来添加额外的参数</li><li>覆盖 <code>run(self, [.args])</code> 方法来实现 <code>Process</code> 启动的时候执行的任务</li></ul><p>创建 <code>Porcess</code> 子类之后，你可以创建它的实例并通过 <code>start()</code> 方法启动它，启动之后会运行 <code>run()</code> 方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 自定义子类进程</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&#x27;called run method in process: %s&#x27;</span> % self.name)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    timestart = timeit.default_timer()</span><br><span class="line">    jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            p = MyProcess()</span><br><span class="line">            jobs.append(p)</span><br><span class="line">            p.start()</span><br><span class="line">            p.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Time elapsed:&#x27;</span>, (timeit.default_timer() - timestart))</span><br></pre></td></tr></table></figure><p><code>join()</code> 命令可以让主进程等待其他进程结束最后退出。</p><h4 id="进程中交换对象"><a href="#进程中交换对象" class="headerlink" title="进程中交换对象"></a>进程中交换对象</h4><blockquote><p>并行应用常常需要在进程之间交换数据。Multiprocessing库有两个Communication Channel可以交换对象：队列(queue)和管道（pipe）</p></blockquote><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/communication-channel.png" alt="../_images/communication-channel.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, queue</span>):</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            item = random.randint(<span class="number">0</span>, <span class="number">256</span>)</span><br><span class="line">            self.queue.put(item)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Process Producer : item %d appended to queue %s&quot;</span> % (item, self.name))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;The size of queue is %s&quot;</span> % self.queue.qsize())</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, queue</span>):</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> self.queue.empty():</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;the queue is empty&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                item = self.queue.get()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Process Consumer : item %d popped from by %s \n&#x27;</span> % (item, self.name))</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    queue = multiprocessing.Queue()</span><br><span class="line">    process_producer = Producer(queue)</span><br><span class="line">    process_consumer = Consumer(queue)</span><br><span class="line">    process_producer.start()</span><br><span class="line">    process_consumer.start()</span><br><span class="line">    process_producer.join()</span><br><span class="line">    process_consumer.join()</span><br></pre></td></tr></table></figure><p>队列还有一个 <code>JoinableQueue</code> 子类，它有以下两个额外的方法：</p><ul><li><code>task_done()</code>: 此方法意味着之前入队的一个任务已经完成，比如， <code>get()</code> 方法从队列取回item之后调用。所以此方法只能被队列的消费者调用。</li><li><code>join()</code>: 此方法将进程阻塞，直到队列中的item全部被取出并执行。</li></ul><p>此外还可以通过Pipe交换对象.</p><h4 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h4><p>多个进程可以协同工作来完成一项任务。通常需要共享数据。所以在多进程之间保持数据的一致性就很重要了。需要共享数据协同的进程必须以适当的策略来读写数据。相关的同步原语和线程的库很类似。</p><p>进程的同步原语如下：</p><ul><li><strong>Lock</strong>: 这个对象可以有两种装填：锁住的（locked）和没锁住的（unlocked）。一个Lock对象有两个方法， <code>acquire()</code> 和 <code>release()</code> ，来控制共享数据的读写权限。</li><li><strong>Event</strong>: 实现了进程间的简单通讯，一个进程发事件的信号，另一个进程等待事件的信号。 <code>Event</code> 对象有两个方法， <code>set()</code> 和 <code>clear()</code> ，来管理自己内部的变量。</li><li><strong>Condition</strong>: 此对象用来同步部分工作流程，在并行的进程中，有两个基本的方法： <code>wait()</code> 用来等待进程， <code>notify_all()</code> 用来通知所有等待此条件的进程。</li><li><strong>Semaphore</strong>: 用来共享资源，例如，支持固定数量的共享连接。</li><li><strong>Rlock</strong>: 递归锁对象。其用途和方法同 <code>Threading</code> 模块一样。</li><li><strong>Barrier</strong>: 将程序分成几个阶段，适用于有些进程必须在某些特定进程之后执行。处于障碍（Barrier）之后的代码不能同处于障碍之前的代码并行。</li></ul><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">from multiprocessing import Barrier, Lock, Process</span><br><span class="line">from time import time</span><br><span class="line">from datetime import datetime</span><br><span class="line"></span><br><span class="line">def test<span class="constructor">_with_barrier(<span class="params">synchronizer</span>, <span class="params">serializer</span>)</span>:</span><br><span class="line">    name = multiprocessing.current<span class="constructor">_process()</span>.name</span><br><span class="line">    synchronizer.wait<span class="literal">()</span></span><br><span class="line">    now = time<span class="literal">()</span></span><br><span class="line">    <span class="keyword">with</span> serializer:</span><br><span class="line">        print(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line">def test<span class="constructor">_without_barrier()</span>:</span><br><span class="line">    name = multiprocessing.current<span class="constructor">_process()</span>.name</span><br><span class="line">    now = time<span class="literal">()</span></span><br><span class="line">    print(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__<span class="operator"> == </span>&#x27;__main__&#x27;:</span><br><span class="line">    synchronizer = <span class="constructor">Barrier(2)</span></span><br><span class="line">    serializer = <span class="constructor">Lock()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p1</span> - <span class="params">test_with_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_with_barrier</span>, <span class="params">args</span>=(<span class="params">synchronizer</span>,<span class="params">serializer</span>)</span>).start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p2</span> - <span class="params">test_with_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_with_barrier</span>, <span class="params">args</span>=(<span class="params">synchronizer</span>,<span class="params">serializer</span>)</span>).start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p3</span> - <span class="params">test_without_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_without_barrier</span>)</span>.start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p4</span> - <span class="params">test_without_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_without_barrier</span>)</span>.start<span class="literal">()</span></span><br></pre></td></tr></table></figure><h4 id="进程之间管理状态"><a href="#进程之间管理状态" class="headerlink" title="进程之间管理状态"></a>进程之间管理状态</h4><p>Python的多进程模块提供了在所有的用户间管理共享信息的管理者(Manager)。一个管理者对象控制着持有Python对象的服务进程，并允许其它进程操作共享对象。</p><p>管理者有以下特性：</p><ul><li>它控制着管理共享对象的服务进程</li><li>它确保当某一进程修改了共享对象之后，所有的进程拿到额共享对象都得到了更新</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker</span>(<span class="params">dictionary,key,item</span>):</span></span><br><span class="line">    dictionary[key] = item</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;key = %d value = %d&quot;</span> %(key,item))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    mgr = multiprocessing.Manager()</span><br><span class="line">    dictionary = mgr.<span class="built_in">dict</span>()</span><br><span class="line">    jobs = [ multiprocessing.Process(target=worker,args=(dictionary,i,i*<span class="number">2</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">        j.start()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span>  jobs:</span><br><span class="line">        j.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Results:&#x27;</span>,dictionary)</span><br></pre></td></tr></table></figure><h4 id="使用进程池"><a href="#使用进程池" class="headerlink" title="使用进程池"></a>使用进程池</h4><p>多进程库提供了 <code>Pool</code> 类来实现简单的多进程任务。 <code>Pool</code> 类有以下方法：</p><ul><li><code>apply()</code>: 直到得到结果之前一直阻塞。</li><li><code>apply_async()</code>: 这是 <code>apply()</code> 方法的一个变体，返回的是一个result对象。这是一个异步的操作，在所有的子类执行之前不会锁住主进程。</li><li><code>map()</code>: 这是内置的 <code>map()</code> 函数的并行版本。在得到结果之前一直阻塞，此方法将可迭代的数据的每一个元素作为进程池的一个任务来执行。</li><li><code>map_async()</code>: 这是 <code>map()</code> 方法的一个变体，返回一个result对象。如果指定了回调函数，回调函数应该是callable的，并且只接受一个参数。当result准备好时会自动调用回调函数（除非调用失败）。回调函数应该立即完成，否则，持有result的进程将被阻塞。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_square</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">12</span>)</span><br><span class="line">    result = data*data</span><br><span class="line">    <span class="built_in">print</span>(multiprocessing.current_process().pid)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inputs = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">    pool = multiprocessing.Pool(<span class="number">2</span>)</span><br><span class="line">    pool_outputs = pool.map_async(function_square, inputs)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Pool:&#x27;</span>, pool_outputs)</span><br><span class="line">    <span class="comment"># p.daemon = True</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="异步编程"><a href="#异步编程" class="headerlink" title="异步编程"></a>异步编程</h3><h4 id="concurrent-futures"><a href="#concurrent-futures" class="headerlink" title="concurrent.futures"></a>concurrent.futures</h4><p><code>concurrent.futures</code> 模块，这个模块具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能。</p><p>此模块由以下部分组成：</p><ul><li><code>concurrent.futures.Executor</code>: 这是一个虚拟基类，提供了异步执行的方法。</li><li><code>submit(function, argument)</code>: 调度函数（可调用的对象）的执行，将 <code>argument</code> 作为参数传入。</li><li><code>map(function, argument)</code>: 将 <code>argument</code> 作为参数执行函数，以 <strong>异步</strong> 的方式。</li><li><code>shutdown(Wait=True)</code>: 发出让执行者释放所有资源的信号。</li><li><code>concurrent.futures.Future</code>: 其中包括函数的异步执行。Future对象是submit任务（即带有参数的functions）到executor的实例。</li></ul><p>Executor是抽象类，可以通过子类访问，即线程或进程的 <code>ExecutorPools</code> 。因为，线程或进程的实例是依赖于资源的任务，所以最好以“池”的形式将他们组织在一起，作为可以重用的launcher或executor。</p><blockquote><p>线程池或进程池是用于在程序中优化和简化线程/进程的使用。通过池，你可以提交任务给executor。池由两部分组成，一部分是内部的队列，存放着待执行的任务；另一部分是一系列的进程或线程，用于执行这些任务。池的概念主要目的是为了重用：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。重用不是必须的规则，但它是程序员在应用中使用池的主要原因。</p></blockquote><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/pooling-management.png" alt="../_images/pooling-management.png" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">number_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_item</span>(<span class="params">x</span>):</span></span><br><span class="line">    result_item = count(x)</span><br><span class="line">    <span class="keyword">return</span> result_item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">number</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10000000</span>):</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> i * number</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> number_list:</span><br><span class="line">        <span class="built_in">print</span>(evaluate_item(item))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sequential execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line">    start_time_1 = time.time()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">5</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        futures = [executor.submit(evaluate_item, item) <span class="keyword">for</span> item <span class="keyword">in</span> number_list]</span><br><span class="line">        <span class="keyword">for</span> future <span class="keyword">in</span> concurrent.futures.as_completed(futures):</span><br><span class="line">            <span class="built_in">print</span>(future.result())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Thread pool execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time_1), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line">    start_time_2 = time.time()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor(max_workers=<span class="number">5</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        futures = [executor.submit(evaluate_item, item) <span class="keyword">for</span> item <span class="keyword">in</span> number_list]</span><br><span class="line">        <span class="keyword">for</span> future <span class="keyword">in</span> concurrent.futures.as_completed(futures):</span><br><span class="line">            <span class="built_in">print</span>(future.result())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Process pool execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time_2), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个计算平方的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个ThreadPoolExecutor对象，设置线程池中的线程数量为2</span></span><br><span class="line"><span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">2</span>) <span class="keyword">as</span> executor:</span><br><span class="line">    <span class="comment"># 提交任务给线程池，并获取Future对象</span></span><br><span class="line">    future1 = executor.submit(square, <span class="number">5</span>)</span><br><span class="line">    future2 = executor.submit(square, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取任务的执行结果</span></span><br><span class="line">    result1 = future1.result()</span><br><span class="line">    result2 = future2.result()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Result 1: <span class="subst">&#123;result1&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Result 2: <span class="subst">&#123;result2&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Asyncio管理事件循环"><a href="#Asyncio管理事件循环" class="headerlink" title="Asyncio管理事件循环"></a>Asyncio管理事件循环</h4><p>Python的Asyncio模块提供了管理事件、协程、任务和线程的方法，以及编写并发代码的原语。此模块的主要组件和概念包括：</p><ul><li><strong>事件循环</strong>: 在Asyncio模块中，每一个进程都有一个事件循环。</li><li><strong>协程</strong>: 这是子程序的泛化概念。协程可以在执行期间暂停，这样就可以等待外部的处理（例如IO）完成之后，从之前暂停的地方恢复执行。</li><li><strong>Futures</strong>: 定义了 <code>Future</code> 对象，和 <code>concurrent.futures</code> 模块一样，表示尚未完成的计算。</li><li><strong>Tasks</strong>: 这是Asyncio的子类，用于封装和管理并行模式下的协程。</li></ul><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable"><span class="keyword">while</span></span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="variable">events</span> = <span class="function"><span class="title">getEvents</span>();</span></span><br><span class="line"><span class="function">    <span class="variable">for</span> (<span class="variable">e</span> <span class="variable"><span class="keyword">in</span></span> <span class="variable">events</span>)</span></span><br><span class="line">        <span class="function"><span class="title">processEvent</span>(<span class="variable">e</span>);</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>可以产生事件的实体叫做事件源</strong>，<strong>能处理事件的实体叫做事件处理者</strong>。此外，<strong>还有一些第三方实体叫做事件循环</strong>。它的作用是管理所有的事件，在整个程序运行过程中不断循环执行，追踪事件发生的顺序将它们放到队列中，当主线程空闲的时候，调用相应的事件处理者处理事件。</p></blockquote><p>Asyncio提供了一下方法来管理事件循环：</p><ul><li><code>loop = get_event_loop()</code>: 得到当前上下文的事件循环。</li><li><code>loop.call_later(time_delay, callback, argument)</code>: 延后 <code>time_delay</code> 秒再执行 <code>callback</code> 方法。</li><li><code>loop.call_soon(callback, argument)</code>: 尽可能快调用 <code>callback</code>, <code>call_soon()</code> 函数结束，主线程回到事件循环之后就会马上调用 <code>callback</code> 。</li><li><code>loop.time()</code>: 以float类型返回当前时间循环的内部时间。</li><li><code>asyncio.set_event_loop()</code>: 为当前上下文设置事件循环。</li><li><code>asyncio.new_event_loop()</code>: 根据此策略创建一个新的时间循环并返回。</li><li><code>loop.run_forever()</code>: 在调用 <code>stop()</code> 之前将一直运行。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_1</span>(<span class="params">end_time,loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function_1 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_2, end_time,loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_2</span>(<span class="params">end_time,loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function_2 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> loop.time() + <span class="number">1.0</span> &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_3, end_time,loop)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_3</span>(<span class="params">end_time, loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;function_3 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_1, end_time, loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_4</span>(<span class="params">end_time, loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;function_5 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_4, end_time, loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    end_loop = loop.time() + <span class="number">9.0</span></span><br><span class="line">    loop.call_soon(function_1, end_loop,loop)</span><br><span class="line">    loop.run_forever()</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Asyncio管理协程"><a href="#Asyncio管理协程" class="headerlink" title="Asyncio管理协程"></a>Asyncio管理协程</h4><p>子程序不能单独执行，只能在主程序的请求下执行，主程序负责协调使用各个子程序。协程就是子程序的泛化。和子程序一样的事，协程只负责计算任务的一步</p><p>和子程序不一样的是，协程没有主程序来进行调度。这是因为协程通过管道连接在一起，没有监视函数负责顺序调用它们。在协程中，执行点可以被挂起，可以被从之前挂起的点恢复执行。通过协程池就可以插入到计算中：运行第一个任务，直到它返回(yield)执行权，然后运行下一个，这样顺着执行下去。</p><p>协程的另外一些重要特性如下：</p><ul><li>协程可以有多个入口点，并可以yield多次</li><li>协程可以将执行权交给其他协程</li></ul><p>yield表示协程在此暂停，并且将执行权交给其他协程。因为协程可以将值与控制权一起传递给另一个协程，所以“yield一个值”就表示将值传给下一个执行的协程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">StartState</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start State called \n&quot;</span>)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State2(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Resume of the Transition : \nStart State calling &quot;</span> + result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State2</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 2 with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Evaluating...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (input_value == <span class="number">0</span>):</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State3(input_value)</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">EndState</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;End State with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Stop Computation...&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> outputValue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State3</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 3 with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Evaluating...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> EndState(input_value)</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State1</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 1 with transition value = &quot;</span> + <span class="built_in">str</span>(transition_value) + <span class="string">&quot; \n&quot;</span>)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;...Evaluating...&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State3(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State2(input_value)</span><br><span class="line">    result = <span class="string">&quot;State 1 calling &quot;</span> + result</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(StartState())</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>object async_generator can&#39;t be used in &#39;await&#39; expression</code> async函数中如果使用了yield相当于<code>async_generator</code>,后者中不能使用await.</p><h4 id="Asyncio控制任务"><a href="#Asyncio控制任务" class="headerlink" title="Asyncio控制任务"></a>Asyncio控制任务</h4><blockquote><p>Asyncio是用来处理事件循环中的异步进程和并发任务执行的。它还提供了 <code>asyncio.Task()</code> 类，可以在任务中使用协程。它的作用是，在同一事件循环中,运行某一个任务的同时可以并发地运行多个任务。当协程被包在任务中，它会自动将任务和事件循环连接起来，当事件循环启动的时候，任务自动运行。这样就提供了一个可以自动驱动协程的机制。</p></blockquote><p><code>asyncio.Task(coroutine)</code> 方法来处理计算任务，它可以调度协程的执行。任务对协程对象在事件循环的执行负责。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">factorial</span>(<span class="params">number</span>):</span></span><br><span class="line">    f = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, number + <span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: Compute factorial(%s)&quot;</span> % (i))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        f *= i</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: factorial(%s) = %s&quot;</span> % (number, f))</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span>(<span class="params">number</span>):</span></span><br><span class="line">    a,b = <span class="number">0</span>,<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: Compute fibonacci(%s)&quot;</span> % (i))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        a,b = b, a + b</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: fibonacci(%s) = %s&quot;</span> % (number, a))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tasks = [asyncio.Task(factorial(<span class="number">10</span>)), asyncio.Task(fibonacci(<span class="number">10</span>))]</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><h4 id="使用asyncio和futures"><a href="#使用asyncio和futures" class="headerlink" title="使用asyncio和futures"></a>使用asyncio和futures</h4><blockquote><p>Asyncio 模块的另一个重要的组件是 <code>Future</code> 类。它和 <code>concurrent.futures.Futures</code> 很像，但是针对Asyncio的事件循环做了很多定制。 <code>asyncio.Futures</code> 类代表还未完成的结果（有可能是一个Exception）。所以综合来说，它是一种抽象，代表还没有做完的事情。</p></blockquote><ul><li><code>cancel()</code>: 取消future的执行，调度回调函数</li><li><code>result()</code>: 返回future代表的结果</li><li><code>exception()</code>: 返回future中的Exception</li><li><code>add_done_callback(fn)</code>: 添加一个回调函数，当future执行的时候会调用这个回调函数</li><li><code>remove_done_callback(fn)</code>: 从“call whten done”列表中移除所有callback的实例</li><li><code>set_result(result)</code>: 将future标为执行完成，并且设置result的值</li><li><code>set_exception(exception)</code>: 将future标为执行完成，并设置Exception</li></ul><p>类似于js的Promise?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line">future = asyncio.Future</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">first_coroutine</span>(<span class="params">future, N</span>):</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N + <span class="number">1</span>):</span><br><span class="line">        count += i</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">3</span>)</span><br><span class="line">    future.set_result(<span class="string">&quot;First coroutine total count: &quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">second_coroutine</span>(<span class="params">future, N</span>):</span></span><br><span class="line">    count = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, N + <span class="number">1</span>):</span><br><span class="line">        count *= i</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future.set_result(<span class="string">&quot;Second coroutine total count: &quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">got_result</span>(<span class="params">future</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(future.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    N = <span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    future1 = asyncio.Future()</span><br><span class="line">    future2 = asyncio.Future()</span><br><span class="line">    tasks = [</span><br><span class="line">        first_coroutine(future1, N),</span><br><span class="line">        second_coroutine(future2, N)</span><br><span class="line">    ]</span><br><span class="line">    future1.add_done_callback(got_result)</span><br><span class="line">    future2.add_done_callback(got_result)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/chapter2/01_Introduction.html">1. 介绍 — python-parallel-programming-cookbook-cn 1.0 文档 (python-parallel-programmning-cookbook.readthedocs.io)</a></li><li><a href="https://blog.csdn.net/weixin_45665318/article/details/106686332">[Python 多线程] 详解daemon属性值None,False,True的区别_daemon=true-CSDN博客</a></li><li><a href="https://ruanyifeng.com/blog/2019/11/python-asyncio.html">Python 异步编程入门 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://www.freecodecamp.org/chinese/news/introduction-to-python-threading/">介绍 Python 线程及其实现 (freecodecamp.org)</a></li><li><a href="https://medium.com/dev-bits/a-minimalistic-guide-for-understanding-asyncio-in-python-52c436c244ea">A minimalistic guide for understanding asyncio in Python | by Naren Yellavula | Dev bits | Medium</a></li><li><a href="https://tutorialedge.net/python/concurrency/getting-started-with-asyncio-python/">Getting Started with Asyncio in Python | TutorialEdge.net</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;主要是因为Python库的设计很不错,通过这门语言进一步学习并行,涉及到进程线程以及异步编程等.建议是对性能有要求的利用其他语言实现,但是基本的思想、方法是一样的.&lt;br&gt;</summary>
    
    
    
    
    <category term="parallel" scheme="https://www.sekyoro.top/tags/parallel/"/>
    
  </entry>
  
  <entry>
    <title>目标检测学习_P2</title>
    <link href="https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/"/>
    <id>https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/</id>
    <published>2023-10-17T02:21:51.000Z</published>
    <updated>2023-11-02T13:59:11.299Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。</p><span id="more"></span><p>R-CNN家族中的模型都是基于regions的。检测分为两个阶段：</p><p>（1）首先，该模型通过<strong>选择搜索</strong>或<strong>区域建议网络</strong>来提出一组感兴趣的区域。所提出的区域是稀疏的，因为潜在的边界框候选者可以是无限的。</p><p>（2） 然后分类器只处理候选区域。</p><p>这里深入细节实现R-CNN系列的检测网络.</p><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><blockquote><p>R-CNN (<a href="https://arxiv.org/abs/1311.2524">Girshick et al., 2014</a>) is short for “Region-based Convolutional Neural Networks”. The main idea is composed of two steps. First, using <a href="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#selective-search">selective search</a>, it identifies a manageable number of bounding-box object region candidates (“region of interest” or “RoI”). And then it extracts CNN features from each region independently for classification.</p></blockquote><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png" alt="img"></p><h4 id="选择算法-selective-search"><a href="#选择算法-selective-search" class="headerlink" title="选择算法(selective search)"></a>选择算法(selective search)</h4><p>主要涉及到选择算法,用于提供可能包含对象的区域建议。它建立在图像分割输出的基础上，并使用基于区域的特征（注意：不仅仅是单个像素的属性）来进行自下而上的分层分组。</p><ol><li>在初始化阶段，首先应用Felzenszwalb和Huttenlocher的基于图的图像分割算法来创建区域。</li><li>使用贪婪算法迭代地将区域分组在一起：<ul><li>首先计算所有相邻区域之间的相似性。</li><li>将两个最相似的区域分组在一起，并计算得到的区域与其相邻区域之间的新相似性。</li></ul></li><li>重复对最相似区域进行分组的过程（步骤2），直到整个图像变成单个区域。</li></ol><p>可以使用颜色,材质,大小和形状作为相似度量.</p><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/selective-search-algorithm.png" alt="img" style="zoom:67%;" /></p><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p>R-CNN流程:</p><ol><li><p>使用一个预训练CNN网络,假设网络输出是K类.</p></li><li><p>通过选择性搜索提出与类别无关的感兴趣区域（每个图像约2k个候选）。这些区域可能包含目标对象，并且它们具有不同的大小。</p></li><li><p>区域被扭曲成一个固定大小.</p></li><li><p>对于第K+1类,在一个扭曲的候选区域上微调CNN(附加的一个类指的是背景（没有感兴趣的对象)）。在微调阶段，我们应该使用更小的学习率，并且小批量对阳性病例进行过采样，因为大多数提出的区域只是背景。</p></li><li><p>给定每个图像区域，通过CNN的一次正向传播生成一个特征向量。然后，该特征向量输入针对每个类独立训练的二进制SVM。</p><p>正样本是IoU&gt;=0.3的区域，而负样本是不相关的其他区域。</p></li><li><p>为了减少定位误差，训练回归模型来使用CNN特征校正边界框校正偏移上的预测检测窗口。</p></li></ol><p><img data-src="https://i.imgur.com/8NYb1o7.png" alt="image-20231022172302856"></p><h4 id="常用技巧"><a href="#常用技巧" class="headerlink" title="常用技巧"></a>常用技巧</h4><ol><li>Non-Maximum Suppression</li></ol><p>模型可能能够为同一对象找到多个边界框。非极大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。然后跳过与这个边界框具有高IoU（即大于0.5）的剩余框,重复这个过程直到挑选出需要数量的bbox</strong></p><blockquote><p>非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于人脸的概率 分别为A、B、C、D、E、F。</p><ol><li>从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</li><li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</li><li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</li><li>就这样一直重复，找到所有被保留下来的矩形框。</li></ol></blockquote><ol><li>Hard Negative Mining</li></ol><p>我们将没有对象的边界框视为Negative示例。</p><p>并非所有的Negative例子都同样难以识别。例如，如果它包含纯空背景，那么它很可能是一个“容易否定的”；但是，<strong>如果盒子中包含奇怪的嘈杂纹理或部分对象，可能很难被识别为背景，这些都是“硬阴性”</strong>。严厉的反面例子很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。</p><blockquote><p>也就说增加容易被FP的数据</p></blockquote><p>通过查看R-CNN的学习步骤，您可以很容易地发现训练R-CNN模型既昂贵又缓慢，因为以下步骤需要大量工作：</p><ol><li>运行选择性搜索，为每个图像提出2000个区域候选</li><li>为每个图像区域生成CNN特征向量（N个图像*2000）</li><li>整个过程分别涉及三个模型，没有太多的共享计算：用于图像分类和特征提取的卷积神经网络；用于识别目标对象的顶部SVM分类器；以及用于收紧区域边界框的回归模型。</li></ol><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>opencv实现了选择性算法.</p><p>可以参考<a href="https://github.com/Hulkido/RCNN/tree/master">Hulkido/RCNN: FULL Implementation of RCNN from scratch (github.com)</a></p><h4 id="生成Region-proposals"><a href="#生成Region-proposals" class="headerlink" title="生成Region proposals"></a>生成Region proposals</h4><blockquote><p>区域建议(Region proposals)只是图像的较小区域，可能包含我们在输入图像中搜索的对象。为了减少R-CNN中的区域建议，使用了一种称为选择性搜索的贪婪算法。</p></blockquote><p>首先需要定义IoU计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculating dimension of common area between these two boxes.</span></span><br><span class="line">x_left = <span class="built_in">max</span>(bb1[<span class="string">&#x27;x1&#x27;</span>], bb2[<span class="string">&#x27;x1&#x27;</span>])</span><br><span class="line">y_bottom = <span class="built_in">max</span>(bb1[<span class="string">&#x27;y1&#x27;</span>], bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">x_right = <span class="built_in">min</span>(bb1[<span class="string">&#x27;x2&#x27;</span>], bb2[<span class="string">&#x27;x2&#x27;</span>])</span><br><span class="line">y_top = <span class="built_in">min</span>(bb1[<span class="string">&#x27;y2&#x27;</span>], bb2[<span class="string">&#x27;y2&#x27;</span>])</span><br><span class="line"><span class="comment"># if there is no overlap output 0 as intersection area is zero.</span></span><br><span class="line"><span class="keyword">if</span> x_right &lt; x_left <span class="keyword">or</span> y_bottom &lt; y_top:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"><span class="comment"># calculating intersection area.</span></span><br><span class="line"><span class="comment"># 计算交集</span></span><br><span class="line">intersection_area = (x_right - x_left) * (y_top - y_bottom)</span><br><span class="line"><span class="comment"># individual areas of both these bounding boxes.</span></span><br><span class="line"><span class="comment"># 计算各自区域面积</span></span><br><span class="line">bb1_area = (bb1[<span class="string">&#x27;x2&#x27;</span>] - bb1[<span class="string">&#x27;x1&#x27;</span>]) * (bb1[<span class="string">&#x27;y2&#x27;</span>] - bb1[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">bb2_area = (bb2[<span class="string">&#x27;x2&#x27;</span>] - bb2[<span class="string">&#x27;x1&#x27;</span>]) * (bb2[<span class="string">&#x27;y2&#x27;</span>] - bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line"><span class="comment"># union area = area of bb1_+ area of bb2 - intersection of bb1 and bb2.</span></span><br><span class="line"><span class="comment"># 并集就是各自之和减去交集</span></span><br><span class="line">iou = intersection_area / <span class="built_in">float</span>(bb1_area + bb2_area - intersection_area)</span><br></pre></td></tr></table></figure><p>遍历选择性搜索得到的区域,计算每个区域与对应bbox(bounding box)的IoU.</p><p>然后将iou大于阈值(这里设置为0.7)的定为region proposal,同时resize这个区域建议.</p><blockquote><p>本身应该是warp,但我觉得差别不大.另外region proposal和ROI差别其实也不大,有说法是<strong>region proposal是对图像而言的，roi是针对feature map上的</strong>.</p></blockquote><p><img data-src="https://i.imgur.com/j8O2rCN.png" alt="image-20231022171507221"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">cv2.setUseOptimized(<span class="literal">True</span>);</span><br><span class="line">ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span><br><span class="line">ss.setBaseImage(image)   <span class="comment"># setting given image as base image</span></span><br><span class="line">            ss.switchToSelectiveSearchFast()     <span class="comment"># running selective search on bae image</span></span><br><span class="line">            ssresults = ss.process()     <span class="comment"># processing to get the outputs</span></span><br><span class="line">            imout = image.copy()</span><br><span class="line">            counter = <span class="number">0</span></span><br><span class="line">            falsecounter = <span class="number">0</span></span><br><span class="line">            flag = <span class="number">0</span></span><br><span class="line">            fflag = <span class="number">0</span></span><br><span class="line">            bflag = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> e,result <span class="keyword">in</span> <span class="built_in">enumerate</span>(ssresults):</span><br><span class="line">                <span class="keyword">if</span> e &lt; <span class="number">2000</span> <span class="keyword">and</span> flag == <span class="number">0</span>:     <span class="comment"># till 2000 to get top 2000 regions only</span></span><br><span class="line">                    <span class="keyword">for</span> gtval <span class="keyword">in</span> gtvalues:</span><br><span class="line">                        x,y,w,h = result</span><br><span class="line">                        iou = get_iou(gtval,&#123;<span class="string">&quot;x1&quot;</span>:x,<span class="string">&quot;x2&quot;</span>:x+w,<span class="string">&quot;y1&quot;</span>:y,<span class="string">&quot;y2&quot;</span>:y+h&#125;)  <span class="comment"># calculating IoU for each of the proposed regions</span></span><br><span class="line">                        <span class="keyword">if</span> counter &lt; <span class="number">30</span>:       <span class="comment"># getting only 30 psoitive examples</span></span><br><span class="line">                            <span class="keyword">if</span> iou &gt; <span class="number">0.70</span>:     <span class="comment"># IoU or being positive is 0.7</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">1</span>)</span><br><span class="line">                                counter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            fflag =<span class="number">1</span>              <span class="comment"># to insure we have collected all psotive examples</span></span><br><span class="line">                        <span class="keyword">if</span> falsecounter &lt;<span class="number">30</span>:      <span class="comment"># 30 negatve examples are allowed only</span></span><br><span class="line">                            <span class="keyword">if</span> iou &lt; <span class="number">0.3</span>:         <span class="comment"># IoU or being negative is 0.3</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">0</span>)</span><br><span class="line">                                falsecounter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            bflag = <span class="number">1</span>             <span class="comment">#to ensure we have collected all negative examples</span></span><br><span class="line">                    <span class="keyword">if</span> fflag == <span class="number">1</span> <span class="keyword">and</span> bflag == <span class="number">1</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&quot;inside&quot;</span>)</span><br><span class="line">                        flag = <span class="number">1</span>        <span class="comment"># to signal the complition of data extaction from a particular image</span></span><br></pre></td></tr></table></figure><p>上面一共最多遍历生成的2000个ROI,选取其中的正例(超过阈值)的30,负例30.</p><p>下面是一张图像与其得到的正例和负例</p><p><img data-src="https://i.imgur.com/elheTvF.png" alt="image-20231022174714613"></p><p><img data-src="https://i.imgur.com/YTCcKjA.png" alt="image-20231022174748063"></p><h4 id="使用CNN模型二分类"><a href="#使用CNN模型二分类" class="headerlink" title="使用CNN模型二分类"></a>使用CNN模型二分类</h4><p>一般直接使用预训练模型,这里使用keras,加载VGG16模型.这里只做目标是否在而不做具体分类,所以输出单个值作为二分类.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vgg = tf.keras.applications.vgg16.VGG16(include_top=<span class="literal">True</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>, input_tensor=<span class="literal">None</span>, input_shape=<span class="literal">None</span>, pooling=<span class="literal">None</span>, classes=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.layers[:-<span class="number">2</span>]:</span><br><span class="line">  layer.trainable = <span class="literal">False</span></span><br><span class="line">x = vgg.get_layer(<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">last_output =  x.output</span><br><span class="line">x = tf.keras.layers.Dense(<span class="number">1</span>,activation = <span class="string">&#x27;sigmoid&#x27;</span>)(last_output)</span><br><span class="line">model = tf.keras.Model(vgg.<span class="built_in">input</span>,x)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = <span class="string">&quot;adam&quot;</span>,</span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              metrics = [<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line">model.fit(X_new,Y_new,batch_size = <span class="number">64</span>,epochs = <span class="number">3</span>, verbose = <span class="number">1</span>,validation_split=<span class="number">.05</span>,shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="预训练模型提取特征再使用SVM二分类"><a href="#预训练模型提取特征再使用SVM二分类" class="headerlink" title="预训练模型提取特征再使用SVM二分类"></a>预训练模型提取特征再使用SVM二分类</h4><p>原文中为每一类使用了一个二分类的SVM(毕竟一般的一个SVM只能二分类)</p><p>可以再上面的预训练模型只使用特征提取层,然后加个SVM</p><h4 id="bbox-regression"><a href="#bbox-regression" class="headerlink" title="bbox regression"></a>bbox regression</h4><p>最后需要求的bbox的回归用于修正误差.</p><blockquote><p>回归后得到四个参数，即x，y中心点偏移量和高、宽缩放因子，利用这四个参数对剩余的高质量目标建议框进行调整，取得分最高的称为Bounding Box，完成定位任务。</p></blockquote><p><img data-src="https://i.imgur.com/2xOfOpy.png" alt="image-20231031112707018"></p><p>如图,x,y坐标的修正由p~w~d~x~(p)+p~x~,而w,h由p~w~exp(d~w~(p))修正,已知t~i~函数,需要求d~i~(p)的回归值.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">curr_iou = iou([gta[bbox_num, <span class="number">0</span>], gta[bbox_num, <span class="number">2</span>], gta[bbox_num, <span class="number">1</span>], gta[bbox_num, <span class="number">3</span>]], [x1_anc, y1_anc, x2_anc, y2_anc])</span><br><span class="line"><span class="comment"># calculate the regression targets if they will be needed</span></span><br><span class="line"><span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class="keyword">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class="line">cx = (gta[bbox_num, <span class="number">0</span>] + gta[bbox_num, <span class="number">1</span>]) / <span class="number">2.0</span> <span class="comment"># center point of bbox</span></span><br><span class="line">cy = (gta[bbox_num, <span class="number">2</span>] + gta[bbox_num, <span class="number">3</span>]) / <span class="number">2.0</span></span><br><span class="line">cxa = (x1_anc + x2_anc)/<span class="number">2.0</span> <span class="comment"># center point of anchor box which scales to resized image</span></span><br><span class="line">cya = (y1_anc + y2_anc)/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">tx = (cx - cxa) / (x2_anc - x1_anc) <span class="comment"># regression targets</span></span><br><span class="line">ty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class="line">tw = np.log((gta[bbox_num, <span class="number">1</span>] - gta[bbox_num, <span class="number">0</span>]) / (x2_anc - x1_anc))</span><br><span class="line">th = np.log((gta[bbox_num, <span class="number">3</span>] - gta[bbox_num, <span class="number">2</span>]) / (y2_anc - y1_anc))</span><br><span class="line">                            <span class="keyword">if</span> img_data[<span class="string">&#x27;bboxes&#x27;</span>][bbox_num][<span class="string">&#x27;class&#x27;</span>] != <span class="string">&#x27;bg&#x27;</span>:</span><br><span class="line"></span><br><span class="line"><span class="comment"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class="line"><span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class="line">best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class="line">best_iou_for_bbox[bbox_num] = curr_iou</span><br><span class="line">best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class="line">best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]</span><br></pre></td></tr></table></figure><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>为了使R-CNN更快，Girshick（2015）通过将三个独立的模型统一到一个联合训练的框架中并增加共享计算结果（称为Fast R-CNN）来改进训练过程。</p><p>不同于R-CNN对于每个region proposals提取特征,，而是将它们聚合到整个图像上的一个 CNN 前向传递中，并且region proposals共享此特征矩阵。然后，将相同的特征矩阵分支出来，用于学习对象分类器和边界框回归器。总之，计算共享加速了R-CNN。</p><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/fast-RCNN.png" alt="img"></p><h4 id="ROI-Pooling"><a href="#ROI-Pooling" class="headerlink" title="ROI Pooling"></a>ROI Pooling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">16</span>).reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">rois = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])</span><br><span class="line">torchvision.ops.roi_pool(X,rois, output_size=(<span class="number">2</span>, <span class="number">2</span>), spatial_scale=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-pooling.png" alt="img" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roi_pooling</span>(<span class="params">features, rois, output_size</span>):</span></span><br><span class="line">    <span class="comment"># features: 输入特征图 (N, C, H, W)</span></span><br><span class="line">    <span class="comment"># rois: 区域候选框 (N, 4) 其中每行表示一个候选框的坐标 (x1, y1, x2, y2)</span></span><br><span class="line">    <span class="comment"># output_size: ROI Pooling的输出尺寸 (H&#x27;, W&#x27;)</span></span><br><span class="line"></span><br><span class="line">    num_rois = rois.size(<span class="number">0</span>)</span><br><span class="line">    output = torch.zeros(num_rois, features.size(<span class="number">1</span>), output_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_rois):</span><br><span class="line">        roi = rois[i]</span><br><span class="line">        x1, y1, x2, y2 = roi</span><br><span class="line">        roi_features = features[:, :, y1:y2 + <span class="number">1</span>, x1:x2 + <span class="number">1</span>]</span><br><span class="line">        roi_features = F.adaptive_max_pool2d(roi_features, output_size)</span><br><span class="line">        output[i] = roi_features</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><blockquote><p>上面的算法其实使用的是pytorch的adaptive_max_pool2d,这两个东西并不一样,但很多简易实现就是利用了这个函数</p></blockquote><p>由于要进行分类和回归(都使用一个FC),需要一个固定大小的输入.由于此时输入已经成了可训练的feature map而不是直接的图像,需要一个可微分的操作.</p><p>RoI 池化类似于 max-pooling。说白了就是将原本的region proposals分成hxw的grid,这里的h,w就是后面fc需要的输入,在每个grid中做max pooling</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastRCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        vgg = torchvision.models.vgg19_bn(pretrained=<span class="literal">True</span>)</span><br><span class="line">        self.features = nn.Sequential(*<span class="built_in">list</span>(vgg.features.children())[:-<span class="number">1</span>])</span><br><span class="line">        self.roipool = ROIPooling(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">        <span class="comment">#roipooling之后得到B,C,7,7直接</span></span><br><span class="line">        self.output = nn.Sequential(*<span class="built_in">list</span>(vgg.classifier.children())[:-<span class="number">1</span>])</span><br><span class="line">        self.prob = nn.Linear(<span class="number">4096</span>, num_classes+<span class="number">1</span>)</span><br><span class="line">        self.loc = nn.Linear(<span class="number">4096</span>, <span class="number">4</span> * (num_classes + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.cat_loss = nn.CrossEntropyLoss()</span><br><span class="line">        self.loc_loss = nn.SmoothL1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img, rois, roi_idx</span>):</span></span><br><span class="line">        res = self.features(img)</span><br><span class="line">        res = self.roipool(res, rois, roi_idx)</span><br><span class="line">        res = res.view(res.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        features = self.output(res)</span><br><span class="line">        prob = self.prob(features)</span><br><span class="line">        loc = self.loc(features).view(-<span class="number">1</span>, self.num_classes+<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> prob, loc</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, prob, bbox, label, gt_bbox, lmb=<span class="number">1.0</span></span>):</span></span><br><span class="line">        loss_cat = self.cat_loss(prob, label)</span><br><span class="line">        lbl = label.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        mask = (label != <span class="number">0</span>).<span class="built_in">float</span>().view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        loss_loc = self.loc_loss(gt_bbox * mask, bbox.gather(<span class="number">1</span>, lbl).squeeze(<span class="number">1</span>) * mask)</span><br><span class="line">        loss = loss_cat + lmb * loss_loc</span><br><span class="line">        <span class="keyword">return</span> loss, loss_cat, loss_loc</span><br></pre></td></tr></table></figure><ol><li>首先，在图像分类任务上预训练卷积神经网络。</li><li>通过选择性搜索提出区域建议（每张图像~2k个候选者）。</li><li>更改预训练的 CNN：</li><li><ul><li>将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。</li><li>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。</li></ul></li><li>最后，模型分为两个输出:K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。</li></ol><p>损失函数有分类损失和回归损失,回归损失用于计算bouding box的回归损失,可使用L1损失。</p><script type="math/tex; mode=display">L_1^\text{smooth}(x) = \begin{cases}    0.5 x^2             & \text{if } \vert x \vert < 1\\    \vert x \vert - 0.5 & \text{otherwise}\end{cases}</script><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/l1-smooth.png" alt="img"></p><blockquote><p>在Fast RCNN中,改进并不显著，因为区域提案是由另一个模型单独生成的，而且非常耗时。</p></blockquote><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h4><p>首先，在图像分类任务上预训练卷积神经网络。</p><p>通过选择性搜索提出区域建议（每张图像~2k个候选者）。</p><p>更改预训练的 CNN：将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。</p><p>RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。</p><p>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。最后，模型分支为两个输出层：K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。</p><p>其中关键是如何利用选择搜索得到的region proposals映射到通过CNN得到的feature map上,这样就只用在整个图像上进行一次CNN而不是单独在每个region proposal上滤波.如果只进行一次滤波,如何<code>准确地将输入图像的一个区域投影到卷积特征图的一个区域上</code>,此外还有ROI Pooling得到固定的ROI区域也是关键.</p><p>SPPNet介绍了 ROI Projection,看起来貌似就是算回去,不过在设计CNN时,padding = kernel_size/2这样避免计算复杂</p><p><img data-src="https://i.imgur.com/xsrzh3i.png" alt="image-20231029125650323"></p><p>​    </p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*U95lm-Jkwkpy3p8X6IOKlQ.png" alt="img"></p><p>Loss包括分类损失和bouding box回归损失</p><p><img data-src="https://i.imgur.com/2leLfbr.png" alt="image-20231029204052670"></p><h3 id="Faster-RNN"><a href="#Faster-RNN" class="headerlink" title="Faster RNN"></a>Faster RNN</h3><p>重点说一下faster rcnn</p><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/faster-RCNN.png" alt="img"></p><p>将选择性搜索算法(也就是用于生成region proposals的算法)融合到深度学习模型中</p><p><img data-src="http://zh.d2l.ai/_images/faster-rcnn.svg" alt="../_images/faster-rcnn.svg"></p><p>比如下面将一张图像通过一个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">ef nn_base(input_tensor=<span class="literal">None</span>, trainable=<span class="literal">False</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Determine proper input shape</span></span><br><span class="line">    <span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_first&#x27;</span>:</span><br><span class="line">        input_shape = (<span class="number">3</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_shape = (<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_tensor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img_input = Input(shape=input_shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> K.is_keras_tensor(input_tensor):</span><br><span class="line">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img_input = input_tensor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_last&#x27;</span>:</span><br><span class="line">        bn_axis = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bn_axis = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    x = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(img_input)</span><br><span class="line"></span><br><span class="line">    x = Convolution2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">&#x27;conv1&#x27;</span>, trainable = trainable)(x)</span><br><span class="line">    x = FixedBatchNormalization(axis=bn_axis, name=<span class="string">&#x27;bn_conv1&#x27;</span>)(x)</span><br><span class="line">    x = Activation(<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    x = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;a&#x27;</span>, strides=(<span class="number">1</span>, <span class="number">1</span>), trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;a&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;d&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;a&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;d&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;e&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;f&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>提出了<strong>Region Proposal Networks</strong></p><h4 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h4><p>主要更改就是对于roi projection,之前还是在原图上使用ss得到region proposals然后映射到feature map上,现在用区域提案网络 (RPN)替代,其他部分不变.</p><p>RPN,简单地说，它是一个小型的全卷积网络，它接受feature map，并输出一组区域和每个区域的“objectness”分数（该区域包含对象的可能性）。</p><p><img data-src="https://i.imgur.com/BL8dkf1.png" alt="image-20231029172807621"></p><p>首先使用一个cnn模型提取特征得到feature map,而RPN就会利用这个feature map,并且提出了anchor boxes概念,在feature map上使用一个大小为3x3的sliding window,在sliding window移动到一个位置的时候生成多个不同大小的anchor boxes.</p><p>在生成相关数据的时候,对于一个feature map,由于它已经经过多次卷积包含许多信息,在这个feature map上生成anchor box.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialise empty output objectives</span></span><br><span class="line">y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))</span><br><span class="line">y_is_box_valid = np.zeros((output_height, output_width, num_anchors))</span><br><span class="line">y_rpn_regr = np.zeros((output_height, output_width, num_anchors * <span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>其中num_anchors设定为9,包含三种不同大小以及比例的的anchor box.</p><p><code>y_rpn_overlap</code>表示这个anchor box是否包含物体,在生成数据时会将与bbox的iou大于一个阈值(比如0.7)的作为pos,小于一个阈值(0.3)作为neg表示不包含物体,也就是背景.在代码中,如果一个大于0.7的anchor box都没有那就直接取与bounding box的iou最大的作为包含物体的anchor box正例.</p><p><code>y_is_box_valid</code>表示所有正负例anchor box,去掉iou在0.3~0.7的,认为这些anchor box不明显.一般这个值会设置一个固定值,注意,生成的anchor box如果是pos,只表示其与其中一个或多个bbox的iou大于阈值,并不能知道其与哪个物体位置更相近.</p><p><code>y_rpn_regr</code>表示<code>anchor box</code>的具体位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_cls, y_rpn_regr = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class="number">1</span>)</span><br><span class="line">y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class="number">4</span>, axis=<span class="number">1</span>), y_rpn_regr], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>计算rpn输出时为什么要caoncat这些数据?这是在代码实现层面的事,calc_rpn计算得到固定个数的正负例作为训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative</span></span><br><span class="line"><span class="comment"># regions. We also limit it to 256 regions.</span></span><br><span class="line">num_regions = <span class="number">256</span></span><br><span class="line"><span class="comment"># 对于一张图 只选取256个anchor box</span></span><br><span class="line"><span class="comment"># change from len(pos_locs[0]) to len(pos_locs)</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pos_locs) &gt; num_regions/<span class="number">2</span>:</span><br><span class="line">val_locs = random.sample(<span class="built_in">range</span>(<span class="built_in">len</span>(pos_locs[<span class="number">0</span>])), <span class="built_in">len</span>(pos_locs[<span class="number">0</span>]) - num_regions/<span class="number">2</span>)</span><br><span class="line">y_is_box_valid[<span class="number">0</span>, pos_locs[<span class="number">0</span>][val_locs], pos_locs[<span class="number">1</span>][val_locs], pos_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br><span class="line">num_pos = num_regions/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(neg_locs) + num_pos &gt; num_regions:</span><br><span class="line">val_locs = random.sample(<span class="built_in">range</span>(<span class="built_in">len</span>(neg_locs[<span class="number">0</span>])), <span class="built_in">len</span>(neg_locs[<span class="number">0</span>]) - num_pos)</span><br><span class="line">y_is_box_valid[<span class="number">0</span>, neg_locs[<span class="number">0</span>][val_locs], neg_locs[<span class="number">1</span>][val_locs], neg_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  这里 y_rpn_regr 为什么要concatenate四个正例?</span></span><br><span class="line">y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class="number">1</span>)</span><br><span class="line">y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class="number">4</span>, axis=<span class="number">1</span>), y_rpn_regr], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#  y_rpn_cls  (1, 18, 26, 35)</span></span><br><span class="line"><span class="comment">#  y_rpn_regr (1, 72, 26, 35)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpn_accuracy_rpn_monitor = []</span><br><span class="line">rpn_accuracy_for_epoch = []</span><br></pre></td></tr></table></figure><p>训练代码中设计了<code>rpn_accuracy_rpn_monitor</code>和<code>rpn_accuracy_rpn_monitor</code>,前者会在处理过的图像达到一定数量后计算rpn的准确率,也就是生成的roi是否总是正例.</p><p>后者会在每次epoch完后计算mean loss和acc,因为在这个目标检测任务中,每次epoch的batch_size并不是像其他任务取多张图像,而是每次epoch中又取epoch_length,每次取一张图片.</p><h5 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h5><blockquote><p>RPN如何设计的,3x3的sliding window有什么用,如果是为了生成anchor box,有必要设计一个sliding window吗?</p></blockquote><p>在通过一个预训练模型的feature层后(比如vgg,resnet,inception等等),再使用一个3x3的sliding window,通道数256(可以理解为再聚合一下信息),然后在原本的feature map上针对每个pixel生成多个anchor box.</p><p>遍历一个图像上的所有bbox,看它与生成的anchor box的iou,取一个大于0.7的最大的anchor box的pos,其余的小于0.3的是neg.</p><p>首先将feature map做一次3x3卷积,在进行全卷积的输出分别得到针对一个anchor box的二分类以及修正坐标(坐标回归)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn</span>(<span class="params">base_layers,num_anchors</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># important! same width,hight</span></span><br><span class="line">    x = Convolution2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_initializer=<span class="string">&#x27;normal&#x27;</span>, name=<span class="string">&#x27;rpn_conv1&#x27;</span>)(base_layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get class and regr output</span></span><br><span class="line">    <span class="comment"># fully convolution</span></span><br><span class="line">    x_class = Convolution2D(num_anchors, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>, kernel_initializer=<span class="string">&#x27;uniform&#x27;</span>, name=<span class="string">&#x27;rpn_out_class&#x27;</span>)(x)</span><br><span class="line">    x_regr = Convolution2D(num_anchors * <span class="number">4</span>, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;linear&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>, name=<span class="string">&#x27;rpn_out_regress&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure><p>然后利用roi pooling以及classifier输出针对于21类的概率和20类回归坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span>(<span class="params">base_layers, input_rois, num_rois, nb_classes = <span class="number">21</span>, trainable=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> K.backend() == <span class="string">&#x27;tensorflow&#x27;</span>:</span><br><span class="line">        pooling_regions = <span class="number">14</span></span><br><span class="line">        input_shape = (num_rois,<span class="number">14</span>,<span class="number">14</span>,<span class="number">1024</span>)</span><br><span class="line">    <span class="keyword">elif</span> K.backend() == <span class="string">&#x27;theano&#x27;</span>:</span><br><span class="line">        pooling_regions = <span class="number">7</span></span><br><span class="line">        input_shape = (num_rois,<span class="number">1024</span>,<span class="number">7</span>,<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class="line">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    out = TimeDistributed(Flatten())(out)</span><br><span class="line"></span><br><span class="line">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class="string">&#x27;softmax&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>), name=<span class="string">&#x27;dense_class_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(nb_classes))(out)</span><br><span class="line">    <span class="comment"># note: no regression target for bg class</span></span><br><span class="line">    out_regr = TimeDistributed(Dense(<span class="number">4</span> * (nb_classes-<span class="number">1</span>), activation=<span class="string">&#x27;linear&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>), name=<span class="string">&#x27;dense_regress_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(nb_classes))(out)</span><br><span class="line">    <span class="keyword">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure><p>这样就训练三个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_rpn = Model(img_input, rpn[:<span class="number">2</span>])</span><br><span class="line">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class="line"></span><br><span class="line"><span class="comment"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class="line">model_all = Model([img_input, roi_input], rpn[:<span class="number">2</span>] + classifier)</span><br></pre></td></tr></table></figure><p>rpn损失计算,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_regr</span>(<span class="params">num_anchors</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_regr_fixed_num</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line"><span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_first&#x27;</span>:</span><br><span class="line">x = y_true[:, <span class="number">4</span> * num_anchors:, :, :] - y_pred</span><br><span class="line">x_abs = K.<span class="built_in">abs</span>(x)</span><br><span class="line">x_bool = K.less_equal(x_abs, <span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_regr * K.<span class="built_in">sum</span>(</span><br><span class="line">y_true[:, :<span class="number">4</span> * num_anchors, :, :] * (x_bool * (<span class="number">0.5</span> * x * x) + (<span class="number">1</span> - x_bool) * (x_abs - <span class="number">0.5</span>))) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :<span class="number">4</span> * num_anchors, :, :])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">x = y_true[:, :, :, <span class="number">4</span> * num_anchors:] - y_pred</span><br><span class="line">x_abs = K.<span class="built_in">abs</span>(x)</span><br><span class="line">x_bool = K.cast(K.less_equal(x_abs, <span class="number">1.0</span>), tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> lambda_rpn_regr * K.<span class="built_in">sum</span>(</span><br><span class="line">y_true[:, :, :, :<span class="number">4</span> * num_anchors] * (x_bool * (<span class="number">0.5</span> * x * x) + (<span class="number">1</span> - x_bool) * (x_abs - <span class="number">0.5</span>))) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :, :, :<span class="number">4</span> * num_anchors])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rpn_loss_regr_fixed_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_cls</span>(<span class="params">num_anchors</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_cls_fixed_num</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line"><span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_last&#x27;</span>:</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_class * K.<span class="built_in">sum</span>(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_class * K.<span class="built_in">sum</span>(y_true[:, :num_anchors, :, :] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, num_anchors:, :, :])) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :num_anchors, :, :])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure><p>计算得到P_rpn也就是在feature maps上通过RPN网络得到的300个anchor boxes坐标以及sigmoid分数(表示是否包含物体)</p><p>然后调整anchor box使其区域在feature map内,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">anchor_x = (anchor_size * anchor_ratio[<span class="number">0</span>])/C.rpn_stride</span><br><span class="line">anchor_y = (anchor_size * anchor_ratio[<span class="number">1</span>])/C.rpn_stride</span><br><span class="line">X, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>, :, :, curr_layer] = X - anchor_x/<span class="number">2</span></span><br><span class="line">A[<span class="number">1</span>, :, :, curr_layer] = Y - anchor_y/<span class="number">2</span></span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = anchor_x</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = anchor_y</span><br><span class="line"></span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = np.maximum(<span class="number">1</span>, A[<span class="number">2</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = np.maximum(<span class="number">1</span>, A[<span class="number">3</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] += A[<span class="number">0</span>, :, :, curr_layer]</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] += A[<span class="number">1</span>, :, :, curr_layer]</span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>, :, :, curr_layer] = np.maximum(<span class="number">0</span>, A[<span class="number">0</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">1</span>, :, :, curr_layer] = np.maximum(<span class="number">0</span>, A[<span class="number">1</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = np.minimum(cols-<span class="number">1</span>, A[<span class="number">2</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = np.minimum(rows-<span class="number">1</span>, A[<span class="number">3</span>, :, :, curr_layer])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此外就是NMS了,输入是anchor box shape是(h*w*9,4)以及对应的一个概率对应着(h*w*9)值在0-1之间表示对应anchor box包含物体的概率.将从一个图像中得到的所有anchor box经过NMS,这样一张图像中的anchor box最多也就一定数量(比如300)了.</p><p>然后对于这些anchor box,计算与其iou最大的bbox,要求最大的iou大于一定值(比如0.7),那么这个就是roi,其类别就是对应bbox类别,并且设置其位置(修正位置).</p><p>然后设置一定数量需要的rois,尽量均匀分正负例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression_fast</span>(<span class="params">boxes, probs, overlap_thresh=<span class="number">0.9</span>, max_boxes=<span class="number">300</span></span>):</span></span><br><span class="line">    <span class="comment"># code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</span></span><br><span class="line">    <span class="comment"># if there are no boxes, return an empty list</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(boxes) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grab the coordinates of the bounding boxes</span></span><br><span class="line">    x1 = boxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = boxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = boxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = boxes[:, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    np.testing.assert_array_less(x1, x2)</span><br><span class="line">    np.testing.assert_array_less(y1, y2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if the bounding boxes integers, convert them to floats --</span></span><br><span class="line">    <span class="comment"># this is important since we&#x27;ll be doing a bunch of divisions</span></span><br><span class="line">    <span class="keyword">if</span> boxes.dtype.kind == <span class="string">&quot;i&quot;</span>:</span><br><span class="line">        boxes = boxes.astype(<span class="string">&quot;float&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the list of picked indexes</span></span><br><span class="line">    pick = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the areas</span></span><br><span class="line">    area = (x2 - x1) * (y2 - y1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort the bounding boxes</span></span><br><span class="line">    idxs = np.argsort(probs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep looping while some indexes still remain in the indexes</span></span><br><span class="line">    <span class="comment"># list</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(idxs) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># grab the last index in the indexes list and add the</span></span><br><span class="line">        <span class="comment"># index value to the list of picked indexes</span></span><br><span class="line">        last = <span class="built_in">len</span>(idxs) - <span class="number">1</span></span><br><span class="line">        i = idxs[last]</span><br><span class="line">        pick.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the intersection</span></span><br><span class="line"></span><br><span class="line">        xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class="line">        yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class="line">        xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class="line">        yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class="line"></span><br><span class="line">        ww_int = np.maximum(<span class="number">0</span>, xx2_int - xx1_int)</span><br><span class="line">        hh_int = np.maximum(<span class="number">0</span>, yy2_int - yy1_int)</span><br><span class="line"></span><br><span class="line">        area_int = ww_int * hh_int</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the union</span></span><br><span class="line">        area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the ratio of overlap</span></span><br><span class="line">        overlap = area_int / (area_union + <span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># delete all indexes from the index list that have</span></span><br><span class="line">        idxs = np.delete(idxs, np.concatenate(([last],</span><br><span class="line">                                               np.where(overlap &gt; overlap_thresh)[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pick) &gt;= max_boxes:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># return only the bounding boxes that were picked using the integer data type</span></span><br><span class="line">    boxes = boxes[pick].astype(<span class="string">&quot;int&quot;</span>)</span><br><span class="line">    probs = probs[pick]</span><br><span class="line">    <span class="keyword">return</span> boxes, probs</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>大致逻辑是选取probs最大的(也就是通过RPN算出来包含物体概率最大的anchor box),计算其与剩余的anchor box的iou,去掉大于某个阈值(比如0.9)的,然后再重复,选择剩下来的probs最大的anchor box.一共选择固定数量的anchor box.返回相应的boxes以及probs.</p><p>可以看看这个<a href="https://zhuanlan.zhihu.com/p/43812909">Faster R-CNN 论文阅读记录（一）：概览 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/31426458">一文读懂Faster RCNN - 知乎 (zhihu.com)</a></p><h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/mask-rcnn.png" alt="img" style="zoom:67%;" /></p><p>主要是提出了ROIAlign将模型用于实例分割中,可以与原先的任务并行,</p><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-align.png" alt="img" style="zoom:50%;" /></p><p>RoIAlign 层旨在修复 RoI 池化中由量化(quantization)引起的位置错位。RoIAlign 删除哈希量化，例如，使用 x/16 而不是 [x/16]，以便提取的特征可以与输入像素正确对齐。<strong>双线性插值</strong>用于计算输入中的浮点位置值。</p><blockquote><p>作者认为Faster RCNN中ROI pooling的取整(quantization )操作会使得定位不准确</p><p>这种quantization还体现在proposals(在原图上)映射到特征层上的操作,因为这会导致不对准,对于detection任务有较大影响,</p></blockquote><p><img data-src="https://i.imgur.com/Md5zxS2.png" alt="image-20231102202646398"></p><p>上图蓝框就是proposals到feature map上的框,没有取整.然后在每个roi中选择采样点计算这些采样的均值即为每个roi的值</p><p>可以看看这篇文章<a href="https://blog.csdn.net/qq_37541097/article/details/123754766">【精选】Mask R-CNN网络详解_mask rcnn详解-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_37541097/article/details/112564822">【精选】双线性插值-CSDN博客</a></p><h3 id="Region-based-Fully-Convolutional-Network-R-FCN"><a href="#Region-based-Fully-Convolutional-Network-R-FCN" class="headerlink" title="Region-based Fully Convolutional Network (R-FCN)"></a>Region-based Fully Convolutional Network (R-FCN)</h3><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png" alt="r-fcn image"></p><p>可以了解一下全卷积网络(<a href="http://zh.d2l.ai/chapter_computer-vision/fcn.html">13.11. 全卷积网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>).1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性</p><blockquote><p>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数,最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测</p></blockquote><p>R-FCN核心操作是将ROI pooling改为了position-sensitive score maps.而且原本在ROI pooling之后的卷积层和全连接层(被认为是位置不敏感的操作,这种操作会影响目标检测的精度而且浪费神经网络的分类能力),所以文章将全部操作改为卷积</p><p><img data-src="https://i.imgur.com/BaAJNpE.png" alt="image-20231102205517036"></p><p>大致流程如下:</p><ul><li>首先选择一张需要处理的图片，并对这张图片进行相应的预处理操作；</li><li>接着，我们将预处理后的图片送入一个预训练好的分类网络中（这里使用了ResNet-101网络的Conv4之前的网络），固定其对应的网络参数；</li><li>接着，在预训练网络的最后一个卷积层获得的feature map上存在3个分支，第1个分支就是在该feature map上面进行RPN操作，获得相应的ROI；第2个分支就是在该feature map上获得一个K*K*（C+1）维的位置敏感得分映射（position-sensitive score map），用来进行分类；第3个分支就是在该feature map上获得一个4*K*K维的位置敏感得分映射，用来进行回归；</li><li>最后，在K*K*（C+1）维的位置敏感得分映射和4<em>K</em>K维的位置敏感得分映射上面分别执行位置敏感的ROI池化操作（Position-Sensitive Rol Pooling，这里使用的是平均池化操作），获得对应的类别和位置信息。</li></ul><p>具体来说,通过一个CNN网络后得到feature map,一方面使用全卷积网络将通道数调整为k*k*(C+1)(对于分类任务),此外也需要一个RPN网络,用于提取roi区域,得到roi之后,将每个roi分为k*k个bins,这个时候每个bin就对应一个类别中k*k个通道之一,池化操作也就在这个通道上进行操作.i,j表示每个roi中的某个bin,c表示通道数,其中n表示bin中的像素数,相当于对于某个类别c中的某个bin,计算得到的scores(其实就是一个通道的和)除以bin中的像素数量.论文中的所谓vote就是简单地使用均值得到k*k个position-sensitive scores.</p><p><img data-src="https://pic1.zhimg.com/80/v2-0c3fd2c903db6887128ec390b8981ef0_720w.webp" alt="img"></p><p>然后按此计算某个类别的池化值,也就是k*k个bin的值的和,每个bin是映射在某个score map上的avg pooling.</p><p><img data-src="https://i.imgur.com/fwsdL0f.png" alt="image-20231102214505751"></p><p>得到C+1个值然后做softmax作为评价交叉熵巡视以及对roi的排名.</p><p><img data-src="https://i.imgur.com/9R3Qfia.png" alt="image-20231102214651913"></p><p><a href="https://zhuanlan.zhihu.com/p/30867916">详解R-FCN - 知乎 (zhihu.com)</a></p><h3 id="Summary-of-R-CNN-family"><a href="#Summary-of-R-CNN-family" class="headerlink" title="Summary of R-CNN family"></a>Summary of R-CNN family</h3><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png" alt="img"></p><p>以上都是two-stage  detector,另一种不同的方法跳过区域建议阶段，直接在可能位置的密集采样上运行检测。这就是单阶段目标检测算法的工作原理。这更快、更简单，但可能会降低performance。</p><p>在One-stage中对象检测是一个简单的回归问题，需要输入并学习概率类和边界框坐标。YOLO、YOLO v2、SSD、RetinaNet等属于一个相位检测器。对象检测是图像分类的一种高级形式，其中神经网络预测图像中的对象，并以边界框的形式引起人们的注意。</p><h2 id="论文相关"><a href="#论文相关" class="headerlink" title="论文相关"></a>论文相关</h2><h3 id="Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="Rich feature hierarchies for accurate object detection and semantic segmentation"></a>Rich feature hierarchies for accurate object detection and semantic segmentation</h3><p>2014年的论文,为后面RCNN系列目标检测方法奠定基础</p><h4 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h4><blockquote><p>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.The best-performing methods are <strong>complex ensemble systems that typically combine multiple low-level image features with high-level context.</strong>In this paper, we propose a simple and <strong>scalable detection algorithm that improves mean average precision</strong> (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%.Our approach combines two key insights: (1) <strong>one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects</strong> and (2) when labeled training data is scarce, <strong>supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning,</strong> yields a significant performance boost.</p></blockquote><h3 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask-RCNN"></a>Mask-RCNN</h3><h4 id="abs-1"><a href="#abs-1" class="headerlink" title="abs"></a>abs</h4><blockquote><p>We present a conceptually simple, flexible, and general framework for <strong>object instance segmentation</strong>. Our approach efficiently <strong>detects objects</strong> in an image while <strong>simultaneously generating a high-quality segmentation mask for each instance</strong>.</p><p>The method, called Mask R-CNN, <strong>extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition</strong>. Mask R-CNN is <strong>simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps</strong>.</p><p>Moreover, Mask R-CNN is <strong>easy to generalize to other tasks</strong>, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection.</p></blockquote><h3 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h3><h4 id="abs-2"><a href="#abs-2" class="headerlink" title="abs"></a>abs</h4><blockquote><p>We present <strong>region-based</strong>, <strong>fully convolutional networks</strong> for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply <strong>a costly per-region subnetwork hundreds of times</strong>, our region-based detector is fully convolutional with almost all computation shared on the entire image.</p><p>To achieve this goal, we propose <strong>position-sensitive score maps</strong> to <strong>address a dilemma between translation-invariance in image classification and translation-variance in object detection</strong>.</p><p>Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/">Object Detection Part 4: Fast Detection Models | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://www.analyticsvidhya.com/blog/2022/09/object-detection-using-yolo-and-mobilenet-ssd/?utm_source=reading_list&amp;utm_medium=https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/">Object Detection Using YOLO And Mobilenet SSD Computer Vision - (analyticsvidhya.com)</a></li><li><a href="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/">Object Detection for Dummies Part 3: R-CNN Family | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://tjmachinelearning.com/lectures/1718/obj/">Object Detection | TJHSST Machine Learning Club (tjmachinelearning.com)</a></li><li><a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN | by Dhruv Parthasarathy | Athelas</a></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li>PyTorch: <a href="https://github.com/longcw/faster_rcnn_pytorch">https://github.com/longcw/faster_rcnn_pytorch</a></li><li>Keras:<a href="https://github.com/drowning-in-codes/Keras-frcnn">drowning-in-codes/Keras-frcnn: Keras Implementation of Faster R-CNN (github.com)</a> 目前许多keras项目版本有点老,keras版本目前到了2.14,tensorflow也是.   我之前学过keras,也是新版本的了,大概5年前的老代码差异还是有的.</li><li>PyTorch: <a href="https://github.com/felixgwu/mask_rcnn_pytorch">https://github.com/felixgwu/mask_rcnn_pytorch</a></li><li><a href="https://blog.csdn.net/yx123919804/article/details/114800885">【精选】保姆级 Keras 实现 Faster R-CNN 一_keras实现rcnn-CSDN博客</a> l</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。&lt;/p&gt;</summary>
    
    
    
    
    <category term="SSD" scheme="https://www.sekyoro.top/tags/SSD/"/>
    
    <category term="YOLO" scheme="https://www.sekyoro.top/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>DDS学习与实战</title>
    <link href="https://www.sekyoro.top/2023/10/14/DDS%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AE%9E%E6%88%98/"/>
    <id>https://www.sekyoro.top/2023/10/14/DDS%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AE%9E%E6%88%98/</id>
    <published>2023-10-14T07:03:24.000Z</published>
    <updated>2023-10-15T10:18:34.534Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>数据分发服务（DDS）是一种以数据为中心的通信协议，用于分布式软件应用程序通信。它描述了实现数据提供者和数据消费者之间通信的通信应用程序编程接口（API）和通信语义。</p><span id="more"></span><h3 id="DDS"><a href="#DDS" class="headerlink" title="DDS"></a>DDS</h3><p>在实现过程中定义了三个关键的应用实体：</p><ol><li>发布实体(publication entities)，定义了信息生成对象及其属性；</li><li>订阅实体(subscription entities)，定义信息消费对象及其属性；</li><li>配置实体(configuration entities)，其定义作为主题传输的信息类型，并创建具有其服务质量（QoS）属性的发布者和订阅者，从而确保上述实体的正确性能。</li></ol><p>DDS使用QoS来定义DDS实体的行为特征。QoS由单独的QoS策略（从QoSPolicy派生的类型的对象）组成</p><p>在DCPS模型中，为通信应用程序系统的开发定义了四个基本元素。</p><p><strong>Publisher</strong>:它是DCPS实体，负责创建和配置它所实现的DataWriters。DataWriter是负责实际发布消息的实体。每个都将有一个指定的主题，在该主题下发布消息.</p><p><strong>Subscriber</strong>:DCPS实体负责接收在其订阅的主题下发布的数据。它为一个或多个DataReader对象提供服务，这些对象负责将新数据的可用性传达给应用程序</p><p><strong>Topic</strong>:它是绑定发布和订阅的实体。它在DDS域中是唯一的。通过TopicDescription，它可以统一发布和订阅的数据类型。</p><p><strong>Domain</strong>:这是一个用于链接属于一个或多个应用程序的所有发布者和订阅者的概念，这些应用程序在不同的主题下交换数据。这些参与域的单独应用程序称为DomainParticipant。DDS域由域ID标识。DomainParticipant定义域ID以指定其所属的DDS域。具有不同ID的两个DomainParticipants不知道对方在网络中的存在。因此，可以创建几个通信信道。这适用于涉及多个DDS应用程序的场景，它们各自的DomainParticipants相互通信，但这些应用程序不得干扰。DomainParticipant充当其他DCPS实体的容器，充当发布服务器、订阅服务器和主题实体的工厂，并在域中提供管理服务</p><p><img data-src="https://fast-dds.docs.eprosima.com/en/latest/_images/dds_domain.svg" alt=""></p><h3 id="RTPS"><a href="#RTPS" class="headerlink" title="RTPS"></a>RTPS</h3><p>为支持DDS应用程序而开发的实时发布-订阅（RTPS）协议是一种通过UDP/IP等尽力传输的发布-订阅通信中间件。此外，Fast DDS还支持TCP和共享内存（SHM）传输。</p><p>它被设计为同时支持单播和多播通信。在继承自DDS的RTPS的顶部，可以找到Domain，它定义了一个单独的通信平面。多个域可以同时独立共存。域包含任意数量的RTPSP参与者，即能够发送和接收数据的元素。为此，RTPSP参与者使用他们的终点：</p><p>域表示一个单独的通信平面。它在共享公共通信基础设施的实体之间创建了逻辑分离。从概念上讲，它可以被视为一个虚拟网络，将运行在同一域上的所有应用程序链接起来，并将它们与运行在不同域上的应用程序隔离开来。通过这种方式，几个独立的分布式应用程序可以共存于同一物理网络中，而不会发生干扰，甚至不会相互了解。每个域都有一个唯一的标识符，称为domainId，它被实现为uint32值。共享此domainId的应用程序属于同一个域，并且能够进行通信。</p><p>DomainParticipant是应用程序到域的入口点。每个DomainParticipant从创建起就链接到一个域，并包含与该域相关的所有实体。它还充当发布服务、订阅服务和主题的工厂。DomainParticipant的行为可以使用DomainParticipantQos上指定的QoS值进行修改。</p><p>发布是通过DataWriter与发布服务(Publisher)的关联来定义的。若要开始发布数据实例的值，应用程序将在发布服务器中创建一个新的DataWriter。此DataWriter将绑定到描述正在传输的数据类型的Topic。与此主题匹配的远程订阅将能够从DataWriter接收数据值更新。</p><p>订阅是通过DataReader与订阅服务(subscriber)的关联来定义的。若要开始接收发布的更新，应用程序将在订阅服务器中创建一个新的DataReader。此DataReader将绑定到描述将要接收的数据类型的Topic。然后，DataReader将开始接收来自与此主题匹配的远程发布的数据值更新。当订阅服务器接收到数据时，它会通知应用程序新数据可用。然后，应用程序可以使用DataReader来获取接收到的数据。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;数据分发服务（DDS）是一种以数据为中心的通信协议，用于分布式软件应用程序通信。它描述了实现数据提供者和数据消费者之间通信的通信应用程序编程接口（API）和通信语义。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测(Object Detection)学习_P1</title>
    <link href="https://www.sekyoro.top/2023/10/11/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-Detection-%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/10/11/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-Detection-%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-10-11T11:56:21.000Z</published>
    <updated>2023-10-23T09:54:14.304Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>目标检测也是属于cv热点方向之一,但之前我做得并不多,这里通过微软的教程以及d2l学习一下.<br><span id="more"></span></p><blockquote><p>在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。 然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。 在计算机视觉里，我们将这类任务称为<em>目标检测</em>（object detection）或<em>目标识别</em>（object recognition）</p></blockquote><p>目标检测在无人驾驶和机器人、摄像头里用得很多.</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="bouding-box"><a href="#bouding-box" class="headerlink" title="bouding box"></a>bouding box</h3><p>在目标检测中，我们通常使用<em>边界框</em>（bounding box）来描述对象的空间位置。 边界框是矩形的，由矩形左上角的以及右下角的x和y坐标决定。 另一种常用的边界框表示方法是边界框中心的(x,y)轴坐标以及框的宽度和高度</p><p><img data-src="http://zh.d2l.ai/_images/output_bounding-box_d6b70e_70_0.svg" alt=""></p><blockquote><p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<em>真实边界框</em>（ground-truth bounding box）。 不同的模型使用的区域采样方法可能不同。 这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。 这些边界框被称为<em>锚框</em>（anchor box）</p></blockquote><p>也就是采样区域的设置方式,这里使用不同的缩放比和宽高比生成锚框,假设缩放比为有n个,m个宽高比,比较简单的组合就是</p><script type="math/tex; mode=display">(s_1,r_1),(s_1,r_2),\ldots,(s_1,r_m),(s_2,r_1),(s_3,r_1),\ldots,(s_n,r_1).</script><p><img data-src="http://zh.d2l.ai/_images/output_anchor_f592d1_66_0.svg" alt=""></p><p>可以通过杰卡德系数,也就是交并比IoU衡量锚框与真实边界框之间的相似性</p><script type="math/tex; mode=display">J(\mathcal{A},\mathcal{B})=\frac{|\mathcal{A}\cap\mathcal{B}|}{|\mathcal{A}\cup\mathcal{B}|}.</script><p>在训练集中，我们需要给每个锚框两种类型的标签。一个是与锚框中目标检测的类别，另一个是锚框相对于真实边界框的偏移量。</p><p>在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框</p><h4 id="Naive-approach"><a href="#Naive-approach" class="headerlink" title="Naive approach"></a>Naive approach</h4><p>假设我们想在照片上找到一只猫，一种非常简单的物体检测方法如下：</p><ol><li>将图片分解为多个平铺(tiles)</li><li>对每个平铺运行图像分类。</li><li>那些识别率较高的tiles可以被认为包含所讨论的对象。</li></ol><p>然而，这种方法远非理想，因为它只允许算法非常不精确地定位对象的边界框(bounding box )。为了获得更精确的位置，我们需要运行某种回归(<strong>regression</strong>)来预测边界框的坐标——为此，我们需要特定的数据集。</p><h3 id="目标检测的指标"><a href="#目标检测的指标" class="headerlink" title="目标检测的指标"></a>目标检测的指标</h3><h5 id="Intersection-over-Union"><a href="#Intersection-over-Union" class="headerlink" title="Intersection over Union"></a>Intersection over Union</h5><p>指的就是上面的交并比</p><blockquote><p>虽然对于图像分类，很容易测量算法的性能，但对于对象检测，我们需要测量类的正确性以及推断的边界框位置的精度。对于后者，我们使用所谓的并集交集（IoU），它测量两个bouding box（或两个任意区域）重叠的程度。</p></blockquote><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/iou_equation.png" alt="IoU" style="zoom:50%;" /></p><p>们用两个图形的并集面积来划分两个图形之间的相交面积。对于两个相同的区域，IoU将是1，而对于完全不相交的区域，它将是0。否则，它将在0到1之间变化。我们<strong>通常只考虑IoU超过某个值的边界框。</strong></p><h5 id="Average-Precision"><a href="#Average-Precision" class="headerlink" title="Average Precision"></a>Average Precision</h5><p>PR(Precision-Recall)曲线下的面积,这与混淆矩阵相关.</p><ol><li>考虑精度-召回曲线显示了取决于检测阈值（从0到1）的精度。</li><li>根据阈值的不同，我们会得到或多或少的图像中检测到的对象，以及不同的精度和召回率值。</li></ol><p><img data-src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecall.png" alt="img" style="zoom:67%;" /></p><p>计算多个不同阈值(threshold)的precision.比如设置10个均分的阈值</p><script type="math/tex; mode=display">AP=\frac1{11}\sum_{i=0}^{10}\text{Precision}(\mathrm{Recall}=\frac i{10})</script><h5 id="AP与IoU"><a href="#AP与IoU" class="headerlink" title="AP与IoU"></a>AP与IoU</h5><p>我们将仅考虑IoU高于特定值的那些检测。包括计算AP时也只考虑IoU高于阈值的</p><p><img data-src="https://github.com/shwars/NeuroWorkshop/raw/master/images/ObjDetectionPrecisionRecallIoU.png" alt="img" style="zoom:67%;" /></p><h4 id="Mean-Average-Precision-mAP"><a href="#Mean-Average-Precision-mAP" class="headerlink" title="Mean Average Precision - mAP"></a>Mean Average Precision - mAP</h4><p>目标检测的主要指标称为平均精度（mAP）。它是Average Precision的值，是所有对象类的平均值，有时也计算在给定的几个IoU阈值上的均值.</p><h5 id="Different-Object-Detection-Approaches"><a href="#Different-Object-Detection-Approaches" class="headerlink" title="Different Object Detection Approaches"></a>Different Object Detection Approaches</h5><p>对象检测算法有两大类：<strong>Region Proposal Networks</strong> （R-CNN、Fast R-CNN、Faster R-CNN）.主要思想是生成ROI(Regions of Interests),在每个ROI上运行CNN.</p><p><strong>One-pass</strong> (YOLO, SSD, RetinaNet) 在这些体系结构中，我们设计网络以一次性预测类和ROI.</p><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><p>R-CNN使用选择性搜索<a href="http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf">rcnn_pami.pdf (ulsan.ac.kr)</a><a href="https://arxiv.org/pdf/1311.2524.pdf">1311.2524.pdf (arxiv.org)</a>生成ROI区域的分层结构，然后通过CNN特征提取器和SVM分类器来确定对象类别，并通过线性回归来确定边界框坐标</p><p>选择性搜索<a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf">UijlingsIJCV2013.pdf (uva.nl)</a>：</p><p>1.生成初始子分割，我们生成许多候选区域</p><p>2.使用贪婪算法递归地将相似的区域组合成更大的区域</p><p>3.使用生成的区域来生成最终的候选区域提案</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*REPHY47zAyzgbNKC6zlvBQ.png" alt="img"></p><p>得到2000个区域proposals后,CNN充当特征提取器，并且输出密集层由从图像中提取的特征组成，并且提取的特征被馈送到SVM中以对该候选区域提议内的对象的存在进行分类。除了预测区域建议内对象之外，该算法还预测四个值，这些值是偏移值，以增加边界框的精度。例如，给定一个区域提案，该算法本来可以预测一个人的存在，但该区域提案中那个人的脸可能会被减半。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/rcnn2.png" alt="RCNN-1"></p><h5 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h5><ul><li>训练网络仍然需要大量的时间，因为每张图像需要对2000个区域建议进行分类。它不能实时实现，因为每个测试图像大约需要47秒。</li><li>选择性搜索算法是一种固定的算法。因此，在那个阶段没有学习。这可能导致产生糟糕的候选地区proposals。</li></ul><h4 id="F-RCNN"><a href="#F-RCNN" class="headerlink" title="F-RCNN"></a>F-RCNN</h4><p>这种方法类似于R-CNN，但ROI区域是在应用卷积层后定义的。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/f-rcnn.png" alt="FRCNN"></p><p>将输入图像提供给CNN以生成卷积特征图。从卷积特征图中识别proposal regions，并通过使用RoI池化层将其扭曲(warp )成正方形，我们将其重塑为固定大小，以便将其馈送到完全连接的层中。</p><p>根据RoI特征向量，我们使用softmax层来预测所提出区域的类别以及边界框的偏移值</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*0pMP3aY8blSpva5tvWbnKA.png" alt="img"></p><p>可以看出,生成proposal regions的算法效率很重要,R-CNN使用过选择性搜索算法,F-RCNN通过先将图像通过卷积层得到feature map,在feature map上进行生成region of proposals 并warp成相同大小的ROI再输入到FC进行分类.</p><blockquote><p>“Fast R-CNN”比R-CNN快的原因是，你不必每次向卷积神经网络提供2000个区域建议。相反，每个图像只进行一次卷积运算，并从中生成特征图。</p></blockquote><h4 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h4><p>这种方法的主要思想是使用神经网络来预测ROI，即所谓的区域建议网络。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/faster-rcnn.png" alt="FasterRCNN" style="zoom:67%;" /></p><p>上述两种算法（R-CNN和Fast R-CNN）都使用选择性搜索来找出区域建议。选择性搜索是一个缓慢而耗时的过程，会影响网络的性能。<a href="https://arxiv.org/pdf/1506.01497v1.pdf">1506.01497v1.pdf (arxiv.org)</a>提出了一种对象检测算法，该算法没有使用选择性搜索算法，并让网络学习区域建议。</p><p>类似于Fast R-CNN，图像被提供作为卷积网络的输入，该卷积网络提供卷积特征图。不是在特征图上使用选择性搜索算法来识别区域建议，而是使用单独的网络来预测区域建议。</p><p>不是在特征图上使用选择性搜索算法来识别区域建议，而是使用单独的网络来预测区域建议。然后使用RoI池化层对预测的区域建议进行整形，该RoI池层随后用于对建议区域内的图像进行分类并预测边界框的偏移值。</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*4gGddZpKeNIPBoVxYECd5w.png" alt="img"></p><h4 id="R-FCN-Region-Based-Fully-Convolutional-Network"><a href="#R-FCN-Region-Based-Fully-Convolutional-Network" class="headerlink" title="R-FCN: Region-Based Fully Convolutional Network"></a>R-FCN: Region-Based Fully Convolutional Network</h4><ol><li>我们使用ResNet-101提取特征</li><li>特征由<strong>Position-Sensitive Score Map</strong>处理,每个类对应的对象由k*k个块构成,也就是说通过一个特征图得到一个d<em>C\</em>k*k的输出,d表示d个regional proposals,d中的每一块可能对应图像中的某一部分,相当于把每一部分分为大小相同且对应不同类的特征,然后通过pool.</li><li>对于k*k区域的每一块,对每一个对象类vote(其实就是计算类概率),最大值就是对应的类.</li></ol><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png" alt="r-fcn image"></p><h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>YOLO是一种实时one-pass算法</p><ol><li>图像被分成SXS块</li><li>对于每块区域,CNN预测n个可能的类,bounding box坐标,以及置信度(置信度=概率*IoU)</li></ol><p>YOLO的工作原理是，我们拍摄一张图像，并将其分割成一个SxS网格，在每个网格中我们取m个边界框。对于每个边界框，网络输出该边界框的类概率和偏移值。具有高于阈值的类概率的边界框被选择并用于在图像内定位对象。</p><p>YOLO算法的局限性在于它很难处理图像中的小物体，例如，它可能很难检测到一群鸟。这是由于算法的空间约束。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491">Object detection with neural networks — a simple tutorial using keras | by Johannes Rieke | Towards Data Science</a></li><li><a href="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/11-ObjectDetection/README.md">AI-For-Beginners/lessons/4-ComputerVision/11-ObjectDetection/README.md at main · microsoft/AI-For-Beginners (github.com)</a></li><li><a href="http://zh.d2l.ai/chapter_computer-vision/anchor.html">13.4. 锚框 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li><a href="https://github.com/jrieke/shape-detection">jrieke/shape-detection: 🟣 Object detection of abstract shapes with neural networks (github.com)</a></li><li><a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms | by Rohith Gandhi | Towards Data Science</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;目标检测也是属于cv热点方向之一,但之前我做得并不多,这里通过微软的教程以及d2l学习一下.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>cmake学习</title>
    <link href="https://www.sekyoro.top/2023/10/11/cmake%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/10/11/cmake%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-10-11T09:26:00.000Z</published>
    <updated>2023-10-14T11:40:36.351Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在Windows上开发c++相比Linux还是有点不方便,这里介绍CMake,跨平台的构建工具.<br><span id="more"></span></p><p>在Windows上可选择的构建后端有vs,codeblocks这种软件的文件结构,或者单纯的Makefiles以及Ninja.相当于忽略了几个项目构建的差异.</p><p><img data-src="https://i.imgur.com/0mXZUbA.png" alt="image-20231013105520974" style="zoom:50%;" /></p><h2 id="常用变量"><a href="#常用变量" class="headerlink" title="常用变量"></a>常用变量</h2><p><code>PROJECT_BINARY_DIR</code></p><p>编译生成项目的目录</p><p><code>PROJECT_SOURCE_DIR</code></p><p>顶层目录</p><p><code>EXECUTABLE_OUTPUT_PATH</code>以及<code>LIBRARY_OUTPUT_PATH</code></p><p>分别用来重新定义最终结果的存放目录</p><p><code>CMAKE_ARCHIVE_OUTPUT_DIRECTORY</code>：默认存放静态库的文件夹位置；</p><p><code>CMAKE_LIBRARY_OUTPUT_DIRECTORY</code>：默认存放动态库的文件夹位置；</p><p><code>LIBRARY_OUTPUT_PATH</code>：默认存放库文件的位置，如果产生的是静态库并且没有指定 CMAKE_ARCHIVE_OUTPUT_DIRECTORY 则存放在该目录下，动态库也类似；</p><p><code>CMAKE_RUNTIME_OUTPUT_DIRECTORY</code>：存放可执行软件的目录；</p><p><code>CMAKE_CXX_FLAGS</code>和<code>CMAKE_C_FLAGS</code></p><p>设置C/ C++编译选项,<code>CMAKE_C_COMPILER</code>设置对应编译器路径.</p><p><code>BUILD_SHARED_LIBS</code></p><p>用来控制默认的库编译方式,如果不进行设置,使用ADD_LIBRARY 并没有指定库类型的情况下,默认编译生成的库都是静态库.</p><p>此外还有一些系统信息</p><p><img data-src="https://i.imgur.com/cozY007.png" alt="image-20231013163518609"></p><p>使用$ENV{}调用系统变量.</p><h2 id="指定生成程序"><a href="#指定生成程序" class="headerlink" title="指定生成程序"></a>指定生成程序</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(&lt;name&gt; [STATIC | SHARED | MODULE]</span><br><span class="line">            [EXCLUDE_FROM_ALL]</span><br><span class="line">            source1 [source2 ...])</span><br></pre></td></tr></table></figure><p>将源码source构建成一个库， 供他人使用</p><p><code>[STATIC | SHARED | MODULE]</code> ：类型有三种</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(&lt; name&gt; [WIN32] [MACOSX_BUNDLE]</span><br><span class="line">                [EXCLUDE_FROM_ALL]  source1 source2 … sourceN)</span><br></pre></td></tr></table></figure><p>使用给定的源文件，为工程引入一个可执行文件。引入一个名为&lt; name&gt;的可执行目标，该目标会由调用该命令时在源文件列表中指定的源文件来构建</p><h2 id="添加头文件目录和库"><a href="#添加头文件目录和库" class="headerlink" title="添加头文件目录和库"></a>添加头文件目录和库</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">include_directories</span>([AFTER|BEFORE] [SYSTEM]  dir1  dir2 ...)</span><br><span class="line"><span class="keyword">target_include_directories</span>()</span><br></pre></td></tr></table></figure><p>将给定目录 dir1 dir2 加给编译器搜索到的包含文件 .默认情况下，加到目录列表的最后,target_include_directories可以指定针对目标文件添加头文件目录.</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(&lt;<span class="keyword">target</span>&gt; [item1] [item2] [...]</span><br><span class="line">                      [[debug|optimized|general] &lt;item&gt;] ...)</span><br></pre></td></tr></table></figure><p>该指令的作用为将目标文件与库文件进行链接。</p><p>上述指令中的<code>&lt;target&gt;</code>是指通过add_executable()和add_library()指令生成已经创建的目标文件.</p><p>可以使用<code>&lt;a&gt;_FOUND</code>检查是否通过find加载成功,之后使用target_link_libraries连接.</p><h2 id="find-package-amp-find-path-amp-find-library"><a href="#find-package-amp-find-path-amp-find-library" class="headerlink" title="find_package&amp;find_path&amp;find_library"></a>find_package&amp;find_path&amp;find_library</h2><p>find_path和find_library分别用来找头文件和库.找到之后可以使用include_directory或者target_link_libraries用来使用.</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FIND_PATH</span>(myCeres NAMES ceress.h PATHS /ceres/<span class="keyword">include</span>/ceres NO_DEFAULT_PATH)</span><br><span class="line"><span class="keyword">INCLUDE_DIRECTORIES</span>(<span class="variable">$&#123;myCeres&#125;</span>)</span><br></pre></td></tr></table></figure><h2 id="编译时消息输出"><a href="#编译时消息输出" class="headerlink" title="编译时消息输出"></a>编译时消息输出</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MESSAGE</span>(STATUS <span class="string">&quot;HELLO&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="设置变量"><a href="#设置变量" class="headerlink" title="设置变量"></a>设置变量</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_LIBRARY_OUTPUT_DIRECTORY <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/lib/x86)</span><br></pre></td></tr></table></figure><p>set设置变量,后续使用${}使用变量</p><h2 id="控制结构"><a href="#控制结构" class="headerlink" title="控制结构"></a>控制结构</h2><p>if elseif else endif</p><p>文件中可以使用条件,循环等控制语句.可以用来判断构建时系统的一些环境.</p><p>比如</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># include dynamic link path</span></span><br><span class="line"><span class="keyword">if</span>(CMAKE_SYSTEM_PROCESSOR <span class="keyword">MATCHES</span> <span class="string">&quot;x86&quot;</span>)</span><br><span class="line">    <span class="keyword">link_directories</span>(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../nwpu_std_msgs/lib/x86)</span><br><span class="line">    <span class="keyword">link_directories</span>(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../nwpucutils/lib/x86)</span><br><span class="line"><span class="keyword">elseif</span>(CMAKE_SYSTEM_PROCESSOR <span class="keyword">MATCHES</span> <span class="string">&quot;arm&quot;</span>)</span><br><span class="line">    <span class="keyword">link_directories</span>(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../nwpu_std_msgs/lib/arm)</span><br><span class="line">    <span class="keyword">link_directories</span>(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../nwpucutils/lib/arm)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure><h2 id="添加其他子目录"><a href="#添加其他子目录" class="headerlink" title="添加其他子目录"></a>添加其他子目录</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_subdirectory</span>(source_dir [binary_dir] [EXCLUDE_FROM_ALL] [SYSTEM])</span><br></pre></td></tr></table></figure><p><strong>添加一个子目录并构建该子目录</strong>。source_dir指定源CMakeLists.txt和代码文件所在的目录。</p><p>一般用在嵌套的项目中,顶层CMakeLists.txt文件添加子目录,让子目录先构建完成之后添加其中生成的库和头文件.</p><h2 id="获取文件"><a href="#获取文件" class="headerlink" title="获取文件"></a>获取文件</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FILE</span> (GLOB ALL_SOURCES <span class="string">&quot;*.cpp&quot;</span> <span class="string">&quot;*.c&quot;</span> <span class="string">&quot;./AFolder/*.cpp&quot;</span> )</span><br></pre></td></tr></table></figure><p>使用正则匹配响应文件并存到一个变量中</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">aux_source_directory</span>(dir VAR) </span><br></pre></td></tr></table></figure><p>发现一个目录下所有的<strong>源代码文件</strong>并将列表存储在一个变量中.</p><h2 id="vs中显示头文件"><a href="#vs中显示头文件" class="headerlink" title="vs中显示头文件"></a>vs中显示头文件</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">file</span>(GLOB_RECURSE pipe_header_files  <span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span>/*.h )</span><br><span class="line"><span class="keyword">source_group</span>(<span class="string">&quot;Header Files&quot;</span> FILES <span class="variable">$&#123;pipe_header_files&#125;</span>) </span><br></pre></td></tr></table></figure><p>使用source_group增加文件</p><p>并添加到生成目标中</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_library</span>( lib_pipe_shared SHARED <span class="variable">$&#123;pipe_src&#125;</span> <span class="variable">$&#123;pipe_header_files&#125;</span>)</span><br></pre></td></tr></table></figure><h2 id="option与add-definitions"><a href="#option与add-definitions" class="headerlink" title="option与add_definitions"></a>option与add_definitions</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">option</span>(&lt;variable&gt; <span class="string">&quot;&lt;help_text&gt;&quot;</span> [value])</span><br></pre></td></tr></table></figure><p>可以在cmake命令中指定该值.</p><p>而add_definition用于指定编译器参数，比如<code>add_definitions(&quot;-Wall -g&quot;)</code>,此外更推荐使用add_compile_definitions将预处理器定义添加到编译器命令行,使用add_compile_options命令添加其它选项。</p><p>比如下面文件,使用add_definition定义了TEST_DEBUG,option定义为OFF并在cmake执行时指定为on,然后在cmake文件中指定option为on,这样就执行了<code>add_definitions(-DTEST_DEBUG)</code>,定义了该宏.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">cmake -DTEST_DEBUG=ON .</span><br><span class="line">cmake --build .</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">project</span>(<span class="keyword">test</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">option</span>(TEST_DEBUG <span class="string">&quot;option for debug&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="keyword">if</span> (TEST_DEBUG)</span><br><span class="line"><span class="keyword">add_definitions</span>(-DTEST_DEBUG)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;test.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> TEST_DEBUG</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>CMake中的命令特别多,事实上并不需要去一个一个记住,通常只要知道一个项目的大致构建流程以及可能需要的命令就行了.</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://blog.csdn.net/llffss/article/details/120121617">cmake指令汇总_cmake命令大全_nuosen123的博客-CSDN博客</a></li><li><a href="https://www.runoob.com/w3cnote/cpp-static-library-and-dynamic-library.html">C++静态库与动态库 | 菜鸟教程 (runoob.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/578843962">使用C++创建并调用动态链接库(dll) - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/Op_chaos/article/details/110476264?spm=1001.2101.3001.6650.7&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-7-110476264-blog-120121617.235^v38^pc_relevant_anti_t3_base&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-7-110476264-blog-120121617.235^v38^pc_relevant_anti_t3_base&amp;utm_relevant_index=11">CMake指令详解_cmake -d-CSDN博客</a></li><li><a href="https://blog.csdn.net/qq_25160757/article/details/79813428">VS的包含目录、库目录、引用目录、可执行目录解释_vs包含目录和引用目录-CSDN博客</a></li><li><a href="https://www.cnblogs.com/pandamohist/p/13674438.html">cmake之Visual studio无法显示头文件 - mohist - 博客园 (cnblogs.com)</a></li><li><a href="https://blog.csdn.net/sandalphon4869/article/details/100589747">Linux之cmake的指令以及内部构建和外部构建_cmake 外部编译-CSDN博客</a></li><li><a href="https://www.cnblogs.com/guoshuai-ouc/p/cmake_variable.html">cmake 常用变量和常用环境变量 - 小果子啊 - 博客园 (cnblogs.com)</a></li><li><a href="https://blog.csdn.net/qq_38410730/article/details/102477162">【CMake】CMakeLists.txt的超傻瓜手把手教程（附实例源码）_【cmake】cmakelists.txt的超傻瓜手把手教程(附实例源码)-CSDN博客</a></li><li><a href="https://developer.aliyun.com/article/243229#:~:text=官网不推荐使用l,aries使用。">make的link_directories命令不起作用-阿里云开发者社区 (aliyun.com)</a></li><li><a href="https://cmake.org/cmake/help/latest/">CMake Reference Documentation — CMake 3.28.0-rc1 Documentation</a></li><li><a href="https://blog.csdn.net/afei__/article/details/81201039">CMakeLists.txt 语法介绍与实例演练-CSDN博客</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Windows上开发c++相比Linux还是有点不方便,这里介绍CMake,跨平台的构建工具.&lt;br&gt;</summary>
    
    
    
    
    <category term="CMake" scheme="https://www.sekyoro.top/tags/CMake/"/>
    
  </entry>
  
  <entry>
    <title>stable_diffusion学习</title>
    <link href="https://www.sekyoro.top/2023/10/04/stable-diffusion%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/10/04/stable-diffusion%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-10-04T06:30:57.000Z</published>
    <updated>2023-10-04T08:31:01.778Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>比较火的text-img的模型,看一下其中的模块和流程.<br><span id="more"></span><br><a href="https://course.fast.ai/Lessons/lesson9.html以及[The">https://course.fast.ai/Lessons/lesson9.html以及[The</a> Illustrated Stable Diffusion – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)](<a href="https://jalammar.github.io/illustrated-stable-diffusion/)都是比较好的资料">https://jalammar.github.io/illustrated-stable-diffusion/)都是比较好的资料</a>.</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>stable diffusion前身是latent diffusion<a href="https://arxiv.org/abs/2112.10752">[2112.10752] High-Resolution Image Synthesis with Latent Diffusion Models (arxiv.org)</a></p><blockquote><p>扩散模型已经显示出在生成图像数据方面实现了最先进的结果。但扩散模型的一个缺点是反向去噪过程很慢。此外，这些模型消耗了大量内存，因为它们在像素空间中工作，在生成高分辨率图像时，像素空间变得不合理地昂贵。因此，训练这些模型并将其用于推理是具有挑战性的。</p></blockquote><p>latent diffusion可以通过在<strong>低维潜在空间上应用扩散过程</strong>而不是使用实际像素空间来降低内存和计算复杂性。这是标准扩散和潜在扩散模型之间的关键区别：<strong>在潜在扩散中，模型被训练以生成图像的潜在（压缩）表示</strong></p><p>潜在扩散有三个主要成分。 一种自动编码器(VAE),U-Net,text-encoder，例如CLIP的文本编码器。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/article-Figure3-1-1536x762.png" alt="img"></p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>VAE模型有两个部分，一个编码器和一个解码器。</p><p>编码器用于将图像转换为低维的潜在表示，该低维潜在表示将用作U-Net模型的输入。相反，解码器将潜在的表示转换回图像。</p><p>在潜在扩散训练期间，编码器用于获得前向扩散过程的图像的潜在表示（latent），前向扩散在每一步应用越来越多的噪声。</p><p>在推断过程中，使用VAE解码器将反向扩散过程生成的去噪潜伏时间转换回图像。正如我们将在推理过程中看到的，我们只需要VAE解码器。</p><h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p><img data-src="https://img1.imgtp.com/2023/10/04/0K8yjheX.png" alt="image-20231004151321726"></p><p>U-Net具有编码器部分和解码器部分，两者都由ResNet块组成。编码器将图像表示压缩成较低分辨率的图像表示，并且解码器将较低分辨率图像表示解码回假定噪声较小的原始较高分辨率图像表示。更具体地，<strong>U-Net输出预测可用于计算预测的去噪图像表示的噪声残差</strong>。</p><p>该体系结构的一些亮点包括：</p><ul><li>该模型预测与输入大小相同的图像</li><li>该模型使输入图像经过几个ResNet层块</li><li>这些层将图像大小减半2然后通过相同数量的块再次对其进行上采样。</li><li>skip connections将下采样路径上的特征链接到上采样路径中的相应层。</li></ul><p>为了防止U-Net在下采样时丢失重要信息，<strong>通常在编码器的下采样ResNet和解码器的上采样ResNets之间添加short-cut connections</strong>。</p><p>此外，稳定扩散U-Net能够通过跨注意力层将其输出条件设置在文本嵌入上。交叉注意层被添加到U-Net的编码器和解码器部分，通常在ResNet块之间。</p><h3 id="Text-encoder"><a href="#Text-encoder" class="headerlink" title="Text-encoder"></a>Text-encoder</h3><p>Text-encoder负责将输入提示（例如“一个骑马的天文数字”）转换为U-Net可以理解的嵌入空间。它通常是一个简单的基于转换器的编码器，将输入token序列映射到潜在文本嵌入序列。</p><p>Stable Diffusion在训练期间不训练Text-encoder，而是简单地使用CLIP的已经训练的文本编码器</p><p>由于潜在扩散模型的U-Net在低维空间上运行，与像素空间扩散模型相比，它大大降低了内存和计算需求。</p><h2 id="diffusion-process"><a href="#diffusion-process" class="headerlink" title="diffusion process"></a>diffusion process</h2><p>扩散过程包括<strong>取所需输出大小的随机噪声</strong>，并将其通过模型多次。该过程在<strong>给定数量的步骤</strong>后结束，并且输出图像应该表示根据模型的训练数据分布的样本，例如蝴蝶的图像。</p><p>在训练过程中，我们展示了许多给定分布的样本，例如蝴蝶的图像。经过训练后，该模型将能够处理随机噪声，生成类似的蝴蝶图像。</p><p>该模型通常不会被训练成直接预测噪声稍低的图像，而是预测“噪声残差”，即<strong>噪声较低的图像和输入图像之间的差异</strong>（对于称为“DDPM”的扩散模型），或者类似地，两个时间步长之间的梯度（如称为“Score VE”的扩散模式）。也就是说Unet被训练为一个去噪器,被用于输出噪声差异.</p><p>因此，为了进行去噪过程，需要一种特定的噪声调度算法，并包裹(wrap)模型，以定义推理需要多少扩散步骤，以及如何从模型的输出中计算噪声较小的图像。扩散器库的不同<strong>调度器</strong>在这里发挥作用。</p><p>在训练时,我们给定text,图像</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-autoencoder.png" alt="img"></p><p>首先将图像通过encoder得到潜在变量用于后续处理,这也是为了内存和计算资源.这种压缩（以及后来的解压缩）是通过自动编码器完成的。自动编码器使用其编码器将图像压缩到潜在空间中，然后使用解码器仅使用压缩信息来重建图像。</p><p>现在，正向扩散过程是在压缩的compressed latents上完成的。噪声应用于那些latents的噪声，而不是应用于本身的像素图像的噪声。因此，<strong>噪声预测器实际上被训练来预测压缩表示（潜在空间）中的噪声</strong>。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-latent-forward-process-v2.png" alt="img"></p><p>正向过程（使用自动编码器的编码器）是我们生成数据以训练噪声预测器的方式。一旦经过训练，我们就可以通过运行反向过程（使用自动编码器的解码器）来生成图像。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-forward-and-reverse-process-v2.png" alt="img"></p><p>通过一张图片(潜在变量)以及添加与一个值(noise amount)相关的噪声,得到新的数据.</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-forward-diffusion-training-example-2.png" alt="img"></p><p>有了这个数据集，我们可以训练噪声预测器，并最终获得一个出色的噪声预测器。当在特定配置下运行时，它实际上可以创建图像</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-u-net-noise-training-step.png" alt="img"></p><p>我们目的是得到一个噪声预测器,通过之前得到的带有噪声的图像数据集以及相应的加噪声的值,这个预测器将这些作为输入,输出就是噪声,将带噪声图像数据集减去噪声看是否与原图一样,这样就得到一个noise predictor.</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-denoising-step-2v2.png" alt="img" style="zoom:50%;" /></p><p>Dall-E2和谷歌的Imagen也是类似原理.</p><p>注意上面的输入并没有加入text embedding,Transformer语言模型被用作语言理解组件，该组件接受文本提示并生成 token embeddings。发布的stable diffusion使用ClipText（一种基于GPT的模型），而论文中使用的BERT。</p><h3 id="CLIP训练"><a href="#CLIP训练" class="headerlink" title="CLIP训练"></a>CLIP训练</h3><p>CLIP是在图像及其字幕的数据集上进行训练的。想象一下这样的数据集，只有4亿张图像和它们的标题(captions)</p><blockquote><p>事实上，CLIP是根据从网络上抓取的图像及其“alt”标签进行训练的。</p></blockquote><p>CLIP是图像编码器和文本编码器的组合。它的训练过程可以简化为拍摄图像及其说明(caption)。我们分别用图像和文本编码器对它们进行编码。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/clip-training-step-1.png" alt="img" style="zoom: 50%;" /></p><p>然后，我们使用<strong>余弦相似性</strong>来比较结果嵌入。当我们开始训练过程时，即使文本正确地描述了图像，相似度也会很低。我们更新这两个模型，以便下次嵌入它们时，得到的嵌入是相似的。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/clip-training-step-3.png" alt="img" style="zoom:50%;" /></p><p>通过在数据集中重复，并使用大批量，我们最终发现编码器能够生成狗的图像和句子“狗的照片”相似的嵌入。就像在word2vec中一样，训练过程也需要包括不匹配的图像和描述(captions )的负面示例，并且模型需要为它们分配低相似度分数。</p><blockquote><p>captions本身是字幕的意思,这里表示图像的类似标签,标题,名字的含义.</p></blockquote><h3 id="加入text信息"><a href="#加入text信息" class="headerlink" title="加入text信息"></a>加入text信息</h3><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-unet-inputs-v2.png" alt="img" style="zoom:67%;" /></p><p>为了使文本成为图像生成过程的一部分，我们必须调整我们的噪声预测器，以使用文本作为输入。</p><p>现在的数据集包括编码文本。由于我们在潜在空间中操作，输入图像和预测噪声都在潜在空间内。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/stable-diffusion-text-dataset-v2.png" alt="img"></p><h4 id="如果不包含text信息"><a href="#如果不包含text信息" class="headerlink" title="如果不包含text信息"></a>如果不包含text信息</h4><p>在Unet中,如果没有text信息,那么结构如下</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unet-inputs-outputs-v2.png" alt="img"></p><ol><li>Unet是一系列用于转换latents数组的层</li><li>每个层对上一层的输出进行操作</li><li>一些输出（通过residual connections）被馈送到网络中稍后的处理中</li><li>时间步长(timestep)被转换为时间步长嵌入向量( time step embedding vector)，</li></ol><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unit-resnet-steps-v2.png" alt="img"></p><h4 id="如果包含text信息"><a href="#如果包含text信息" class="headerlink" title="如果包含text信息"></a>如果包含text信息</h4><p>需要对结构进行的主要更改是在ResNet块之间添加一个注意力层，以添加对文本输入的支持。</p><p><img data-src="https://jalammar.github.io/images/stable-diffusion/unet-with-text-steps-v2.png" alt="img"></p><p>请注意，ResNet块不会直接查看文本。但注意力层将这些文本表示合并到了latents中。现在，下一个ResNet可以在处理过程中利用合并的文本信息。</p><h2 id="Stable-Diffusion-during-inference"><a href="#Stable-Diffusion-during-inference" class="headerlink" title="Stable Diffusion during inference"></a>Stable Diffusion during inference</h2><p>训练时通过一个prompt,也就是text文本,然后通过一个laten seed随机生成一个潜变量图像(正态分布噪声),这些作为unet的输入</p><p><img data-src="https://img1.imgtp.com/2023/10/04/kHc7pOee.png" alt="image-20231004150708585"></p><p>代码如下,使用hugging face的diffusers库.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch_device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPTextModel, CLIPTokenizer</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoencoderKL, UNet2DConditionModel, PNDMScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load the autoencoder model which will be used to decode the latents into image space.</span></span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;vae&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Load the tokenizer and text encoder to tokenize and encode the text.</span></span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. The UNet model for generating the latents.</span></span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;unet&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line">scheduler = LMSDiscreteScheduler.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;scheduler&quot;</span>)</span><br><span class="line">vae = vae.to(torch_device)</span><br><span class="line">text_encoder = text_encoder.to(torch_device)</span><br><span class="line">unet = unet.to(torch_device)</span><br><span class="line">prompt = [<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>]</span><br><span class="line"></span><br><span class="line">height = <span class="number">512</span>                        <span class="comment"># default height of Stable Diffusion</span></span><br><span class="line">width = <span class="number">512</span>                         <span class="comment"># default width of Stable Diffusion</span></span><br><span class="line"></span><br><span class="line">num_inference_steps = <span class="number">100</span>            <span class="comment"># Number of denoising steps</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">7.5</span>                <span class="comment"># Scale for classifier-free guidance</span></span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">32</span>)   <span class="comment"># Seed generator to create the inital latent noise</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">text_input = tokenizer(prompt, padding=<span class="string">&quot;max_length&quot;</span>, max_length=tokenizer.model_max_length, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br><span class="line">  max_length = text_input.input_ids.shape[-<span class="number">1</span>]</span><br><span class="line">uncond_input = tokenizer(</span><br><span class="line">    [<span class="string">&quot;&quot;</span>] * batch_size, padding=<span class="string">&quot;max_length&quot;</span>, max_length=max_length, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br><span class="line">  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br><span class="line">  latents = torch.randn(</span><br><span class="line">  (batch_size, unet.in_channels, height // <span class="number">8</span>, width // <span class="number">8</span>),</span><br><span class="line">  generator=generator,</span><br><span class="line">)</span><br><span class="line">latents = latents.to(torch_device)</span><br><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br><span class="line">latents = latents * scheduler.init_noise_sigma</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm(scheduler.timesteps):</span><br><span class="line">  <span class="comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span><br><span class="line">  latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  latent_model_input = scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># predict the noise residual</span></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">  <span class="comment"># perform guidance</span></span><br><span class="line">  noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">  latents = scheduler.step(noise_pred, t, latents).prev_sample</span><br><span class="line">  <span class="comment"># scale and decode the image latents with vae</span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  image = vae.decode(latents).sample</span><br><span class="line">  image = (image / <span class="number">2</span> + <span class="number">0.5</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">image = image.detach().cpu().permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).numpy()</span><br><span class="line">images = (image * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line">pil_images = [Image.fromarray(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br></pre></td></tr></table></figure><p>本人水平有限,如果上文中关于diffusion过程有错误,还请斧正.</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ol><li><a href="https://colab.research.google.com/drive/1MOY7vkpKUosqPSk993g0o4VLjJKAJrwK#scrollTo=BBsdAj9pDPOv">stable_diffusion.ipynb - Colaboratory (google.com)</a></li><li><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">Diffusers.ipynb - Colaboratory (google.com)</a></li><li><a href="https://jalammar.github.io/illustrated-stable-diffusion/">The Illustrated Stable Diffusion – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></li><li><a href="https://rekil156.github.io/rekilblog/posts/lesson9_stableDissufion/Lesson9.html">rekilblog - A different way to look at Stable Diffusion (rekil156.github.io)</a></li><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil’Log (lilianweng.github.io)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;比较火的text-img的模型,看一下其中的模块和流程.&lt;br&gt;</summary>
    
    
    
    
    <category term="stable diffusion" scheme="https://www.sekyoro.top/tags/stable-diffusion/"/>
    
  </entry>
  
</feed>
