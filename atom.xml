<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sekyoro的博客小屋</title>
  
  
  <link href="https://www.sekyoro.top/atom.xml" rel="self"/>
  
  <link href="https://www.sekyoro.top/"/>
  <updated>2023-09-20T05:40:13.644Z</updated>
  <id>https://www.sekyoro.top/</id>
  
  <author>
    <name>Sekyoro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ZTM-pytorchForDL</title>
    <link href="https://www.sekyoro.top/2023/09/16/ZTM-pytorchForDL/"/>
    <id>https://www.sekyoro.top/2023/09/16/ZTM-pytorchForDL/</id>
    <published>2023-09-16T08:35:22.000Z</published>
    <updated>2023-09-20T05:40:13.644Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>ZeroToMastery<a href="https://www.learnpytorch.io/">Zero to Mastery Learn PyTorch for Deep Learning</a>上的课程学习<br><span id="more"></span></p><h2 id="chapter-1"><a href="#chapter-1" class="headerlink" title="chapter 1"></a>chapter 1</h2><p>设置seed,计算tensor的点乘以及tensor数据类型等基础操作,搭配官方文档即可.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random tensor</span></span><br><span class="line">x = torch.rand(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove single dimensions</span></span><br><span class="line">y = torch.squeeze(x)</span><br><span class="line"><span class="comment"># Print out tensors and their shapes</span></span><br><span class="line"><span class="built_in">print</span>(x,x.shape)</span><br><span class="line"><span class="built_in">print</span>(y,y.shape)</span><br></pre></td></tr></table></figure><p>此外还有一些常用方法,比如<code>torch.squeeze</code>,<code>torch.cat</code>,<code>torch.stack</code>,<code>torch.unsqueeze</code>,<code>torch.Tensor.view</code>,<code>torch.Tensor.reshape</code>,<code>torch.Tensor.transpose</code>,<code>torch.Tensor.permute</code>等等</p><h2 id="chapter-2"><a href="#chapter-2" class="headerlink" title="chapter 2"></a>chapter 2</h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;ZeroToMastery&lt;a href=&quot;https://www.learnpytorch.io/&quot;&gt;Zero to Mastery Learn PyTorch for Deep Learning&lt;/a&gt;上的课程学习&lt;br&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://www.sekyoro.top/tags/pytorch/"/>
    
    <category term="DeepLearning" scheme="https://www.sekyoro.top/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习——初探</title>
    <link href="https://www.sekyoro.top/2023/09/12/pytorch%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%9D%E6%8E%A2/"/>
    <id>https://www.sekyoro.top/2023/09/12/pytorch%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%88%9D%E6%8E%A2/</id>
    <published>2023-09-12T01:04:36.000Z</published>
    <updated>2023-09-15T01:14:05.751Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>我并没有系统地翻阅Pytorch文档,一般都是看别人pytorch实现的网络代码,哪里有不懂的再去看.现在找到一些tutorial并做一些简单的尝试.</p><span id="more"></span><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><img data-src="https://img.proanimer.com/imgs/image-20230914101921453.pngundefined" alt="image-20230914101921453"></p><p><img data-src="https://img.proanimer.com/imgs/image-20230912094601559.png" alt="image-20230912094601559" style="zoom:67%;" /></p><p>首先定义网络架构,注意这里也有很多要点,比如像写代码一样,使用模块嵌套,最终形成一个个小模块组成的大模块,模块的超参设置也很重要.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>损失函数需要根据任务去确定,常用交叉熵.优化器基本没有太多改进空间了,常用的Adam或者RMSprop.在训练时,通过梯度更新参数,再进行验证,通过这样来判断是否过拟合等等.</p><p><img data-src="https://img.proanimer.com/imgs/image-20230912095110476.png" alt="image-20230912095110476" style="zoom:67%;" /></p><p>以上是关于网络的训练,对于一般的任务,对于数据集的处理也是非常重要的,导入之前需要做一些transforms,不同任务做的变化不一样,比如图像的话,一般数据集是PIL数据,需要将其转为Tensor类型数据,还可能需要normalize等等.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>上面用的Dataset都是自带的,很多时候需要用我们自己的数据集,那么需要重写Dataset</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, file</span>):</span></span><br><span class="line">      <span class="comment"># read data and preprocess</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">      <span class="comment"># return one sample at a time</span></span><br><span class="line">      <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="comment"># return the size of the dataset</span></span><br><span class="line">      <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>然后使用dataloader方便读入batch以及shuffle打乱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = MyDataset(data, <span class="string">&#x27;./data/train.csv&#x27;</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>Pytorch中的dim与numpy中的axis一样的,有的时候类似这种术语经常出现混乱.</p><p>常用操作</p><p>transpose 互换维度</p><p>squeezz 去掉指定的长度为1的维度</p><p>unsqueezz 增加一个长度唯一的维度</p><p>cat   将给定维度中的seq张量的给定序列连接起来。所有张量必须具有相同的形状（连接维度除外）或为空</p><p>stack  沿着一个新的维度连接一系列张量。 所有张量的大小都必须相同。</p><h4 id="tensor数据类型"><a href="#tensor数据类型" class="headerlink" title="tensor数据类型"></a>tensor数据类型</h4><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td><code>torch.float32</code> or <code>torch.float</code></td><td><code>torch.FloatTensor</code></td><td><code>torch.cuda.FloatTensor</code></td></tr><tr><td>64-bit floating point</td><td><code>torch.float64</code> or <code>torch.double</code></td><td><code>torch.DoubleTensor</code></td><td><code>torch.cuda.DoubleTensor</code></td></tr><tr><td>32-bit integer (signed)</td><td><code>torch.int32</code> or <code>torch.int</code></td><td><code>torch.IntTensor</code></td><td><code>torch.cuda.IntTensor</code></td></tr></tbody></table></div><p>pytorch中的tensor与numpy都有shape和dtype属性</p><p>在方法上,pytorch用于变更维度的有reshape和view(很多时候看别人代码里用view不要忘了其作用).此外numpy也有squeezz,但是没有unsqueezz,而是使用expand_dims</p><p>而torch.Tensor与np.ndarray比较大的差别是tensor可以在GPU上跑,同时也可以设置梯度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available() <span class="comment"># 查看GPU是否可用</span></span><br></pre></td></tr></table></figure><p>通过torch.tensor创造tensor时,会根据输入设置dtype,如果输入是int,那就是int32或者int64,跟os有关,如果是浮点数就是float32.如果使用torch.ones这种来创建tensor,dtype默认是float32.</p><h3 id="torch-nn定义模型"><a href="#torch-nn定义模型" class="headerlink" title="torch.nn定义模型"></a>torch.nn定义模型</h3><h4 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear"></a>nn.Linear</h4><p><img data-src="https://img.proanimer.com/imgs/image-20230914094934618.png" alt="image-20230914094934618"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(). __init__()</span><br><span class="line">    self.net = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">10</span>,<span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = self.net(x)</span><br></pre></td></tr></table></figure><p>对于每一个batch,首先需要使用optimizer.zero_grad()去除gradient,然后使用loss.backward()通过损失函数计算梯度,最后使用optimizer.step更新梯度.</p><p>标准<strong>训练</strong>流程如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(). __init__()</span><br><span class="line">    self.net = nn.Sequential( <span class="comment"># 也可以使用nn.Linear  nn.Sigmoid连续写</span></span><br><span class="line">        nn.Linear(<span class="number">10</span>,<span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>,<span class="number">1</span>)</span><br><span class="line">    )    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = self.net(x)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = MyModel().to(DEVICE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> x,y <span class="keyword">in</span> tr_set:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    x,y = x.to(DEVICE),y.to(DEVICE)</span><br><span class="line">    pred = model(x)</span><br><span class="line">    loss = criterion(pred,y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>验证时,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> te_set:</span><br><span class="line">    x,y = x.to(DEVICE),y.to(DEVICE)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">      pred = model(x)</span><br><span class="line">      loss = criterion(pred,y)</span><br><span class="line">      total_loss += loss.cpu().item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>: loss = <span class="subst">&#123;total_loss&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>注意</strong> 计算出的loss除了在计算backward时,其他地方需要放在cpu上并移除梯度,应该注意这些细节,也就是将原本在cuda上的数据放在cpu上,并且将tensor数据转为python的数据类型.</p><p><a href="https://zhuanlan.zhihu.com/p/497192910">Pytorch训练过程中，显存（内存）爆炸解决方法 - 知乎 (zhihu.com)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> tt_test:</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pred = model(x)</span><br><span class="line">        preds.append(pred.cpu())</span><br></pre></td></tr></table></figure><p><img data-src="https://img.proanimer.com/imgs/image-20230914224438072.png" alt="image-20230914224438072"></p><p>在训练验证时需要使用model.eval与model.train切换模型中每层的行为,在测试时防止将测试数据放入模型中计算.</p><p>此外要多翻阅Pytorch文档.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.youtube.com/watch?v=6dEp6oRN2NE&amp;ab_channel=Hung-yiLee">【機器學習 2023】 PyTorch Tutorial (introduction + documentation) - YouTube</a></li><li><a href="https://pytorch.org/docs/stable/tensors">torch.Tensor — PyTorch 2.0 documentation</a></li><li><a href="https://github.com/wkentaro/pytorch-for-numpy-users">wkentaro/pytorch-for-numpy-users: PyTorch for Numpy users. https://pytorch-for-numpy-users.wkentaro.com (github.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;我并没有系统地翻阅Pytorch文档,一般都是看别人pytorch实现的网络代码,哪里有不懂的再去看.现在找到一些tutorial并做一些简单的尝试.&lt;/p&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://www.sekyoro.top/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Autoencoder学习</title>
    <link href="https://www.sekyoro.top/2023/09/01/Autoencoder%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/09/01/Autoencoder%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-09-01T12:28:30.000Z</published>
    <updated>2023-09-07T07:22:29.266Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这种encoder-decoder结构很重要,同时也可以作为学习GAN的前置<br><span id="more"></span></p><h2 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a>Autoencoders</h2><p>autoencoders是在深度学习经常听到的词,简单来说是基于latent vector,manifold这种概念上的模型,</p><blockquote><p>利用输入数据  本身作为监督，来指导神经网络尝试学习一个映射关系，从而得到一个重构输出 。在时间序列异常检测场景下，异常对于正常来说是少数，所以我们认为，如果使用自编码器重构出来的输出跟原始输入的差异超出一定阈值（threshold）的话，原始时间序列即存在了异常。</p><p>An autoencoder is a type of algorithm with the primary purpose of learning an “informative” representation of the data that can be used for different applicationsa by learning to reconstruct a set of input observations well enough.</p></blockquote><p>latent feature又可以叫做潜在向量,潜在特征,bottleneck等等,叫法很多,不要听见新的说法发怵.简单来说就是encoder-decoder架构,不过进行自监督,使用损失函数比较输入和输出. 使用,重建误差(Reconsrtuction Error)体现,重建误差（RE）是一个指标，它可以指示自动编码器能够重建输入观测x的好坏。相比于全连接网络和卷积网络,AE并不需要labeled data, 我们可以使用图像同时作为输入和输出.</p><p><strong>The main idea of autoencoder is that we will have an encoder network that converts input image into some latent space (normally it is just a vector of some smaller size), then the decoder network, whose goal would be to reconstruct the original image</strong></p><p><img data-src="https://s2.loli.net/2023/08/31/ABn4DgqX7xWGI6y.png" alt=""></p><p>常用于如下用途</p><ul><li>降低图像的维度以进行可视化或训练图像嵌入。通常，自动编码器比PCA给出更好的结果，因为它考虑了图像的空间性质和层次特征。</li><li>去噪，即从图像中去除噪声。由于噪声携带了大量无用的信息，自动编码器无法将其全部放入相对较小的潜在空间，因此只能捕获图像的重要部分。在训练去噪器时，我们从原始图像开始，并使用带有人工添加噪声的图像作为自动编码器的输入。</li><li>超分辨率，提高图像分辨率。我们从高分辨率图像开始，使用分辨率较低的图像作为自动编码器输入。</li><li>生成模型。一旦我们训练了自动编码器，解码器部分就可以用来从随机潜在向量开始创建新的对象。</li></ul><p>缺点是 传统的AE(autoencoders)潜在向量往往没有太多的语义含义,换句话说，以MNIST数据集为例，弄清楚哪些数字对应于不同的潜在向量并不是一项容易的任务，因为接近的潜在向量不一定对应于同一个数字</p><p>尝试改变潜变量大小,看看效果.</p><p>尝试看看不同图像的潜变量,增加噪声后再查看效果.</p><h3 id="AE常用去噪和超分"><a href="#AE常用去噪和超分" class="headerlink" title="AE常用去噪和超分."></a>AE常用去噪和超分.</h3><p>对于去噪来说,将没有噪声的图片人工加噪,训练时使用噪声图片作为输入,正常无噪图片作为输出.</p><blockquote><p>To train super-resolution network, we will start with high-resolution images, and automatically downscale them to produce network inputs. We will then feed autoencoder with small images as inputs and high-resolution images as outputs.</p></blockquote><p>对于超分将宽高缩小的图片作为输入,将正常图作为输出.</p><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = AutoEncoder().to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)</span><br><span class="line">loss_fn = nn.BCELoss()</span><br><span class="line">noisy_tensor = torch.FloatTensor(noisify([<span class="number">256</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])).to(device)</span><br><span class="line">test_noisy_tensor = torch.FloatTensor(noisify([<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])).to(device)</span><br><span class="line">noisy_tensors = (noisy_tensor, test_noisy_tensor)</span><br><span class="line">train(dataloaders, model, loss_fn, optimizer, <span class="number">100</span>, device, noisy=noisy_tensors)</span><br></pre></td></tr></table></figure><p>预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">predictions = []</span><br><span class="line">noise = []</span><br><span class="line">plots = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_dataset):</span><br><span class="line">    <span class="keyword">if</span> i == plots:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    shapes = data[<span class="number">0</span>].shape</span><br><span class="line">    noisy_data = data[<span class="number">0</span>] + test_noisy_tensor[<span class="number">0</span>].detach().cpu()</span><br><span class="line">    noise.append(noisy_data)</span><br><span class="line">    predictions.append(model(noisy_data.to(device).unsqueeze(<span class="number">0</span>)).detach().cpu())</span><br><span class="line">plotn(plots, noise)</span><br><span class="line">plotn(plots, predictions)</span><br></pre></td></tr></table></figure><p>对于超分,因为输入变化了,考虑潜变量不变,所以encoder需要做一些变化.</p><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>对于图像降维来说影响不大,但要训练生成模型，最好对潜在空间有一些了解。这个想法使我们想到了变分自动编码器(VAE)</p><p>VAE是一种自动编码器，它学习预测潜在参数的统计分布，即所谓的潜在分布。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/images/vae.png" alt="img" style="zoom: 50%;" /></p><p>VAE是一种自动编码器，它学习预测潜在参数的统计分布，即所谓的潜在分布。例如，我们可能希望潜在向量正态分布，具有一些均值z~mean~和标准差z~sigma~（均值和标准差都是一些维度d的向量）。VAE中的编码器学习预测这些参数，然后解码器从这个分布中提取一个随机向量来重建对象。 </p><p>相比于AE简单的损失函数,变分自动编码器使用由两部分组成的复杂损失函数：</p><ul><li>重建损失是显示重建图像离目标有多近的损失函数（它可以是均方误差或MSE）。它与普通自动编码器中的损失函数相同。</li><li>KL损失，确保潜在变量分布保持接近正态分布。它基于Kullback-Leibler散度的概念，这是一个估计两个统计分布相似程度的指标。</li></ul><p>VAE的一个重要优势是，它们使我们能够相对容易地生成新图像，<strong>因为我们知道从哪个分布对潜在向量进行采样</strong>。例如，如果我们在MNIST上用2D潜在向量训练VAE，那么我们可以改变潜在向量的分量以获得不同的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vae_loss</span>(<span class="params">preds, targets, z_vals</span>):</span></span><br><span class="line">    mse = nn.MSELoss()</span><br><span class="line">    reconstruction_loss = mse(preds, targets.view(targets.shape[<span class="number">0</span>], -<span class="number">1</span>)) * <span class="number">784.0</span></span><br><span class="line">    temp = <span class="number">1.0</span> + z_vals[<span class="number">1</span>] - torch.square(z_vals[<span class="number">0</span>]) - torch.exp(z_vals[<span class="number">1</span>]) <span class="comment"># ?尽可能使得潜变量与期望的分布kl相近</span></span><br><span class="line">    <span class="comment"># 期望正态分布 均值0 方差1</span></span><br><span class="line">    kl_loss = -<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(temp, axis=-<span class="number">1</span>)  <span class="comment">#  </span></span><br><span class="line">    <span class="keyword">return</span> torch.mean(reconstruction_loss + kl_loss)</span><br></pre></td></tr></table></figure><p>关键是这里的KL loss,需要使得潜变量与正态分布kl更小,分布更相似. 当我们通过encoder计算出均值和方差的log之后,需要计算其与正态分布的KL,</p><p><img data-src="https://s2.loli.net/2023/09/03/HURDP5TfoBuVjyC.png" alt="image-20230903182226844" style="zoom: 80%;" /></p><p>计算正态分布之间的KL散度公式如上.关于VAE这里的KL 散度比较好的回答<a href="https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes/370048#370048">kullback leibler - Deriving the KL divergence loss for VAEs - Cross Validated (stackexchange.com)</a></p><blockquote><p><strong>KL散度</strong>，是指当某分布q (x)被用于近似p (x)时的信息损失。 KL Divergence 也就是说，q (x)能在多大程度上表达p (x)所包含的信息，KL散度越大，表达效果越差。</p></blockquote><p>所以计算KL时应该是KL(z|n)其中z表示潜变量,n表示正态分布. 目的是利用正态分布描述潜变量的损失.</p><h3 id="Training-a-VAE-with-The-Reparametrization-Trick"><a href="#Training-a-VAE-with-The-Reparametrization-Trick" class="headerlink" title="Training a VAE with The Reparametrization Trick"></a>Training a VAE with The Reparametrization Trick</h3><p>VAE在反向传播时存在一些计算问题.使用了Reparametrization Trick</p><p><a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">mathematical statistics - How does the reparameterization trick for VAEs work and why is it important? - Cross Validated (stackexchange.com)</a></p><h2 id="AAE"><a href="#AAE" class="headerlink" title="AAE"></a>AAE</h2><p>结合GAN和VAE的结构</p><p>对抗性自动编码器是生成对抗性网络和变分自动编码器的组合。编码器将是生成器，鉴别器将学习区分编码器输出的真实图像和生成的图像。编码器的输出是一个分布，从这个输出解码器将尝试解码图像。</p><p><img data-src="https://s2.loli.net/2023/09/03/mbhaVXe7jrO63D8.png" alt="image-20230903230742551" style="zoom: 67%;" /></p><p>众所周知,GAN的生成器在训练时使用噪声作为输入,</p><p><img data-src="https://s2.loli.net/2023/09/04/dcLtUFBSzg41rVl.png" alt="image-20230904105728118" style="zoom:67%;" /></p><p><strong>纠正</strong>:是Adversarial. 注意,上面一层的autoencoder的encoder也是一个generator,相当于共用了GAN的generator和autoencoder的encoder.首先encoder使用一张图像作为输入,生成的潜变量在VAE中需要减小其与正态分布之间的相似度,也就是优化KL散度,但由于<strong>KL散度项的积分除了少数分布之外没有闭合形式的解析解</strong>,所以利用GAN的鉴别器,从正态分布中采样的数据与生成的潜变量作为鉴别器的输入进行鉴别,利用这个鉴别器的损失更新鉴别器</p><script type="math/tex; mode=display">L_D=-\frac1m\sum_{k=1}^mlog(D(z'))+log(1-D(z))</script><p>更新后,再使用encoder(同时也是generator)以原始图像作为输入,生成潜变量,输入给更新后的鉴别器,鉴别器将其判断为真的概率.</p><script type="math/tex; mode=display">L_G=-\frac1m\sum_{k=1}^mlog(D(z))</script><p>以这种方式定义的损失将迫使鉴别器能够识别假样本，同时推动生成器欺骗鉴别器.</p><p>首先，由于编码器的输出必须遵循高斯分布，我们在其最后一层不使用任何非线性。解码器的输出具有S形非线性，这是因为我们使用的输入以其值在0和1之间的方式归一化。鉴别器网络的输出只是0和1之间的一个数字，表示输入来自真实先验分布的概率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Encoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Q_net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Q_net, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(X_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3gauss = nn.Linear(N, z_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.droppout(self.lin1(x), p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.droppout(self.lin2(x), p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        xgauss = self.lin3gauss(x)</span><br><span class="line">        <span class="keyword">return</span> xgauss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decoder</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">P_net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(P_net, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(z_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3 = nn.Linear(N, X_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.lin1(x)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.lin2(x)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.25</span>, training=self.training)</span><br><span class="line">        x = self.lin3(x)</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Discriminator</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D_net_gauss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(D_net_gauss, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(z_dim, N)</span><br><span class="line">        self.lin2 = nn.Linear(N, N)</span><br><span class="line">        self.lin3 = nn.Linear(N, <span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.dropout(self.lin1(x), p=<span class="number">0.2</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.dropout(self.lin2(x), p=<span class="number">0.2</span>, training=self.training)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> F.sigmoid(self.lin3(x))</span><br></pre></td></tr></table></figure><p>所以这里的损失函数定义为重建损失(一般为BCEloss或者cross-entropy loss)和GAN的损失,而GAN的训练一般都是G和D一个训练一下,而之前autoencoder训练时也是把encoder-decoder作为整个模型训练的loss.而这里为了在编码器（也是对抗性网络的生成器）的优化过程中具有独立性，我们为网络的这一部分定义了两个优化器,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">10</span>)</span><br><span class="line">Q, P = Q_net() = Q_net(), P_net(<span class="number">0</span>)     <span class="comment"># Encoder/Decoder</span></span><br><span class="line">D_gauss = D_net_gauss()                <span class="comment"># Discriminator adversarial</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    Q = Q.cuda()</span><br><span class="line">    P = P.cuda()</span><br><span class="line">    D_cat = D_gauss.cuda()</span><br><span class="line">    D_gauss = D_net_gauss().cuda()</span><br><span class="line"><span class="comment"># Set learning rates</span></span><br><span class="line">gen_lr, reg_lr = <span class="number">0.0006</span>, <span class="number">0.0008</span></span><br><span class="line"><span class="comment"># Set optimizators</span></span><br><span class="line">P_decoder = optim.Adam(P.parameters(), lr=gen_lr)</span><br><span class="line">Q_encoder = optim.Adam(Q.parameters(), lr=gen_lr)</span><br><span class="line">Q_generator = optim.Adam(Q.parameters(), lr=reg_lr)</span><br><span class="line">D_gauss_solver = optim.Adam(D_gauss.parameters(), lr=reg_lr)</span><br></pre></td></tr></table></figure><p>通过编码器/解码器部分进行正向计算，计算重建损失并更新编码器Q和解码器P网络的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z_sample = Q(X)</span><br><span class="line">X_sample = P(z_sample)</span><br><span class="line">recon_loss = F.binary_cross_entropy(X_sample + TINY, </span><br><span class="line">                                    X.resize(train_batch_size, X_dim) + TINY)</span><br><span class="line">recon_loss.backward()</span><br><span class="line">P_decoder.step()</span><br><span class="line">Q_encoder.step()</span><br></pre></td></tr></table></figure><p>创建一个潜在表示z=Q（x），并从先前的p（z）中提取样本z’，通过鉴别器运行每个样本，并计算分配给每个样本的分数（D（z）和D（z’））</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="module-access"><span class="module"><span class="identifier">Q</span>.</span></span>eval<span class="literal">()</span>    </span><br><span class="line">z_real_gauss = <span class="constructor">Variable(<span class="params">torch</span>.<span class="params">randn</span>(<span class="params">train_batch_size</span>, <span class="params">z_dim</span>)</span><span class="operator"> * </span><span class="number">5</span>)   # Sample from <span class="constructor">N(0,5)</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is<span class="constructor">_available()</span>:</span><br><span class="line">    z_real_gauss = z_real_gauss.cuda<span class="literal">()</span></span><br><span class="line">z_fake_gauss = <span class="constructor">Q(X)</span></span><br></pre></td></tr></table></figure><p>训练过程,首先利用encoder-decoder,得到重建后的输出,这里使用二分类交叉熵,计算梯度后更新encoder和decoder的值.然后使用generator(同时也是encoder)使用从高斯分布采样得到的变量作为输入,注意此时需要设置dropout和normalization模式为测试,因为正则目的是为了防止过拟合、加快拟合过程,所以测试、验证时并不需要正则了. 这里Q.eval目的是只需要得到一个生成的潜变量,而不是进行训练.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute discriminator outputs and loss</span></span><br><span class="line">D_real_gauss, D_fake_gauss = D_gauss(z_real_gauss), D_gauss(z_fake_gauss)</span><br><span class="line">D_loss_gauss = -torch.mean(torch.log(D_real_gauss + TINY) + torch.log(<span class="number">1</span> - D_fake_gauss + TINY))</span><br><span class="line">D_loss_gauss.backward()       <span class="comment"># Backpropagate loss</span></span><br><span class="line">D_gauss_solver.step()   <span class="comment"># Apply optimization step</span></span><br></pre></td></tr></table></figure><p>计算Generator的loss，并相应地更新Q网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generator</span></span><br><span class="line">Q.train()   <span class="comment"># Back to use dropout</span></span><br><span class="line">z_fake_gauss = Q(X)</span><br><span class="line">D_fake_gauss = D_gauss(z_fake_gauss)</span><br><span class="line"></span><br><span class="line">G_loss = -torch.mean(torch.log(D_fake_gauss + TINY))</span><br><span class="line">G_loss.backward()</span><br><span class="line">Q_generator.step()</span><br></pre></td></tr></table></figure><p>由于需要更新Generator,恢复训练模式,</p><h3 id="Supervised-approach"><a href="#Supervised-approach" class="headerlink" title="Supervised approach"></a>Supervised approach</h3><blockquote><p>特征学习最稳健的方法是尽可能多地分解因素，尽可能少地丢弃有关数据的信息</p></blockquote><p>通常来说,如果我们能提供更多信息,将其作为一个全监督的模型.</p><p><code>disentangled representations</code>类似于风格迁移中概念,可以将一张图像中的东西分为<strong>内容</strong>和<strong>风格</strong>,进行解耦表示。</p><p>我们可以加上类标签的独热码,这其实就是所谓的Conditional GAN</p><blockquote></blockquote><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/aae_super.png" alt="aae_semi" style="zoom:33%;" /></p><p>这样在代码上就会增加两个损失函数和一个鉴别器用于分辨产生的label的独热码和真实的label的独热码.</p><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/dis_2.png" alt="disentanglement1" style="zoom: 33%;" /></p><p>这是教程<a href="https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/">Adversarial Autoencoders (with Pytorch) (paperspace.com)</a>的图片,使得每一列潜变量第一部分也就是正态分布一样,但类标签不一样,</p><h3 id="Semi-supervised-approach"><a href="#Semi-supervised-approach" class="headerlink" title="Semi-supervised approach"></a>Semi-supervised approach</h3><p>下面这种半监督的方式,我们需要将label加入,而这种加入并不是直接把label作为输入给decoder的,而是类似刚才AAE的方式通过GAN使得潜变量分为两个部分,分别是想要的某种分布和label,将class label的one-hot编码,比如说MNIST数据集中,3这个图像的label就是数字三,one-hot编码是0011(因为一共十个数字,需要4位.</p><p>我们可以修改以前的体系结构，使AAE产生一个由矢量级联组成的潜变量y指示类或标签（使用Softmax）和连续潜在变量z(使用线性层). 使用softmax作为最后一层的激活函数,这样最后一层输出shape就是(Batch_size,4) 每个值在0-1之间,</p><p>通过这种方法还能通过encoder产生的潜变量中的类标签的独热码进行对图像分类,</p><p><img data-src="https://raw.githubusercontent.com/fducau/AAE_pytorch/master/img/aae_semi.png" alt="aae002" style="zoom: 33%;" /></p><p>希望向量y表现为一个独热码，我们通过使用鉴别器Dcat的对抗性网络来强制它遵循类别分布。</p><p>编码器现在是q（z，y|x）。解码器使用类标签和连续潜变量来重建图像</p><h2 id="Conditional-Variational-Autoencoders"><a href="#Conditional-Variational-Autoencoders" class="headerlink" title="Conditional Variational Autoencoders"></a>Conditional Variational Autoencoders</h2><p>条件变分自动编码器对编码器和解码器都有一个额外的输入。</p><p><img data-src="https://cdn.sekyoro.top/imgs/image-20230907105548807.png" alt="image-20230907105548807" style="zoom: 67%;" /></p><p>在训练时，将其图像被馈送的数字提供给编码器和解码器。在这种情况下，它将被表示为一个热向量。</p><p>要生成特定数字的图像，只需将该数字与从标准正态分布采样的潜在空间中的随机点一起输入解码器即可。即使输入同一点来产生两个不同的数字，这个过程也会正确工作，因为系统不再依赖潜在空间来编码你正在处理的数字。相反，潜在空间对其他信息进行编码，如笔划宽度或数字写入的角度。</p><p><img data-src="https://s2.loli.net/2023/09/04/HCPMwute1Z2LBV7.png" alt="image-20230904223354814" style="zoom:50%;" /></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/09-Autoencoders/README.md">AI-For-Beginners/lessons/4-ComputerVision/09-Autoencoders/README.md at main · microsoft/AI-For-Beginners (github.com)</a></li><li><a href="https://arxiv.org/pdf/2201.03898.pdf">*2201.03898.pdf (arxiv.org)</a></li><li><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler Divergence Explained — Count Bayesie</a></li><li><a href="https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians">normal distribution - KL divergence between two univariate Gaussians - Cross Validated (stackexchange.com)</a></li><li><a href="https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/">Adversarial Autoencoders (with Pytorch) (paperspace.com)</a></li><li><a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html">Conditional Variational Autoencoders (ijdykeman.github.io)</a></li><li><a href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">mathematical statistics - How does the reparameterization trick for VAEs work and why is it important? - Cross Validated (stackexchange.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这种encoder-decoder结构很重要,同时也可以作为学习GAN的前置&lt;br&gt;</summary>
    
    
    
    
    <category term="VAE" scheme="https://www.sekyoro.top/tags/VAE/"/>
    
    <category term="autoencoder" scheme="https://www.sekyoro.top/tags/autoencoder/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础知识(二)</title>
    <link href="https://www.sekyoro.top/2023/08/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(%E4%BA%8C)/"/>
    <id>https://www.sekyoro.top/2023/08/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86(%E4%BA%8C)/</id>
    <published>2023-08-12T10:16:29.000Z</published>
    <updated>2023-08-18T02:49:56.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>深度学习知识第二部分</p><span id="more"></span><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>解决序列数据</p><p>输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加， 因此需要一个近似方法来使这个计算变得容易处理。 本章后面的大部分内容将围绕着如何有效估计<img data-src="https://s2.loli.net/2023/08/11/c82anIiD6Cmr9OE.png" alt="image-20230811182733764">展开。 简单地说，它归结为以下两种策略。</p><p>第一种策略，假设在现实情况下相当长的序列可能是不必要的， 因此我们只需要满足某个长度为l的时间跨度。 当下获得的最直接的好处就是参数的数量总是不变的， 至少在t&gt;l时如此，这就使我们能够训练一个上面提及的深度网络。 这种模型被称为<em>自回归模型</em>（autoregressive models）， 因为它们是对自己执行回归。</p><p>第二种策略， 是保留一些对过去观测的总结ℎ， 并且同时更新预测和总结ℎ。 这就产生了基于<img data-src="https://s2.loli.net/2023/08/11/RZ8LQh2qyYW4Su6.png" alt="image-20230811182855054">计x， 以及公式<img data-src="https://s2.loli.net/2023/08/11/2mkvsVQXMIpajlY.png" alt="image-20230811182927632">更新的模型。 由于ℎ从未被观测到，这类模型也被称为 <em>隐变量自回归模型</em>（latent autoregressive models</p><p><img data-src="https://zh-v2.d2l.ai/_images/sequence-model.svg" alt="../_images/sequence-model.svg"></p><h4 id="文本预处理方式"><a href="#文本预处理方式" class="headerlink" title="文本预处理方式"></a>文本预处理方式</h4><p>步骤通常包括：</p><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。</li></ol><p><em>词元</em>（token）是文本的基本单位，词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做<em>词表</em>（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。</p><p>将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为<em>语料</em>（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</p><p>自然语言特征:</p><ol><li><p>词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law），</p><p><img data-src="https://s2.loli.net/2023/08/12/5CnHizhOuTlW1wc.png" alt="image-20230812181916062"></p></li><li><p>除了一元语法词，单词序列似乎也遵循齐普夫定律， 尽管公式中的指数α更小 （指数的大小受序列长度的影响）；</p></li><li>词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望；</li><li>很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。</li></ol><h3 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p><img data-src="https://zh.d2l.ai/_images/rnn.svg" alt="../_images/rnn.svg" style="zoom: 67%;" /></p><h4 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h4><p><img data-src="https://s2.loli.net/2023/08/16/zpKICRmb4FvML2g.png" alt="image-20230815235353077"></p><p>隐状态H,有上一个隐状态与本次输入控制.</p><p><img data-src="https://s2.loli.net/2023/08/16/kWAnvRDGEHc2Bxi.png" alt="image-20230816000019295"></p><p>输出O</p><h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><p><img data-src="https://s2.loli.net/2023/08/16/EPUuOp3Gf1Szlxq.png" alt="image-20230816130933516"></p><h4 id="简单的RNN缺点"><a href="#简单的RNN缺点" class="headerlink" title="简单的RNN缺点"></a>简单的RNN缺点</h4><ul><li>我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。</li><li>我们可能会遇到这样的情况：一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来<em>跳过</em>隐状态表示中的此类词元。</li><li>我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来<em>重置</em>我们的内部状态表示</li></ul><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><blockquote><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 下面我们将详细讨论各类门控</p></blockquote><p>引入重置门和更新门. 输入是由当前时间步的输入和前一时间步的隐状态给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给</p><p><img data-src="https://zh-v2.d2l.ai/_images/gru-1.svg" alt="../_images/gru-1.svg" style="zoom: 67%;" /></p><p>利用重置门的输出与常规隐状态集成,得到一个候选隐状态.如如果重置门输出为1,则是普通的隐状态,由本次输入与上次隐状态作为输入,如果重置门输出为0,则候选隐状态只受输入影响,也就是进行了重置.</p><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t=\tanh \left(\mathbf{X}_t \mathbf{W}_{x h}+\left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{h h}+\mathbf{b}_h\right),</script><p><img data-src="https://zh-v2.d2l.ai/_images/gru-2.svg" alt="../_images/gru-2.svg"></p><p>结合更新门确定最终隐状态,如果输出为1,不进行更新,保持之前的隐状态,如果是0则将候选隐状态作为新的隐状态.</p><script type="math/tex; mode=display">\mathbf{H}_t=\mathbf{Z}_t \odot \mathbf{H}_{t-1}+\left(1-\mathbf{Z}_t\right) \odot \tilde{\mathbf{H}}_t .</script><p><img data-src="https://zh-v2.d2l.ai/_images/gru-3.svg" alt="../_images/gru-3.svg"></p><blockquote><ul><li>门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。</li><li>重置门有助于捕获序列中的短期依赖关系。</li><li>更新门有助于捕获序列中的长期依赖关系。</li><li>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</li></ul></blockquote><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>从时间上来说,LSTM比GRU结构要早,结构也更复杂.</p><blockquote><ul><li>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了<em>记忆元</em>（memory cell），或简称为<em>单元</em>（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为<em>输出门</em>（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为<em>输入门</em>（input gate）。 我们还需要一种机制来重置单元的内容，由<em>遗忘门</em>（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。</li></ul></blockquote><p>引入输入门,忘记门,输出门用于控制隐状态.</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x i}+\mathbf{H}_{t-1} \mathbf{W}_{h i}+\mathbf{b}_i\right), \\\mathbf{F}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x f}+\mathbf{H}_{t-1} \mathbf{W}_{h f}+\mathbf{b}_f\right), \\\mathbf{O}_t & =\sigma\left(\mathbf{X}_t \mathbf{W}_{x o}+\mathbf{H}_{t-1} \mathbf{W}_{h o}+\mathbf{b}_o\right),\end{aligned}</script><p>同时还有候选记忆元,</p><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t=\tanh \left(\mathbf{X}_t \mathbf{W}_{x c}+\mathbf{H}_{t-1} \mathbf{W}_{h c}+\mathbf{b}_c\right)</script><p><img data-src="https://zh-v2.d2l.ai/_images/lstm-1.svg" alt="../_images/lstm-1.svg"></p><p>利用忘记门和输入门控制上一次的记忆元和候选记忆元,隐状态的计算就是根据输出门和记忆元.</p><script type="math/tex; mode=display">\mathbf{H}_t=\mathbf{O}_t \odot \tanh \left(\mathbf{C}_t\right)</script><p>只要输出门接近1，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，我们只保留记忆元内的所有信息，而不需要更新隐状态。</p><p><img data-src="https://zh-v2.d2l.ai/_images/lstm-3.svg" alt="../_images/lstm-3.svg"></p><h3 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h3><p><img data-src="https://zh-v2.d2l.ai/_images/deep-rnn.svg" alt="../_images/deep-rnn.svg" style="zoom: 80%;" /></p><script type="math/tex; mode=display">\mathbf{H}_t^{(l)}=\phi_l\left(\mathbf{H}_t^{(l-1)} \mathbf{W}_{x h}^{(l)}+\mathbf{H}_{t-1}^{(l)} \mathbf{W}_{h h}^{(l)}+\mathbf{b}_h^{(l)}\right)</script><script type="math/tex; mode=display">\mathbf{H}_t^{(l)}=\phi_l\left(\mathbf{H}_t^{(l-1)} \mathbf{W}_{x h}^{(l)}+\mathbf{H}_{t-1}^{(l)} \mathbf{W}_{h h}^{(l)}+\mathbf{b}_h^{(l)}\right)</script><script type="math/tex; mode=display">\mathbf{O}_t=\mathbf{H}_t^{(L)} \mathbf{W}_{h q}+\mathbf{b}_q</script><p>与多层感知机一样，隐藏层数目L和隐藏单元数目ℎ都是超参数。 也就是说，它们可以由我们调整的。 另外，用门控循环单元或长短期记忆网络的隐状态 来代替隐状态进行计算， 可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。</p><ul><li>在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。</li><li>有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。</li><li>总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。</li></ul><h3 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h3><p>处在序列中间的文字明显可以收到两边的影响.</p><p><img data-src="https://zh-v2.d2l.ai/_images/birnn.svg" alt="../_images/birnn.svg"></p><p>其中ℎ是隐藏单元的数目。 前向和反向隐状态的更新如下</p><script type="math/tex; mode=display">\begin{aligned}& \overrightarrow{\mathbf{H}}_t=\phi\left(\mathbf{X}_t \mathbf{W}_{x h}^{(f)}+\overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{h h}^{(f)}+\mathbf{b}_h^{(f)}\right) \\& \overleftarrow{\mathbf{H}}_t=\phi\left(\mathbf{X}_t \mathbf{W}_{x h}^{(b)}+\overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{h h}^{(b)}+\mathbf{b}_h^{(b)}\right)\end{aligned}</script><script type="math/tex; mode=display">\mathbf{O}_t=\mathbf{H}_t \mathbf{W}_{h q}+\mathbf{b}_q .</script><blockquote><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。 但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。 具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。</p><p>另一个严重问题是，双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链</p></blockquote><p>双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，<strong>填充缺失的单词</strong>、<strong>词元注释</strong>（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤<strong>对序列进行编码</strong></p><h4 id="数据集一般处理流程"><a href="#数据集一般处理流程" class="headerlink" title="数据集一般处理流程"></a>数据集一般处理流程</h4><p>将数据进行预处理(比如替换不间断空格,小写,单词和标点之间插入空格)、词元化后得到词元之后,建立词表.</p><p>由于机器翻译数据集由语言对组成， 因此我们可以分别为源语言和目标语言构建两个词表。 使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。 为了缓解这一问题，这里我们将<strong>出现次数少于2次的低频率词元 视为相同的未知（“<unk>”）词元</strong>。 除此之外，我们还指定了额外的特定词元， 例如在<strong>小批量时用于将序列填充到相同长度的填充词元（“<pad>”）</strong>， 以及<strong>序列的开始词元（“<bos>”）和结束词元（“<eos>”）</strong>。 这些特殊词元在自然语言处理任务中比较常见.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = np.array([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).astype(np.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。</li><li>使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。</li><li>通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><h3 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h3><p><img data-src="https://zh-v2.d2l.ai/_images/encoder-decoder.svg" alt="../_images/encoder-decoder.svg"></p><p>前面处理机器翻译时输入和输出长度都是固定的.</p><p>机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个<em>编码器</em>（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是<em>解码器</em>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为<em>编码器-解码器</em>（encoder-decoder）架构.</p><ul><li>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。</li><li>编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。</li><li>解码器将具有固定形状的编码状态映射为长度可变的序列。</li></ul><h3 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h3><blockquote><p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq.svg" alt="../_images/seq2seq.svg"></p><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><script type="math/tex; mode=display">\mathbf{h}_{t}=f(\mathbf{x}_{t},\mathbf{h}_{t-1}).</script><p>隐状态根据本次输入和上次的隐状态输出.</p><script type="math/tex; mode=display">\mathbf{c}=q(\mathbf{h}_1,\ldots,\mathbf{h}_T).</script><p>常常会使用一个嵌入层,获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（<code>vocab_size</code>）， 其列数等于特征向量的维度（<code>embed_size</code>）。 对于任意输入词元的索引i， 嵌入层获取权重矩阵的第i行（从0开始）以返回其特征向量</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><script type="math/tex; mode=display">\mathbf{s}_{t^{\prime}}=g(y_{t^{\prime}-1},\mathbf{c},\mathbf{s}_{t^{\prime}-1}).</script><p>在获得解码器的隐状态之后， 我们可以使用输出层和softmax操作 来计算在时间步t′时输出y~t~′的条件概率分布</p><p>损失函数使用交叉熵,在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化,此外应该将填充词元的预测排除在损失函数的计算之外屏蔽不相关项.</p><h4 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h4><p>训练时,特定的序列开始词元（“<bos>”）和 原始的输出序列（不包括序列结束词元“<eos>”） 拼接在一起作为解码器的输入。 这被称为<em>强制教学</em>（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的<em>预测</em>得到的词元作为解码器的当前输入.</p><p>预测时,为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（“<bos>”） 在初始时间步被输入到解码器中.</p><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq-predict.svg" alt="../_images/seq2seq-predict.svg"></p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p><img data-src="https://zh-v2.d2l.ai/_images/qkv.svg" alt="../_images/qkv.svg"></p><p>将注意力简单地分为自主性和非自主性,利用这两种注意力提示,用神经网络来设计注意力机制的框架.</p><p>“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）。 给定任何查询，注意力机制通过<em>注意力汇聚</em>（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为<em>值</em>（value）。 更通俗的解释，每个值都与一个<em>键</em>（key）配对， 这可以想象为感官输入的非自主提示。 </p><ul><li>人类的注意力是有限的、有价值和稀缺的资源。</li><li>受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。</li><li>注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。</li><li>由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。</li><li>注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。</li><li>可视化查询和键之间的注意力权重是可行的。</li></ul><p><img data-src="https://zh-v2.d2l.ai/_images/attention-output.svg" alt="../_images/attention-output.svg"></p><p>高斯核指数部分可以视为<em>注意力评分函数</em>（attention scoring function）， 简称<em>评分函数</em>（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和</p><script type="math/tex; mode=display">f(\mathbf{q},(\mathbf{k}_1,\mathbf{v}_1),\ldots,(\mathbf{k}_m,\mathbf{v}_m))=\sum_{i=1}^m\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\in\mathbb{R}^v,</script><script type="math/tex; mode=display">\alpha(\mathbf{q},\mathbf{k}_i)=\mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i))=\frac{\exp(a(\mathbf{q},\mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q},\mathbf{k}_j))}\in\mathbb{R}.</script><p>注意力机制涉及到q,k,v分别代表查询,键,值. 计算注意力评分有多种方法,常用的有加性注意力和缩放点积注意力.</p><script type="math/tex; mode=display">a(\mathbf{q}, \mathbf{k})=\mathbf{w}_v^{\top} \tanh \left(\mathbf{W}_q \mathbf{q}+\mathbf{W}_k \mathbf{k}\right) \in \mathbb{R}</script><script type="math/tex; mode=display">\operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt d}\right)\mathbf{V}\in\mathbb{R}^{n\times v}.</script><p>有了评分函数后,继续考虑注意力模型问题.循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量， 然后循环神经网络解码器<strong>根据生成的词元和上下文变量</strong> 按词元生成输出（目标）序列词元。</p><blockquote><p>即使并非所有输入（源）词元都对解码某个词元都有用， 在每个解码步骤中仍使用编码<em>相同</em>的上下文变量。 有什么方法能改变上下文变量呢</p></blockquote><h4 id="Bahdanau注意力"><a href="#Bahdanau注意力" class="headerlink" title="Bahdanau注意力"></a>Bahdanau注意力</h4><p>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过<strong>将上下文变量视为注意力集中的输出</strong>来实现的.</p><p>其中解码时间步t~’~都会被c~t’~替换,是作为查询(query)的上一步解码器隐状态和与编码器隐状态</p><script type="math/tex; mode=display">\mathbf{c}_{t^{\prime}}=\sum_{t=1}^T\alpha(\mathbf{s}_{t^{\prime}-1},\mathbf{h}_t)\mathbf{h}_t,</script><p>时间步t′−1时的解码器隐状态s~t~′−1是查询， 编码器隐状态ℎ~t~既是键，也是值. 注意力权重可以使用加性注意力打分.</p><p><img data-src="https://zh-v2.d2l.ai/_images/seq2seq-attention-details.svg" alt="../_images/seq2seq-attention-details.svg"></p><p>定义Bahdanau注意力,只需要改变解码器就行了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqAttentionDecoder</span>(<span class="params">AttentionDecoder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout=<span class="number">0</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span></span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X, state</span>):</span></span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>首先，初始化解码器的状态，需要下面的输入：</p><p><strong>编码器</strong>在所有时间步的<strong>最终层隐状态，将作为注意力的键和值</strong>；</p><p><strong>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态</strong>；</p><p><strong>编码器有效长度</strong>（排除在注意力池中填充词元）。</p><p>结合了注意力机制与编码器-解码器,使得解码器中每个解码时间步是注意力模型的输出,查询q是上一步的隐状态,键和值都是编码器的最终隐状态.</p><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><blockquote><p>在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 <em>子空间表示</em>（representation subspaces）可能是有益的</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/multi-head-attention.svg" alt="../_images/multi-head-attention.svg"></p><p>可以用独立学习得到的ℎ组不同的<em>线性投影</em>（linear projections）来变换查询、键和值。 然后，这ℎ组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这ℎ个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。</p><p>多头注意力机制,对于h个注意力汇聚输出,每一个注意力汇聚都被称作一个”头”.</p><script type="math/tex; mode=display">\mathbf{h}_i=f(\mathbf{W}_i^{(q)}\mathbf{q},\mathbf{W}_i^{(k)}\mathbf{k},\mathbf{W}_i^{(v)}\mathbf{v})\in\mathbb{R}^{p_v},</script><script type="math/tex; mode=display">\mathbf{W}_o\begin{bmatrix}\mathbf{h}_1\\\vdots\\\mathbf{h}_h\end{bmatrix}\in\mathbb{R}^{p_o}.</script><p>其中的W均是可学习的参数,f是注意力汇聚的函数.</p><ul><li>多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</li><li>基于适当的张量操作，可以实现多头注意力的并行计算。</li></ul><h3 id="自注意力和位置编码"><a href="#自注意力和位置编码" class="headerlink" title="自注意力和位置编码"></a>自注意力和位置编码</h3><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;深度学习知识第二部分&lt;/p&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://www.sekyoro.top/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>GAN深入学习</title>
    <link href="https://www.sekyoro.top/2023/08/11/GAN%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/08/11/GAN%E6%B7%B1%E5%85%A5%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-08-11T10:08:06.000Z</published>
    <updated>2023-09-19T09:56:55.249Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>深入GAN学习<br><span id="more"></span></p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/images/gan_architecture.png" alt="img"></p><p>注意,实验复现时最好设置随机种子固定<a href="https://pytorch.org/docs/stable/notes/randomness.html">Reproducibility — PyTorch 2.0 documentation</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># seed setting</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">same_seeds</span>(<span class="params">seed</span>):</span></span><br><span class="line">    <span class="comment"># Python built-in random module</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    <span class="comment"># Numpy</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    <span class="comment"># Torch</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed(seed)</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">same_seeds(<span class="number">2023</span>)</span><br></pre></td></tr></table></figure><h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><p>标题<strong>Generative Adversarial Nets</strong> 2014年</p><p><strong>摘要</strong></p><p>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples</p><p><img data-src="https://pic4.zhimg.com/80/v2-044f8d58f378b088c9a85a8f19dae363_720w.webp" alt="img" style="zoom:67%;" /></p><p>主要贡献:提出GAN  定义G和D以及损失函数.由于GAN中使用的极小极大（minmax）优化，训练可能非常不稳定。</p><script type="math/tex; mode=display">\min_G\max_DV=E_{x\sim\text{p}_r}[\log D(x)]+E_{x\sim\text{p}_g}[\log(1-D(x))]</script><p>存在问题：梯度不稳定,梯度消失,<strong>模式崩溃</strong>(特别是NS-GAN,使用了the - log D trick),也就是生成器的损失改为-logD(x)<a href="https://zhuanlan.zhihu.com/p/25071913">令人拍案叫绝的Wasserstein GAN - 知乎 (zhihu.com)</a></p><p>首先求得生成器固定,最大化V的D</p><script type="math/tex; mode=display">\mathrm{P}_r(x)\log D(x)+P_g(x)\log[1-D(x)]</script><p>对D(x)求导,让导数为0</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\mathrm{P}_r(x)}{D(x)}-\frac{\mathrm{P}_g(x)}{1-D(x)}=0\\\\&\text{化简上式,得最优的D表达式为}.\\\\&D^*(x)=\frac{\mathrm{P}_r(x)}{\mathrm{P}_r(x)+\mathrm{P}_g(x)}\end{aligned}</script><p>将这个最优的D带入一开始的式子</p><script type="math/tex; mode=display">\begin{aligned}&\operatorname*{min}_{G}V=E_{x\sim\mathrm{p}_{r}}[\operatorname{log}D(x)]+E_{x\sim\mathrm{p}_{g}}[\operatorname{log}(1-D(x))] \\&\text{将最大化的D即式}1\text{的}D^*(x)\text{代入式得}: \\&\operatorname*{min}_{G}V=E_{x\sim\mathrm{p}_{r}}[\operatorname{log}(\frac{\mathrm{p}_{r}(x)}{\mathrm{p}_{r}(x)+\mathrm{p}_{g}(x)})]+E_{x\sim\mathrm{p}_{g}}[\operatorname{log}(\frac{\mathrm{p}_{g}(x)}{\mathrm{p}_{r}(x)+\mathrm{p}_{g}(x)})] \\&\text{再化简一步得式:} \\&\min_GV=E_{x\sim\mathrm{p}_r}[\log(\frac{\mathrm{p}_r(x)}{\frac12(\mathrm{p}_r(x)+\mathrm{p}_g(x))})]+E_{x\sim\mathrm{p}_g}[\log(\frac{\mathrm{p}_g(x)}{\frac12(\mathrm{p}_r(x)+\mathrm{p}_g(x))}]-2\log2\end{aligned}</script><p>将JS散度带入,有</p><script type="math/tex; mode=display">\min_GV=2JS(P_r||P_g)-2\log2</script><p>所以当判别器达到固定G情况下最优时,如果两个分布重叠则为JS则为0,否则JS为log2.梯度一直为0,G得不到更新,所以这种原始GAN会面临<strong>梯度消失问题</strong>,导致训练困难.</p><blockquote><p>上述的推导都是建立在最优判别器的基础上的，但是在我们实操过程中往往一开始判别器性能是不理想的，所以生成器还是有梯度更新的</p></blockquote><p>如果使用logD-trick,</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{x\sim P_{g}}\left[-\log D^{*}(x)\right]& =KL(P_{g}||P_{r})-\mathbb{E}_{x\sim P_{g}}\log[1-D^{*}(x)]  \\&=KL(P_g||P_r)-2JS(P_r||P_g)+2\log2+\mathbb{E}_{x\sim P_r}[\log D^*(x)]\end{aligned}</script><p>所以最后需要最小化前面两项,因为后面两项与G无关. 这个最小化目标需要同时最小化KL散度又要最大化JS散度,直观上荒谬,数值结果上<strong>导致梯度不稳定</strong>,此外第一项的KL散度表示</p><script type="math/tex; mode=display">P_{g}(x)\log\frac{P_{g}(x)}{P_{r}(x)}</script><p>当P~g~(x)趋近于1,P~r~(x)趋近于0这种情况与当P~g~(x)趋近于0,P~r~(x)趋近于1这种情况对于KL散度情况不一致,由于要最小化KL散度,会导致后者这种情况,也就是</p><p>这种情况下,梯度可能不会消失,但会存在梯度不稳定,模式崩溃的问题.</p><p>以上内容部分是WGAN中的,从理论上解释了GAN训练的一些问题.</p><h4 id="使用tensorboard记录损失"><a href="#使用tensorboard记录损失" class="headerlink" title="使用tensorboard记录损失"></a>使用tensorboard记录损失</h4><p>大致流程是首先将损失计入到一个文件,然后使用tensorboard读取,便能使用tensorboard打开一个端口,在网页上查看。<a href="https://pytorch.org/docs/stable/tensorboard.html">torch.utils.tensorboard — PyTorch 2.0 documentation</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorboard</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))])</span><br><span class="line">trainset = datasets.MNIST(<span class="string">&#x27;mnist_train&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">model = torchvision.models.resnet50(<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># Have ResNet model take in grayscale rather than RGB</span></span><br><span class="line">model.conv1 = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(trainloader))</span><br><span class="line"></span><br><span class="line">grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images&#x27;</span>, grid, <span class="number">0</span>)</span><br><span class="line">writer.add_graph(model, images)</span><br><span class="line"><span class="comment"># 关闭writer</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = np.random.random(<span class="number">1000</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&#x27;distribution centers&#x27;</span>, x + i, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><img data-src="https://pytorch.org/docs/stable/_images/add_histogram.png" alt="_images/add_histogram.png" style="zoom: 67%;" /></p><p>在google colab使用需要搭配一些magic func</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext tensorboard  <span class="comment">#使用tensorboard 扩展</span></span><br><span class="line">%tensorboard --logdir logs  <span class="comment">#定位tensorboard读取的文件目录</span></span><br></pre></td></tr></table></figure><h4 id="使用visdom可视化"><a href="#使用visdom可视化" class="headerlink" title="使用visdom可视化"></a>使用visdom可视化</h4><p>visdom一般搭配pytorch,毕竟都是meta的.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pip install visdom</span><br><span class="line">python -m visdom.server</span><br><span class="line">iz = Visdom()</span><br><span class="line">  </span><br><span class="line">viz.line([<span class="number">0.</span>],    <span class="comment">#Y的第一个点</span></span><br><span class="line">         [<span class="number">0.</span>],    <span class="comment">#X的第一个点</span></span><br><span class="line">         win=<span class="string">&quot;train loss&quot;</span>,   <span class="comment">#右上角窗口的名称 </span></span><br><span class="line">         opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train_loss&#x27;</span>) <span class="comment">#opt的参数都可以用python字典的格式传入，还有很多其他的类似matplotlib美化图形的参数参考官网</span></span><br><span class="line">        )  </span><br><span class="line">        </span><br><span class="line">viz.line([<span class="number">1</span>,],<span class="comment">#Y的下一个点</span></span><br><span class="line">         [<span class="number">1.</span>],<span class="comment">#X的下一个点</span></span><br><span class="line">         win=<span class="string">&quot;train loss&quot;</span>,</span><br><span class="line">         update=<span class="string">&#x27;append&#x27;</span><span class="comment">#添加到下一个点后面</span></span><br><span class="line">         )</span><br></pre></td></tr></table></figure><p>这里还是推荐选择两者之一即可.</p><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="*DCGAN"></a>*DCGAN</h3><p>标题<strong>WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</strong></p><p><strong>intro</strong></p><p>Learning reusable feature representations from large unlabeled datasets has been an area of active research. In the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classification. We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques. One can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. There has been very limited published research in trying to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs. In this paper, we make the following contributions </p><p>• We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings. We name this class of architectures Deep Convolutional GANs (DCGAN) </p><p>• We use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. </p><p>• We visualize the filters learnt by GANs and empirically show that specific filters have learned to draw specific objects.</p><p>We show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated sample</p><p>贡献:提出卷积GAN,卷积层替代全连接,使用训练过的判别器用于分类任务,可视化了生成器中的某层,显示出良好的绘制特定对象的能力.生成器的向量显示出能控制样本的语义质量行为.介绍了一些超参的初始化.</p><p>• Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). </p><p>• Use batchnorm in both the generator and the discriminator. • Remove fully connected hidden layers for deeper architectures. </p><p>• Use ReLU activation in generator for all layers except for the output, which uses Tanh. • Use LeakyReLU activation in the discriminator for all layers</p><p>使用了三个数据集</p><ul><li>批量标准化是两个网络中必须的。</li><li>卷积层替代全连接层。</li><li>使用strided卷积(步幅大于1)可以代替池化</li><li>ReLU激活（<em>几乎</em>总是）会有帮助。</li></ul><p><img data-src="https://s2.loli.net/2023/08/29/L1aVuqWoiDHfS2z.png" alt="image-20230829192803486"></p><p>原论文中D判别函数使用的是ReLU,但现在代码中很多其实还是用的LeakyReLU.此外不使用池化,而是使用deconvolution或者叫分数步长卷积(fractionally-strided convolutions).</p><p><img data-src="https://s2.loli.net/2023/08/29/cktnUB7Lj5g6mlI.png" alt="image-20230829214218494"></p><p>pytorch实现中,D判别器使用nn.AvgPool2d平均池化操作.</p><p><img data-src="https://s2.loli.net/2023/08/30/eIhKBaf3t85gVqx.png" alt="image-20230830120457639"></p><p>layer normalization RNN,nlp任务中,每个token的特征数不同,针对每个token</p><p>instance normalization GAN中,针对单个图像不同的通道</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> are very similar, but have some subtle differences. <a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> is applied on each channel of channeled data like RGB images, but <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> is usually applied on entire sample and often in NLP tasks. Additionally, <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm"><code>LayerNorm</code></a> applies elementwise affine transform, while <a href="https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a> usually don’t apply affine transform</p><p>ConvTranspose2d</p><p>逆卷积fractionally-strided convolutions,可以利用<code>torchsummary</code>这个库查看模型相关信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">myModel = Discriminator().to(DEVICE)</span><br><span class="line">summary(myModel,(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (type)               Output Shape         Param #</span><br><span class="line">================================================================</span><br><span class="line">            Conv2d<span class="string">-1</span>          [<span class="string">-1</span>, 512, 14, 14]           4,608</span><br><span class="line">       BatchNorm2d<span class="string">-2</span>          [<span class="string">-1</span>, 512, 14, 14]           1,024</span><br><span class="line">         LeakyReLU<span class="string">-3</span>          [<span class="string">-1</span>, 512, 14, 14]               0</span><br><span class="line">            Conv2d<span class="string">-4</span>            [<span class="string">-1</span>, 256, 7, 7]       1,179,648</span><br><span class="line">       BatchNorm2d<span class="string">-5</span>            [<span class="string">-1</span>, 256, 7, 7]             512</span><br><span class="line">         LeakyReLU<span class="string">-6</span>            [<span class="string">-1</span>, 256, 7, 7]               0</span><br><span class="line">            Conv2d<span class="string">-7</span>            [<span class="string">-1</span>, 128, 4, 4]         294,912</span><br><span class="line">       BatchNorm2d<span class="string">-8</span>            [<span class="string">-1</span>, 128, 4, 4]             256</span><br><span class="line">         LeakyReLU<span class="string">-9</span>            [<span class="string">-1</span>, 128, 4, 4]               0</span><br><span class="line">        AvgPool2d<span class="string">-10</span>            [<span class="string">-1</span>, 128, 1, 1]               0</span><br><span class="line">           Linear<span class="string">-11</span>                    [<span class="string">-1</span>, 1]             129</span><br><span class="line">          Sigmoid<span class="string">-12</span>                    [<span class="string">-1</span>, 1]               0</span><br><span class="line">================================================================</span><br><span class="line">Total params: 1,481,089</span><br><span class="line">Trainable params: 1,481,089</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): 0.00</span><br><span class="line">Forward/backward pass size (MB): 2.63</span><br><span class="line">Params size (MB): 5.65</span><br><span class="line">Estimated Total Size (MB): 8.28</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>我在测试github上一个DCGAN的代码时,发现其在生成器上除了最后一层使用tanh激活函数,其他层都使用leak激活函数,但是这样生成器会逐渐变大.</p><p><img data-src="https://s2.loli.net/2023/08/31/6QlMEzeH8vsLy7S.png" alt="image-20230831165159320" style="zoom:67%;" /></p><p>因为LeakyReLU照顾到了负数,使得每一线性层输出为负值时也有梯度,这样也许能使得生成器跳出</p><h3 id="WGAN"><a href="#WGAN" class="headerlink" title="*WGAN"></a>*WGAN</h3><p>使用EM距离<a href="https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">GAN — Wasserstein GAN &amp; WGAN-GP. Training GAN is hard. Models may never… | by Jonathan Hui | Medium</a></p><ul><li>判别器最后一层去掉sigmoid</li><li>生成器和判别器的loss不取log</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p><img data-src="https://pic1.zhimg.com/80/v2-b783ce95d8bdf1499fc88994e170a02c_720w.webp" alt="img"></p><p>上面这个公式是基于推图距离的计算</p><p><img data-src="https://img1.imgtp.com/2023/09/15/B6W4SQCF.png" alt="image-20230915205616738" style="zoom: 67%;" /></p><p><img data-src="https://pic2.zhimg.com/80/v2-fe9ef30af6166a5eea47c9006bfc27cd_720w.webp" alt=""></p><p>下面是WGAN论文的intro</p><p><img data-src="https://s2.loli.net/2023/09/05/u4hjRmwN5i3E9oG.png" alt="image-20230905224350055" style="zoom: 80%;" /></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*5jF5gbIDwU6k9m1ILl0Utg.jpeg" alt="img"></p><p>一开始的GAN的损失函数设计被认为有问题,与KL,JS散度有关.</p><script type="math/tex; mode=display">KL(P_1||P_2)=E_{x\sim P_1}log\frac{P_1}{P_2}</script><script type="math/tex; mode=display">KL(P_1||P_2)=\int\limits_xP_1\log\frac{P_1}{P_2}dx\text{或}KL(P_1||P_2)=\sum p_1\log\frac{P_1}{P_2}</script><p>KL散度是熵与交叉熵的差,它不是对称的.</p><p>而JS散度和KL散度是有关联的,可以看出JS散度是对称的,</p><script type="math/tex; mode=display">JS(P_1||P_2)=\frac12KL(P_1||\frac{P_1+P_2}2)+\frac12KL(P_2||\frac{P_1+P_2}2)</script><p>经证明,当两个分布不重叠时,JS散度为log2<a href="https://blog.csdn.net/Invokar/article/details/88917214">GAN：两者分布不重合JS散度为log2的数学证明_为什么深度学习wganjs散度等于log2</a></p><blockquote><p>从理论和经验上来说，真实的数据分布通常是一个<strong>低维流形</strong>，简单地说就是数据不具备高维特性，而是存在一个嵌入在高维度的低维空间内,在实际操作中，我们的维度空间远远不止3维，有可能是上百维，在这样的情况下，数据就更加难于重合.</p></blockquote><p>WGAN打算训练网络得到一个函数,这个函数满足1-Lipschitz,同时也是D辨别器,这样能使得损失函数更有意义,也能解决梯度与模式崩溃问题. </p><p>WGAN贡献:解决GAN训练不稳定与模式崩溃问题,有一个指标(EM距离),这个值越小训练得越好.</p><h4 id="WGAN-GP"><a href="#WGAN-GP" class="headerlink" title="WGAN-GP"></a>WGAN-GP</h4><p>这里的GP就是gradient penalty的意思.在发了第一篇GAN的文章之后,作者又发了这篇.</p><blockquote><p>The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often <strong>due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior</strong></p></blockquote><p>WGAN以及其衍生主要都是为了满足Lipschitz constraint,包括后面的Spectral Normalizaton<a href="https://arxiv.org/pdf/1802.05957.pdf">1802.05957.pdf (arxiv.org)</a>.</p><p>意思是强制使用梯度裁剪(clamp)到一个范围会导致不想要的行为,因为本身想要的是让critic满足Lipschitz,所以粗暴地使用了梯度裁剪.</p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*NVBkG5vDwwz-1ad-zwxddA.jpeg" alt="img"></p><p>需要使得判别器f的梯度范数处处小于1,WGAN-GP证明了需要使得在真实数据和生成的数据之间插值的点对于f应该具有1的梯度范数。</p><p>范数有多种.<img data-src="https://img1.imgtp.com/2023/09/17/ZwMIQPEx.png" alt="image-20230917103610368"></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*TErKpfBkilA-G24FNFg0FA.png" alt="img"></p><p>所以需要使用到梯度,而且是对于输入的梯度,通过限制输入的梯度,而不是WGAN中限制每次模型的weight和bias的值.<a href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html">torch.autograd.grad — PyTorch 2.0 documentation</a>在pytorch中使用autograd.grad计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autograd</span><br><span class="line"><span class="comment"># demo</span></span><br><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = torch.<span class="built_in">sum</span>(x**<span class="number">2</span>)</span><br><span class="line">grads = autograd.grad(outputs=y, inputs=x,create_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(grads)</span><br></pre></td></tr></table></figure><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*r8472Sg5fDJ1XKQPUbRC4Q.png" alt=""></p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*gi2isFNxtXE-pNiQ_CrZ8w.jpeg" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradient Penalty (e.g. gradients w.r.t x_penalty)</span></span><br><span class="line">eps = torch.rand(batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).to(DEVICE) <span class="comment"># x shape: (64, 1, 28, 28)</span></span><br><span class="line">x_penalty = eps*x + (<span class="number">1</span>-eps)*x_fake</span><br><span class="line">x_penalty = x_penalty.view(x_penalty.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># n 1 28*28</span></span><br><span class="line">p_outputs = D(x_penalty, y)  <span class="comment"># N,1</span></span><br><span class="line">xp_grad = autograd.grad(outputs=p_outputs, inputs=x_penalty, grad_outputs=D_labels, <span class="comment"># N 1</span></span><br><span class="line">                        create_graph=<span class="literal">True</span>, retain_graph=<span class="literal">True</span>, only_inputs=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(xp_grad)</span><br><span class="line">grad_penalty = p_coeff * torch.mean(torch.<span class="built_in">pow</span>(torch.norm(xp_grad[<span class="number">0</span>], <span class="number">2</span>, <span class="number">1</span>) - <span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>对于辨别器,WGAN一般叫做critic,损失函数,而生成器依旧是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Wasserstein loss</span></span><br><span class="line">x_outputs = D(x, y)</span><br><span class="line">z_outputs = D(x_fake, y)</span><br><span class="line">D_x_loss = torch.mean(x_outputs)</span><br><span class="line">D_z_loss = torch.mean(z_outputs)</span><br><span class="line">D_loss = D_z_loss - D_x_loss + grad_penalty</span><br></pre></td></tr></table></figure><p>此外不使用BN,批次标准化会在同一批次中的样本之间创建相关性。它<strong>影响了梯度惩罚的有效性</strong>，实验证实了这一点。</p><p>一般可以使用Layer Normalization也就是对单个样本进行归一化.</p><h3 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="*Conditional GAN"></a>*Conditional GAN</h3><p>某种程度上里程碑作品,能够控制GAN生成的东西了,通过添加label,也就是condition.</p><h3 id="InfoGAN"><a href="#InfoGAN" class="headerlink" title="InfoGAN"></a>InfoGAN</h3><p>提出利用互信息诱导潜变量.该方法将信息最大化引入到标准GAN网络中。</p><p><a href="https://medium.com/mlearning-ai/infogan-interpretable-representation-learning-to-distangle-data-unsupervised-33a4089d7c09">InfoGAN: Interpretable Representation Learning to Distangle Data Unsupervised | by Renee LIN | MLearning.ai | Medium</a></p><p>期望有良好的特征解耦关系.</p><blockquote><p>GAN公式使用简单的连续输入噪声矢量z，同时对G使用噪声的方式没有限制。因此，噪声可能会被生成器以高度纠缠(entangled)的方式使用，导致 z 的各个维度与数据的语义特征不对应。</p><p>在本文中，将输入噪声向量分解为两部分，而不是使用单个非结构化噪声向量：（i）z，它被视为不可压缩噪声源;（ii） c，我们称之为潜在代码，将针对数据分布的显著结构化语义特征。</p></blockquote><script type="math/tex; mode=display">I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)</script><p>引入互信息,在G输入时加入一个潜变量c,潜在代码 C 和生成器分布 G(z,c) 之间应该有高度的互信息。因此I(c;G(z,c)) 应该很高。给定任何 x ∼ P~G~(x),希望 P~G~（c|x） 有一个较小的熵。换句话说，潜在代码c中的信息不应该在生成过程中丢失。</p><script type="math/tex; mode=display">\operatorname*{min}_{G}\operatorname*{max}_{D}V_{I}(D,G)=V(D,G)-\lambda I(c;G(z,c))</script><p>然而上面互信息的计算涉及后验概率分布P(c|x)，而后者在实际中是很难获取的，所以需要定义一个辅助性的概率分布Q(c|x)，采用Variational Information Maximization对互信息进行下界拟合.</p><script type="math/tex; mode=display">\begin{aligned}I(c;G(z,c))& =H(c)-H(c|G(z,c))  \\&=\mathbb{E}_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log P(c^{\prime}|x)]]+H(c) \\&=\mathbb{E}_{x\sim G(z,c)}[\underbrace{D_{\mathrm{KL}}(P(\cdot|x)\parallel Q(\cdot|x))}_{\geq0}+\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c) \\&\geq\mathbb{E}_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c)\end{aligned}</script><p>这样互信息计算就能确定最小值,继续推导有</p><script type="math/tex; mode=display">\begin{aligned}L_{I}(G,Q)& =E_{c\sim P(c),x\sim G(z,c)}[\log Q(c|x)]+H(c)  \\&=E_{x\sim G(z,c)}[\mathbb{E}_{c^{\prime}\sim P(c|x)}[\log Q(c^{\prime}|x)]]+H(c) \\&\leq I(c;G(z,c))\end{aligned}</script><p>最后目标函数为</p><p><img data-src="https://pic4.zhimg.com/80/v2-ede0624e4acb54575483852435d0ec2b_720w.webp" alt="img"></p><p><img data-src="https://miro.medium.com/v2/resize:fit:543/1*c0wSI0WJR9-yagc0ruFGGg.png" alt="img" style="zoom: 67%;" /></p><p>z,c均为采样得到,z依旧是正态分布采样,c由两部分组成,一部分是离散分布另一部分是连续分布.论文中使用Categorical与Unif分布,是离散均匀分布与连续均匀分布.</p><p><img data-src="https://img1.imgtp.com/2023/09/18/TvDtnXUu.png" alt="image-20230918102341583" style="zoom:67%;" /></p><p>在MNIST数据集上,比如使用c~1~作为离散变量控制生成的数字的类型,其他的c~2~和c~3~作为连续变量控制其他.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_noise</span>(<span class="params">batch_size, n_noise, n_c_discrete, n_c_continuous, label=<span class="literal">None</span>, supervised=<span class="literal">False</span></span>):</span></span><br><span class="line">    z = torch.randn(batch_size, n_noise).to(DEVICE) <span class="comment">#正态分布 潜变量 与VAE的中间变量类似. bottleneck</span></span><br><span class="line">    <span class="comment"># 离散分布 控制数字类型也就是类别 如果supervised 会根据label的值</span></span><br><span class="line">    <span class="keyword">if</span> supervised:</span><br><span class="line">        c_discrete = to_onehot(label).to(DEVICE) <span class="comment"># (B,10)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 否则随机离散均匀生成</span></span><br><span class="line">        c_discrete = to_onehot(torch.LongTensor(batch_size, <span class="number">1</span>).random_(<span class="number">0</span>, n_c_discrete)).to(DEVICE) <span class="comment"># (B,10)</span></span><br><span class="line">    <span class="comment"># 连续分布 控制其他属性 </span></span><br><span class="line">    c_continuous = torch.zeros(batch_size, n_c_continuous).uniform_(-<span class="number">1</span>, <span class="number">1</span>).to(DEVICE) <span class="comment"># (B,2)</span></span><br><span class="line">    c = torch.cat((c_discrete.<span class="built_in">float</span>(), c_continuous), <span class="number">1</span>) <span class="comment">#c (B,12)</span></span><br><span class="line">    <span class="keyword">return</span> z, c</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># Training Discriminator</span></span><br><span class="line">x = images.to(DEVICE)</span><br><span class="line">x_outputs, _, = D(x)</span><br><span class="line">D_x_loss = bce_loss(x_outputs, D_labels)</span><br><span class="line"></span><br><span class="line">z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=<span class="literal">True</span>)</span><br><span class="line">z_outputs, _, = D(G(z, c))</span><br><span class="line">D_z_loss = bce_loss(z_outputs, D_fakes)</span><br><span class="line">D_loss = D_x_loss + D_z_loss</span><br><span class="line"></span><br><span class="line">D_opt.zero_grad()</span><br><span class="line">D_loss.backward()</span><br><span class="line">D_opt.step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_gaussian</span>(<span class="params">c, mu, var</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    criterion for Q(condition classifier)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> -((c - mu)**<span class="number">2</span>)/(<span class="number">2</span>*var+<span class="number">1e-8</span>) - <span class="number">0.5</span>*torch.log(<span class="number">2</span>*np.pi*var+<span class="number">1e-8</span>)</span><br><span class="line">    </span><br><span class="line"> <span class="comment"># Training Generator</span></span><br><span class="line">z, c = sample_noise(batch_size, n_noise, n_c_discrete, n_c_continuous, label=labels, supervised=<span class="literal">True</span>)</span><br><span class="line">c_discrete_label = torch.<span class="built_in">max</span>(c[:, :-<span class="number">2</span>], <span class="number">1</span>)[<span class="number">1</span>].view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">z_outputs, features = D(G(z, c)) <span class="comment"># (B,1), (B,10), (B,4)</span></span><br><span class="line">c_discrete_out, cc_mu, cc_var = Q(features)</span><br><span class="line"></span><br><span class="line">G_loss = bce_loss(z_outputs, D_labels)</span><br><span class="line">Q_loss_discrete = ce_loss(c_discrete_out, c_discrete_label.view(-<span class="number">1</span>))</span><br><span class="line">Q_loss_continuous = -torch.mean(torch.<span class="built_in">sum</span>(log_gaussian(c[:, -<span class="number">2</span>:], cc_mu, cc_var), <span class="number">1</span>)) <span class="comment"># N(x | mu,var) -&gt; (B, 2) -&gt; (,1)</span></span><br><span class="line">mutual_info_loss = Q_loss_discrete + Q_loss_continuous*<span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">GnQ_loss = G_loss + mutual_info_loss</span><br><span class="line"></span><br><span class="line">G_opt.zero_grad()</span><br><span class="line">GnQ_loss.backward()</span><br><span class="line">G_opt.step()</span><br></pre></td></tr></table></figure><p>离散分布的c求损失使用交叉熵,利用一个Q网络,输入是D的倒数第二层输出,得到离散输出与连续输出的均值和logV. 相当于利用Q的输出与D的倒数第二层输出计算损失,判断在生成过程中是否有损失.</p><p>其中log_gaussian是在计算log(q(x)),看来还是要学好数理统计和矩阵论才行.</p><h3 id="BIGGAN"><a href="#BIGGAN" class="headerlink" title="BIGGAN"></a>BIGGAN</h3><h3 id="proGAN"><a href="#proGAN" class="headerlink" title="proGAN"></a>proGAN</h3><h3 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h3><h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><h3 id="SAGAN"><a href="#SAGAN" class="headerlink" title="SAGAN"></a>SAGAN</h3><h3 id="TelDiGAN"><a href="#TelDiGAN" class="headerlink" title="TelDiGAN"></a>TelDiGAN</h3><h3 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p><a href="https://zhuanlan.zhihu.com/p/44926155">盘点各种GAN及资源整理（1） - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://github.com/soumith/ganhacks">soumith/ganhacks: starter from “How to Train a GAN?” at NIPS2016 (github.com)</a></p></li><li><p><a href="https://github.com/Yangyangii/GAN-Tutorial/tree/master">Yangyangii/GAN-Tutorial: Simple Implementation of many GAN models with PyTorch. (github.com)</a> 在一些数据集上的GAN</p></li><li><p><a href="https://github.com/eriklindernoren/PyTorch-GAN">eriklindernoren/PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks. (github.com)</a>pytorch实现的GAN</p></li><li><p><a href="https://github.com/ccc013/GAN_Study">ccc013/GAN_Study: 学习GAN的笔记和代码 (github.com)</a></p></li><li><p><a href="https://github.com/torchgan/torchgan">torchgan/torchgan: Research Framework for easy and efficient training of GANs based on Pytorch (github.com)</a> pytorch实现的库</p></li><li><p><a href="https://github.com/tensorflow/models/tree/master/research/gan">File not found (github.com)</a>tensorflow实现的GAN</p></li><li><p><a href="https://github.com/eriklindernoren/Keras-GAN">eriklindernoren/Keras-GAN: Keras implementations of Generative Adversarial Networks. (github.com)</a>keras实现的GAN</p></li><li><p><a href="https://github.com/zhangqianhui/AdversarialNetsPapers">zhangqianhui/AdversarialNetsPapers: Awesome paper list with code about generative adversarial nets (github.com)</a>GAN论文与代码</p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;深入GAN学习&lt;br&gt;</summary>
    
    
    
    
    <category term="GAN" scheme="https://www.sekyoro.top/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>tailwind速成</title>
    <link href="https://www.sekyoro.top/2023/08/04/tailwind%E9%80%9F%E6%88%90/"/>
    <id>https://www.sekyoro.top/2023/08/04/tailwind%E9%80%9F%E6%88%90/</id>
    <published>2023-08-04T14:09:28.000Z</published>
    <updated>2023-08-05T12:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>很火的CSS框架<br><span id="more"></span></p><p><a href="https://www.tailwindcss.cn/docs/installation">安装 - TailwindCSS中文文档 | TailwindCSS中文网</a></p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p><img data-src="https://s2.loli.net/2023/08/04/7d4CpYb5QeXHguV.png" alt="image-20230804221132617"></p><p><strong>Mobile first</strong></p><h2 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h2><h2 id="自定义设置值"><a href="#自定义设置值" class="headerlink" title="自定义设置值"></a>自定义设置值</h2><p>在<code>tailwind.config.js</code>文件中,由于 Tailwind 是一个用于构建定制用户界面的框架，因此在设计之初就考虑到了定制化。</p><p>默认情况下，Tailwind会在项目根目录下查找一个可选的 tailwind.config.js 文件，您可以在其中定义任何自定义内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx tailwindcss init</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  <span class="attr">theme</span>: &#123;</span><br><span class="line">    <span class="attr">colors</span>: &#123;</span><br><span class="line">      <span class="attr">primary</span>: <span class="string">&#x27;#5c6ac4&#x27;</span>,</span><br><span class="line">      <span class="attr">secondary</span>: <span class="string">&#x27;#ecc94b&#x27;</span>,</span><br><span class="line">      <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://www.tailwindcss.cn/docs/customizing-colors#naming-your-colors">Customizing Colors - TailwindCSS中文文档 | TailwindCSS中文网</a></p><h2 id="重用"><a href="#重用" class="headerlink" title="重用"></a>重用</h2><p><a href="https://www.tailwindcss.cn/docs/reusing-styles#extracting-classes-with-apply">Reusing Styles - TailwindCSS中文文档 | TailwindCSS中文网</a></p><p>一般使用两种方法,一种使用@layer和@apply,另一种使用react或者vue框架定义component.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;很火的CSS框架&lt;br&gt;</summary>
    
    
    
    
    <category term="css" scheme="https://www.sekyoro.top/tags/css/"/>
    
    <category term="Tailwind" scheme="https://www.sekyoro.top/tags/Tailwind/"/>
    
  </entry>
  
  <entry>
    <title>深度知识基础学习(一)</title>
    <link href="https://www.sekyoro.top/2023/08/02/%E6%B7%B1%E5%BA%A6%E7%9F%A5%E8%AF%86%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0(%E4%B8%80)/"/>
    <id>https://www.sekyoro.top/2023/08/02/%E6%B7%B1%E5%BA%A6%E7%9F%A5%E8%AF%86%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0(%E4%B8%80)/</id>
    <published>2023-08-02T03:59:56.000Z</published>
    <updated>2023-09-05T07:05:44.320Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这方面的知识实在是很多<br><span id="more"></span></p><p>首先写一下计算机视觉方向的,介绍一下各种Net的发展史.</p><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>Padding Stride 卷积核大小</p><p>p表示填充padding,k表示卷积核宽高,s表示stride</p><script type="math/tex; mode=display">n_{\mathrm{out}}=\frac{n_{\mathrm{in}}+2p-k}{s}+1</script><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><blockquote><p>LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。 这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像 (<a href="https://zh.d2l.ai/chapter_references/zreferences.html#id90">LeCun <em>et al.</em>, 1998</a>)中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果</p></blockquote><p><img data-src="https://s2.loli.net/2023/08/03/wQkVZHUIGzYyvMs.png" alt="image-20230803191703397"></p><p>​    每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。</p><p>​    为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(in_features=<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, out_features=<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out = F.relu(self.conv1(x)) <span class="comment"># 3*32*32 -&gt; 6*28*28</span></span><br><span class="line">        out = F.max_pool2d(out,<span class="number">2</span>) <span class="comment"># 6*28*28 -&gt; 6*14*14</span></span><br><span class="line">        out = F.relu(self.conv2(out)) <span class="comment"># 6*14*14 -&gt; 16*10*10</span></span><br><span class="line">        out = F.max_pool2d(out, <span class="number">2</span>) <span class="comment"># 16*10*10 -&gt; 16*5*5</span></span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>) <span class="comment"># 16*5*5 -&gt; 400</span></span><br><span class="line"></span><br><span class="line">        out = F.relu(self.fc1(out))</span><br><span class="line">        out = F.relu(self.fc2(out))</span><br><span class="line">        out = self.fc3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。</p><p>AlexNet和LeNet的架构非常相似</p><p><img data-src="https://s2.loli.net/2023/08/03/olDupY69gcaJGH3.png" alt="image-20230803213645408" style="zoom:67%;" /></p><p>AlexNet和LeNet的设计理念非常相似，但也存在显著差异。</p><ol><li>AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li><li>AlexNet使用ReLU而不是sigmoid作为其激活函数。</li></ol><p>AlexNet通过<code>DropOut</code>控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。 在 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/image-augmentation.html#sec-image-augmentation">13.1节</a>中更详细地讨论数据扩增。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>), <span class="comment"># 3*224*224 -&gt; 64*55*55</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 64*55*55 -&gt; 64*27*27</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), <span class="comment"># 64*27*27 -&gt; 192*27*27</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 192*27*27 -&gt; 192*13*13</span></span><br><span class="line">            nn.Conv2d(<span class="number">192</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 192*13*13 -&gt; 384*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 384*13*13 -&gt; 256*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), <span class="comment"># 256*13*13 -&gt; 256*13*13</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment"># 256*13*13 -&gt; 256*6*6</span></span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Linear(<span class="number">256</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out = self.features(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><blockquote><p>AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/vgg.svg" alt="../_images/vgg.svg" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;VGG11&#x27;</span>: [<span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG13&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG16&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;VGG19&#x27;</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">&#x27;M&#x27;</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vgg_name</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VGG, self).__init__()</span><br><span class="line">        self.features = self._make_layers(cfg[vgg_name])</span><br><span class="line">        self.classifier = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.features(x)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.classifier(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layers</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        layers = []</span><br><span class="line">        in_channels = <span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> cfg:</span><br><span class="line">            <span class="keyword">if</span> x == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">                layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [nn.Conv2d(in_channels, x, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                           nn.BatchNorm2d(x),</span><br><span class="line">                           nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">                in_channels = x</span><br><span class="line">        layers += [nn.AvgPool2d(kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG16</span>():</span></span><br><span class="line">    <span class="keyword">return</span> VGG(<span class="string">&#x27;VGG16&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG19</span>():</span></span><br><span class="line">    <span class="keyword">return</span> VGG(<span class="string">&#x27;VGG19&#x27;</span>)</span><br></pre></td></tr></table></figure><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。</p><h4 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h4><p><strong>1x1卷积</strong></p><blockquote><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 <em>网络中的网络</em>（<em>NiN</em>）提供了一个非常简单的解决方案：<strong>在每个像素的通道上分别使用多层感知机</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参考AlexNet设计</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NiN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_labels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NiN, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            self.nin_block(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            self.nin_block(in_channels=<span class="number">384</span>, out_channels=num_labels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line">        self.init_weight()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weight</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(layer.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">                nn.init.constant_(layer.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,</span><br><span class="line">                      padding=padding),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_output_shape</span>(<span class="params">self</span>):</span></span><br><span class="line">        test_img = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">227</span>, <span class="number">227</span>), dtype=torch.float32)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.net:</span><br><span class="line">            test_img = layer(test_img)</span><br><span class="line">            <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, test_img.shape)</span><br></pre></td></tr></table></figure><p>NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为1×1卷积层，或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）</p><h4 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h4><p>GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 GoogleNet核心是提出了<code>Inception</code>这种模块.</p><p>这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 </p><p><img data-src="https://zh-v2.d2l.ai/_images/inception.svg" alt="../_images/inception.svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img data-src="https://zh-v2.d2l.ai/_images/inception-full.svg" alt="../_images/inception-full.svg" style="zoom: 67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p>对由多个输入平面组成的输入信号进行二维自适应平均池化处理。</p><p>对于任何输入尺寸，输出的尺寸都是 H x W。输出特征的数量等于输入平面的数量。</p></blockquote><p>nn.AdaptiveAvgPool2d((1,1))，首先这句话的含义是使得池化后的每个通道上的大小是一个1x1的，也就是每个通道上只有一个像素点。（1，1）表示的outputsize。</p><h5 id="Inception-V1"><a href="#Inception-V1" class="headerlink" title="Inception V1"></a>Inception V1</h5><p>使用了全局平均池化</p><h5 id="Inception-V2"><a href="#Inception-V2" class="headerlink" title="Inception V2"></a>Inception V2</h5><p>使用Batch Normalization，加快模型训练速度；<br>使用两个3x3的卷积代替5x5的大卷积，降低了参数数量并减轻了过拟合</p><h5 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception V3"></a>Inception V3</h5><blockquote><p>Inception V3一个最重要的改进是卷积分解（Factorization），将7x7卷积分解成两个一维的卷积串联（1x7和7x1），3x3卷积分解为两个一维的卷积串联（1x3和3x1），这样既可以加速计算，又可使网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU）</p></blockquote><p><img data-src="https://img-blog.csdnimg.cn/20190827133529424.png" alt="img" style="zoom: 67%;" /></p><h5 id="Inception-V4"><a href="#Inception-V4" class="headerlink" title="Inception V4"></a>Inception V4</h5><p>inception v4把原来的inception结构中加入了ResNet中的Residual Blocks结构，把一些层的输出加上前几层的输出，这样中间这几层学习的实际上是残差。</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>假设我们的原始输入为x，而希望学出的理想映射为f(x)（作为上方激活函数的输入）。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出残差映射f(x)−x。 残差映射在现实中往往更容易优化。 以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)，我们只需将右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么f(x)即为恒等映射。 实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 右图是ResNet的基础架构–<em>残差块</em>（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。</p><p><img data-src="https://zh-v2.d2l.ai/_images/residual-block.svg" alt="../_images/residual-block.svg" style="zoom:67%;" /></p><blockquote><p>ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算</p></blockquote><p><img data-src="https://zh-v2.d2l.ai/_images/resnet-block.svg" alt="../_images/resnet-block.svg" style="zoom:80%;" /></p><p><img data-src="https://zh-v2.d2l.ai/_images/resnet18.svg" alt="../_images/resnet18.svg" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差块  通过卷积</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span></span><br><span class="line">      <span class="built_in">super</span>().__init__()</span><br><span class="line">      self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">      self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">      <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">          self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                  kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self.conv3 = <span class="literal">None</span></span><br><span class="line">      self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">      self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">      Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">      Y = self.bn2(self.conv2(Y))</span><br><span class="line">      <span class="keyword">if</span> self.conv3:</span><br><span class="line">          X = self.conv3(X)</span><br><span class="line">      Y += X</span><br><span class="line">      <span class="keyword">return</span> F.relu(Y)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#残差模块</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add(nn.Conv2D(<span class="number">64</span>, kernel_size=<span class="number">7</span>, strides=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm(), nn.Activation(<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        nn.MaxPool2D(pool_size=<span class="number">3</span>, strides=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">net.add(resnet_block(<span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>),</span><br><span class="line">        resnet_block(<span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">        resnet_block(<span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">        resnet_block(<span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">net.add(nn.GlobalAvgPool2D(), nn.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p><img data-src="https://zh-v2.d2l.ai/_images/densenet-block.svg" alt="../_images/densenet-block.svg"></p><p>ResNet和DenseNet的关键区别在于，DenseNet输出是<em>连接</em>（用图中的[,]表示）而不是如ResNet的简单相加</p><p><img data-src="https://zh-v2.d2l.ai/_images/densenet.svg" alt="../_images/densenet.svg"></p><p>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂</p><h5 id="denseblock"><a href="#denseblock" class="headerlink" title="denseblock"></a>denseblock</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h5 id="transition-layer"><a href="#transition-layer" class="headerlink" title="transition layer"></a>transition layer</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度.</p><h5 id="DenseNet-1"><a href="#DenseNet-1" class="headerlink" title="DenseNet"></a>DenseNet</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">input_channels, num_channels</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">blks = []</span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels += num_convs * growth_rate</span><br><span class="line">    <span class="comment"># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        blks.append(transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, *blks,</span><br><span class="line">    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(num_channels, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，DenseNet则使用过渡层来减半高和宽，并减半通道数。</p><ul><li>在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。</li><li>DenseNet的主要构建模块是稠密块和过渡层。</li><li>在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量  </li></ul><h2 id="正则化方式"><a href="#正则化方式" class="headerlink" title="正则化方式"></a>正则化方式</h2><h3 id="不同的normalization"><a href="#不同的normalization" class="headerlink" title="不同的normalization"></a>不同的normalization</h3><blockquote><p>为什么需要批量规范化层呢？让我们来回顾一下训练神经网络时出现的一些实际挑战。</p><p>首先，数据预处理的方式通常会对最终结果产生巨大影响。 使用真实数据时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。 直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。</p><p>第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：<strong>不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛</strong>。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。</p><p>第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。</p></blockquote><p>Normalization有多种方式,包括BN，IN，GN，LN.</p><h4 id="BN-Batch-Normalization"><a href="#BN-Batch-Normalization" class="headerlink" title="BN Batch Normalization"></a>BN Batch Normalization</h4><p><img data-src="https://s2.loli.net/2023/08/11/YpgyB1C3ZQkTbuh.png" alt="image-20230811102021894" style="zoom:50%;" /></p><p><img data-src="https://s2.loli.net/2023/08/10/GpsxiLJhErfyOTb.png" alt="image-20230810201247075"></p><p>其中N表示样本数,H、W表示高和宽.得到均值和标准差,利用这两个值标准化.</p><p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于<em>批量</em>统计的<em>标准化</em>，才有了<em>批量规范化</em>的名称. 简单来说,就是对于每个batch每个通道计算.得到三对均值和方差,然后对每个通道规范化.</p><h4 id="IN-Instance-Normalization"><a href="#IN-Instance-Normalization" class="headerlink" title="IN Instance Normalization"></a>IN Instance Normalization</h4><p><img data-src="https://s2.loli.net/2023/08/11/PoVaWGy6FLzpfAi.png" alt="image-20230811102050186" style="zoom:67%;" /></p><blockquote><p>Instance Normalization (IN) 最初用于图像的风格迁移。作者发现，在生成模型中， feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格，因此可以先把图像在 channel 层面归一化，然后再用目标风格图片对应 channel 的均值和标准差“去归一化”，以期获得目标图片的风格。IN 操作也在单个样本内部进行，不依赖 batch。</p></blockquote><p><img data-src="https://s2.loli.net/2023/08/10/HlRGrK9joUJxkip.png" alt="image-20230810204827706"></p><p>简单点来说,就是对于每个样本,一张彩图三个通道计算,batch=1 用在特定任务比如风格迁移上.</p><h4 id="GN-Group-Normalization"><a href="#GN-Group-Normalization" class="headerlink" title="GN Group Normalization"></a>GN Group Normalization</h4><p>对于特定任务,batch不能过大,否则存在显存占用问题.而一般的BN这时候表现较差.GN 计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。</p><p><img data-src="https://s2.loli.net/2023/08/10/Ryfsc9CJhqEMLv3.png" alt="image-20230810205259430" style="zoom: 67%;" /></p><p>对通道进行分组，统计每个分组通道的高度和宽度，增强对批量大小的稳定性</p><h4 id="LN-Layer-Normalization"><a href="#LN-Layer-Normalization" class="headerlink" title="LN Layer Normalization"></a>LN Layer Normalization</h4><p>在使用BN层时，需要的假设是每个mini batch应该是同分布（或者近似同分布）的，如果不同mini batch的分布差异较大，相当于这个BN层需要学习不同的变换，这便无法解决<strong>Internal Covariate Shift</strong>（ICS,也就是内部偏移）问题。因此，在使用BN层时，batchsize尽可能调大、且数据集彻底打乱，否则BN的效果会显著变差。显而易见，BN也并不适用于需要先后输入数据的RNN模型。</p><p>BN并不适用于序列模型（RNN），对于序列数据，我们其实更加关心独立的数据样本（例如一个句子的特定位置的单词），因此Layer Normalization将每一条数据做归一化。</p><p><img data-src="https://s2.loli.net/2023/08/10/6xSbuFan71pBQYJ.png" alt="image-20230810211021828"></p><p>简单来说,就是计算所有通道上的数据得到均值和标准差.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/u010986753/article/details/99191760">GoogleNet、AleXNet、VGGNet、ResNet等总结_小麦粒的博客-CSDN博客</a></li><li><a href="https://zh-v2.d2l.ai/chapter_convolutional-modern/batch-norm.html">7.5. 批量规范化 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li><a href="https://zhuanlan.zhihu.com/p/91965772">BN、LN、IN、GN的简介 - 知乎 (zhihu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这方面的知识实在是很多&lt;br&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://www.sekyoro.top/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>增加live2d看板娘</title>
    <link href="https://www.sekyoro.top/2023/08/01/%E5%A2%9E%E5%8A%A0live2d%E7%9C%8B%E6%9D%BF%E5%A8%98/"/>
    <id>https://www.sekyoro.top/2023/08/01/%E5%A2%9E%E5%8A%A0live2d%E7%9C%8B%E6%9D%BF%E5%A8%98/</id>
    <published>2023-08-01T13:40:48.000Z</published>
    <updated>2023-08-02T02:25:50.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>看到可爱的看板娘了吗?<br><span id="more"></span><br>很多个人博客或者网站都有一个可爱的看板娘,这里主要用的就是一位大佬的代码(详细看参考资料),全都导入后,用于获取以及处理live2d人物以及对话消息.</p><p><img data-src="https://s2.loli.net/2023/08/02/GLDCzqIf9XHAn2Q.png" alt="image-20230802091503678"></p><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>现在的需求就是,后端的api用的是别人的服务,live2d人物的衣服放在github上,如果不翻的话很可能看不了,所以现在主要就是把后端api放到自己的网站上,同时看能不能增加一些模型.</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>如果有自己的服务器,那么上面如果有php的话直接放上去基本就可以了,如果不想这么做,可以使用其他的托管工具,比如<code>Vercel</code>,事实上Vercel是支持php的,利用了serverless  functions.至于增加更多模型,就需要到处找找或者自己使用<code>cubsim</code>制作了.</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://github.com/vercel-community/php/#️-faq">vercel-community/php: 🐘 PHP Runtime for ▲ Vercel Serverless Functions (github.com)</a></li><li><a href="https://github.com/stevenjoezhang/live2d-widget">stevenjoezhang/live2d-widget: 把萌萌哒的看板娘抱回家 (ノ≧∇≦)ノ | Live2D widget for web platform (github.com)</a></li><li><a href="https://github.com/xiaoski/live2d_models_collection/tree/master">xiaoski/live2d_models_collection: Collections of live2d models (github.com)</a></li><li><a href="https://www.live2d.com/zh-CHS/">Live2D - Live2D Cubism【官网】</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;看到可爱的看板娘了吗?&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>浏览器JS</title>
    <link href="https://www.sekyoro.top/2023/07/30/%E6%B5%8F%E8%A7%88%E5%99%A8JS/"/>
    <id>https://www.sekyoro.top/2023/07/30/%E6%B5%8F%E8%A7%88%E5%99%A8JS/</id>
    <published>2023-07-30T07:46:22.000Z</published>
    <updated>2023-08-01T01:44:22.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>浏览器的一些常用js方法.主要是浏览的宽高什么的,我经常弄混.<br><span id="more"></span></p><p>由于经常涉及浏览器里的元素<strong>宽高属性</strong>以及<strong>与其他元素间的距离</strong>,而且这设计到浏览器兼容性问题,相同的获取滚动条距离什么的方法在不同浏览器很可能效果不同,这里就优先考虑谷歌,Edge这些Chromium内核的.</p><p><img data-src="https://www.runoob.com/wp-content/uploads/2021/10/L0hUTUw15byA5Y-R5paH5qGjL2ltYWdlcy9Dc3NCb3hNb2RlbC5wbmc.png" alt="img" style="zoom: 67%;" /></p><p>这个图上,由内到外分别是content,padding,border,margin,然后是其他外层元素.通过这个图可以有一个大致了解.</p><div class="table-container"><table><thead><tr><th>元素尺寸属性</th><th>说明</th></tr></thead><tbody><tr><td>clientWidth</td><td>获取元素可视部分的宽度，即 CSS 的 width 和 padding 属性值之和，元素边框和滚动条不包括在内，也不包含任何可能的滚动区域</td></tr><tr><td>clientHeight</td><td>获取元素可视部分的高度，即 CSS 的 height 和 padding 属性值之和，元素边框和滚动条不包括在内，也不包含任何可能的滚动区域</td></tr><tr><td>offsetWidth</td><td>元素在页面中占据的宽度总和，包括 width、padding、border 以及滚动条的宽度</td></tr><tr><td>offsetHeight</td><td>元素在页面中占据的高度总和，包括 height、padding、border 以及滚动条的宽度</td></tr><tr><td>scrollWidth</td><td>当元素设置了 overflow:visible 样式属性时，元素的总宽度，也称滚动宽度。在默认状态下，如果该属性值大于 clientWidth 属性值，则元素会显示滚动条，以便能够翻阅被隐藏的区域</td></tr><tr><td>scrollHeight</td><td>当元素设置了 overflow:visible 样式属性时，元素的总高度，也称滚动高度。在默认状态下，如果该属性值大于 clientWidth 属性值，则元素会显示滚动条，以便能够翻阅被隐藏的区域</td></tr></tbody></table></div><p><img data-src="https://s2.loli.net/2023/07/31/nP1rODAi6NhLcy5.png" alt="image-20230731223846471"></p><p>比如打开一个有滚动条的网站.</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">document</span>.documentElement.offsetWidth</span><br><span class="line">&gt;<span class="number">863</span></span><br><span class="line"><span class="built_in">document</span>.documentElement.clientWidth</span><br><span class="line">&gt;<span class="number">863</span></span><br><span class="line"><span class="built_in">document</span>.documentElement.scrollWidth</span><br><span class="line">&gt;<span class="number">1200</span></span><br></pre></td></tr></table></figure><p>其他的xxxLeft,xxxTop含义类似.比如clienLeft表示border的宽度.如果设置盒模型为border-box,那么clientWidth就等于width-border-滚动条,因为border-box的宽高包括了padding和border,而clientWidth不包括border以及滚动条.</p><p>而offsetLeft表示<code>当前元素左上角相对于offsetParent左边界的偏移</code>,offsetTop同理,计算时会使用元素的offsetParent元素的padding、margin,border以及自己的margin计算.</p><p>最后scrolLeft表示<code>一个元素的内容水平滚动的像素数</code>,比较好理解,就是如果有滚动条的话,就是滚动条滚动的距离</p><h3 id="获取可视区域的宽高"><a href="#获取可视区域的宽高" class="headerlink" title="获取可视区域的宽高"></a>获取可视区域的宽高</h3><ul><li>document.documentElement.clientWidth</li><li>document.documentElement.clientHeight</li></ul><p><img data-src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bd22ffc2355e4de99d1893ffb1f9667e~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom: 50%;" /></p><p>注意,这个方法在老的IE浏览器上行不通.可以看到这个宽高不包括margin,border以及可能的滚动条宽高.此外注意这个宽高顾名思义<strong>可视区域</strong>,如果打开发者工具,会发现这个工具也是占宽高的.浏览器上面的任务栏以及滚动条也是占距离的.</p><h3 id="获取滚动条距离"><a href="#获取滚动条距离" class="headerlink" title="获取滚动条距离"></a>获取滚动条距离</h3><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> scrollTop = <span class="built_in">document</span>.body.scrollTop || (<span class="built_in">document</span>.documentElement &amp;&amp; <span class="built_in">document</span>.documentElement.scrollTop);</span><br></pre></td></tr></table></figure><p>在谷歌这类浏览器中通常使用<code>document.documentElement.scrollTop</code></p><p>获取元素</p><p><img data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5e7d4a30e10c421faf01df217a71894b~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp" alt="img" style="zoom:67%;" /></p><h3 id="屏幕分辨率"><a href="#屏幕分辨率" class="headerlink" title="屏幕分辨率"></a>屏幕分辨率</h3><p>屏幕分辨率的高：window.screen.height</p><p>屏幕分辨率的宽：window.screen.width</p><p>屏幕可工作区域的高：window.screen.availHeight</p><p>屏幕可工作区域的宽：window.screen.availWidth</p><p>height和availHeight差别是会受到电脑任务栏的影响.此外这个值跟实际屏幕像素不同是因为有<code>屏幕缩放因子：window.devicePixelRatio</code></p><p>屏幕逻辑分辨率：window.screen.width * window.devicePixelRatio (缩放因子与物理分辨率的乘积)</p><p><img data-src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2240a67351d6413e8a93d5d7431ea129~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom:50%;" /></p><p>window.screenTop：浏览器相对于屏幕左上角的横向偏移值<br>window.screenLeft：浏览器相对于屏幕左上角的纵向偏移值</p><h3 id="浏览器的宽高"><a href="#浏览器的宽高" class="headerlink" title="浏览器的宽高"></a>浏览器的宽高</h3><p><img data-src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50b6dd61a0a5433fa03ef2c4a17d332f~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?" alt="image.png" style="zoom:67%;" /></p><p>浏览器总高度：window.outerHeight </p><p>浏览器总宽度：window.outerWidth  </p><p>此外还有不包括工具栏的:window.innerHeight  window.innerWidth </p><p><img data-src="https://s2.loli.net/2023/07/31/2xLB5U1Cwgo9lka.png" alt="image-20230731235947161"></p><p>这两个都包括滚动条,如果没有滚动条</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innerHeight = <span class="built_in">document</span>.documentElement.clientHeight</span><br><span class="line">innerWidth = <span class="built_in">document</span>.documentElement.clientWidth</span><br></pre></td></tr></table></figure><h3 id="元素方法"><a href="#元素方法" class="headerlink" title="元素方法"></a>元素方法</h3><ol><li><p><code>getBoundingClientRect()</code> :</p><p><code>getBoundingClientRect()</code> ： 得到矩形元素的界线，返回的是一个对象，包含top、right、bottom、left四个属性值，大小都是相对于浏览器窗口top、left的距离。返回的内容类似于：<code>&#123;top: 143, right: 1196, bottom: 165, left: 889&#125;</code>；</p><hr></li><li><p><code>scrollIntoView()</code> :</p><p><code>obj.scrollIntoView()</code> 让元素滚动到可视区域（<code>HTML5标准</code>），参数true与浏览器对齐，false元素在窗口居中显示；</p><hr></li><li><p>event.clientX / event.clientY ： 相对于window，为鼠标相对于浏览器窗口的偏移量。</p><p><code>event.clientX</code>鼠标在文档的水平坐标；</p><p><code>even.clientY</code>鼠标在文档的垂直坐标；</p></li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><p><a href="https://github.com/XIANFESchool/FE-problem-collection/issues/31">js获取浏览器中各种宽、高、偏移值 · Issue #31 · XIANFESchool/FE-problem-collection (github.com)</a></p></li><li><p><a href="https://github.com/sophianbj/JavaScript-accumulation/blob/master/前端开发中的各种宽高整理.md">JavaScript-accumulation/前端开发中的各种宽高整理.md at master · sophianbj/JavaScript-accumulation (github.com)</a></p></li><li><p><a href="https://www.runoob.com/jsref/prop-element-clientleft.html">HTML DOM clientLeft 属性 | 菜鸟教程 (runoob.com)</a></p></li><li><p><a href="https://juejin.cn/post/7116306912198524959#heading-3">scrollTop、clientHeight、 scrollHeight…学完真的理解了 - 掘金 (juejin.cn)</a></p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;浏览器的一些常用js方法.主要是浏览的宽高什么的,我经常弄混.&lt;br&gt;</summary>
    
    
    
    
    <category term="js" scheme="https://www.sekyoro.top/tags/js/"/>
    
    <category term="dom" scheme="https://www.sekyoro.top/tags/dom/"/>
    
    <category term="bom" scheme="https://www.sekyoro.top/tags/bom/"/>
    
  </entry>
  
  <entry>
    <title>CSS_Modules介绍</title>
    <link href="https://www.sekyoro.top/2023/07/29/CSS-Modules%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.sekyoro.top/2023/07/29/CSS-Modules%E4%BB%8B%E7%BB%8D/</id>
    <published>2023-07-29T07:53:28.000Z</published>
    <updated>2023-07-30T04:15:06.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前在用Vue框架时,为了便于不同template之间的样式不相互影响,都设置了css的scoped属性.现在在学习React,发现并没有自带这种功能,通常可以使用多种方案解决.</p><span id="more"></span><p><img data-src="https://i.imgur.com/62Mf7KC.png" alt="image-20230721195018708"></p><p>一般在用CSS Modules或者CSS in Js的方法,不过感觉CSS Modules用的比较多,设置也并不复杂,这里写一下简单的教程.</p><blockquote><p>为了让 CSS 也能适用软件工程方法，程序员想了各种办法，让它变得像一门编程语言。从最早的Less、SASS，到后来的 PostCSS，再到最近的 CSS in JS，都是为了解决这个问题。</p></blockquote><p>在React中使用creat-react-app或者使用vite都直接用就行了,一般的脚手架都默认集成了CSS Modules.</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>首先创建css文件,要求命名为xxx.module.css</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* style.module.css */</span></span><br><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">color</span>: green;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当从JS模块导入CSS模块时，它会导出一个包含所有本地名称到全局名称的映射的对象。</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> styles <span class="keyword">from</span> <span class="string">&quot;./style.module.css&quot;</span>;</span><br><span class="line"><span class="comment">// import &#123; className &#125; from &quot;./style.css&quot;;</span></span><br><span class="line">element.innerHTML = <span class="string">&#x27;&lt;div class=&quot;&#x27;</span> + styles.className + <span class="string">&#x27;&quot;&gt;&#x27;</span>;</span><br></pre></td></tr></table></figure><p>在编译时会给类名前加上一个唯一值用以区分,事实上vue的scoped也是这个原理.</p><p><img data-src="https://i.imgur.com/nCJDs00.png" alt="image-20230721201226474"></p><p>使用<code>import styles from &quot;./xxxx.module.css&quot;;</code>导入后,事实上不管是否使用,都会将这个css文件里的规则用上,比如这个文件</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.nav</span> &#123;</span><br><span class="line">  <span class="attribute">display</span>: flex;</span><br><span class="line">  <span class="attribute">align-items</span>: center;</span><br><span class="line">  <span class="attribute">justify-content</span>: space-between;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">ul</span> &#123;</span><br><span class="line">  <span class="attribute">list-style</span>: none;</span><br><span class="line">  <span class="attribute">display</span>: flex;</span><br><span class="line">  <span class="attribute">align-items</span>: center;</span><br><span class="line">  gap: <span class="number">4rem</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:link</span>,</span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span><span class="selector-pseudo">:visited</span> &#123;</span><br><span class="line">  <span class="attribute">text-decoration</span>: none;</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">var</span>(--color-light--<span class="number">2</span>);</span><br><span class="line">  <span class="attribute">text-transform</span>: uppercase;</span><br><span class="line">  <span class="attribute">font-size</span>: <span class="number">1.5rem</span>;</span><br><span class="line">  <span class="attribute">font-weight</span>: <span class="number">600</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* CSS Modules feature */</span></span><br><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span>:<span class="built_in">global</span>(.active) &#123;</span><br><span class="line">  color: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">a</span><span class="selector-class">.ctaLink</span><span class="selector-pseudo">:link</span>,</span><br><span class="line"><span class="selector-tag">a</span><span class="selector-class">.ctaLink</span><span class="selector-pseudo">:visited</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">var</span>(--color-dark--<span class="number">0</span>);</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0.8rem</span> <span class="number">2rem</span>;</span><br><span class="line">  <span class="attribute">border-radius</span>: <span class="number">7px</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-tag">div</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>:red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里<code>div &#123;  background-color:red;&#125;</code>会直接用上,但是如果定义了类并使用的话,这个类就会生成唯一类名.</p><h3 id="命名方式"><a href="#命名方式" class="headerlink" title="命名方式"></a>命名方式</h3><p>对于本地类名，建议使用驼峰命名法，但不强制要求。</p><p>这是因为常见的替代方案，连字符命名法，在尝试访问style.class-name时可能会导致意外行为，因为它作为点符号表示法。您仍然可以使用方括号表示法（例如style[‘class-name’]）来解决连字符命名法，但是style.className更清晰。</p><p>简单来说命名需要驼峰.</p><h3 id="global全局使用css"><a href="#global全局使用css" class="headerlink" title=":global全局使用css"></a>:global全局使用css</h3><p>:global切换到当前选择器的全局作用域，对应的标识符。:global(.xxx) 对应的@keyframes :global(xxx)声明括号中的内容在全局作用域中。</p><p>例如,如果自定义了nav类,下面有类名为active的元素,这时不能使用.nav .active进行选择,因为会给active类名字前生成区分的id,又要在相关元素上加上. 为了避免麻烦,可以使用:global</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.nav</span> <span class="selector-tag">a</span>:<span class="built_in">global</span>(.active) &#123;</span><br><span class="line">  color: <span class="built_in">var</span>(--color-brand--<span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h3><p>使用<code>compose</code>组合其他选择器的规则</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: blue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.title</span> &#123;</span><br><span class="line">  composes: className;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果继承其他文件里的</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.className</span> &#123;</span><br><span class="line">  <span class="attribute">background-color</span>: blue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.title</span> &#123;</span><br><span class="line">  composes: className from <span class="string">&#x27;./another.css&#x27;</span>;</span><br><span class="line">  <span class="attribute">color</span>: red;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://www.ruanyifeng.com/blog/2016/06/css_modules.html">CSS Modules 用法教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://github.com/css-modules/css-modules">css-modules/css-modules: Documentation about css-modules (github.com)</a></li><li><a href="https://juejin.cn/post/7031528538209681444">react 中 css modules-基本使用 - 掘金 (juejin.cn)</a></li><li><a href="https://www.jianshu.com/p/694f9c14ab35">在 React 中使用 CSS Modules - 简书 (jianshu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前在用Vue框架时,为了便于不同template之间的样式不相互影响,都设置了css的scoped属性.现在在学习React,发现并没有自带这种功能,通常可以使用多种方案解决.&lt;/p&gt;</summary>
    
    
    
    
    <category term="css" scheme="https://www.sekyoro.top/tags/css/"/>
    
  </entry>
  
  <entry>
    <title>青龙面板部署项目</title>
    <link href="https://www.sekyoro.top/2023/07/09/%E9%9D%92%E9%BE%99%E9%9D%A2%E6%9D%BF%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/"/>
    <id>https://www.sekyoro.top/2023/07/09/%E9%9D%92%E9%BE%99%E9%9D%A2%E6%9D%BF%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE/</id>
    <published>2023-07-09T03:21:18.000Z</published>
    <updated>2023-07-09T04:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>青龙面板主要用于在上面放一些自动化脚本,可以跑一些类似签到的程序.之前我部署过一些,结果服务器出现了一些问题,现在使用docker重装一下并建立一个新的docker image.<br><span id="more"></span></p><p>首先官方仓库<a href="https://github.com/whyour/qinglong">whyour/qinglong: 支持 Python3、JavaScript、Shell、Typescript 的定时任务管理平台（Timed task management platform supporting Python3, JavaScript, Shell, Typescript） (github.com)</a>,推荐使用docker的方式,卸载安装都很方便.</p><p>启动一个容器后,在本地打开设置的端口,这里不推荐使用5700,因为会有机器人扫描端口暴力破解,我之前就遇到过.此外要在防火墙上打开对应的端口.</p><p><img data-src="https://s2.loli.net/2023/07/09/DWxV5MUmoeqikjN.png" alt="image-20230709112516871"></p><p>然后在本地打开对应端口的网页,选择推送方式,我一般九用PushPlus的一对一推送,官方文档<a href="https://www.pushplus.plus/push1.html">一对一消息|pushplus(推送加)-微信消息推送平台</a></p><p>然后设置账号和密码.这样搭建就完成了,这里推荐几个脚本,一般来说越新的越好.</p><h3 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h3><p>有许多库可以使用,我之前就用的faker.详情可以看看文尾参考链接</p><h3 id="阿里云盘签到"><a href="#阿里云盘签到" class="headerlink" title="阿里云盘签到"></a>阿里云盘签到</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ql repo https:<span class="regexp">//gi</span>thub.com<span class="regexp">/mrabit/</span>aliyundriveDailyCheck.git <span class="string">&quot;autoSignin&quot;</span> <span class="string">&quot;&quot;</span> <span class="string">&quot;qlApi&quot;</span></span><br></pre></td></tr></table></figure><p>阿里云盘的自动签到</p><h3 id="美团-饿了吗"><a href="#美团-饿了吗" class="headerlink" title="美团 饿了吗"></a>美团 饿了吗</h3><ul><li>美团</li><li>定时规则：10 8,11,18,21 <em> </em> *</li><li>脚本地址：<a href="https://raw.githubusercontent.com/leafTheFish/DeathNote/main/meituanV3.js">https://raw.githubusercontent.com/leafTheFish/DeathNote/main/meituanV3.js</a></li><li></li><li>饿了么</li><li>定时规则：10 8,11,18,21 <em> </em> *</li><li>脚本地址：<a href="https://raw.githubusercontent.com/leafTheFish/DeathNote/main/elmV3.js">https://raw.githubusercontent.com/leafTheFish/DeathNote/main/elmV3.js</a></li></ul><h3 id="B站"><a href="#B站" class="headerlink" title="B站"></a>B站</h3><p>我fork了一下源项目,把输出限制在5000字以配合PushPlus的免费推送额度</p><p><a href="https://github.com/drowning-in-codes/BiliBiliToolPro/blob/main/qinglong/README.md">BiliBiliToolPro/qinglong/README.md at main · drowning-in-codes/BiliBiliToolPro · GitHub</a></p><p>注意配置相关cookie用于登录</p><p><img data-src="https://s2.loli.net/2023/07/09/czmZdETvkt41i3b.png" alt="image-20230709121439385" style="zoom:67%;" /></p><h2 id="打包镜像"><a href="#打包镜像" class="headerlink" title="打包镜像"></a>打包镜像</h2><p>由于之前装了一堆脚本结果服务器出现了一点问题,这里打算打包镜像,镜像里就带着这些脚本以及一些cookie</p><p>使用<code>docker commit</code></p><p><img data-src="https://s2.loli.net/2023/07/09/iknuaYByJEV7X8z.png" alt="image-20230709122034155"></p><p>查询刚才的新镜像</p><p><img data-src="https://s2.loli.net/2023/07/09/vYIqbO6c2BPuo9Z.png" alt="image-20230709122209545"></p><p>可以使用<code>docker tag [IMAGE ID] [NEW NAME]</code>重命名,同时删除之前的镜像</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ol><li>[2023最新青龙面板京东脚本库（7月2日，持续更新中） - 便利空间 (conveniencespace.com)]</li><li>[<a href="https://www.cnblogs.com/anyview/p/17030881.html">青龙面板自动领京东京豆，做农场浇水、萌宠、种豆、签到等任务 - 视觉书虫 - 博客园 (cnblogs.com)</a>(<a href="https://conveniencespace.com/index.php/2022/05/03/2022最新青龙面板京东脚本库（持续更新中）/">https://conveniencespace.com/index.php/2022/05/03/2022最新青龙面板京东脚本库（持续更新中）/</a>)</li><li><a href="https://www.dujin.org/20920.html">使用青龙面板挂载「阿里云盘账号」自动签到领会员福利-缙哥哥 (dujin.org)</a></li><li><a href="https://blog.renzicu.com/2022/qinglong-meituanelm/index.html">码农大叔博客 - 技术交流 | 「薅羊毛」青龙面板 – 美团&amp;饿了么 (renzicu.com)</a></li><li><a href="https://www.runoob.com/w3cnote/docker-use-container-create-image.html">Docker 使用容器来创建镜像 | 菜鸟教程 (runoob.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;青龙面板主要用于在上面放一些自动化脚本,可以跑一些类似签到的程序.之前我部署过一些,结果服务器出现了一些问题,现在使用docker重装一下并建立一个新的docker image.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Astro+Vercel部署博客</title>
    <link href="https://www.sekyoro.top/2023/07/07/Astro-Vercel%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/"/>
    <id>https://www.sekyoro.top/2023/07/07/Astro-Vercel%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2/</id>
    <published>2023-07-07T07:21:56.000Z</published>
    <updated>2023-09-01T13:58:00.121Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>类似于hexo,这里使用Astro部署博客.创建一个Github项目部署在Vercel,主要目的是记录一些琐碎的学习记录,顺便学习一下很热的Astro.</p><span id="more"></span><h2 id="Astro介绍"><a href="#Astro介绍" class="headerlink" title="Astro介绍"></a>Astro介绍</h2><p>Astro 是<strong>集多功能于一体的 Web 框架</strong>，用于构建<strong>快速、以内容为中心</strong>的网站。</p><h3 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h3><p><a href="https://docs.astro.build/zh-cn/getting-started/#主要特性">标题部分 主要特性</a></p><ul><li><strong>组件群岛:</strong> 用于构建更快网站的新 web 架构。</li><li><strong>服务器优先的 API 设计:</strong> 从用户设备上去除高成本的 Hydration。</li><li><strong>默认零 JS:</strong> 没有 JavaScript 运行时开销来减慢你的速度。</li><li><strong>边缘就绪:</strong> 在任何地方部署，甚至像 Deno 或 Cloudflare 这样的全球边缘运行时。</li><li><strong>可定制:</strong> Tailwind, MDX 和 100 多个其他集成可供选择。</li><li><strong>不依赖特定 UI:</strong> 支持 React, Preact, Svelte, Vue, Solid, Lit 等等</li></ul><p>可以使用<a href="[CodeSandbox: Code, Review and Deploy in Record Time](https://codesandbox.io/">codesandbox</a>)与<a href="[StackBlitz | Instant Dev Environments | Click. Code. Done.](https://stackblitz.com/">stackblitz</a>)在线写一些代码.</p><p>可以在这个网站<a href="https://astro.new/latest/">Getting Started | astro.new</a>玩玩一些常用模板.</p><p>也有其他用户的例子<a href="https://astro.build/themes/">Themes | Astro</a>，关于主题的.</p><p>另外可以使用astro方便添加其他库,比如添加tailwind.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx astro add tailwind</span><br></pre></td></tr></table></figure><p><img data-src="https://s2.loli.net/2023/07/08/h6JqXOmDHubWRzy.png" alt="image-20230708152345258"></p><p>这里增加了tailwind以及react</p><h3 id="Astro的配置"><a href="#Astro的配置" class="headerlink" title="Astro的配置"></a>Astro的配置</h3><blockquote><p>在 <code>astro.config.mjs</code> 文件中自定义 Astro 的运行方式。它在 Astro 项目中十分常见，所有官方的示例模板和主题都默认附带。</p></blockquote><p>这里我使用了Blog模板,用于记录博客的.</p><blockquote><ul><li>Edit this page in <code>src/pages/index.astro</code></li><li>Edit the site header items in <code>src/components/Header.astro</code></li><li>Add your name to the footer in <code>src/components/Footer.astro</code></li><li>Check out the included blog posts in <code>src/pages/blog/</code></li><li>Customize the blog post page layout in <code>src/layouts/BlogPost.astro</code></li></ul></blockquote><h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h3><blockquote><p>Astro 为你的项目提供了一个有想法的文件夹布局。每个 Astro 项目的根目录下都应该包括以下目录和文件：</p><ul><li><code>src/*</code> - 你的项目源代码（组件、页面、样式等）。</li><li><code>public/*</code> - 你的非代码、未处理的资源（字体、图标等）。</li><li><code>package.json</code> - 项目列表。</li><li><code>astro.config.mjs</code> - Astro 配置文件（可选）。</li><li><code>tsconfig.json</code> - TypeScript 配置文件（可选）。</li></ul></blockquote><h4 id="src-components"><a href="#src-components" class="headerlink" title="src/components"></a><code>src/components</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srccomponents">标题部分 src/components</a></p><p><strong>组件</strong>是你在 HTML 页面中可重复使用的代码单元。它可以是 <a href="https://docs.astro.build/zh-cn/core-concepts/astro-components/">Astro 组件</a> 或是像 React 或 Vue 这样的<a href="https://docs.astro.build/zh-cn/core-concepts/framework-components/">前端组件</a>。通常将你项目中所有组件都分组放在这个文件夹中。</p><p>这在 Astro 项目中是个习惯，但不过你可以自由地根据喜好进行管理。</p><h4 id="src-layouts"><a href="#src-layouts" class="headerlink" title="src/layouts"></a><code>src/layouts</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srclayouts">标题部分 src/layouts</a></p><p><a href="https://docs.astro.build/zh-cn/core-concepts/layouts/">布局</a>是特殊的组件，它将一些内容包裹在一个更大的页面布局中。通常用在 <a href="https://docs.astro.build/zh-cn/core-concepts/astro-pages/">Astro 页面</a>和 <a href="https://docs.astro.build/zh-cn/guides/markdown-content/">Markdown 页面</a>中以定义页面的布局。</p><p>和 <code>src/components</code> 一样，这个目录也只是约定俗成。</p><h4 id="src-pages"><a href="#src-pages" class="headerlink" title="src/pages"></a><code>src/pages</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srcpages">标题部分 src/pages</a></p><p><a href="https://docs.astro.build/zh-cn/core-concepts/astro-pages/">页面</a>是一种用于创建新的页面的特殊组件。一个页面可以是一个 Astro 组件，也可以是一个 Markdown 文件，它代表你网站的一些内容页面。</p><h4 id="src-styles"><a href="#src-styles" class="headerlink" title="src/styles"></a><code>src/styles</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#srcstyles">标题部分 src/styles</a></p><p>在 <code>src/styles</code> 目录下存储你的 CSS 或 Sass 文件仍只是个习惯。只要你的样式在 <code>src/</code> 目录下的某个地方，并且正确导入，Astro 就能处理并压缩它们。</p><h4 id="public"><a href="#public" class="headerlink" title="public/"></a><code>public/</code></h4><p><a href="https://docs.astro.build/zh-cn/core-concepts/project-structure/#public">标题部分 public/</a></p><p><code>public/</code> 目录用于文件和资源，它不会在 Astro 构建过程中处理。这些文件将不加修改地被直接复制到构建文件夹。</p><p>这种行为使得 <code>public/</code> 成为存放图片和字体等普通资源或 <code>robots.txt</code> 和 <code>manifest.webmanifest</code> 等特殊文件的理想选择。</p><h3 id="撰写组件"><a href="#撰写组件" class="headerlink" title="撰写组件"></a>撰写组件</h3><blockquote><p>Astro 组件非常灵活的。通常情况下，Astro 组件会包含一些<strong>可在页面中复用的 UI</strong>，如 header 或简介卡。在其他时候，Astro 组件可能包含一个较小的 HTML 片段，像是常见的使 SEO 更好的 <code>&lt;meta&gt;</code> 标签集合。Astro 组件甚至可以包含整个页面布局。</p><p>Astro 组件中最重要的一点是，它们在构建过程中会被<strong>渲染成 HTML</strong>。即使你在组件内运行 JavaScript 代码，它也会抢先一步运行从呈现给用户的最终页面中剥离出来。其最终使得网站变得更快，且默认不用任何 JavaScript。</p></blockquote><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://docs.astro.build/zh-cn/getting-started/">入门指南 🚀 Astro 文档</a></li><li><a href="https://2022.stateofjs.com/en-US/">State of JavaScript 2022 (stateofjs.com)</a></li><li><a href="https://astro.new/latest/">Getting Started | astro.new</a></li><li><a href="https://www.youtube.com/watch?v=NniT0vKyn-E&amp;ab_channel=developedbyed">(1) Astro Crash Course in 60 Minutes - YouTube</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;类似于hexo,这里使用Astro部署博客.创建一个Github项目部署在Vercel,主要目的是记录一些琐碎的学习记录,顺便学习一下很热的Astro.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Astro" scheme="https://www.sekyoro.top/tags/Astro/"/>
    
    <category term="Vercel" scheme="https://www.sekyoro.top/tags/Vercel/"/>
    
    <category term="Github" scheme="https://www.sekyoro.top/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>AI写作工作流</title>
    <link href="https://www.sekyoro.top/2023/06/12/AI%E5%86%99%E4%BD%9C%E5%B7%A5%E4%BD%9C%E6%B5%81/"/>
    <id>https://www.sekyoro.top/2023/06/12/AI%E5%86%99%E4%BD%9C%E5%B7%A5%E4%BD%9C%E6%B5%81/</id>
    <published>2023-06-12T11:17:40.000Z</published>
    <updated>2023-06-12T13:33:46.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>使用ChatGPT可以帮助写作,但其本身是个对话AI,有专门帮助写作的AI.这里做一个介绍同时尝试整合到日常工作流中.<br><span id="more"></span></p><p>之前想在一些自媒体号上发布一些好用的ChatGPT以及微软的Bing替代品,没想过还通过不了审核,甚至有些平台ChatGPT都不好提.于是打算在这里写写,也可以关注我的微信公众号或者个人网站。</p><p>这里推荐的AI写作工具有几个要求:</p><ol><li>有免费计划或者额度</li><li>最好提供API便于开发</li><li>专注于写作,不需要一句一句去询问.如果能生成一些图片辅助写作就更好了.</li><li>多语言支持</li></ol><p>我将体验下面的每个应用并给出个人见解。</p><h3 id="Chatsonic-writesonic"><a href="#Chatsonic-writesonic" class="headerlink" title="Chatsonic(writesonic)"></a>Chatsonic(writesonic)</h3><p><a href="https://writesonic.com/?via=devinder14">Writesonic - Best AI Writer, Copywriting &amp; Paraphrasing Tool</a></p><p>可以说是比较推荐的工具了,有免费方案,功能很多,还有浏览器扩展以及app.</p><p><img data-src="https://i.imgur.com/NC0RjRO.png" alt="image-20230612201548133"></p><p>如果你没有精力去搞其他工具,那光使用这个就行了,可以使用浏览器扩展或者手机App.</p><p>具体来说有以下优点</p><p>1.来源于事实和实时信息</p><p>如果您正在寻找一种有效的方式来生成具有最新新闻、事实和数字的内容，Chatsonic是最好的ChatGPT替代品。为了提供实时更新，Chatsonic的人工智能聊天机器人与谷歌搜索集成。</p><p>通过使用自然语言处理（NLP）和机器学习，Chatsonic对有关当前新闻和趋势的问题提供准确的答案，并提供详细的总结和见解。</p><p>2.理解语音命令</p><p>由于NLP—ChatSonic可以理解语音命令。</p><p>你只需要发出一个语音命令，它就会生成你所需要的东西。此外，它还可以响应像Siri或谷歌助手的语音命令，最终可以帮助您节省更多的时间，使整体体验更加愉快。</p><p>3.充当你的个人助理</p><p>把Chatsonic想象成你的虚拟助理，他在你的指尖上给你提供你需要的信息。</p><p>无论您是需要数学方面的帮助，还是需要准备工作面试，或是需要有关屋檐下任何东西的信息，Chatsonic都能为您的内容创作带来革命性的变化。</p><p>在我们都以提高生产力为目标的时候，Chatsonic帮助用户更快地写作，并使这个过程变得愉快。</p><p>4.将想法转化为独特的数字艺术</p><p>目前，所有像Chat GPT这样的网站都有一个共同的局限性：它们不能生成支持内容的令人惊叹的AI艺术。</p><p>这就是Chatsonic作为一个差异化因素出现的地方。您可以快速轻松地创建令人惊叹的数字AI艺术，因为ChatsSonic已经整合了稳定扩散和DALL-E，只需点击一下就可以创建自定义AI图像。</p><p>5.Chatsonic的API集成到现有的应用程序中</p><p>通过将这个ChatGPT的替代品整合到您现有的应用程序中，释放Chatsonic的全部潜力。这样，您就可以在不切换标签的情况下获得最新信息。</p><p>Chatsonic的API可以与您的网站和移动应用程序整合，建立强大的客户服务和沟通能力。</p><p>想了解如何在自己的系统中使用ChatGPT API，请阅读30多个使用案例 - <a href="https://writesonic.com/blog/chatgpt-api-use-cases/">ChatGPT API use cases: 35 ways to implement ChatSonic API (writesonic.com)</a>。</p><p>这里的API跟ChatGPT类似,有空我也打算写一个套壳对话机器人。</p><h3 id="YouChat"><a href="#YouChat" class="headerlink" title="YouChat"></a>YouChat</h3><p><a href="https://you.com/">The AI Search Engine You Control | AI Chat &amp; Apps</a></p><p>YouChat是一个很好的ChatGPT替代品，用于研究，同时给人一种日常体验。该工具是由You[点]com，一个搜索引擎推出的。它的工作原理与其他一般的聊天工具基本相同。该工具使用自然语言处理和人工智能，像人一样自然地进行交谈。</p><p>YouChat可以写代码和电子邮件，翻译，总结文本，并回答一般的询问。YouChat在回答基本级别的查询方面非常出色，但由于它仍在学习中，可能会错过一些特定的小众内容。该工具提供的所有信息并不总是正确的。</p><p>此外也有YouWrite以及YouImage功能,也是有免费的。</p><h3 id="copy-ai"><a href="#copy-ai" class="headerlink" title="copy.ai"></a>copy.ai</h3><p><a href="https://app.copy.ai/projects">Copy.ai</a> 免费的支持中文,网站介绍说用于写博客以及写邮件.</p><p><img data-src="https://i.imgur.com/7Xufnib.png" alt="image-20230612205040139"></p><p>不得不说在网上一搜有太多相关内容了,但实际使用之后才知道哪些好用哪些真的不便于使用。</p><p>比如有浏览器扩展的肯定是非常好的优点,不像其他的还需要打开相关网站.所以我这里是强烈推荐Chatsonic的。</p><h3 id="Rytr"><a href="#Rytr" class="headerlink" title="Rytr"></a>Rytr</h3><p><a href="https://app.rytr.me/create">Rytr · Best AI Writer, Content Generator &amp; Writing Assistant</a></p><p>Rytr是一个人工智能写作助手，可以帮助你创造高质量的内容，只需几秒钟的时间，而成本却很低!</p><p>免费版本每个月只能生成5000个字符</p><p><img data-src="https://i.imgur.com/zIKQzAS.png" alt="image-20230612213317657" style="zoom: 33%;" /></p><p>下面详细介绍一下Chatsonic.</p><p>以下是浏览器插件</p><p><img data-src="https://i.imgur.com/c4C1znZ.png" alt="image-20230612210508225" style="zoom: 50%;" /></p><p>如果用相同的问题询问基于GPT3.5的ChatGPT会得到非常糟糕的回答,但这个的回答就好很多.虽然也有一些问题.</p><p><img data-src="https://i.imgur.com/XwAjLUN.png" alt="image-20230612210640326" style="zoom:50%;" /></p><p>免费额度大概25000字,有以下具体功能</p><p><img data-src="https://i.imgur.com/oob3R76.png" alt=" "></p><p>比如我使用其中的Website Copy功能写一个网站的登录页.填写以下信息,我随便填的</p><p><img data-src="https://i.imgur.com/bS6K9Ju.png" alt="image-20230612212038829" style="zoom:50%;" /></p><p>得到结果</p><p><img data-src="https://i.imgur.com/EbQmKoZ.png" alt=" " style="zoom:50%;" /></p><p>效果还是很不错的.</p><p>回到主题写作功能,你可以首先给出主题搜索相关文章或者自己给它文章链接或者文章的文件,然后生成文章并自己修改.</p><p><img data-src="https://i.imgur.com/amfyrWe.png" alt="image-20230612212217201" style="zoom:33%;" /></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.wbolt.com/best-chatgpt-alternatives.html#h-6-colossal-chat">25个最佳ChatGPT替代品 - 闪电博 (wbolt.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/625827740">Writesonic： 2023年最好用的的AI 写作工具 ｜使用 GPT 4 - 知乎 (zhihu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用ChatGPT可以帮助写作,但其本身是个对话AI,有专门帮助写作的AI.这里做一个介绍同时尝试整合到日常工作流中.&lt;br&gt;</summary>
    
    
    
    
    <category term="AI" scheme="https://www.sekyoro.top/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>node的包管理工具介绍</title>
    <link href="https://www.sekyoro.top/2023/06/10/node%E7%9A%84%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D/"/>
    <id>https://www.sekyoro.top/2023/06/10/node%E7%9A%84%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D/</id>
    <published>2023-06-10T07:13:46.000Z</published>
    <updated>2023-06-10T13:39:46.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>不同的语言都有相应的不同包或者环境管理工具,本文介绍一下node的<br><span id="more"></span></p><p>在前端开发中,使用node作为运行时环境时,常使用npm与yarn等作为包管理工具.</p><h2 id="npm与yarn存在的问题"><a href="#npm与yarn存在的问题" class="headerlink" title="npm与yarn存在的问题"></a>npm与yarn存在的问题</h2><h3 id="NPM"><a href="#NPM" class="headerlink" title="NPM"></a>NPM</h3><p>对于npm,其核心是有一个package.json和package-lock.json文件用于记录和追踪包版本和依赖。</p><p>之前版本的npm的node_modules目录结构是</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">node_modules </span><br><span class="line">└─ 依赖A </span><br><span class="line">   ├─ index.<span class="keyword">js </span></span><br><span class="line">   ├─ package.<span class="keyword">json </span></span><br><span class="line">   └─ node_modules </span><br><span class="line">       └─ 依赖<span class="keyword">B </span></span><br><span class="line">       ├─ index.<span class="keyword">js </span></span><br><span class="line">       └─ package.<span class="keyword">json</span></span><br><span class="line"><span class="keyword"></span> └─ 依赖C </span><br><span class="line">   ├─ index.<span class="keyword">js </span></span><br><span class="line">   ├─ package.<span class="keyword">json </span></span><br><span class="line">   └─ node_modules </span><br><span class="line">       └─ 依赖<span class="keyword">B </span></span><br><span class="line">       ├─ index.<span class="keyword">js </span></span><br><span class="line">       └─ package.<span class="keyword">json</span></span><br></pre></td></tr></table></figure><p>但很明显由于这种会重复安装依赖,还无法共享依赖,现在的版本是这样</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">node_modules </span><br><span class="line">└─ 依赖A  </span><br><span class="line">    ├─ index.<span class="keyword">js </span></span><br><span class="line">    ├─ package.<span class="keyword">json </span></span><br><span class="line">    └─ node_modules </span><br><span class="line">└─ 依赖C   </span><br><span class="line">    ├─ index.<span class="keyword">js </span></span><br><span class="line">    ├─ package.<span class="keyword">json </span></span><br><span class="line">    └─ node_modules </span><br><span class="line">└─ 依赖<span class="keyword">B </span></span><br><span class="line">    ├─ index.<span class="keyword">js </span></span><br><span class="line">    ├─ package.<span class="keyword">json </span></span><br><span class="line">    └─ node_modules </span><br></pre></td></tr></table></figure><blockquote><p><code>node_modules</code>下所有的依赖都会平铺到同一层级。由于require寻找包的机制，如果A和C都依赖了B，那么A和C在自己的node_modules中未找到依赖B的时候会向上寻找，并最终在与他们同级的node_modules中找到依赖包B。 这样<strong>就不会出现重复下载的情况。而且依赖层级嵌套也不会太深。因为没有重复的下载，所有的A和C都会寻找并依赖于同一个B包。自然也就解决了实例无法共享数据的问题</strong></p></blockquote><p>这种扁平化结构虽然是解决了之前的嵌套问题，但同时也带来了另外一些问题：</p><ul><li>依赖结构的不确定性</li><li>扁平化算法的复杂度增加</li><li>项目中仍然可以非法访问没有声明过的依赖包(幽灵依赖)</li></ul><p>yarn的输出格式提示以及下载速度比npm更快.</p><h3 id="PNPM"><a href="#PNPM" class="headerlink" title="PNPM"></a>PNPM</h3><p>  主要是采用硬链接和软链接的方式，提高了安装速度、节约了磁盘空间、避免了“依赖分身（doppelgangers）”和“幻影依赖（Phantom dependencies）”的问题。而且 yarn 支持的：安全、离线模式、更快的速度，pnpm 都支持，而且速度还要更快。</p><p>现在一般用的还是npm或者yarn.</p><p>最后再介绍一些打包工具.</p><p>常用的打包工具有Parcel,Rollup与Webpack,现在也常常使用比较火的Vite,其功能也不仅限于打包.</p><p><strong>对于应用使用 webpack或者Vite，对于类库使用 Rollup</strong></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://github.com/pnpm/pnpm">pnpm/pnpm: Fast, disk space efficient package manager (github.com)</a></li><li><a href="https://nodejs.org/en">Node.js (nodejs.org)</a></li><li><a href="https://www.ruanyifeng.com/blog/2020/08/how-nodejs-use-es6-module.html">Node.js 如何处理 ES6 模块 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://www.yarnpkg.cn/">Home | Yarn - JavaScript 软件包管理器 | Yarn中文文档 - Yarn中文网 (yarnpkg.cn)</a></li><li><a href="https://www.yarnpkg.cn/">Home | Yarn - JavaScript 软件包管理器 | Yarn中文文档 - Yarn中文网 (yarnpkg.cn)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;不同的语言都有相应的不同包或者环境管理工具,本文介绍一下node的&lt;br&gt;</summary>
    
    
    
    
    <category term="node" scheme="https://www.sekyoro.top/tags/node/"/>
    
    <category term="npm" scheme="https://www.sekyoro.top/tags/npm/"/>
    
    <category term="yarn" scheme="https://www.sekyoro.top/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>深入Clash配置文件</title>
    <link href="https://www.sekyoro.top/2023/06/03/%E6%B7%B1%E5%85%A5Clash%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"/>
    <id>https://www.sekyoro.top/2023/06/03/%E6%B7%B1%E5%85%A5Clash%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/</id>
    <published>2023-06-03T13:16:28.000Z</published>
    <updated>2023-06-03T14:35:26.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近用代理都是用的Clash,自己也搭了一个代理服务器,顺便看看Clash的配置文件.<br><span id="more"></span></p><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>有一些vmess,ssr,ss转clash配置文件的网站,比如下面的:</p><p><a href="https://sub-web.wcc.best/">Subscription Converter (wcc.best)</a>,<a href="https://a.ppconverter.eu.org/">Proxy Provider Converter (ppconverter.eu.org)</a></p><p><a href="https://acl4ssr-sub.github.io/">ACL4SSR 在线订阅转换 (acl4ssr-sub.github.io)</a></p><p><a href="https://sub.v1.mk/">在线订阅转换工具 (v1.mk)</a></p><p>不过有一些略显冗余而且有自己的网站名,所以打算看看配置文件格式自己改改</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>可以看看一般的配置文件是啥样的,如果你有自己的clash配置文件就可以看看,为了避免代理被使用,我这里就简单说说.</p><p><img data-src="https://s2.loli.net/2023/06/03/HwIUPr4Kq3kjvWx.png" alt="image-20230603214331664"></p><p>port用于配置HTTP(S)代理,此外也有socks-port用于匹配之基于socks的代理</p><p>可以使用mixed-port指定HTTP(S)和SOCKS4(A)/SOCKS5在同一端口,</p><p>allow-lan: true表示局域网内可使用,这个一般适合放在路由器上的服务器.</p><p>mode指定clash路由工作模式,有三种模式包括rule,global,direct</p><blockquote><p>rule: rule-based packet routing</p><p>global: all packets will be forwarded to a single endpoint</p><p>direct: directly forward the packets to the Internet</p></blockquote><p>external-controller指定clash的restfulapi调用监听地址,如果你想自己写请求可以设置这个参数.</p><h3 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h3><p>剩下最重要的三个参数proxies,proxy grops以及Proxy Providers,其实还有rules.</p><p>对于proxies就是用于配置自己的代理的,包括以下几种协议</p><p><img data-src="https://s2.loli.net/2023/06/03/mHEhSLt9gqKAnRO.png" alt="image-20230603220002530"></p><p>常用v2ray的vmess以及ssr.</p><p>对于vmess,配置格式如下.至于如何获得vmess链接,可以参考我之前的文章自己搭建,当然付钱买其他人的也行.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">proxies:</span></span><br><span class="line">  <span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">proxy_name</span>, <span class="attr">server:</span> <span class="string">proxy_ip</span>, <span class="attr">port:</span> <span class="string">proxy_port</span>, <span class="attr">type:</span> <span class="string">vmess</span>, <span class="attr">uuid:</span> <span class="string">proxy_uuid</span>, <span class="attr">alterId:</span> <span class="number">0</span>, <span class="attr">cipher:</span> <span class="string">auto</span>, <span class="attr">tls:</span> <span class="literal">false</span>&#125;</span><br></pre></td></tr></table></figure><p>这里相当于定义了可用的代理,或者如下格式设置</p><p><img data-src="https://s2.loli.net/2023/06/03/lZYbvEq3WmygtIr.png" alt="image-20230603221051028"></p><p>其中,name可以用于后面proxy-group的引用,type就是协议类型,其他的与具体配置有关</p><p>proxy-group设置了clash的显示,相当于一个分组</p><p><img data-src="https://s2.loli.net/2023/06/03/9h85ulEtzCJSIjf.png" alt="image-20230603220831027" style="zoom:50%;" /></p><p>最上面的DIRECT和REJECT是默认的</p><ul><li>Direct：不走代理</li><li>Reject：禁止访问</li></ul><p>此外可以嵌套使用,也就是proxy-group中的proxies也可以是proxy-group中的项目,而type设置为select是用于选择proxy.</p><p><img data-src="https://s2.loli.net/2023/06/03/neiomlIY6NMH2CE.png" alt="image-20230603222406981" style="zoom:50%;" /></p><p>然后对于一些网站我们想直连,对于另一些使用不同的代理,这样就可以使用rules.</p><blockquote><p>一个代理规则主要由三部分组成：</p><ol><li>应用对象，包括完整域名(DOMAIN)、域名后缀(DOMAIN-SUFFIX)、域名关键字(DOMAIN-KEYWORD)、IP地址/段(IP-CIDR)以及GEOIP；</li><li>作用的IP或者域名；</li><li>采取的规则，包括直连(DIRECT)、屏蔽(REJECT)，走某个代理组(就是刚才定义的proxy-groups)</li></ol></blockquote><p><img data-src="https://s2.loli.net/2023/06/03/XMteCPELH5D3O94.png" alt="image-20230603222609199" style="zoom:50%;" /></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://fndroid.github.io/clash-win-docs/contents/ui/profiles.html">配置 Profiles · Clash for Windows (fndroid.github.io)</a></li><li><a href="https://vpsgongyi.com/p/2396/">深入理解Clash配置文件 - VPS攻略 (vpsgongyi.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近用代理都是用的Clash,自己也搭了一个代理服务器,顺便看看Clash的配置文件.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>技术教程:如何自己搭建一个VPN</title>
    <link href="https://www.sekyoro.top/2023/06/03/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B-%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAVPN/"/>
    <id>https://www.sekyoro.top/2023/06/03/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B-%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAVPN/</id>
    <published>2023-06-03T06:52:16.000Z</published>
    <updated>2023-08-01T06:14:42.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>仅供技术学习与交流<br><span id="more"></span></p><h2 id="VPN简单介绍"><a href="#VPN简单介绍" class="headerlink" title="VPN简单介绍"></a>VPN简单介绍</h2><p>首先需要明确的是,VPN本是一种技术,英文是Virtual Private Network,顾名思义是虚拟化的私人网络.简单来说就是基于代理IP然后自己在上面加一些功能.像是自己的手机或者PC其实都有连接VPN的功能.此外也有一些连接VPN的客户端软件.</p><p><img data-src="https://s2.loli.net/2023/06/03/W7lEcMw8eZK9IDo.png" alt="image-20230603150053505" style="zoom:50%;" /></p><h3 id="VPN好处"><a href="#VPN好处" class="headerlink" title="VPN好处"></a>VPN好处</h3><blockquote><ul><li><strong>加密 IP 地址：</strong>VPN 的主要工作就是对 ISP 和其他第三方隐藏您的 IP 地址。这样当您在线发送和接收信息时，就不用担心会有被除了您和 VPN 提供商以外的任何人看到的风险。</li><li><strong>加密协议：</strong>VPN 还应该防止您留下痕迹，例如，互联网历史记录、搜索历史记录和 cookie。加密 cookie 尤其重要，因为能阻止第三方访问保密信息，如个人数据、财务信息和网站上的其他内容。</li><li><strong>自杀开关：</strong>如果您的 VPN 连接突然中断，您的安全连接也会中断。好的 VPN 能检测到这种突然停机并终止预先选择的程序，从而降低数据被泄露的可能性。</li><li><strong>双因素身份验证：</strong>通过使用各种身份验证方法，强大的 VPN 能够检查试图登录的每个人。例如：您可能会被提醒输入密码，然后会将代码发送到您的手机。这使得未经邀请的第三方难以访问您的安全连接。</li></ul></blockquote><p>本篇文章主要目的是动手自己搭建一个访问外网的VPN,为此需要一个搭建在大陆地区以外的的服务器,因此需要一个性价比高的国外云服务器商.</p><p>这里就主要使用<a href="https://www.vultr.com/register/">Vultr</a>云服务商,事实上国内的也行,不过为了安全考虑还是算了.此外Vultr是支持支付宝支付的,不需要再去办一张VISA银行卡了,这就很满足需求了.</p><p>主要方法是使用性价比高的服务商的服务器再搭建VPN服务端软件进行配置.</p><p>一些VPS服务商推荐<a href="https://vpsxueyuan.com/2022-best-vps/">2023年最值得购买的VPS推荐</a></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="注册Vultr"><a href="#注册Vultr" class="headerlink" title="注册Vultr"></a>注册Vultr</h3><p>可以点击我的邀请进行注册<a href="https://www.vultr.com/?ref=9473485">注册地址</a>,此外也有<a href="https://www.vultr.com/?ref=9473486-8H">优惠链接</a></p><p>其他可能已经失效的活动<a href="https://www.vultrcn.com/11.html">Vultr优惠码整理</a>,包括充多少送多少(最多100美金),另外还有送美金余额的,总之还是不错的.</p><p><img data-src="https://s2.loli.net/2023/06/03/8tl2yLZskjWGTcY.png" alt="image-20230603160407745" style="zoom:50%;" /></p><h3 id="充值"><a href="#充值" class="headerlink" title="充值"></a>充值</h3><p>这里使用支付宝支付,此外可以使用一些兑换码获取一些福利,下面细说.</p><p><img data-src="https://s2.loli.net/2023/06/03/tD7BMopWwsmnvI6.png" alt="image-20230603160605588" style="zoom:50%;" /></p><h3 id="购买服务器"><a href="#购买服务器" class="headerlink" title="购买服务器"></a>购买服务器</h3><p><img data-src="https://s2.loli.net/2023/06/03/Uj9iGZ4Wo6emthV.png" alt="image-20230603151410859" style="zoom:50%;" /></p><p>选择Regular Performance,</p><p><img data-src="https://s2.loli.net/2023/06/03/7ckwCSKsh9PLvTo.png" alt="image-20230603155931403"></p><p>最低只要一个月2.5刀,不过貌似这个只支持IPV6,所以只能再加一刀支持IPV4</p><p><img data-src="https://s2.loli.net/2023/06/03/7jpMzXtLNAgZIhU.png" alt="image-20230603155808005"></p><p>最后加上0.7刀备份,所以一个月就是4.2刀.还是比较便宜的。目前大致情况总结如下</p><div class="table-container"><table><thead><tr><th style="text-align:center">CPU</th><th style="text-align:center">内存</th><th style="text-align:center">硬盘</th><th style="text-align:center">带宽</th><th style="text-align:center">IP</th><th style="text-align:center">价格</th></tr></thead><tbody><tr><td style="text-align:center">1 核</td><td style="text-align:center">512MB</td><td style="text-align:center">10GB</td><td style="text-align:center">0.5TB</td><td style="text-align:center">IPv6</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$2.5/月</a></td></tr><tr><td style="text-align:center">1 核</td><td style="text-align:center">512MB</td><td style="text-align:center">10GB</td><td style="text-align:center">0.5TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$3.5/月</a></td></tr><tr><td style="text-align:center">1 核</td><td style="text-align:center">1GB</td><td style="text-align:center">25GB</td><td style="text-align:center">1TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$5/月</a></td></tr><tr><td style="text-align:center">1 核</td><td style="text-align:center">2GB</td><td style="text-align:center">55GB</td><td style="text-align:center">2TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$10/月</a></td></tr><tr><td style="text-align:center">2 核</td><td style="text-align:center">4GB</td><td style="text-align:center">80GB</td><td style="text-align:center">3TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$20/月</a></td></tr><tr><td style="text-align:center">4 核</td><td style="text-align:center">8GB</td><td style="text-align:center">160GB</td><td style="text-align:center">4TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$40/月</a></td></tr><tr><td style="text-align:center">6 核</td><td style="text-align:center">16GB</td><td style="text-align:center">320GB</td><td style="text-align:center">5TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$80/月</a></td></tr><tr><td style="text-align:center">8 核</td><td style="text-align:center">32GB</td><td style="text-align:center">640GB</td><td style="text-align:center">6TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$160/月</a></td></tr><tr><td style="text-align:center">16 核</td><td style="text-align:center">64GB</td><td style="text-align:center">1280GB</td><td style="text-align:center">10TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$320/月</a></td></tr><tr><td style="text-align:center">24 核</td><td style="text-align:center">96GB</td><td style="text-align:center">1600GB</td><td style="text-align:center">15TB</td><td style="text-align:center">IPv4</td><td style="text-align:center"><a href="https://guowaivps.org/go/vultr">$640/月</a></td></tr></tbody></table></div><p>最后购买的服务器如下</p><p><img data-src="https://s2.loli.net/2023/06/03/xnDOYvcGfg7TRb1.png" alt="image-20230603160314905"></p><p>点击实例,进入后查看ip,用户名和密码,这样方便使用终端连接</p><p><img data-src="https://s2.loli.net/2023/06/03/T62l48bHnrwBJjM.png" alt="image-20230603161001092"></p><p>此外测试一下能不能连接,本地ping一下,ping不通可以删掉,删掉后不计费.</p><p><img data-src="https://s2.loli.net/2023/06/03/l28InbCFUPd7yrS.png" alt="image-20230603161143254" style="zoom:50%;" /></p><h3 id="安装服务端VPN"><a href="#安装服务端VPN" class="headerlink" title="安装服务端VPN"></a>安装服务端VPN</h3><p>主要有shadowsocks和V2ray,终端可以使用Xshell或者PuTTY.</p><h4 id="V2Ray"><a href="#V2Ray" class="headerlink" title="V2Ray"></a>V2Ray</h4><p>这里使用V2ray,如果你想使用其他的可以看看参考链接.利用的仓库是<a href="https://github.com/xiaoming2028/FreePAC/wiki/美国VPS-Hostwinds一键脚本搭建V2Ray最新中文教程">一键脚本搭建V2Ray最新中文教程</a></p><blockquote><h4 id="V2Ray的优势"><a href="#V2Ray的优势" class="headerlink" title="V2Ray的优势"></a><strong>V2Ray的优势</strong></h4><blockquote><ul><li><strong>更完善的协议:</strong> V2Ray 使用了新的自行研发的 VMess 协议，改正了 Shadowsocks 一些已有的缺点，更难被墙检测到（不保证可靠性）</li><li><strong>更强大的性能:</strong> 网络性能更好，具体数据可以看 V2Ray 官方博客</li><li><strong>更丰富的功能:</strong></li></ul></blockquote></blockquote><p>使用命令</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash &lt;(curl -s -L https://git.io/v2ray.sh)</span><br></pre></td></tr></table></figure><p>如果出现INTERNAL ERROR 500,可以再试试.如果出现其他错误,自己再改改.</p><p>安装好后出现如下图</p><p><img data-src="https://s2.loli.net/2023/06/03/AfylcpMsmGJRgV9.png" alt="image-20230603164722550" style="zoom:50%;" /></p><p>命令V2ray进行进一步配置,修改协议,端口等</p><p><img data-src="https://s2.loli.net/2023/06/03/CEuhbjH3zeXWNYd.png" alt="image-20230603164801527" style="zoom:50%;" /></p><p>使用v2ray ip查看自己的ip</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=5200/tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><p>防火墙开启端口,这是很有必要的.</p><p>最后可以使用这个网站<a href="https://ping.sx/check-port">Check Port | Ping.Sx</a>检测一下,如果是绿色的reachable就可以了.</p><p><img data-src="https://s2.loli.net/2023/06/03/9diQ3kxVCUaZG5j.png" alt="image-20230603170821614"></p><p>其他命令添加配置：</p><ul><li><code>v2ray add</code> -&gt; 添加配置</li><li><code>v2ray add ss</code> -&gt; 添加一个 Shadowsocks 配置</li><li><code>v2ray add tcp</code> -&gt; 添加一个 VMess-TCP 配置</li><li><code>v2ray add kcpd</code> -&gt; 添加一个 VMess-mKCP-dynamic-port 动态端口配置</li></ul><p><img data-src="https://s2.loli.net/2023/06/03/r5E2pmuKCgSx7RF.png" alt="image-20230603170717004"></p><p>优化v2ray</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v2ray bbr</span><br></pre></td></tr></table></figure><h4 id="Shadowsocks"><a href="#Shadowsocks" class="headerlink" title="Shadowsocks"></a>Shadowsocks</h4><p>搭建Shadowsocks教程如下,基本方法差不多</p><p><a href="https://github.com/xiaoming2028/FreePAC/wiki/VPS一键脚本搭建SSR教程，Youtube看4k视频无压力">VPS一键脚本搭建SSR教程</a></p><p><a href="https://github.com/clown-coding/vpn">clown-coding/vpn: 快速搭建一个自己的VPN翻墙科学上网 (github.com)</a></p><p><a href="https://github.com/sucong426/VPN">快速搭建个人VPN</a></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget –no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x shadowsocks.sh</span><br><span class="line">./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log</span><br></pre></td></tr></table></figure><p>然后根据情况自己配置即可</p><p><img data-src="https://s2.loli.net/2023/06/03/4AVn5yZ3Q6EU8fj.png" alt="image-20230603180917790" style="zoom: 80%;" /></p><p>安装完成后就有相关信息.</p><p>还有另一个脚本</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh</span><br><span class="line">chmod +x shadowsocks-all.sh</span><br><span class="line">./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log</span><br></pre></td></tr></table></figure><p><del>配置更多,但我使用这个有时下载不了</del>后面发现由于我是用Centos 8.x版本的操作系统,EPEL软件包有点问题.参考这篇文章解决<a href="https://zhuanlan.zhihu.com/p/90442949">如何在 CentOS 8 和 RHEL 8 服务器上启用 EPEL 仓库 - 知乎 (zhihu.com)</a> ,最后还是不行的话建议换OS,用Debian,Debian开启某个端口命令<a href="https://www.jianshu.com/p/94e95f0b2a8a">Debian 10 iptables防火墙常用命令 - 简书 (jianshu.com)</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -p tcp --dport 8080 -j ACCEPT</span><br></pre></td></tr></table></figure><p><strong>最后记得重启一下</strong></p><p>可以使用<a href="https://bianyuan.xyz/">边缘@订阅转换API (bianyuan.xyz)</a>转换为clash订阅地址,因为我本身用clash客户端比较多</p><p><img data-src="https://s2.loli.net/2023/06/03/85rJdCYwav49Myg.png" alt="image-20230603173315958"></p><h3 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h3><p>客户端主要有V2ray,ShadowSocks以及现在的Clash,导入服务端对应的链接即可.</p><p><a href="https://www.howru.cc/articles/414.html">Shadowsocks Windows客户端快速使用指南 - 小小的宇宙 (howru.cc)</a></p><p><img data-src="https://s2.loli.net/2023/06/03/WLHpD6Bc5SYVi4P.png" alt="image-20230603172709508" style="zoom:50%;" /></p><p>这里不做过多介绍,可以看看参考资料.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.kaspersky.com.cn/resource-center/definitions/what-is-a-vpn">什么是 VPN，它是怎样运作的？ (kaspersky.com.cn)</a></li><li><a href="https://github.com/yukaiji/buildVpn">yukaiji/buildVpn: 图文教程搭建一个vpn翻墙 (github.com)</a></li><li><a href="https://github.com/sucong426/freevpn">sucong426/freevpn: 新手搭建VPN科学上网教程/自由/v2ray/翻墙梯子/快速最新/免费机场 (github.com)</a></li><li><a href="https://github.com/sucong426/VPN">sucong426/VPN: 快速搭建个人VPN/科学上网/翻墙/教程/ssr/ss/bbr/梯子搭建/自建机场/自由上网/代理服务/VPN/2023最新教程 (github.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/607513892">2023年Vultr服务器是否还值得用 - 知乎 (zhihu.com)</a></li><li><a href="https://github.com/xiaoming2028/FreePAC/wiki">Home · xiaoming2028/FreePAC Wiki (github.com)</a></li><li><a href="https://teddysun.com/486.html">Shadowsocks 一键安装脚本（四合一） | 秋水逸冰 (teddysun.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;仅供技术学习与交流&lt;br&gt;</summary>
    
    
    
    
    <category term="VPN" scheme="https://www.sekyoro.top/tags/VPN/"/>
    
  </entry>
  
  <entry>
    <title>Wox插件编写</title>
    <link href="https://www.sekyoro.top/2023/05/20/Wox%E6%8F%92%E4%BB%B6%E7%BC%96%E5%86%99/"/>
    <id>https://www.sekyoro.top/2023/05/20/Wox%E6%8F%92%E4%BB%B6%E7%BC%96%E5%86%99/</id>
    <published>2023-05-20T02:21:31.000Z</published>
    <updated>2023-08-01T09:15:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近正在使用Wox，这个软件还挺高效的，而且还能自己编写一些插件，这里打算自己写点插件用用.<br><span id="more"></span></p><p><a href="http://www.wox.one/">Wox</a>官网</p><p><a href="http://www.wox.one/plugin">Plugin (wox.one)</a>插件,此外官方也提供了编写文档，<a href="http://doc.wox.one/zh/plugin/create_plugin.html">编写插件 · GitBook (wox.one)</a>提供Python和C#两种优秀的语言编写方案。我这里就使用Python写了.</p><p>首先需要做的是创建新的虚拟环境作隔离，一般使用python自带的<code>venv</code>或者<code>virtualenv</code>,当然也有比较新的<code>pipenv</code>和被认为比较重的<code>Anaconda</code>，后两者也是包管理器，功能比较多。</p><p>以下是创建虚拟环境的代码示例：</p><p>使用 virtualenv：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 virtualenv</span></span><br><span class="line">pip install virtualenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建名为 myenv 的虚拟环境</span></span><br><span class="line">virtualenv myenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活虚拟环境（Windows 平台）</span></span><br><span class="line">myenv\Scripts\activate.bat</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活虚拟环境（Linux 和 Mac 平台）</span></span><br><span class="line"><span class="built_in">source</span> myenv/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出虚拟环境</span></span><br><span class="line">deactivate</span><br></pre></td></tr></table></figure><p>使用 venv：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建名为 myenv 的虚拟环境</span></span><br><span class="line">python3 -m venv myenv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活虚拟环境</span></span><br><span class="line"><span class="built_in">source</span> myenv/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出虚拟环境</span></span><br><span class="line">deactivate</span><br></pre></td></tr></table></figure><p>这一步还是很重要的，环境不能乱。</p><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>可以看看官方案例以及其他插件的目录结构</p><p><img data-src="https://s2.loli.net/2023/05/20/79rv3TcFb4fYPqn.png" alt="image-20230520114456394"></p><p>在创建Wox的时候，用户必须在插件的根目录方式一个名为<code>plugin.json</code>的文件。该文件中包含了该插件的一些基本信息。</p><p><code>plugin.json</code>的格式如下： <strong>请在粘贴下面代码的时候移除其中的注释</strong></p><p>重点是触发词<code>ActionKeyword</code>以及<code>IcoPath</code>插件图标,<code>ID</code>需要一个32位不与其他插件重复的随机数，自己生成即可。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;ID&quot;</span>:<span class="string">&quot;D2D2C23B084D411DB66FE0C79D6C2A6H&quot;</span>,   <span class="comment">//插件ID，32位的UUID</span></span><br><span class="line">  <span class="attr">&quot;ActionKeyword&quot;</span>:<span class="string">&quot;wpm&quot;</span>,                     <span class="comment">//插件默认的触发关键字</span></span><br><span class="line">  <span class="attr">&quot;Name&quot;</span>:<span class="string">&quot;WPM&quot;</span>,                              <span class="comment">//插件名字</span></span><br><span class="line">  <span class="attr">&quot;Description&quot;</span>:<span class="string">&quot;Wox Package Management&quot;</span>,    <span class="comment">//插件介绍</span></span><br><span class="line">  <span class="attr">&quot;Author&quot;</span>:<span class="string">&quot;qianlifeng&quot;</span>,                     <span class="comment">//作者</span></span><br><span class="line">  <span class="attr">&quot;Version&quot;</span>:<span class="string">&quot;1.0.0&quot;</span>,                         <span class="comment">//插件版本，必须是x.x.x的格式</span></span><br><span class="line">  <span class="attr">&quot;Language&quot;</span>:<span class="string">&quot;csharp&quot;</span>,                       <span class="comment">//插件语言，目前支持csharp,python</span></span><br><span class="line">  <span class="attr">&quot;Website&quot;</span>:<span class="string">&quot;http://www.getwox.com&quot;</span>,         <span class="comment">//插件网站或者个人网站</span></span><br><span class="line">  <span class="attr">&quot;IcoPath&quot;</span>: <span class="string">&quot;Images\\pic.png&quot;</span>,              <span class="comment">//插件图标，路径是相对插件根目录的路径</span></span><br><span class="line">  <span class="attr">&quot;ExecuteFileName&quot;</span>:<span class="string">&quot;PluginManagement.dll&quot;</span>   <span class="comment">//执行文件入口，如果是C#插件则填写DLL路径，如果是pyhton则填写python文件路径</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后就是主文件以及插件图标文件</p><h2 id="Python开发"><a href="#Python开发" class="headerlink" title="Python开发"></a>Python开发</h2><blockquote><p>Wox自带了一个打包的Python及其标准库，所以使用Python插件的用户不必自己再安装Python环境。同时，Wox还打包了requests和beautifulsoup4两个库， 方便用户进行网络访问与解析。</p></blockquote><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>使用Python开发需要有一个类继承Wox,注意这个东西并不需要pip下载，是Wox自带的运行时，包括requests和BeautifulSoup4，而其他的第三方包需要自己下载.下载Wox时有一个full-installer包含python解释器，自己可以设置Python解释器文件夹路径为环境变量，一般下载Python时选择加入环境变量即可。</p><p>继承Wox类后关键要继承的方法是<code>query</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> webbrowser</span><br><span class="line"><span class="keyword">from</span> wox <span class="keyword">import</span> Wox,WoxAPI</span><br><span class="line"></span><br><span class="line"><span class="comment">#用户写的Python类必须继承Wox类 https://github.com/qianlifeng/Wox/blob/master/PythonHome/wox.py</span></span><br><span class="line"><span class="comment">#这里的Wox基类做了一些工作，简化了与Wox通信的步骤。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Main</span>(<span class="params">Wox</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request</span>(<span class="params">self,url</span>):</span></span><br><span class="line">    <span class="comment">#如果用户配置了代理，那么可以在这里设置。这里的self.proxy来自Wox封装好的对象</span></span><br><span class="line">    <span class="keyword">if</span> self.proxy <span class="keyword">and</span> self.proxy.get(<span class="string">&quot;enabled&quot;</span>) <span class="keyword">and</span> self.proxy.get(<span class="string">&quot;server&quot;</span>):</span><br><span class="line">      proxies = &#123;</span><br><span class="line">        <span class="string">&quot;http&quot;</span>:<span class="string">&quot;http://&#123;&#125;:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.proxy.get(<span class="string">&quot;server&quot;</span>),self.proxy.get(<span class="string">&quot;port&quot;</span>)),</span><br><span class="line">        <span class="string">&quot;https&quot;</span>:<span class="string">&quot;http://&#123;&#125;:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.proxy.get(<span class="string">&quot;server&quot;</span>),self.proxy.get(<span class="string">&quot;port&quot;</span>))&#125;</span><br><span class="line">      <span class="keyword">return</span> requests.get(url,proxies = proxies)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> requests.get(url)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#必须有一个query方法，用户执行查询的时候会自动调用query方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query</span>(<span class="params">self,key</span>):</span></span><br><span class="line">    r = self.request(<span class="string">&#x27;https://news.ycombinator.com/&#x27;</span>)</span><br><span class="line">    bs = BeautifulSoup(r.text)</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> bs.select(<span class="string">&quot;.comhead&quot;</span>):</span><br><span class="line">      title = i.previous_sibling.text</span><br><span class="line">      url = i.previous_sibling[<span class="string">&quot;href&quot;</span>]</span><br><span class="line">      results.append(&#123;</span><br><span class="line">        <span class="string">&quot;Title&quot;</span>: title ,</span><br><span class="line">        <span class="string">&quot;SubTitle&quot;</span>:title,</span><br><span class="line">        <span class="string">&quot;IcoPath&quot;</span>:<span class="string">&quot;Images/app.ico&quot;</span>,</span><br><span class="line">        <span class="string">&quot;JsonRPCAction&quot;</span>:&#123;</span><br><span class="line">          <span class="comment">#这里除了自已定义的方法，还可以调用Wox的API。调用格式如下：Wox.xxxx方法名</span></span><br><span class="line">          <span class="comment">#方法名字可以从这里查阅https://github.com/qianlifeng/Wox/blob/master/Wox.Plugin/IPublicAPI.cs 直接同名方法即可</span></span><br><span class="line">          <span class="string">&quot;method&quot;</span>: <span class="string">&quot;openUrl&quot;</span>,</span><br><span class="line">          <span class="comment">#参数必须以数组的形式传过去</span></span><br><span class="line">          <span class="string">&quot;parameters&quot;</span>:[url],</span><br><span class="line">          <span class="comment">#是否隐藏窗口</span></span><br><span class="line">          <span class="string">&quot;dontHideAfterAction&quot;</span>:<span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">openUrl</span>(<span class="params">self,url</span>):</span></span><br><span class="line">    webbrowser.<span class="built_in">open</span>(url)</span><br><span class="line">    WoxAPI.change_query(url)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以下代码是必须的</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">Main()</span><br></pre></td></tr></table></figure><p>上面这个例子就很典型，每次在Wox中输入就会调用query方法,其需要返回一个列表，列表中每一项内容如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        <span class="attr">&quot;Title&quot;</span>: title ,</span><br><span class="line">        <span class="attr">&quot;SubTitle&quot;</span>:title,</span><br><span class="line">        <span class="attr">&quot;IcoPath&quot;</span>:<span class="string">&quot;Images/app.ico&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;JsonRPCAction&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;method&quot;</span>: <span class="string">&quot;openUrl&quot;</span>,  </span><br><span class="line">          <span class="attr">&quot;parameters&quot;</span>:[url],</span><br><span class="line">          <span class="attr">&quot;dontHideAfterAction&quot;</span>:True</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p><img data-src="https://s2.loli.net/2023/05/21/rqlJXwZEeTzhdIK.png" alt="image-20230521171823745"></p><p>其中,拿一个翻译插件来说，Title就是要点,SubTitle就是翻译结果,icoPath是左边图标的路径，我测试了必须使用本地文件，而JsonRPCAction就是按Enter键或鼠标左击后的响应，method就是触发的方法，可以自己写，也可以用wox.py<a href="https://github.com/Wox-launcher/Wox/blob/master/JsonRPC/wox.py">Wox/wox.py at master · Wox-launcher/Wox · GitHub</a>中的WoxAPI方法,parameters就是参数，要求传入一个列表.<code>dontHideAfterAction</code>表示点击之后Wox会不会隐藏.</p><p>此外还有个有意思的方法就是<code>context_menu</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">context_menu</span>(<span class="params">self, data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    optional context menu entries for a result</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure><p>就是鼠标右击或者Shift+Enter后的响应，<strong>需要注意的是</strong>,重载的必须是<code>context_menu(self, data)</code>,不能确少data参数，这个data可以在query方法中返回的结果里设置<code>ContextData</code>如下</p><p><img data-src="https://s2.loli.net/2023/05/22/Tu4YXj6OHxD9ngG.png" alt="image-20230522112012105" style="zoom:67%;" /></p><p>比如我在Context menu里添加一项<code>去我的博客看看</code>,下面两项是默认的.</p><p><img data-src="https://s2.loli.net/2023/05/22/vLZGqJ58NnWkBoi.png" alt="image-20230522113028138" style="zoom: 67%;" /></p><p>所以<code>query</code>和<code>context_menu</code>返回的结果就是展示的结果项目。</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><p>像对于这种插件的开发个人认为是比较累的，debug比较麻烦,不能在终端直接显示，不像一般的客户端或者Web开发，所以日志还是很重要的，主要使用Python自带的logging模块.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 日志记录</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Logger</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        filename = os.path.join(os.path.dirname(__file__), <span class="string">&#x27;log.txt&#x27;</span>)</span><br><span class="line">        logging.basicConfig(level=logging.DEBUG, filename=filename, filemode=<span class="string">&#x27;a&#x27;</span>,</span><br><span class="line">                            <span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line">        self.logger = logging.getLogger()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">debug</span>(<span class="params">self, msg</span>):</span></span><br><span class="line">        self.logger.debug(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info</span>(<span class="params">self, msg</span>):</span></span><br><span class="line">        self.logger.info(msg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error</span>(<span class="params">self, msg</span>):</span></span><br><span class="line">        self.logger.error(msg)</span><br></pre></td></tr></table></figure><p>使用时初始化Logger,再掉用实例方法输出信息到文件.</p><h4 id="显示项目"><a href="#显示项目" class="headerlink" title="显示项目"></a>显示项目</h4><p>Wox插件机制叫做<a href="https://www.jsonrpc.org/specification">JSON-RPC 2.0 Specification (jsonrpc.org)</a>,利用这种方法将要展示的信息发给Wox并显示出来.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">query</span>(<span class="params">self, key</span>):</span></span><br><span class="line">    self.results = []</span><br><span class="line">    logger = Logger()</span><br><span class="line">    logger.info(<span class="string">&quot;-------------info--------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">    args = key.split()</span><br><span class="line">    length = <span class="built_in">len</span>(args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">        self.results.append(</span><br><span class="line">            self.add_item(<span class="string">&quot;有道智云翻译(正在开发中)&quot;</span>, <span class="string">&quot;需要配置key&quot;</span>, <span class="string">&#x27;Images/zhiyun.png&#x27;</span>, <span class="string">&quot;configyoudao&quot;</span>, <span class="string">&quot;zy&quot;</span>))</span><br><span class="line">        self.results.append(</span><br><span class="line">            self.add_item(<span class="string">&quot;使用有道翻译免费版本&quot;</span>, <span class="string">&quot;暂只支持中英互译&quot;</span>, <span class="string">&#x27;Images/youdao.png&#x27;</span>, <span class="string">&quot;freetrans&quot;</span>, <span class="string">&quot;yd&quot;</span>))</span><br><span class="line">        self.results.append(self.add_item(<span class="string">&quot;使用百度翻译&quot;</span>, <span class="string">&quot;需要配置key&quot;</span>, <span class="string">&#x27;Images/bd.png&#x27;</span>, <span class="string">&quot;configbaidu&quot;</span>, <span class="string">&quot;bd&quot;</span>))</span><br><span class="line">        self.results.append(self.add_item(<span class="string">&quot;重载插件&quot;</span>, <span class="string">&quot;重新加载插件&quot;</span>, <span class="string">&#x27;Images/pic.jpg&#x27;</span>, <span class="string">&quot;reload&quot;</span>))</span><br><span class="line">        <span class="keyword">return</span> self.results</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.results</span><br></pre></td></tr></table></figure><p>这是我写的部分代码，可以看到返回了一个列表，展示出来就是下面样子的</p><p><img data-src="https://s2.loli.net/2023/05/21/K5TBRAWYpmGbeaf.png" alt="image-20230521172551777"></p><p>可以通过显示项目的方法debug.</p><h3 id="响应方法"><a href="#响应方法" class="headerlink" title="响应方法"></a>响应方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">openUrl</span>(<span class="params">self, url</span>):</span></span><br><span class="line">    webbrowser.<span class="built_in">open</span>(url)</span><br><span class="line">    WoxAPI.change_query(url, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>在返回的项目中，<code>JsonRPCAction</code>的method就填方法名,可以写在一个类中，可以使用WoxAPI改变query也就是搜索的字符串，此外还有其他方法，<strong>注意</strong>，我测试的发现show_msg已经无法正常使用了</p><p><img data-src="https://s2.loli.net/2023/05/21/kwTv5QKbJtL716S.png" alt="image-20230521173212739" style="zoom:50%;" /></p><p><code>JsonRPCAction</code>的parameters传入的列表就对应method中的参数，比如传[1,2],而method中就可以使用method(a,b)来接收,当然也可以使用method(*para)用一个元组接收.</p><p><img data-src="https://s2.loli.net/2023/05/21/7FJRkB6H21wNcsL.png" alt="image-20230521174809680"></p><h3 id="更新到Wox扩展"><a href="#更新到Wox扩展" class="headerlink" title="更新到Wox扩展"></a>更新到Wox扩展</h3><p>目前代码已经传到Github上,Wox本身有一个扩展库.</p><p>欢迎下载<a href="http://www.wox.one/plugin/416">Plugin Detail (wox.one)</a></p><p><img data-src="https://s2.loli.net/2023/07/29/5WwyJq6p7l9oRiK.png" alt="image-20230729211142583"></p><h3 id="B站直播信息查看"><a href="#B站直播信息查看" class="headerlink" title="B站直播信息查看"></a>B站直播信息查看</h3><p>由于平时会看看直播,找到了一个关于B站接口的项目<a href="https://github.com/SocialSisterYi/bilibili-API-collect">GitHub - SocialSisterYi/bilibili-API-collect: 哔哩哔哩-API收集整理【不断更新中….】</a></p><p>利用其中的一些接口使用Wox快速看看某个直播是不是在直播.</p><p>目前打算的方法是让用户填写配置信息,让用户填好Cookie后,直接调用请求查看关注的正在直播的用户.</p><p><img data-src="https://s2.loli.net/2023/07/29/Y8n1iPpI4Qmr9Db.png" alt="框架" style="zoom:67%;" /></p><p>用户在web上登陆后,在配置文件中<code>config.json</code>中写<code>SESSDATA</code></p><p><img data-src="https://s2.loli.net/2023/07/29/vftBx4l81mpG3jS.png" alt="image-20230729221851699"></p><p>然后调用接口即可.</p><p>主要注意两点,一个是在JSONRPC里不要轻易调用原本类里东西.比如我写了个<code>Logger</code>类,并在<code>Main</code>类里实例化了这个类,并增加了<code>searchall</code>方法,通过<code>JsonRPCAction</code>调用.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_item</span>(<span class="params">self, title, subtitle, icopath, method, *parameters</span>):</span></span><br><span class="line">    item = &#123;</span><br><span class="line">        <span class="string">&quot;Title&quot;</span>: title,</span><br><span class="line">        <span class="string">&quot;SubTitle&quot;</span>: subtitle,</span><br><span class="line">        <span class="string">&quot;IcoPath&quot;</span>: icopath,</span><br><span class="line">        <span class="string">&quot;JsonRPCAction&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;method&quot;</span>: method,</span><br><span class="line">            <span class="string">&quot;parameters&quot;</span>: parameters,</span><br><span class="line">            <span class="string">&quot;dontHideAfterAction&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>在这里的<code>searchall</code>方法里是不能使用<code>logger</code>输出日志的.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.results.append(</span><br><span class="line">            self.add_item(<span class="string">&quot;BiliLive&quot;</span>, <span class="string">&quot;查看自己关注的正在直播的主播&quot;</span>, <span class="string">&#x27;Images/pic.png&#x27;</span>,<span class="string">&quot;searchall&quot;</span>,<span class="string">&quot;all&quot;</span>)</span><br><span class="line">        self.results.append(</span><br><span class="line">            self.add_item(<span class="string">&quot;Config Cookie&quot;</span>, <span class="string">&quot;配置你的Cookie&quot;</span>, <span class="string">&#x27;Images/cookie.png&#x27;</span>, <span class="string">&quot;openviewer&quot;</span>))</span><br><span class="line">        self.results.append(</span><br><span class="line">            self.add_item(<span class="string">&quot;reload&quot;</span>, <span class="string">&quot;刷新重载插件&quot;</span>, <span class="string">&#x27;Images/pic.png&#x27;</span>, <span class="string">&quot;reload&quot;</span>))</span><br><span class="line"><span class="keyword">return</span> self.results</span><br></pre></td></tr></table></figure><p><img data-src="https://s2.loli.net/2023/07/30/eV4fpqWTwSZFCmI.png" alt="image-20230730165948141"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>Python编写插件相关资料比较少,C#要多一点</li><li>可以参考这个文件的代码,还是不错的<a href="https://github.com/Wox-launcher/Wox/tree/master/Plugins/HelloWorldPython">Wox/Plugins/HelloWorldPython at master · Wox-launcher/Wox · GitHub</a></li><li>可以看看我的翻译插件,支持免费中英互译,也支持百度的有一定免费额度的API翻译，支持多种语言<br> <img data-src="https://s2.loli.net/2023/05/21/zH9peMTlJg13GYU.png" alt="image-20230521173938835"><br> <img data-src="https://s2.loli.net/2023/05/21/7CioWXbFr4nSx3d.png" alt="image-20230521174103179"></li><li>如果向上传给别人用,需要压缩整个文件夹然后改后缀名为wox<a href="https://github.com/Wox-launcher/Wox/issues/2199">How to create <code>.wox</code> · Issue #2199 · Wox-launcher/Wox (github.com)</a></li><li>最后我在Wox的plugin上上传了这两个插件,欢迎star以及下载使用!</li></ol><p><img data-src="https://s2.loli.net/2023/07/30/TgMsi4I7SWyD9Pj.png" alt="image-20230730120855907"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/81568689#:~:text=virtualenv ：Python 虚拟环境管理工具。 venv ：Python 标准库内置的虚拟环境管理工具，Python 3.3 加入，Python,开始作为管理虚拟环境的推荐工具，用法类似 virtualenv。 如果你使用 Python 3，推荐使用 venv 来替代 virtualenv。">要不我们还是用回 virtualenv/venv 和 pip 吧 </a></li><li><a href="https://github.com/Wox-launcher/Wox/blob/master/JsonRPC/wox.py">Wox/wox.py at master · Wox-launcher/Wox · GitHub</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近正在使用Wox，这个软件还挺高效的，而且还能自己编写一些插件，这里打算自己写点插件用用.&lt;br&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.sekyoro.top/tags/python/"/>
    
    <category term="插件" scheme="https://www.sekyoro.top/tags/%E6%8F%92%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>python的特别方法与装饰器</title>
    <link href="https://www.sekyoro.top/2023/05/07/python%E7%9A%84%E7%89%B9%E5%88%AB%E6%96%B9%E6%B3%95%E4%B8%8E%E8%A3%85%E9%A5%B0%E5%99%A8/"/>
    <id>https://www.sekyoro.top/2023/05/07/python%E7%9A%84%E7%89%B9%E5%88%AB%E6%96%B9%E6%B3%95%E4%B8%8E%E8%A3%85%E9%A5%B0%E5%99%A8/</id>
    <published>2023-05-07T03:31:24.000Z</published>
    <updated>2023-05-07T07:15:14.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>这里将介绍python的所谓魔法方法以及装饰器<br><span id="more"></span></p><h3 id="魔术方法"><a href="#魔术方法" class="headerlink" title="魔术方法"></a>魔术方法</h3><p>一般在类中以双下划线包围的方法就是魔术方法，或者叫特殊方法。</p><p>简单来说，Python的魔术方法是为了利用Python的标准方法以及不用去记住标准操作的名称，实现更统一的接口。</p><p><img data-src="https://s2.loli.net/2023/05/07/2QCAj8iUIsNtrzO.png" alt="image-20230507133421451"></p><p>例如下面的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line">Card = collections.namedtuple(<span class="string">&#x27;Card&#x27;</span>, [<span class="string">&#x27;rank&#x27;</span>, <span class="string">&#x27;suit&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FrenchDeck</span>:</span></span><br><span class="line">    ranks = [<span class="built_in">str</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="number">11</span>)] + <span class="built_in">list</span>(<span class="string">&#x27;JQKA&#x27;</span>)</span><br><span class="line">    suits = <span class="string">&#x27;spades diamonds clubs hearts&#x27;</span>.split()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self._cards = [Card(rank, suit) <span class="keyword">for</span> suit <span class="keyword">in</span> self.suits</span><br><span class="line">                                        <span class="keyword">for</span> rank <span class="keyword">in</span> self.ranks]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._cards)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._cards[item]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(deck))</span><br><span class="line">    <span class="built_in">print</span>(deck[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><code>__len__</code>这个特别方法使得调用len(beer<em>card)实际调用`<em>_len</em></em>`</p><p> <strong>getitem</strong> 方法把 [] 操作交给了 self._cards 列表，所以我们的 deck 类自动支持切 片（slicing）操作。</p><p>输出如下</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">52</span><br><span class="line">Card(<span class="attribute">rank</span>=<span class="string">&#x27;2&#x27;</span>, <span class="attribute">suit</span>=<span class="string">&#x27;spades&#x27;</span>)</span><br></pre></td></tr></table></figure><p>另外，仅仅实现了 <strong>getitem</strong> 方法，对象就变成可迭代的了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> deck:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><p><img data-src="https://s2.loli.net/2023/05/07/nBPSwYrg68y47Ff.png" alt="image-20230507134341143"></p><p>同时<code>__contains__</code>可以实现<code>in</code>方法</p><p>除了上面通过特殊方法实现迭代，len(),切片等方法外，还可以实现类似重载运算符的效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> hypot </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vector</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x=<span class="number">0</span>, y=<span class="number">0</span></span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;Vector(%r, %r)&#x27;</span> % (self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__abs__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> hypot(self.x, self.y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__bool__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bool</span>(<span class="built_in">abs</span>(self))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        x = self.x + other.x</span><br><span class="line">        y = self.y + other.y</span><br><span class="line">        <span class="keyword">return</span> Vector(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__mul__</span>(<span class="params">self, scalar</span>):</span></span><br><span class="line">        <span class="keyword">return</span> Vector(self.x * scalar, self.y * scalar)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v1 = Vector(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">v2 = Vector(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">v3 = v1+v2</span><br><span class="line"><span class="built_in">print</span>(v3)</span><br></pre></td></tr></table></figure><p>输出Vector(7, 9)</p><p>还有<code>__repr__</code>和<code>__str__</code></p><p><strong>repr</strong> 和 <strong>str</strong> 的区别在于，后者是在 str() 函数被使用，或是在用 print 函数打印一个对象的时候才被调用的，并且它返回的字符串对终端用户更友好。</p><p> 如果你只想实现这两个特殊方法中的一个，<strong>repr</strong> 是更好的选择，因为如果一个对象没有 <strong>str</strong> 函数，而 Python 又需要调用它的时候，解释器会用 <strong>repr</strong> 作为替代。</p><blockquote><ul><li><code>&#123;!r&#125;</code>就是使用format语法时候的<code>%r</code>。因此，我们只需要关注<code>%r</code>就好。</li><li><code>%r</code>表示的用<code>repr()</code>处理；类似于的<code>%s</code>表示用<code>str()</code>处理一样</li></ul></blockquote><p>其他特别方法</p><p><img data-src="https://s2.loli.net/2023/05/07/GpmoahVdxD1TNwL.png" alt="image-20230507134927018"></p><h3 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h3><blockquote><p>装饰器的作用就是为已经存在的函数或对象添加额外的功能。<br>装饰器本质上是一个Python函数，它可以让其他函数在不需要做任何代码变动的前提下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码并继续重用。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>():</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[DEBUG]: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(func.__name__))</span><br><span class="line">        <span class="keyword">return</span> func()</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@debug</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"></span><br><span class="line">hello()</span><br></pre></td></tr></table></figure><p>装饰器涉及到闭包的概念,什么是闭包，一句话说就是，在函数中再嵌套一个函数，并且引用外部函数的变量，这就是一个闭包了</p><p>上述无参装饰器可以用于输出日志。</p><p>如果要在wrapper中访问参数,如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">debug</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[DEBUG]: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(func.__name__))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;params&quot;</span>, args, kwargs)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@debug</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">a,b</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;this is a test function&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果要在装饰器中使用参数,还要在外面包围一层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logging</span>(<span class="params">level</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">outwrapper</span>(<span class="params">func</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[&#123;0&#125;]: enter &#123;1&#125;()&quot;</span>.<span class="built_in">format</span>(level, func.__name__))</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outwrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@logging(<span class="params">level=<span class="string">&quot;INFO&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>(<span class="params">a, b, c</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(a, b, c)</span><br></pre></td></tr></table></figure><p>除了使用函数装饰器也可以是使用类装饰器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logging</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, func</span>):</span></span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[DEBUG]: enter &#123;&#125;()&quot;</span>.<span class="built_in">format</span>(self.func.__name__))</span><br><span class="line">        <span class="keyword">return</span> self.func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>(<span class="params">a, b, c</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(a, b, c)</span><br></pre></td></tr></table></figure><p>装饰器中使用参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logging</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, level</span>):</span></span><br><span class="line">        self.level = level</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, func</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[&#123;0&#125;]: enter &#123;1&#125;()&quot;</span>.<span class="built_in">format</span>(self.level, func.__name__))</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@logging(<span class="params">level=<span class="string">&quot;TEST&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>(<span class="params">a, b, c</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(a, b, c)</span><br></pre></td></tr></table></figure><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><blockquote><p>当我们创建一个类，以定义对象时，我们可能会希望一些属性对于外界是只读的，或者希望创建属性只能按特定的方式访问或修改。这时就可以使用 Python 中的属性（Property）。</p><p>在 Python 中，属性是包装在函数中的代码段，它们能够拦截对对象属性的访问和修改。Python 内置了@property 装饰器，可以用来修饰一个方法，使之成为属性调用。同时，Python 还提供了 @property 修饰器的对应方法的 setter 方法，用于设置属性。</p></blockquote><p>简单来说，属性优化了之前使用公共方法来访问私有属性得方法同时有更精细的粒度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myObj</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.__name = name</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;init&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__name</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    obj = myObj(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(obj.name)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">init</span></span><br><span class="line">hello</span><br></pre></td></tr></table></figure><p>如果使用<code>obj.name = &quot;world&quot;</code>更改属性则会报错.</p><p>添加</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@name.setter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name</span>(<span class="params">self, newname</span>):</span></span><br><span class="line">     self.__name = newname</span><br></pre></td></tr></table></figure><p>可以通过name属性更改<code>self.__name</code></p><p>此外还有deleter</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@name.deleter</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name</span>(<span class="params">self</span>):</span></span><br><span class="line">     self.__name = <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>使用del删除属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> obj.name</span><br><span class="line"><span class="built_in">print</span>(obj.name)</span><br></pre></td></tr></table></figure><p>输出就是<code>None</code></p><p>属性也可以通过property() 函数创建.</p><p>基本使用格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">属性名=property(<span class="attribute">fget</span>=None, <span class="attribute">fset</span>=None, <span class="attribute">fdel</span>=None, <span class="attribute">doc</span>=None)</span><br></pre></td></tr></table></figure><p>其中，fget 参数用于指定获取该属性值的类方法，fset 参数用于指定设置该属性值的方法，fdel 参数用于指定删除该属性值的方法，最后的 doc 是一个文档字符串，用于说明此函数的作用。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://zhuanlan.zhihu.com/p/329962624">Python常用魔术方法 - 知乎 (zhihu.com)</a></li><li>《流畅的Python》</li><li><a href="https://blog.csdn.net/luoz_java/article/details/90339876">Python基础（十四）—装饰器 wrapper_luoz_python的博客-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/87353829">python 装饰器详解 - 知乎 (zhihu.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/366156798">Python @property属性详解 - 知乎 (zhihu.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这里将介绍python的所谓魔法方法以及装饰器&lt;br&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.sekyoro.top/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python中的logging模块</title>
    <link href="https://www.sekyoro.top/2023/05/02/python%E4%B8%AD%E7%9A%84logging%E6%A8%A1%E5%9D%97/"/>
    <id>https://www.sekyoro.top/2023/05/02/python%E4%B8%AD%E7%9A%84logging%E6%A8%A1%E5%9D%97/</id>
    <published>2023-05-02T13:55:55.000Z</published>
    <updated>2023-05-03T15:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>开发一个程序或者软件使用日志记录是非常有用的,对于开发者DEBUG,维护以及有经验的程序使用者都可以根据日志判断软件出了什么问题以及能做什么.这里主要介绍使用Python的原生模块logging写日志.<br><span id="more"></span></p><h2 id="日志概念"><a href="#日志概念" class="headerlink" title="日志概念"></a>日志概念</h2><blockquote><p>日志是一种可以追踪某些软件运行时所发生事件的方法。软件开发人员可以向他们的代码中调用日志记录相关的方法来表明发生了某些事情。一个事件可以用一个可包含可选变量数据的消息来描述。</p><p>此外，事件也有重要性的概念，这个重要性也可以被称为严重性级别（level）。</p></blockquote><p>在部署项目时，不可能直接将所有的信息都输出到控制台中，我们可以将这些信息记录到日志文件中，这样不仅方便我们查看程序运行时的情况，也可以在项目出现故障时根据程序运行时产生的日志快速定位问题出现的位置。</p><p>日志等级,在python的logging模块中有以下几个等级以及等级的值</p><blockquote><p>CRITICAL = 50<br>FATAL = CRITICAL<br>ERROR = 40<br>WARNING = 30<br>WARN = WARNING<br>INFO = 20<br>DEBUG = 10<br>NOTSET = 0</p></blockquote><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO, <span class="built_in">format</span>=<span class="string">&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;</span>)</span><br><span class="line">logger = logging.getLogger(__name__)</span><br></pre></td></tr></table></figure><p>使用<code>basicConfig</code>进行基本配置,</p><p><img data-src="https://img.proanimer.com/imgs/02/05image-20230502221157465.png" alt="image-20230502221157465"></p><p>可以设置<code>filename</code>作为日志信息的输入文件.</p><p>可以创造多个 Logger 对象，但是真正输出日志的是root Logger 对象。每个 Logger 对象都可以设置一个名字，如果设置<code>logger = logging.getLogger(__name__)</code>，<strong>name</strong> 是 Python 中的一个特殊内置变量，他代表当前模块的名称（默认为 <strong>main</strong>）。则 Logger 对象的 name 为建议使用使用以点号作为分隔符的命名空间等级制度。</p><h3 id="Logging组件"><a href="#Logging组件" class="headerlink" title="Logging组件"></a>Logging组件</h3><div class="table-container"><table><thead><tr><th>组件名称</th><th>类名</th><th>描述</th></tr></thead><tbody><tr><td>日志器</td><td>Logger</td><td>提供程序一直使用的接口</td></tr><tr><td>处理器</td><td>Handler</td><td>将日志记录发送到合适的目的输出</td></tr><tr><td>过滤器</td><td>Filer</td><td>决定输出哪条日志记录</td></tr><tr><td>格式器</td><td>Formatter</td><td>决定日志记录的最终输出格式</td></tr></tbody></table></div><p>如果直接想要简单使用,可以直接使用<code>logging</code>对象,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.DEBUG,</span><br><span class="line">                    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s %(name)s %(levelname)s %(message)s&quot;</span>,</span><br><span class="line">                    datefmt = <span class="string">&#x27;%Y-%m-%d  %H:%M:%S %a&#x27;</span>   </span><br><span class="line">                    )</span><br><span class="line">logging.debug(<span class="string">&quot;msg1&quot;</span>)</span><br><span class="line">logging.info(<span class="string">&quot;msg2&quot;</span>)</span><br><span class="line">logging.warning(<span class="string">&quot;msg3&quot;</span>)</span><br><span class="line">logging.error(<span class="string">&quot;msg4&quot;</span>)</span><br><span class="line">logging.critical(<span class="string">&quot;msg5&quot;</span>)</span><br></pre></td></tr></table></figure><p>对于<code>format</code>的值</p><div class="table-container"><table><thead><tr><th>字段/属性名称</th><th>使用格式</th><th>描述</th></tr></thead><tbody><tr><td>asctime</td><td>%(asctime)s</td><td>将日志的时间构造成可读的形式，默认情况下是‘2016-02-08 12:00:00,123’精确到毫秒</td></tr><tr><td>name</td><td>%(name)s</td><td>所使用的日志器名称，默认是’root’，因为默认使用的是 rootLogger</td></tr><tr><td>filename</td><td>%(filename)s</td><td>调用日志输出函数的模块的文件名； pathname的文件名部分，包含文件后缀</td></tr><tr><td>funcName</td><td>%(funcName)s</td><td>由哪个function发出的log， 调用日志输出函数的函数名</td></tr><tr><td>levelname</td><td>%(levelname)s</td><td>日志的最终等级（被filter修改后的）</td></tr><tr><td>message</td><td>%(message)s</td><td>日志信息， 日志记录的文本内容</td></tr><tr><td>lineno</td><td>%(lineno)d</td><td>当前日志的行号， 调用日志输出函数的语句所在的代码行</td></tr><tr><td>levelno</td><td>%(levelno)s</td><td>该日志记录的数字形式的日志级别（10, 20, 30, 40, 50）</td></tr><tr><td>pathname</td><td>%(pathname)s</td><td>完整路径 ，调用日志输出函数的模块的完整路径名，可能没有</td></tr><tr><td>process</td><td>%(process)s</td><td>当前进程， 进程ID。可能没有</td></tr><tr><td>processName</td><td>%(processName)s</td><td>进程名称，Python 3.1新增</td></tr><tr><td>thread</td><td>%(thread)s</td><td>当前线程， 线程ID。可能没有</td></tr><tr><td>threadName</td><td>%(thread)s</td><td>线程名称</td></tr><tr><td>module</td><td>%(module)s</td><td>调用日志输出函数的模块名， filename的名称部分，不包含后缀即不包含文件后缀的文件名</td></tr><tr><td>created</td><td>%(created)f</td><td>当前时间，用UNIX标准的表示时间的浮点数表示； 日志事件发生的时间—时间戳，就是当时调用time.time()函数返回的值</td></tr><tr><td>relativeCreated</td><td>%(relativeCreated)d</td><td>输出日志信息时的，自Logger创建以 来的毫秒数； 日志事件发生的时间相对于logging模块加载时间的相对毫秒数</td></tr><tr><td>msecs</td><td>%(msecs)d</td><td>日志事件发生事件的毫秒部分。logging.basicConfig()中用了参数datefmt，将会去掉asctime中产生的毫秒部分，可以用这个加上</td></tr></tbody></table></div><p>对于<code>datefmt</code>字段,对应的是<code>format</code>字段的<code>%(asctime)s</code>,用于修改时间显示格式.</p><p><code>level</code>字段用于显示的<strong>最低等级</strong></p><p>总结来说,简单实用就是配置logging.basicConfig()（用默认日志格式（Formatter）为日志系统建立一个默认的流处理器（StreamHandler），设置基础配置（如日志级别等）并加到root logger（根Logger）中）这几个logging模块级别的函数。</p><h3 id="进阶使用"><a href="#进阶使用" class="headerlink" title="进阶使用"></a>进阶使用</h3><p>进阶使用跟上述的模块四大组件有关,</p><ul><li>日志器（logger）需要通过处理器（handler）将日志信息输出到目标位置，如：文件、sys.stdout、网络等；</li><li>不同的处理器（handler）可以将日志输出到不同的位置；</li><li>日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置；</li><li>每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志；</li><li>每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。</li></ul><h4 id="Logger类"><a href="#Logger类" class="headerlink" title="Logger类"></a>Logger类</h4><blockquote><p>Logger对象有3个任务要做：</p><ul><li>1）向应用程序代码暴露几个方法，使应用程序可以在运行时记录日志消息；</li><li>2）基于日志严重等级（默认的过滤设施）或filter对象来决定要对哪些日志进行后续处理；</li><li>3）将日志消息传送给所有感兴趣的日志handlers。</li></ul><p>Logger对象最常用的方法分为两类：配置方法 和 消息发送方法</p></blockquote><p>一般通过<code>logging.getLogger(__name__)</code>得到logger对象,然后通过<code>logger.setLevel</code>设置logger过滤的等级.</p><p>通过addHandler和addFilter添加处理器和过滤器.</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>Logger.setLevel()</td><td>设置日志器将会处理的日志消息的最低严重级别</td></tr><tr><td>Logger.addHandler() 和 Logger.removeHandler()</td><td>为该logger对象添加 和 移除一个handler对象</td></tr><tr><td>Logger.addFilter() 和 Logger.removeFilter()</td><td>为该logger对象添加 和 移除一个filter对象</td></tr></tbody></table></div><p>使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logger.debug(<span class="string">&quot;DEBUG&quot;</span>)</span><br><span class="line">logger.info(<span class="string">&quot;INFO&quot;</span>)</span><br><span class="line">logger.warning(<span class="string">&quot;WARNING&quot;</span>)</span><br><span class="line">logger.error(<span class="string">&quot;ERROR&quot;</span>)</span><br><span class="line">logger.critical(<span class="string">&quot;CRITICAL&quot;</span>)</span><br><span class="line">logger.log(<span class="number">15</span>,<span class="string">&quot;hello&quot;</span>)</span><br></pre></td></tr></table></figure><p>常用的就是info,debug,warning,error几种日志等级,也可以通过logger.log设置自己的日志等级和消息.</p><h4 id="Handler类"><a href="#Handler类" class="headerlink" title="Handler类"></a>Handler类</h4><p>Handler对象的作用是（基于日志消息的level）将消息分发到handler指定的位置（文件、网络、邮件等）。Logger对象可以通过addHandler()方法为自己添加0个或者更多个handler对象。比如，一个应用程序可能想要实现以下几个日志需求：</p><ul><li>1）把所有日志都发送到一个日志文件中；</li><li>2）把所有严重级别大于等于error的日志发送到stdout（标准输出）；</li><li>3）把所有严重级别为critical的日志发送到一个email邮件地址。这种场景就需要3个不同的handlers，每个handler复杂发送一个特定严重级别的日志到一个特定的位置。</li></ul><p>Handler一般使用StreamHandler和FileHandler用于输入到Stream和磁盘文件.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">handler = logging.StreamHandler()</span><br><span class="line">logger.addHandler(handler)</span><br></pre></td></tr></table></figure><p>handler也可以设置level,过滤通过了logger的日志,此外handler也可以设置Filter用于更细致的过滤以及设置Formatter用于输出格式.</p><div class="table-container"><table><thead><tr><th>Handler</th><th>描述</th></tr></thead><tbody><tr><td>logging.StreamHandler</td><td>将日志消息发送到输出到Stream，如std.out, std.err或任何file-like对象。</td></tr><tr><td>logging.FileHandler</td><td>将日志消息发送到磁盘文件，默认情况下文件大小会无限增长</td></tr><tr><td>logging.handlers.RotatingFileHandler</td><td>将日志消息发送到磁盘文件，并支持日志文件按大小切割</td></tr><tr><td>logging.hanlders.TimedRotatingFileHandler</td><td>将日志消息发送到磁盘文件，并支持日志文件按时间切割</td></tr><tr><td>logging.handlers.HTTPHandler</td><td>将日志消息以GET或POST的方式发送给一个HTTP服务器</td></tr><tr><td>logging.handlers.SMTPHandler</td><td>将日志消息发送给一个指定的email地址</td></tr><tr><td>logging.NullHandler</td><td>该Handler实例会忽略error messages，通常被想使用logging的library开发者使用来避免’No handlers could be found for logger XXX’信息的出现。</td></tr></tbody></table></div><h4 id="Formatter类"><a href="#Formatter类" class="headerlink" title="Formatter类"></a>Formatter类</h4><p>常用的两个参数是</p><ul><li>fmt：指定消息格式化字符串，如果不指定该参数则默认使用message的原始值</li><li>datefmt：指定日期格式字符串，如果不指定该参数则默认使用”%Y-%m-%d %H:%M:%S”</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">formatter = logging.Formatter(<span class="string">&#x27;Hello %(message)s&#x27;</span>)</span><br><span class="line">handler.setFormatter(formatter)</span><br></pre></td></tr></table></figure><p><img data-src="https://img.proanimer.com/imgs/03/05image-20230503222936462.png" alt="image-20230503222936462"></p><h4 id="Filter类"><a href="#Filter类" class="headerlink" title="Filter类"></a>Filter类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoParsingFilter</span>(<span class="params">logging.Filter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter</span>(<span class="params">self, record</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(record.getMessage())</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> record.getMessage().startswith(<span class="string">&#x27;parsing&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_grammar_messages</span>(<span class="params">record</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(record.funcName,<span class="string">&quot;---------------&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> record.funcName == <span class="string">&#x27;load_grammar&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>可以使用类或者函数作为Filter</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logger.addFilter(NoParsingFilter())</span><br><span class="line">logger.addFilter(filter_grammar_messages)</span><br></pre></td></tr></table></figure><p>同时也可以在类中的初始化函数设置类的属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Filter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Filter instances are used to perform arbitrary filtering of LogRecords.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Loggers and Handlers can optionally use Filter instances to filter</span></span><br><span class="line"><span class="string">    records as desired. The base filter class only allows events which are</span></span><br><span class="line"><span class="string">    below a certain point in the logger hierarchy. For example, a filter</span></span><br><span class="line"><span class="string">    initialized with &quot;A.B&quot; will allow events logged by loggers &quot;A.B&quot;,</span></span><br><span class="line"><span class="string">    &quot;A.B.C&quot;, &quot;A.B.C.D&quot;, &quot;A.B.D&quot; etc. but not &quot;A.BB&quot;, &quot;B.A.B&quot; etc. If</span></span><br><span class="line"><span class="string">    initialized with the empty string, all events are passed.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name=<span class="string">&#x27;&#x27;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a filter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Initialize with the name of the logger which, together with its</span></span><br><span class="line"><span class="string">        children, will have its events allowed through the filter. If no</span></span><br><span class="line"><span class="string">        name is specified, allow every event.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.nlen = <span class="built_in">len</span>(name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter</span>(<span class="params">self, record</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Determine if the specified record is to be logged.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Is the specified record to be logged? Returns 0 for no, nonzero for</span></span><br><span class="line"><span class="string">        yes. If deemed appropriate, the record may be modified in-place.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.nlen == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> self.name == record.name:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> record.name.find(self.name, <span class="number">0</span>, self.nlen) != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> (record.name[self.nlen] == <span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure><p>注意Filter也有默认行为,过滤行为设计到初始化的Filer,这里就不细说了.</p><blockquote><h3 id="日志流处理简要流程"><a href="#日志流处理简要流程" class="headerlink" title="日志流处理简要流程"></a>日志流处理简要流程</h3><p>1、创建一个logger</p><p>2、设置下logger的日志的等级</p><p>3、创建合适的Handler(FileHandler要有路径)</p><p>4、设置下每个Handler的日志等级</p><p>5、创建下日志的格式</p><p>6、向Handler中添加上面创建的格式</p><p>7、将上面创建的Handler添加到logger中</p><p>8、打印输出logger.debug\logger.info\logger.warning\logger.error\logger.critical</p></blockquote><h3 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h3><ol><li>重复写日志问题</li></ol><blockquote><p>用Python的logging模块记录日志时，可能会遇到重复记录日志的问题，第一条记录写一次，第二条记录写两次，第三条记录写三次</p></blockquote><p>原因:：第二次调用log的时候，根据getLogger(name)里的name获取同一个logger，而这个logger里已经有了第一次你添加的handler，第二次调用又添加了一个handler，所以，这个logger里有了两个同样的handler，以此类推，调用几次就会有几个handler。</p><p> 解决办法:1. 最后移除handler 2.做判断,若logger已有handler,则不再添加handler</p><p>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">#解决方案1，添加removeHandler语句，每次用完之后移除Handler</span></span><br><span class="line">logger.removeHandler(fh)</span><br><span class="line">logger.removeHandler(ch)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> logger.handlers:</span><br><span class="line">        <span class="comment">#创建handler</span></span><br><span class="line">    fh = logging.FileHandler(<span class="string">&quot;test.log&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">    ch = logging.StreamHandler()</span><br><span class="line">​</span><br><span class="line">        <span class="comment">#设置输出日志格式</span></span><br><span class="line">     formatter = logging.Formatter(</span><br><span class="line">            fmt=<span class="string">&quot;%(asctime)s %(name)s %(filename)s %(message)s&quot;</span>,</span><br><span class="line">            datefmt=<span class="string">&quot;%Y/%m/%d %X&quot;</span></span><br><span class="line">            )</span><br></pre></td></tr></table></figure><h3 id="logging配置文件"><a href="#logging配置文件" class="headerlink" title="logging配置文件"></a>logging配置文件</h3><p>可以使用<code>logging.conf.fileconf</code>进行配置logging</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging.config</span><br><span class="line">logging.config.fileConfig(<span class="string">&#x27;logging.conf&#x27;</span>)</span><br></pre></td></tr></table></figure><p>下面是一个<code>logging.conf</code>文件demo</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">文件配置</span><br><span class="line">配置文件logging.conf如下:</span><br><span class="line">[loggers]</span><br><span class="line"><span class="attribute">keys</span>=root,example01</span><br><span class="line"></span><br><span class="line">[logger_root]</span><br><span class="line"><span class="attribute">level</span>=DEBUG</span><br><span class="line"><span class="attribute">handlers</span>=hand01,hand02</span><br><span class="line"></span><br><span class="line">[logger_example01]</span><br><span class="line"><span class="attribute">handlers</span>=hand01,hand02</span><br><span class="line"><span class="attribute">qualname</span>=example01</span><br><span class="line"><span class="attribute">propagate</span>=0</span><br><span class="line"></span><br><span class="line">[handlers]</span><br><span class="line"><span class="attribute">keys</span>=hand01,hand02</span><br><span class="line"></span><br><span class="line">[handler_hand01]</span><br><span class="line"><span class="attribute">class</span>=StreamHandler</span><br><span class="line"><span class="attribute">level</span>=INFO</span><br><span class="line"><span class="attribute">formatter</span>=form02</span><br><span class="line">args=(sys.stderr,)</span><br><span class="line"></span><br><span class="line">[handler_hand02]</span><br><span class="line"><span class="attribute">class</span>=FileHandler</span><br><span class="line"><span class="attribute">level</span>=DEBUG</span><br><span class="line"><span class="attribute">formatter</span>=form01</span><br><span class="line">args=(<span class="string">&#x27;log.log&#x27;</span>, <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"></span><br><span class="line">[formatters]</span><br><span class="line"><span class="attribute">keys</span>=form01,form02</span><br><span class="line"></span><br><span class="line">[formatter_form01]</span><br><span class="line"><span class="attribute">format</span>=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s```</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/445411809">最棒总结！Python日志库 logging 使用指南来了 - 知乎 (zhihu.com)</a></li><li><a href="https://www.cnblogs.com/yuanyongqiang/p/11913812.html#_label0">Python3 日志(内置logging模块) - 天马行宇 - 博客园 (cnblogs.com)</a></li><li><a href="https://www.jianshu.com/p/feb86c06c4f4">python logging模块使用教程 - 简书 (jianshu.com)</a></li><li><a href="https://www.cnblogs.com/yyds/p/6901864.html">Python之日志处理（logging模块） - 云游道士 - 博客园 (cnblogs.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;开发一个程序或者软件使用日志记录是非常有用的,对于开发者DEBUG,维护以及有经验的程序使用者都可以根据日志判断软件出了什么问题以及能做什么.这里主要介绍使用Python的原生模块logging写日志.&lt;br&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.sekyoro.top/tags/python/"/>
    
    <category term="logging" scheme="https://www.sekyoro.top/tags/logging/"/>
    
    <category term="日志" scheme="https://www.sekyoro.top/tags/%E6%97%A5%E5%BF%97/"/>
    
  </entry>
  
  <entry>
    <title>如何使用word组织一篇学术垃圾</title>
    <link href="https://www.sekyoro.top/2023/04/07/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8word%E7%BB%84%E7%BB%87%E4%B8%80%E7%AF%87%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE/"/>
    <id>https://www.sekyoro.top/2023/04/07/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8word%E7%BB%84%E7%BB%87%E4%B8%80%E7%AF%87%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE/</id>
    <published>2023-04-07T03:57:06.000Z</published>
    <updated>2023-06-09T15:17:10.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>简单来说就是word样式设置</p><span id="more"></span><h3 id="纸张大小设置"><a href="#纸张大小设置" class="headerlink" title="纸张大小设置"></a>纸张大小设置</h3><p><img data-src="https://s2.loli.net/2023/06/09/cRopkws7T1n2dxA.png" alt="image-20230609151901283"></p><h3 id="设置标题样式"><a href="#设置标题样式" class="headerlink" title="设置标题样式"></a>设置标题样式</h3><p><img data-src="https://s2.loli.net/2023/04/08/ebjiD7yvBS63HCW.png" alt="image-20230408221018455" style="zoom:50%;" /></p><p><img data-src="https://s2.loli.net/2023/04/08/PfaiO9EkbVGl7Up.png" alt="image-20230408221508064" style="zoom:50%;" /></p><p><img data-src="https://s2.loli.net/2023/04/08/OYpf3ti2mgGLKbR.png" alt="image-20230408221523407" style="zoom:50%;" /></p><h3 id="题注和引用"><a href="#题注和引用" class="headerlink" title="题注和引用"></a>题注和引用</h3><p><img data-src="https://s2.loli.net/2023/06/09/HSZhj4mEUreQBbD.png" alt="image-20230609170058267"></p><h3 id="页眉页码设置"><a href="#页眉页码设置" class="headerlink" title="页眉页码设置"></a>页眉页码设置</h3><p>需要使用分节符,在wps中可以设置显示分节符,在文件的选项中设置.</p><p><img data-src="https://s2.loli.net/2023/06/09/9wxroRGhuYIN1zE.png" alt="image-20230609160619429"></p><p>然后在封面，目录后插入分节符，在中英摘要之间插入分页符,在摘要与目录之间插入分页符</p><p><img data-src="https://s2.loli.net/2023/06/09/8Kn3TqpXOCoc4zP.png" alt="image-20230609161039060"></p><p>注意,这里摘要,目录使用I,II编号,正文使用阿拉伯数字.</p><p><img data-src="https://s2.loli.net/2023/06/09/hlifCENFVroXu4t.png" alt="image-20230609161112284"></p><p>此外有些论文要求奇偶页页眉不同,取消链接到前一节,勾选奇偶页不同,然后修改域,选择链接和引用,选择标题1.</p><p>同时有论文要求每章节需要分页</p><h3 id="使用公式"><a href="#使用公式" class="headerlink" title="使用公式"></a>使用公式</h3><p>可以使用自带公式编辑,就是鼠标点点点.</p><p>也可以使用在线的编辑器编辑公式复制进去.</p><p>或者安装MathType<a href="https://zhuanlan.zhihu.com/p/531033840#:~:text=说明：WPS里面“宏”的功能是锁定的，无法使用MathType的全部功能，需要上网下载一个“WPS的vba模块”程序。 WPS VBA 宏插件是一款WPS官方推出的WPS VBA模块插件工具，安装后可以开启WPS的宏功能，该WPS VBA,宏插件支持WPS2016和WPS2019版本，兼容Win7和Win10系统。 1.下载wps.vba.exe 链接： pan.baidu.com%2Fs%2F1DD-1j- 提取码：will 2.关闭WPS 3.运行wps.vba.exe，傻瓜式安装（一直下一步）">WPS加载MathType模块 - 知乎 (zhihu.com)</a></p><h3 id="制作三线表"><a href="#制作三线表" class="headerlink" title="制作三线表"></a>制作三线表</h3><p>表格的应用</p><p><img data-src="https://s2.loli.net/2023/04/11/FshAErVzwxaZIOQ.png" alt="image-20230411155154936" style="zoom:67%;" /></p><p><img data-src="https://s2.loli.net/2023/04/11/3PKRuZlpk8wAQvN.png" alt="image-20230411155204407"></p><h3 id="插入参考文献"><a href="#插入参考文献" class="headerlink" title="插入参考文献"></a>插入参考文献</h3><p>推荐使用EndNote或Zotero,详细教程可以看看</p><p><a href="https://www.sekyoro.top/2023/04/02/EndNote基本使用/">EndNote基本使用 | Sekyoro的博客小屋</a></p><p><a href="https://www.sekyoro.top/2022/10/01/Obsidian学习/">Obsidian学习与Zotero联动 | Sekyoro的博客小屋</a></p><h3 id="生成目录"><a href="#生成目录" class="headerlink" title="生成目录"></a>生成目录</h3><p>使用各种标题样式,然后使用引用中的目录.</p><p><img data-src="https://s2.loli.net/2023/06/09/clXritTnsYvjBS1.png" alt="image-20230609163349541"></p><p>注意自动生成的目录格式一般都需要修改.</p><h3 id="导出pdf"><a href="#导出pdf" class="headerlink" title="导出pdf"></a>导出pdf</h3><p>如果需要pdf可以使用在线网站或者wps本身提供的转pdf功能</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;简单来说就是word样式设置&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
