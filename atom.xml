<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sekyoro的博客小屋</title>
  
  
  <link href="https://www.sekyoro.top/atom.xml" rel="self"/>
  
  <link href="https://www.sekyoro.top/"/>
  <updated>2024-01-17T10:46:28.465Z</updated>
  <id>https://www.sekyoro.top/</id>
  
  <author>
    <name>Sekyoro</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Golang学习:使用Gin</title>
    <link href="https://www.sekyoro.top/2024/01/17/Golang%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8Gin/"/>
    <id>https://www.sekyoro.top/2024/01/17/Golang%E5%AD%A6%E4%B9%A0-%E4%BD%BF%E7%94%A8Gin/</id>
    <published>2024-01-17T10:17:45.000Z</published>
    <updated>2024-01-17T10:46:28.465Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Go语言simple并且easy,内置了很多有用的库,对并发支持比较好并且官方(指Google)还是比较重视的.<br><span id="more"></span><br>Go的官方资料就比较好学习<a href="https://go.dev/">The Go Programming Language</a>,有个tutorial还有个examples.写了个使用Colly用来爬取图片和Gin用来显示图片的代码.</p><p>目录结构比较简单</p><p><img data-src="https://s2.loli.net/2024/01/17/xMBdCzhvfZEgVAT.png" alt="image-20240117183718713"></p><p>使用Go的mod进行配置项目,比如<code>go mod init &lt;project name&gt;</code>初始化</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"><span class="string">&quot;sekyoro.top/Goimg/routes&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">router := gin.Default()</span><br><span class="line">r := router.Group(<span class="string">&quot;/api&quot;</span>)</span><br><span class="line">router.Static(<span class="string">&quot;/img&quot;</span>, <span class="string">&quot;./imgs&quot;</span>)</span><br><span class="line">routes.DownloadPicRoutes(r)</span><br><span class="line">routes.ShowPicRoutes(r)</span><br><span class="line">routes.GetPicRoutes(r</span><br><span class="line">router.Run(<span class="string">&quot;:8080&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这里设置了静态资源并使用分组路由构建路由到处理的方法.</p><p>在routes文件夹下就有对应的路由,比如在下载文件下,设置了两个路由.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> routes</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"><span class="string">&quot;sekyoro.top/Goimg/handlers&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DownloadPicRoutes</span><span class="params">(router *gin.RouterGroup)</span></span> &#123;</span><br><span class="line">router.GET(<span class="string">&quot;/pix&quot;</span>, handlers.DownloadPixvisionPicHandler)</span><br><span class="line">router.GET(<span class="string">&quot;/booru/:type&quot;</span>, handlers.DownloadBooruPicHandler)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在downloader目录下写进行处理的方法.比如下面是<code>DownloadPixvisionPicHandler.go</code>去爬取图片并保存到本地</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> handlers</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="string">&quot;log&quot;</span></span><br><span class="line"><span class="string">&quot;net/http&quot;</span></span><br><span class="line"><span class="string">&quot;os&quot;</span></span><br><span class="line"><span class="string">&quot;path/filepath&quot;</span></span><br><span class="line"><span class="string">&quot;time&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"><span class="string">&quot;github.com/gocolly/colly/v2&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DownloadPixvisionPicHandler</span><span class="params">(ctx *gin.Context)</span></span> &#123;</span><br><span class="line">counter  := <span class="number">0</span></span><br><span class="line">allow_img_site := checkAllowSite()</span><br><span class="line"><span class="comment">// fmt.Println(allow_img_site)</span></span><br><span class="line">c := colly.NewCollector(colly.UserAgent(userAgent), colly.AllowedDomains(allow_img_site...),</span><br><span class="line">colly.Async())</span><br><span class="line">c.SetRequestTimeout(<span class="number">20</span> * time.Second)</span><br><span class="line"></span><br><span class="line">c.Limit(&amp;colly.LimitRule&#123;</span><br><span class="line">DomainGlob:  <span class="string">&quot;*pximg.*&quot;</span>,</span><br><span class="line">Parallelism: <span class="number">5</span>,</span><br><span class="line"><span class="comment">//Delay:      5 * time.Second,</span></span><br><span class="line">RandomDelay: <span class="number">500</span> * time.Duration(time.Millisecond),</span><br><span class="line">&#125;)</span><br><span class="line">c.Limit(&amp;colly.LimitRule&#123;</span><br><span class="line">DomainGlob:  <span class="string">&quot;*pixivision.*&quot;</span>,</span><br><span class="line">Parallelism: <span class="number">5</span>,</span><br><span class="line">Delay:       <span class="number">200</span> * time.Duration(time.Millisecond),</span><br><span class="line">RandomDelay: <span class="number">500</span> * time.Duration(time.Millisecond),</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">if</span> proxy != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> proxy[<span class="string">&quot;http&quot;</span>] != <span class="literal">nil</span> &#123;</span><br><span class="line">err := c.SetProxy(fmt.Sprintf(<span class="string">&quot;http://%s&quot;</span>, proxy[<span class="string">&quot;http&quot;</span>].(<span class="keyword">string</span>)))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Panic(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> proxy[<span class="string">&quot;socks5&quot;</span>] != <span class="literal">nil</span> &#123;</span><br><span class="line">err := c.SetProxy(fmt.Sprintf(<span class="string">&quot;socks5://%s&quot;</span>, proxy[<span class="string">&quot;socks5&quot;</span>].(<span class="keyword">string</span>)))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Panic(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">limit_page, ok := conf[<span class="string">&quot;limit_page&quot;</span>].(<span class="keyword">int</span>)</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">log.Panic(<span class="string">&quot;爬取图片目录数配置出错&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> !ok &#123;</span><br><span class="line">log.Panic(<span class="string">&quot;下载路径配置出错&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Find and visit all links</span></span><br><span class="line">c.OnHTML(<span class="string">&quot;a[href]&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(e *colly.HTMLElement)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> e.DOM.Parent().HasClass(<span class="string">&quot;arc__title&quot;</span>) &#123;</span><br><span class="line">log.Default().Println(<span class="string">&quot;Link found:&quot;</span>, e.Attr(<span class="string">&quot;href&quot;</span>))</span><br><span class="line"><span class="keyword">if</span> counter &gt;= limit_page &#123;</span><br><span class="line">ctx.JSON(http.StatusOK, fmt.Sprintf(<span class="string">&quot;success! %d directory image&quot;</span>, limit_page))</span><br><span class="line">&#125;</span><br><span class="line">e.Request.Visit(e.Attr(<span class="string">&quot;href&quot;</span>))</span><br><span class="line">counter += <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">c.OnHTML(<span class="string">&quot;div[class=&#x27;_article-main&#x27;]&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(e *colly.HTMLElement)</span></span> &#123;</span><br><span class="line">title := e.ChildText(<span class="string">&quot;h1[class=&#x27;am__title&#x27;]&quot;</span>)</span><br><span class="line"><span class="comment">// log.Default().Println(&quot;title:&quot;, title)</span></span><br><span class="line"><span class="comment">// p := Pics&#123;title: title, pics: make(map[string]string)&#125;</span></span><br><span class="line">err := os.MkdirAll(filepath.Join(download_root_folder, title), os.ModePerm)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Default().Println(err.Error())</span><br><span class="line">&#125;</span><br><span class="line">e.ForEach(<span class="string">&quot;div.article-item:not(._feature-article-body__paragraph) div.am__work__main&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(i <span class="keyword">int</span>, h *colly.HTMLElement)</span></span> &#123;</span><br><span class="line">log.Default().Println(<span class="string">&quot;pic:&quot;</span>, h.ChildAttr(<span class="string">&quot;img&quot;</span>, <span class="string">&quot;src&quot;</span>))</span><br><span class="line">img_src := h.ChildAttr(<span class="string">&quot;img&quot;</span>, <span class="string">&quot;src&quot;</span>)</span><br><span class="line">h.Request.Visit(img_src)</span><br><span class="line">h.Request.Ctx.Put(<span class="string">&quot;title&quot;</span>, title)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">&#125;)</span><br><span class="line">c.OnResponse(<span class="function"><span class="keyword">func</span><span class="params">(r *colly.Response)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> img_url URL_path = url_path(r.Request.URL.Path)</span><br><span class="line"><span class="comment">// log.Default().Println(&quot;img_name:&quot;, img_name)</span></span><br><span class="line"><span class="keyword">if</span> img_url.isPic() &#123;</span><br><span class="line">img_path := filepath.Join(download_root_folder,r.Ctx.Get(<span class="string">&quot;title&quot;</span>), <span class="keyword">string</span>(img_url.(url_path)))</span><br><span class="line">r.Save(img_path)</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">c.OnRequest(<span class="function"><span class="keyword">func</span><span class="params">(r *colly.Request)</span></span> &#123;</span><br><span class="line">log.Default().Println(<span class="string">&quot;Visiting&quot;</span>, r.URL)</span><br><span class="line"><span class="keyword">if</span> r.URL.Host == <span class="string">&quot;i.pximg.net&quot;</span> &#123;</span><br><span class="line">r.Headers.Set(<span class="string">&quot;Referer&quot;</span>, <span class="string">&quot;https://www.pixivision.net/&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">c.OnError(<span class="function"><span class="keyword">func</span><span class="params">(r *colly.Response, err error)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">log.Default().Println(<span class="string">&quot;Request URL:&quot;</span>, r.Request.URL, <span class="string">&quot;failed with response:&quot;</span>, <span class="keyword">string</span>(r.Body), <span class="string">&quot;\nError:&quot;</span>, err.Error())</span><br><span class="line">&#125;)</span><br><span class="line">c.Visit(pixivision_site)</span><br><span class="line">c.Wait()</span><br><span class="line">ctx.JSON(http.StatusOK, fmt.Sprintf(<span class="string">&quot;success! %d directory image&quot;</span>, limit_page))</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在<code>configure.yaml</code>中进行配置相关信息.</p><p>最后部署可以考虑使用render<a href="https://render.com/">Cloud Application Hosting for Developers | Render</a>,来玩玩吧<a href="https://go-img.onrender.com/api/show">https://go-img.onrender.com/api/show</a></p><p>完整代码可以在我的github<a href="https://github.com/drowning-in-codes/myGo">drowning-in-codes/myGo (github.com)</a>上看,我也上传了docker<a href="https://hub.docker.com/r/proanimer/goimg">proanimer/goimg - Docker Image</a>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull proanimer/goimg</span><br></pre></td></tr></table></figure><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Go语言simple并且easy,内置了很多有用的库,对并发支持比较好并且官方(指Google)还是比较重视的.&lt;br&gt;</summary>
    
    
    
    
    <category term="Golang" scheme="https://www.sekyoro.top/tags/Golang/"/>
    
    <category term="Gin" scheme="https://www.sekyoro.top/tags/Gin/"/>
    
  </entry>
  
  <entry>
    <title>gRPC学习</title>
    <link href="https://www.sekyoro.top/2024/01/05/grpc%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2024/01/05/grpc%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-01-05T11:16:58.000Z</published>
    <updated>2024-01-06T08:03:32.668Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>RPC是远程调用,而google实现了grpc比较方便地实现了远程调用,gRPC是一个现代的开源远程过程调用(RPC)框架<br><span id="more"></span></p><h2 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h2><p>在gRPC中，客户端应用程序可以直接调用另一台计算机上的服务器应用程序上的方法，就好像它是本地对象一样。</p><blockquote><p>远程过程调用是一个分布式计算的客户端-服务器（Client/Server）的例子，它简单而又广受欢迎。<br>远程过程调用总是由客户端对服务器发出一个执行若干过程请求，并用客户端提供的参数。执行结果将返回给客户端。<br>由于存在各式各样的变体和细节差异，对应地派生了各式远程过程调用协议，而且它们并不互相兼容。</p><p>为了允许不同的客户端均能访问服务器，<strong>许多标准化的 RPC 系统应运而生了。其中大部分采用接口描述语言（Interface Description Language，IDL），方便跨平台的远程过程调用</strong>。</p></blockquote><p>与许多RPC系统一样，gRPC基于定义服务的思想，指定可以通过其参数和返回类型远程调用的方法。在服务器端，服务器实现了这个接口，并运行gRPC服务器来处理客户端调用。在客户端，客户端有一个stub（在某些语言中称为客户端），它提供与服务器相同的方法。</p><p>它具有许多特性</p><ol><li><strong>强大的IDL特性</strong><br>RPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议，性能出众，得到了广泛的应用。</li><li>支持多种语言<br>支持C++、Java、Go、Python、Ruby、C#、Node.js、Android Java、Objective-C、PHP等编程语言。</li><li>基于<strong>HTTP/2</strong>标准设计</li></ol><p><img data-src="https://grpc.io/img/landing-2.svg" alt="Concept Diagram"></p><p>默认情况下，gRPC使用<strong>Protocol Buffers</strong>，这是谷歌成熟的开源机制，<strong>用于序列化结构化数据(尽管它可以与JSON等其他数据格式一起使用)</strong></p><h3 id="与REST差异"><a href="#与REST差异" class="headerlink" title="与REST差异"></a>与REST差异</h3><p>RPC 的消息传输可以通过 TCP、UDP 或者 HTTP等，所以有时候我们称之为 RPC over TCP、 RPC over HTTP。</p><p>RPC 通过 HTTP 传输消息的时候和 RESTful的架构是类似的，但是也有不同。</p><blockquote><ul><li>gRPC使用HTTP/2，而REST使用HTTP1.1</li><li>gRPC使用协议缓冲区数据格式，而不是REST API中通常使用的标准JSON数据格式</li><li>使用gRPC,可以利用HTTP/2功能，如服务器端流式传输、客户端流式传输，甚至双向流式传输</li></ul></blockquote><p>首先 RPC 的客户端和服务器端师紧耦合的，客户端需要知道调用的过程的名字，过程的参数以及它们的类型、顺序等。<strong>一旦服务器更改了过程的实现，客户端的实现很容易出问题</strong>。RESTful基于 http的语义操作资源，参数的顺序一般没有关系，也很容易的<strong>通过代理转换链接和资源位置</strong>，从这一点上来说，RESTful 更灵活。</p><p>其次，它们操作的对象不一样。 RPC 操作的是方法和过程，它要操作的是方法对象。 RESTful 操作的是资源(resource)，而不是方法。</p><p>第三，RESTful执行的是对资源的操作，增加、查找、修改和删除等,主要是CURD，所以如果你要实现一个特定目的的操作，比如为名字姓张的学生的数学成绩都加上10这样的操作，<br>RESTful的API设计起来就不是那么直观或者有意义。在这种情况下, RPC的实现更有意义，它可以实现一个直接的方法方法供客户端调用</p><p><strong>RPC over TCP可以通过长连接减少连接的建立所产生的花费</strong>，在调用次数非常巨大的时候(这是目前互联网公司经常遇到的情况,大并发的情况下)，这个花费影响是非常巨大的。<br>当然 RESTful 也可以通过 keep-alive 实现长连接, 但是它最大的一个问题是它的<strong>request-response模型是阻塞的</strong> (http1.0和 http1.1, http 2.0没这个问题)，<br>发送一个请求后只有等到response返回才能发送第二个请求 (有些http server实现了pipeling的功能，但不是标配), RPC的实现没有这个限制。</p><h3 id="其他RPC框架"><a href="#其他RPC框架" class="headerlink" title="其他RPC框架"></a>其他RPC框架</h3><p>目前的 RPC 框架大致有两种不同的侧重方向,一种<strong>偏重于服务治理</strong>,另一种<strong>偏重于跨语言调用</strong>。</p><p>服务治理型的 RPC 框架有Alibabs <strong>Dubbo</strong>、Motan 等，这类的 RPC 框架的特点是功能丰富，<strong>提供高性能的远程调用以及服务发现和治理功能</strong>，适用于大型服务的微服务化拆分以及管理，对于特定语言（Java）的项目可以十分友好的透明化接入。但缺点是语言耦合度较高，跨语言支持难度较大。</p><p>跨语言调用型的 RPC 框架有 Thrift、<strong>gRPC</strong>、Hessian、Finagle 等，<strong>这一类的 RPC 框架重点关注于服务的跨语言调用，能够支持大部分的语言进行语言无关的调用</strong>，非常适合于为不同语言提供通用远程服务的场景。但这类框架没有服务发现相关机制，实际使用时一般需要代理层进行请求转发和负载均衡策略控制。</p><p><a href="https://thrift.apache.org/">thrift</a>是Apache的一个跨语言的高性能的服务框架，也得到了广泛的应用。它的功能类似 gRPC, 支持跨语言，不支持服务治理。</p><p><a href="https://github.com/smallnest/rpcx">rpcx</a> 是一个分布式的Go语言的 RPC 框架，支持Zookepper、etcd、consul多种服务发现方式，多种服务路由方式， 是目前性能最好的 RPC 框架之一</p><h3 id="使用protobuf"><a href="#使用protobuf" class="headerlink" title="使用protobuf"></a>使用protobuf</h3><p>定义要在proto文件中序列化的数据的结构：这是一个扩展名为.proto的普通文本文件。协议缓冲区数据被构造为消息，其中每个消息都是一个包含一系列名值对（称为字段）的信息的小逻辑记录。</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="built_in">string</span> name = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">int32</span> id = <span class="number">2</span>;</span><br><span class="line">  <span class="built_in">bool</span> has_ponycopter = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>指定了数据结构,就可以<strong>使用协议缓冲区编译器protoc从proto定义中生成首选语言的数据访问类</strong>.</p><p>它们为每个字段提供了简单的访问器,如name()和set_name()，以及将整个结构序列化到原始字节/从原始字节解析整个结构的方法。</p><p>因此，例如，如果选择的语言是C++，那么在上面的示例中运行编译器将生成一个名为Person的类。然后，您可以在应用程序中使用此类来填充、序列化和检索Person协议缓冲区消息。</p><p>gRPC使用protoc和一个特殊的gRPC插件从proto文件中生成代码：<strong>可以获得生成的gRPC客户端和服务器代码，以及用于填充、序列化和检索消息类型的常规协议缓冲区代码。</strong>(截至目前protobuf最新版本是v3)</p><h2 id="gRPC-in-Go"><a href="#gRPC-in-Go" class="headerlink" title="gRPC in Go"></a>gRPC in Go</h2><h3 id="下载protoc"><a href="#下载protoc" class="headerlink" title="下载protoc"></a>下载protoc</h3><blockquote><p>虽然不是强制性的，但gRPC应用程序通常利用proto buffer进行服务定义和数据序列化。</p></blockquote><p><a href="https://github.com/protocolbuffers/protobuf/releases">Releases · protocolbuffers/protobuf (github.com)</a></p><p>protoc用于编译.proto文件，其中包含服务和消息定义,Linux或Mac直接使用对应包管理器下载即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install -y protobuf-compiler</span><br><span class="line">protoc --version  <span class="comment"># Ensure compiler version is 3+</span></span><br></pre></td></tr></table></figure><p>Windows在github上下载二进制包<a href="https://github.com/protocolbuffers/protobuf/releases">Releases · protocolbuffers/protobuf (github.com)</a></p><h3 id="protocol-compiler的Go插件"><a href="#protocol-compiler的Go插件" class="headerlink" title="protocol compiler的Go插件"></a>protocol compiler的Go插件</h3><h4 id="下载protoc-go-gen"><a href="#下载protoc-go-gen" class="headerlink" title="下载protoc-go-gen"></a>下载protoc-go-gen</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2</span><br><span class="line">go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28</span><br></pre></td></tr></table></figure><p>下载zip的文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b v1.60.1 --depth 1 https://github.com/grpc/grpc-go</span><br></pre></td></tr></table></figure><h4 id="proto文件"><a href="#proto文件" class="headerlink" title="proto文件"></a>proto文件</h4><p>下面定义了服务</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Interface exported by the server.</span></span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">RouteGuide</span> </span>&#123;</span><br><span class="line">  <span class="comment">// A simple RPC.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Obtains the feature at a given position.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// A feature with an empty name is returned if there&#x27;s no feature at the given</span></span><br><span class="line">  <span class="comment">// position.</span></span><br><span class="line">  <span class="function"><span class="keyword">rpc</span> GetFeature(Point) <span class="keyword">returns</span> (Feature) </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A server-to-client streaming RPC.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Obtains the Features available within the given Rectangle.  Results are</span></span><br><span class="line">  <span class="comment">// streamed rather than returned at once (e.g. in a response message with a</span></span><br><span class="line">  <span class="comment">// repeated field), as the rectangle may cover a large area and contain a</span></span><br><span class="line">  <span class="comment">// huge number of features.</span></span><br><span class="line">  <span class="function"><span class="keyword">rpc</span> ListFeatures(Rectangle) <span class="keyword">returns</span> (stream Feature) </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A client-to-server streaming RPC.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Accepts a stream of Points on a route being traversed, returning a</span></span><br><span class="line">  <span class="comment">// RouteSummary when traversal is completed.</span></span><br><span class="line">  <span class="function"><span class="keyword">rpc</span> RecordRoute(stream Point) <span class="keyword">returns</span> (RouteSummary) </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// A Bidirectional streaming RPC.</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// Accepts a stream of RouteNotes sent while a route is being traversed,</span></span><br><span class="line">  <span class="comment">// while receiving other RouteNotes (e.g. from other users).</span></span><br><span class="line">  <span class="function"><span class="keyword">rpc</span> RouteChat(stream RouteNote) <span class="keyword">returns</span> (stream RouteNote) </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中涉及到一些参数message表示传递数据.</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Points are represented as latitude-longitude pairs in the E7 representation</span></span><br><span class="line"><span class="comment">// (degrees multiplied by 10**7 and rounded to the nearest integer).</span></span><br><span class="line"><span class="comment">// Latitudes should be in the range +/- 90 degrees and longitude should be in</span></span><br><span class="line"><span class="comment">// the range +/- 180 degrees (inclusive).</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">  <span class="built_in">int32</span> latitude = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">int32</span> longitude = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A latitude-longitude rectangle, represented as two diagonally opposite</span></span><br><span class="line"><span class="comment">// points &quot;lo&quot; and &quot;hi&quot;.</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Rectangle</span> </span>&#123;</span><br><span class="line">  <span class="comment">// One corner of the rectangle.</span></span><br><span class="line">  Point lo = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The other corner of the rectangle.</span></span><br><span class="line">  Point hi = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A feature names something at a given point.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If a feature could not be named, the name is empty.</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Feature</span> </span>&#123;</span><br><span class="line">  <span class="comment">// The name of the feature.</span></span><br><span class="line">  <span class="built_in">string</span> name = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The point where the feature is detected.</span></span><br><span class="line">  Point location = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A RouteNote is a message sent while at a given point.</span></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">RouteNote</span> </span>&#123;</span><br><span class="line">  <span class="comment">// The location from which the message is sent.</span></span><br><span class="line">  Point location = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The message to be sent.</span></span><br><span class="line">  <span class="built_in">string</span> <span class="class"><span class="keyword">message</span> = 2;</span></span><br><span class="line"><span class="class">&#125;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">// <span class="title">A</span> RouteSummary is received in response to a RecordRoute rpc.</span></span><br><span class="line"><span class="class">//</span></span><br><span class="line"><span class="class">// It contains the number of individual points received, the number of</span></span><br><span class="line"><span class="class">// detected features, and the total distance covered as the cumulative sum of</span></span><br><span class="line"><span class="class">// the distance between each point.</span></span><br><span class="line"><span class="class">message RouteSummary </span>&#123;</span><br><span class="line">  <span class="comment">// The number of points received.</span></span><br><span class="line">  <span class="built_in">int32</span> point_count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The number of known features passed while traversing the route.</span></span><br><span class="line">  <span class="built_in">int32</span> feature_count = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The distance covered in metres.</span></span><br><span class="line">  <span class="built_in">int32</span> distance = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The duration of the traversal in seconds.</span></span><br><span class="line">  <span class="built_in">int32</span> elapsed_time = <span class="number">4</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生成的两个文件包括message实现和用于server,client的代码</p><p>在pb.go文件中对于每个message实现了其type,比如对于<code>Rectangle</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Rectangle <span class="keyword">struct</span> &#123;</span><br><span class="line">state         protoimpl.MessageState</span><br><span class="line">sizeCache     protoimpl.SizeCache</span><br><span class="line">unknownFields protoimpl.UnknownFields</span><br><span class="line"></span><br><span class="line"><span class="comment">// One corner of the rectangle.</span></span><br><span class="line">Lo *Point <span class="string">`protobuf:&quot;bytes,1,opt,name=lo,proto3&quot; json:&quot;lo,omitempty&quot;`</span></span><br><span class="line"><span class="comment">// The other corner of the rectangle.</span></span><br><span class="line">Hi *Point <span class="string">`protobuf:&quot;bytes,2,opt,name=hi,proto3&quot; json:&quot;hi,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *Rectangle)</span> <span class="title">Reset</span><span class="params">()</span></span> &#123;</span><br><span class="line">*x = Rectangle&#123;&#125;</span><br><span class="line"><span class="keyword">if</span> protoimpl.UnsafeEnabled &#123;</span><br><span class="line">mi := &amp;file_routeguide_route_guide_proto_msgTypes[<span class="number">1</span>]</span><br><span class="line">ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))</span><br><span class="line">ms.StoreMessageInfo(mi)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *Rectangle)</span> <span class="title">String</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> protoimpl.X.MessageStringOf(x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(*Rectangle)</span> <span class="title">ProtoMessage</span><span class="params">()</span></span> &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *Rectangle)</span> <span class="title">ProtoReflect</span><span class="params">()</span> <span class="title">protoreflect</span>.<span class="title">Message</span></span> &#123;</span><br><span class="line">mi := &amp;file_routeguide_route_guide_proto_msgTypes[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> protoimpl.UnsafeEnabled &amp;&amp; x != <span class="literal">nil</span> &#123;</span><br><span class="line">ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))</span><br><span class="line"><span class="keyword">if</span> ms.LoadMessageInfo() == <span class="literal">nil</span> &#123;</span><br><span class="line">ms.StoreMessageInfo(mi)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ms</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> mi.MessageOf(x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Deprecated: Use Rectangle.ProtoReflect.Descriptor instead.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(*Rectangle)</span> <span class="title">Descriptor</span><span class="params">()</span> <span class="params">([]<span class="keyword">byte</span>, []<span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> file_routeguide_route_guide_proto_rawDescGZIP(), []<span class="keyword">int</span>&#123;<span class="number">1</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *Rectangle)</span> <span class="title">GetLo</span><span class="params">()</span> *<span class="title">Point</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> x != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> x.Lo</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *Rectangle)</span> <span class="title">GetHi</span><span class="params">()</span> *<span class="title">Point</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> x != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> x.Hi</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于Client来说,定义接口.首先使用grpc库中的<code>grpc.ClientConnInterface</code>作为routeGuideClient的成员.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> routeGuideClient <span class="keyword">struct</span> &#123;</span><br><span class="line">cc grpc.ClientConnInterface</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>给结构体<code>routeGuideClient</code>实现多个方法,并定义接口</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> RouteGuideClient <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// A simple RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Obtains the feature at a given position.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// A feature with an empty name is returned if there&#x27;s no feature at the given</span></span><br><span class="line"><span class="comment">// position.</span></span><br><span class="line">GetFeature(ctx context.Context, in *Point, opts ...grpc.CallOption) (*Feature, error)</span><br><span class="line"><span class="comment">// A server-to-client streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Obtains the Features available within the given Rectangle.  Results are</span></span><br><span class="line"><span class="comment">// streamed rather than returned at once (e.g. in a response message with a</span></span><br><span class="line"><span class="comment">// repeated field), as the rectangle may cover a large area and contain a</span></span><br><span class="line"><span class="comment">// huge number of features.</span></span><br><span class="line">ListFeatures(ctx context.Context, in *Rectangle, opts ...grpc.CallOption) (RouteGuide_ListFeaturesClient, error)</span><br><span class="line"><span class="comment">// A client-to-server streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Accepts a stream of Points on a route being traversed, returning a</span></span><br><span class="line"><span class="comment">// RouteSummary when traversal is completed.</span></span><br><span class="line">RecordRoute(ctx context.Context, opts ...grpc.CallOption) (RouteGuide_RecordRouteClient, error)</span><br><span class="line"><span class="comment">// A Bidirectional streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Accepts a stream of RouteNotes sent while a route is being traversed,</span></span><br><span class="line"><span class="comment">// while receiving other RouteNotes (e.g. from other users).</span></span><br><span class="line">RouteChat(ctx context.Context, opts ...grpc.CallOption) (RouteGuide_RouteChatClient, error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后通过<code>NewRouteGuideClient</code>返回结构体</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewRouteGuideClient</span><span class="params">(cc grpc.ClientConnInterface)</span> <span class="title">RouteGuideClient</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> &amp;routeGuideClient&#123;cc&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现结构体的方法</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *routeGuideClient)</span> <span class="title">GetFeature</span><span class="params">(ctx context.Context, in *Point, opts ...grpc.CallOption)</span> <span class="params">(*Feature, error)</span></span> &#123;</span><br><span class="line">out := <span class="built_in">new</span>(Feature)</span><br><span class="line">err := c.cc.Invoke(ctx, RouteGuide_GetFeature_FullMethodName, in, out, opts...)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> out, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *routeGuideClient)</span> <span class="title">ListFeatures</span><span class="params">(ctx context.Context, in *Rectangle, opts ...grpc.CallOption)</span> <span class="params">(RouteGuide_ListFeaturesClient, error)</span></span> &#123;</span><br><span class="line">stream, err := c.cc.NewStream(ctx, &amp;RouteGuide_ServiceDesc.Streams[<span class="number">0</span>], RouteGuide_ListFeatures_FullMethodName, opts...)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">x := &amp;routeGuideListFeaturesClient&#123;stream&#125;</span><br><span class="line"><span class="keyword">if</span> err := x.ClientStream.SendMsg(in); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> err := x.ClientStream.CloseSend(); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> x, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果涉及stream还有新的结构,定义结构体,方法和接口</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">type</span> RouteGuide_ListFeaturesClient <span class="keyword">interface</span> &#123;</span><br><span class="line">Recv() (*Feature, error)</span><br><span class="line">grpc.ClientStream</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> routeGuideListFeaturesClient <span class="keyword">struct</span> &#123;</span><br><span class="line">grpc.ClientStream</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(x *routeGuideListFeaturesClient)</span> <span class="title">Recv</span><span class="params">()</span> <span class="params">(*Feature, error)</span></span> &#123;</span><br><span class="line">m := <span class="built_in">new</span>(Feature)</span><br><span class="line"><span class="keyword">if</span> err := x.ClientStream.RecvMsg(m); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> m, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于server类似,定义接口</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> RouteGuideServer <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// A simple RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Obtains the feature at a given position.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// A feature with an empty name is returned if there&#x27;s no feature at the given</span></span><br><span class="line"><span class="comment">// position.</span></span><br><span class="line">GetFeature(context.Context, *Point) (*Feature, error)</span><br><span class="line"><span class="comment">// A server-to-client streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Obtains the Features available within the given Rectangle.  Results are</span></span><br><span class="line"><span class="comment">// streamed rather than returned at once (e.g. in a response message with a</span></span><br><span class="line"><span class="comment">// repeated field), as the rectangle may cover a large area and contain a</span></span><br><span class="line"><span class="comment">// huge number of features.</span></span><br><span class="line">ListFeatures(*Rectangle, RouteGuide_ListFeaturesServer) error</span><br><span class="line"><span class="comment">// A client-to-server streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Accepts a stream of Points on a route being traversed, returning a</span></span><br><span class="line"><span class="comment">// RouteSummary when traversal is completed.</span></span><br><span class="line">RecordRoute(RouteGuide_RecordRouteServer) error</span><br><span class="line"><span class="comment">// A Bidirectional streaming RPC.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Accepts a stream of RouteNotes sent while a route is being traversed,</span></span><br><span class="line"><span class="comment">// while receiving other RouteNotes (e.g. from other users).</span></span><br><span class="line">RouteChat(RouteGuide_RouteChatServer) error</span><br><span class="line">mustEmbedUnimplementedRouteGuideServer()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="实现server和client"><a href="#实现server和client" class="headerlink" title="实现server和client"></a>实现server和client</h4><p>当使用protoc生成文件之后,就可以写server和client了.</p><h5 id="server"><a href="#server" class="headerlink" title="server"></a>server</h5><p>定义结构体,注意定义时加上<code>pb.UnimplementedRouteGuideServer</code>这样避免有些方法没有实现.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> routeGuideServer <span class="keyword">struct</span> &#123;</span><br><span class="line">pb.UnimplementedRouteGuideServer</span><br><span class="line">savedFeatures []*pb.Feature <span class="comment">// read-only after initialized</span></span><br><span class="line"></span><br><span class="line">mu         sync.Mutex <span class="comment">// protects routeNotes</span></span><br><span class="line">routeNotes <span class="keyword">map</span>[<span class="keyword">string</span>][]*pb.RouteNote</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现接口满足的方法.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *routeGuideServer)</span> <span class="title">GetFeature</span><span class="params">(ctx context.Context, point *pb.Point)</span> <span class="params">(*pb.Feature, error)</span></span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *routeGuideServer)</span> <span class="title">ListFeatures</span><span class="params">(rect *pb.Rectangle, stream pb.RouteGuide_ListFeaturesServer)</span></span> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *routeGuideServer)</span> <span class="title">RecordRoute</span><span class="params">(stream pb.RouteGuide_RecordRouteServer)</span></span> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *routeGuideServer)</span> <span class="title">RouteChat</span><span class="params">(stream pb.RouteGuide_RouteChatServer)</span></span></span><br></pre></td></tr></table></figure><p>然后使用tcp链接新建grpc服务</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.Parse()</span><br><span class="line">lis, err := net.Listen(<span class="string">&quot;tcp&quot;</span>, fmt.Sprintf(<span class="string">&quot;localhost:%d&quot;</span>, *port))</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;failed to listen: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> opts []grpc.ServerOption</span><br><span class="line"><span class="keyword">if</span> *tls &#123;</span><br><span class="line"><span class="keyword">if</span> *certFile == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">*certFile = data.Path(<span class="string">&quot;x509/server_cert.pem&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> *keyFile == <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line">*keyFile = data.Path(<span class="string">&quot;x509/server_key.pem&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line">creds, err := credentials.NewServerTLSFromFile(*certFile, *keyFile)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;Failed to generate credentials: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">opts = []grpc.ServerOption&#123;grpc.Creds(creds)&#125;</span><br><span class="line">&#125;</span><br><span class="line">grpcServer := grpc.NewServer(opts...)</span><br><span class="line">pb.RegisterRouteGuideServer(grpcServer, newServer())</span><br><span class="line">grpcServer.Serve(lis)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面使用<code>pb.RegisterRouteGuideServer</code>注册服务,参数分别是grpc服务和<code>newServer</code>返回的结构体</p><h5 id="client"><a href="#client" class="headerlink" title="client"></a>client</h5><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">printFeature</span><span class="params">(client pb.RouteGuideClient, point *pb.Point)</span></span> &#123;</span><br><span class="line">log.Printf(<span class="string">&quot;Getting feature for point (%d, %d)&quot;</span>, point.Latitude, point.Longitude)</span><br><span class="line">ctx, cancel := context.WithTimeout(context.Background(), <span class="number">10</span>*time.Second)</span><br><span class="line"><span class="keyword">defer</span> cancel()</span><br><span class="line">feature, err := client.GetFeature(ctx, point)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;client.GetFeature failed: %v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">log.Println(feature)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>利用context创建ctx和cancel,利用<code>client := pb.NewRouteGuideClient(conn)</code>创建client调用方法.</p><p><code>NewRouteGuideClient</code>返回对应的client接口</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewRouteGuideClient</span><span class="params">(cc grpc.ClientConnInterface)</span> <span class="title">RouteGuideClient</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> &amp;routeGuideClient&#123;cc&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="gRPC面临的挑战"><a href="#gRPC面临的挑战" class="headerlink" title="gRPC面临的挑战"></a>gRPC面临的挑战</h2><p>虽然gRPC确实允许您使用这些较新的技术，但gRPC服务的原型设计更具挑战性，因为<strong>无法使用Postman HTTP客户端等工具来轻松地与您公开的gRPC服务交互</strong>。你确实有一些选择可以实现这一点，但这并不是一种在本地就可以获得的东西。</p><p>有一些选项可以使用诸如特使之类的工具来反转代理标准JSON请求，并将其转码为正确的数据格式，但这是一个额外的依赖项，对于简单的项目来说，设置它可能很困难。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://grpc.io/docs/">Documentation | gRPC</a></li><li><a href="https://tutorialedge.net/golang/go-grpc-beginners-tutorial/">Go gRPC Beginners Tutorial | TutorialEdge.net</a></li><li><a href="https://www.bookstack.cn/read/go-rpc-programming-guide/part1-gorpc.md">Go RPC开发简介 - 官方RPC库 - 《Go RPC开发指南 [中文文档]》 - 书栈网 · BookStack</a></li><li><a href="http://ruanyifeng.com/blog/2018/10/restful-api-best-practices.html">RESTful API 最佳实践 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://grpc.io/docs/languages/go/basics/">Basics tutorial | Go | gRPC</a></li><li><a href="https://grpc.io/docs/what-is-grpc/">What is gRPC? | gRPC</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;RPC是远程调用,而google实现了grpc比较方便地实现了远程调用,gRPC是一个现代的开源远程过程调用(RPC)框架&lt;br&gt;</summary>
    
    
    
    
    <category term="grpc" scheme="https://www.sekyoro.top/tags/grpc/"/>
    
  </entry>
  
  <entry>
    <title>gradio学习</title>
    <link href="https://www.sekyoro.top/2023/12/25/gradio%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/12/25/gradio%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-12-25T13:12:10.000Z</published>
    <updated>2023-12-26T04:22:16.789Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Gradio好啊,好啊,好啊.Hugging Face好啊,好啊.<br><span id="more"></span></p><blockquote><p>Gradio是一个开源Python包，允许您为机器学习模型、API或任何任意Python函数快速构建演示或web应用程序。然后，您可以使用Gradio的内置共享功能，在几秒钟内共享演示或web应用程序的链接。无需JavaScript、CSS或网络托管经验！</p></blockquote><h2 id="Hot-reload"><a href="#Hot-reload" class="headerlink" title="Hot reload"></a>Hot reload</h2><p><a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Developing Faster With Reload Mode (gradio.app)</a></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradio run.py</span><br></pre></td></tr></table></figure><p>在使用重载模式时,Gradio专门在代码中寻找一个名为demo的Gradio Blocks/Interface演示。如果您将您的demo命名为其他名称，则需要将演示的名称作为代码中的第二个参数传入。所以，如果你的run.py文件是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks() <span class="keyword">as</span> my_demo:</span><br><span class="line">    gr.Markdown(<span class="string">&quot;# Greetings from Gradio!&quot;</span>)</span><br><span class="line">    inp = gr.Textbox(placeholder=<span class="string">&quot;What is your name?&quot;</span>)</span><br><span class="line">    out = gr.Textbox()</span><br><span class="line"></span><br><span class="line">    inp.change(fn=<span class="keyword">lambda</span> x: <span class="string">f&quot;Welcome, <span class="subst">&#123;x&#125;</span>!&quot;</span>,</span><br><span class="line">               inputs=inp,</span><br><span class="line">               outputs=out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    my_demo.launch()<span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks() <span class="keyword">as</span> my_demo:</span><br><span class="line">    gr.Markdown(<span class="string">&quot;# Greetings from Gradio!&quot;</span>)</span><br><span class="line">    inp = gr.Textbox(placeholder=<span class="string">&quot;What is your name?&quot;</span>)</span><br><span class="line">    out = gr.Textbox()</span><br><span class="line"></span><br><span class="line">    inp.change(fn=<span class="keyword">lambda</span> x: <span class="string">f&quot;Welcome, <span class="subst">&#123;x&#125;</span>!&quot;</span>,</span><br><span class="line">               inputs=inp,</span><br><span class="line">               outputs=out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    my_demo.launch()</span><br></pre></td></tr></table></figure><p>使用下面命令启动reload模式</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradio run.py my_demo.</span><br></pre></td></tr></table></figure><p>我发现开发时使用reload最好把launch放在<strong>name</strong> == “<strong>main</strong>“下<a href="https://github.com/gradio-app/gradio/issues/4755">Unable to launch with reload mode with default port · Issue #4755 · gradio-app/gradio (github.com)</a></p><h3 id="launch参数"><a href="#launch参数" class="headerlink" title="launch参数"></a>launch参数</h3><p>在reload模式下没用,开发完毕后可以使用,用于改变端口、获得公网地址用于分享等.</p><h2 id="Interface-Class"><a href="#Interface-Class" class="headerlink" title="Interface Class"></a><strong><code>Interface</code> Class</strong></h2><p>Interface类旨在为机器学习模型创建演示，这些模型接受一个或多个输入，并返回一个或更多输出.</p><p>Interface类有三个核心参数：</p><ul><li>fn：包装用户界面（UI）的函数</li><li>inputs：用于输入的Gradio组件。组件的数量应与函数中的参数数量相匹配。</li><li>outputs：用于输出的Gradio组件。组件的数量应该与函数返回值的数量相匹配。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">demo = gr.Interface(</span><br><span class="line">    fn=greet,</span><br><span class="line">    inputs=[gr.components.Textbox(placeholder=<span class="string">&quot;input your words&quot;</span>), gr.Textbox(placeholder=<span class="string">&quot;&quot;</span>),gr.components.Slider()],</span><br><span class="line">    outputs=[<span class="string">&quot;text&quot;</span>,gr.Checkbox(label=<span class="string">&quot;选择&quot;</span>)],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以多个输入,多个输出,输出由fn计算得到,但是貌似这只能构建一个单组件.</p><h2 id="Blocks"><a href="#Blocks" class="headerlink" title="Blocks"></a>Blocks</h2><blockquote><p>Blocks是Gradio的低级API，它允许您创建比Interfaces更多的自定义web应用程序和演示（但仍然完全使用Python）。</p></blockquote><p>与Interface类相比，Blocks提供了更多的灵活性和控制：</p><p>（1）组件的布局（</p><p>2）触发功能执行的事件</p><p>（3）数据流（例如，输入可以触发输出，这可以触发下一级的输出）</p><p>Blocks还提供了将相关演示分组在一起的方法，例如使用选项卡。块的基本用法如下：创建一个块对象，然后将其用作上下文（使用“with”语句），然后在块上下文中定义布局、组件或事件。最后，调用launch（）方法来启动演示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Hello&quot;</span> + name + <span class="string">&quot;!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks() <span class="keyword">as</span> demo:</span><br><span class="line">    gr.Markdown(<span class="string">&quot;## Hello World&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> gr.Row():</span><br><span class="line">        textbox = gr.Textbox(placeholder=<span class="string">&quot;input your words&quot;</span>)</span><br><span class="line">        slider = gr.components.Slider()</span><br><span class="line">    btn = gr.Button(<span class="string">&quot;Run&quot;</span>)</span><br><span class="line">    btn.click(fn=update,<span class="built_in">input</span>=textbox,output=slider)</span><br><span class="line">demo.launch()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increase</span>(<span class="params">num</span>):</span></span><br><span class="line">    <span class="keyword">return</span> num + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks() <span class="keyword">as</span> demo:</span><br><span class="line">    a = gr.Number(label=<span class="string">&quot;a&quot;</span>)</span><br><span class="line">    b = gr.Number(label=<span class="string">&quot;b&quot;</span>)</span><br><span class="line">    atob = gr.Button(<span class="string">&quot;a &gt; b&quot;</span>)</span><br><span class="line">    btoa = gr.Button(<span class="string">&quot;b &gt; a&quot;</span>)</span><br><span class="line">    atob.click(increase, a, b)</span><br><span class="line">    btoa.click(increase, b, a)</span><br><span class="line"></span><br><span class="line">demo.launch()</span><br></pre></td></tr></table></figure><h2 id="TabbedInterface"><a href="#TabbedInterface" class="headerlink" title="TabbedInterface"></a>TabbedInterface</h2><p>TabbedInterface是通过提供一个接口列表来创建的，每个接口都在一个单独的选项卡中呈现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Hello&quot;</span> + name + <span class="string">&quot;!&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> gr.Blocks(theme=gr.themes.Glass()) <span class="keyword">as</span> test:</span><br><span class="line">    gr.Markdown(<span class="string">&quot;## Hello World&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> gr.Row():</span><br><span class="line">        textbox = gr.Textbox(placeholder=<span class="string">&quot;input your words&quot;</span>,label=<span class="string">&quot;name&quot;</span>)</span><br><span class="line">        slider = gr.components.Slider(label=<span class="string">&quot;Greet&quot;</span>,interactive=<span class="literal">True</span>)</span><br><span class="line">    btn = gr.Button(<span class="string">&quot;Run&quot;</span>)</span><br><span class="line">    btn.click(fn=update, inputs=textbox, outputs=slider)</span><br><span class="line"></span><br><span class="line">stt_demo = gr.load(</span><br><span class="line">    <span class="string">&quot;huggingface/facebook/wav2vec2-base-960h&quot;</span>,</span><br><span class="line">    title=<span class="literal">None</span>,</span><br><span class="line">    inputs=<span class="string">&quot;mic&quot;</span>,</span><br><span class="line">    description=<span class="string">&quot;Let me try to guess what you&#x27;re saying!&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">demo = gr.TabbedInterface([stt_demo,test],[<span class="string">&quot;STT&quot;</span>,<span class="string">&quot;Hello World&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo.launch()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="ChatInterface"><a href="#ChatInterface" class="headerlink" title="ChatInterface"></a>ChatInterface</h2><blockquote><p>聊天机器人是大型语言模型的一个流行应用程序。使用gradio，您可以轻松地构建聊天机器人模型的演示并与用户共享，或者使用直观的聊天机器人UI自己尝试。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> gradio <span class="keyword">as</span> gr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_response</span>(<span class="params">message, history</span>):</span></span><br><span class="line"> <span class="keyword">return</span> random.choice([<span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>])</span><br><span class="line"></span><br><span class="line">gr.ChatInterface(random_response).launch()</span><br></pre></td></tr></table></figure><h3 id="streaming"><a href="#streaming" class="headerlink" title="streaming"></a>streaming</h3><p>如果应用程序预计流量会很大，请使用queue（）方法来控制处理速率。</p><p>可以搭配Openai或者Hugging Face上的大语言模型使用.同时搭配LangChain使用.</p><p>上面就是基本的四个大模块,此外还有许多组件,重点是<strong>一些组件如何组合</strong>,一般来说使用<code>gr.Blocks</code>进行构建.</p><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><ol><li><a href="https://www.gradio.app/guides">Gradio Guides</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Gradio好啊,好啊,好啊.Hugging Face好啊,好啊.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>dpsc:深度学习短课程学习</title>
    <link href="https://www.sekyoro.top/2023/12/24/dpsc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%AD%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.sekyoro.top/2023/12/24/dpsc-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%AD%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-12-24T12:55:08.000Z</published>
    <updated>2023-12-29T10:39:45.783Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Andrew Ng的Deep Learning短课程<a href="https://www.deeplearning.ai/short-courses/">Short Courses | Learn Generative AI from DeepLearning.AI</a>,此外还有Cousera上的课程.学的东西比较实用还比较新.</p><span id="more"></span><p>这些课程通常会使用一些公司的产品,比如<strong>Hugging Face</strong>的Gradio,diffusers,transformers,或者W&amp;B的wandb等等(这两个我平常都在用),此外还有谷歌、微软以及Langchain,这些工具都比较实用. 如果关注生成领域,那Diffusion Model肯定要看,如果关注LLM以及chatbot那<strong>Langchain</strong>最好利用起来,如果自己训练部署模型,那也可以使用<strong>wandb</strong>.</p><p>这里我主要关注三部分:<strong>生成式人工智能</strong>,<strong>LLM</strong>,<strong>模型部署和训练辅助工具</strong>.</p><h2 id="Reinforcement-Learning-from-Human-Feedback"><a href="#Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Reinforcement Learning from Human Feedback"></a>Reinforcement Learning from Human Feedback</h2><p><img data-src="https://i.imgur.com/81KDzqo.png" alt="image-20231224221652027"></p><p><img data-src="https://i.imgur.com/TpXVnGU.png" alt="image-20231224231207302"></p><h2 id="Evaluating-and-Debugging-Generative-AI-Models-Using-Weights-and-Biases"><a href="#Evaluating-and-Debugging-Generative-AI-Models-Using-Weights-and-Biases" class="headerlink" title="Evaluating and Debugging Generative AI Models Using Weights and Biases"></a>Evaluating and Debugging Generative AI Models Using Weights and Biases</h2><p>作为模型训练者可能会用到的网站.文档<a href="https://docs.wandb.ai/ref/python/">Python Library | Weights &amp; Biases Documentation (wandb.ai)</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wandb.init(</span><br><span class="line">    project=<span class="string">&quot;gpt5&quot;</span>,</span><br><span class="line">    config=config,</span><br><span class="line">)</span><br><span class="line">wandb.log(metrics)</span><br></pre></td></tr></table></figure><p>首先找到项目(如果没有就会另外创建),并且会根据config创建一个run.使用wandb.log输出最后结果.wandb会保存运行时系统环境信息,github repo甚至仓库文件信息.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">config</span>):</span></span><br><span class="line">    <span class="string">&quot;Train a model with a given config&quot;</span></span><br><span class="line">    </span><br><span class="line">    wandb.init(</span><br><span class="line">        project=<span class="string">&quot;gpt5&quot;</span>,</span><br><span class="line">        config=config,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the data</span></span><br><span class="line">    train_dl, valid_dl = get_dataloaders(DATA_DIR, </span><br><span class="line">                                         config.batch_size, </span><br><span class="line">                                         config.slice_size, </span><br><span class="line">                                         config.valid_pct)</span><br><span class="line">    n_steps_per_epoch = math.ceil(<span class="built_in">len</span>(train_dl.dataset) / config.batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># A simple MLP model</span></span><br><span class="line">    model = get_model(config.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make the loss and optimizer</span></span><br><span class="line">    loss_func = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=config.lr)</span><br><span class="line"></span><br><span class="line">    example_ct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(config.epochs), total=config.epochs):</span><br><span class="line">        model.train()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dl):</span><br><span class="line">            images, labels = images.to(DEVICE), labels.to(DEVICE)</span><br><span class="line"></span><br><span class="line">            outputs = model(images)</span><br><span class="line">            train_loss = loss_func(outputs, labels)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            train_loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            example_ct += <span class="built_in">len</span>(images)</span><br><span class="line">            metrics = &#123;</span><br><span class="line">                <span class="string">&quot;train/train_loss&quot;</span>: train_loss,</span><br><span class="line">                <span class="string">&quot;train/epoch&quot;</span>: epoch + <span class="number">1</span>,</span><br><span class="line">                <span class="string">&quot;train/example_ct&quot;</span>: example_ct</span><br><span class="line">            &#125;</span><br><span class="line">            wandb.log(metrics)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Compute validation metrics, log images on last epoch</span></span><br><span class="line">        val_loss, accuracy = validate_model(model, valid_dl, loss_func)</span><br><span class="line">        <span class="comment"># Compute train and validation metrics</span></span><br><span class="line">        val_metrics = &#123;</span><br><span class="line">            <span class="string">&quot;val/val_loss&quot;</span>: val_loss,</span><br><span class="line">            <span class="string">&quot;val/val_accuracy&quot;</span>: accuracy</span><br><span class="line">        &#125;</span><br><span class="line">        wandb.log(val_metrics)</span><br><span class="line">    </span><br><span class="line">    wandb.finish()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>如何在同一个run更改,更新现有运行的配置.下面是我匿名使用wandb跑的一次run</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">api = wandb.<span class="built_in">Api</span>()</span><br><span class="line"></span><br><span class="line">run = api.<span class="built_in">run</span>(<span class="string">&quot;anony-mouse-988582345570149472/gpt5/&lt;run_id&gt;&quot;</span>)</span><br><span class="line">run.config[<span class="string">&quot;key&quot;</span>] = updated_value</span><br><span class="line">run.<span class="built_in">update</span>()</span><br></pre></td></tr></table></figure><p>将单个运行的指标导出到CSV文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">api = wandb.Api()</span><br><span class="line"></span><br><span class="line"><span class="comment"># run is specified by &lt;entity&gt;/&lt;project&gt;/&lt;run_id&gt;</span></span><br><span class="line">run = api.run(<span class="string">&quot;anony-mouse-946987442323310233/dlai_sprite_diffusion/&lt;run_id&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save the metrics for the run to a csv file</span></span><br><span class="line">metrics_dataframe = run.history()</span><br><span class="line">metrics_dataframe.to_csv(<span class="string">&quot;metrics.csv&quot;</span>)</span><br></pre></td></tr></table></figure><p>读取运行指标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">api = wandb.Api()</span><br><span class="line"></span><br><span class="line">run = api.run(<span class="string">&quot;anony-mouse-946987442323310233/dlai_sprite_diffusion/&lt;run_id&gt;&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> run.state == <span class="string">&quot;finished&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> i, row <span class="keyword">in</span> run.history().iterrows():</span><br><span class="line">      <span class="built_in">print</span>(row[<span class="string">&quot;_timestamp&quot;</span>], row[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure><p>当您从历史中提取数据时，默认情况下会对其采样到500点。使用run.scan_history（）获取所有记录的数据点。下面是下载所有记录在历史中的丢失数据点的示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">api = wandb.Api()</span><br><span class="line"></span><br><span class="line">run = api.run(<span class="string">&quot;anony-mouse-946987442323310233/dlai_sprite_diffusion/&lt;run_id&gt;&quot;</span>)</span><br><span class="line">history = run.scan_history()</span><br><span class="line">losses = [row[<span class="string">&quot;loss&quot;</span>] <span class="keyword">for</span> row <span class="keyword">in</span> history]</span><br></pre></td></tr></table></figure><h3 id="wandb-Table"><a href="#wandb-Table" class="headerlink" title="wandb Table"></a>wandb Table</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table = wandb.Table(columns=[<span class="string">&quot;input_noise&quot;</span>, <span class="string">&quot;ddpm&quot;</span>, <span class="string">&quot;ddim&quot;</span>, <span class="string">&quot;class&quot;</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> noise, ddpm_s, ddim_s, c <span class="keyword">in</span> <span class="built_in">zip</span>(noises, </span><br><span class="line">                                    ddpm_samples, </span><br><span class="line">                                    ddim_samples, </span><br><span class="line">                                    to_classes(ctx_vector)):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># add data row by row to the Table</span></span><br><span class="line">    table.add_data(wandb.Image(noise),</span><br><span class="line">                   wandb.Image(ddpm_s), </span><br><span class="line">                   wandb.Image(ddim_s),</span><br><span class="line">                   c)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> wandb.init(project=<span class="string">&quot;dlai_sprite_diffusion&quot;</span>, </span><br><span class="line">                job_type=<span class="string">&quot;samplers_battle&quot;</span>, </span><br><span class="line">                config=config):</span><br><span class="line">    </span><br><span class="line">    wandb.log(&#123;<span class="string">&quot;samplers_table&quot;</span>:table&#125;)</span><br></pre></td></tr></table></figure><p>先创建<code>wandb.Table</code>,再使用其添加数据,最后使用log推上去</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">table = wandb.Table(columns=[<span class="string">&quot;prompt&quot;</span>, <span class="string">&quot;generation&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> prompt <span class="keyword">in</span> prompts:</span><br><span class="line">    input_ids = tokenizer.encode(prefix + prompt, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    output = model.generate(input_ids, do_sample=<span class="literal">True</span>, max_new_tokens=<span class="number">50</span>, top_p=<span class="number">0.3</span>)</span><br><span class="line">    output_text = tokenizer.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    table.add_data(prefix + prompt, output_text)</span><br><span class="line">    </span><br><span class="line">wandb.log(&#123;<span class="string">&#x27;tiny_generations&#x27;</span>: table&#125;)</span><br></pre></td></tr></table></figure><h3 id="使用W-amp-B-sweep进行超参搜索调整"><a href="#使用W-amp-B-sweep进行超参搜索调整" class="headerlink" title="使用W&amp;B sweep进行超参搜索调整"></a>使用W&amp;B sweep进行超参搜索调整</h3><blockquote><p>在高维超参数空间中搜索以找到最具性能的模型可能会变得非常困难。超参数扫描提供了一种有组织、高效的方式来进行一系列模型的战斗，并选择最准确的模型。它们通过自动搜索超参数值的组合（例如学习率、批量大小、隐藏层的数量、优化器类型）来找到最优化的值。Sweep结合了一种尝试一堆超参数值的策略和评估代码</p></blockquote><ol><li>定义sweep：通过创建一个字典或YAML文件来实现这一点，该文件指定了要搜索的参数、搜索策略、优化指标等。</li></ol><p>首先选择搜索策略,包括网格,随机和贝叶斯.</p><blockquote><ul><li><strong><code>grid</code> Search</strong> – Iterate over every combination of hyperparameter values. Very effective, but can be computationally costly.</li><li><strong><code>random</code> Search</strong> – Select each new combination at random according to provided <code>distribution</code>s. Surprisingly effective!</li><li><strong><code>bayes</code>ian Search</strong> – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sweep_config = &#123;</span><br><span class="line">    <span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;random&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">metric = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;goal&#x27;</span>: <span class="string">&#x27;minimize&#x27;</span>   </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sweep_config[<span class="string">&#x27;metric&#x27;</span>] = metric</span><br></pre></td></tr></table></figure><p>然后是训练网络的一些超参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">parameters_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;values&#x27;</span>: [<span class="string">&#x27;adam&#x27;</span>, <span class="string">&#x27;sgd&#x27;</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">    <span class="string">&#x27;fc_layer_size&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;values&#x27;</span>: [<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">    <span class="string">&#x27;dropout&#x27;</span>: &#123;</span><br><span class="line">          <span class="string">&#x27;values&#x27;</span>: [<span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">sweep_config[<span class="string">&#x27;parameters&#x27;</span>] = parameters_dict</span><br></pre></td></tr></table></figure><p>通常情况下，有些超参数我们不想在这次扫描中发生变化，但我们仍然想在扫描_配置中设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters_dict.update(&#123;</span><br><span class="line">    <span class="string">&#x27;epochs&#x27;</span>: &#123;</span><br><span class="line">        <span class="string">&#x27;value&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><p><code>`rand</code>搜索策略可以指定一个正态分布进行选参数,默认是均匀分布.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">parameters_dict.update(&#123;</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: &#123;</span><br><span class="line">        <span class="comment"># a flat distribution between 0 and 0.1</span></span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;min&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">        <span class="string">&#x27;max&#x27;</span>: <span class="number">0.1</span></span><br><span class="line">      &#125;,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: &#123;</span><br><span class="line">        <span class="comment"># integers between 32 and 256</span></span><br><span class="line">        <span class="comment"># with evenly-distributed logarithms </span></span><br><span class="line">        <span class="string">&#x27;distribution&#x27;</span>: <span class="string">&#x27;q_log_uniform_values&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;q&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&#x27;min&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">&#x27;max&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure><p>所以参数包括<code>metho</code>,<code>metric</code>和<code>parameter</code>.此外还有一些这里就不介绍了</p><ol><li>初始化扫描：用一行代码初始化扫描并传入扫描配置字典：sweep_id=wandb.sweep（sweep_config）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sweep_id = wandb.sweep(sweep_config, project=<span class="string">&quot;pytorch-sweeps-demo&quot;</span>)</span><br></pre></td></tr></table></figure><p>创建一个sweep</p><ol><li>运行扫描代理：也可以用一行代码完成，我们调用wandb.agent（）并传递要运行的sweep_id，以及一个定义模型架构并对其进行训练的函数：wandb.agent（sweep_id，function=train）</li></ol><p>开始正常的训练.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">config=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># Initialize a new wandb run</span></span><br><span class="line">    <span class="keyword">with</span> wandb.init(config=config):</span><br><span class="line">        <span class="comment"># If called by wandb.agent, as below,</span></span><br><span class="line">        <span class="comment"># this config will be set by Sweep Controller</span></span><br><span class="line">        config = wandb.config</span><br><span class="line"></span><br><span class="line">        loader = build_dataset(config.batch_size)</span><br><span class="line">        network = build_network(config.fc_layer_size, config.dropout)</span><br><span class="line">        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config.epochs):</span><br><span class="line">            avg_loss = train_epoch(network, loader, optimizer)</span><br><span class="line">            wandb.log(&#123;<span class="string">&quot;loss&quot;</span>: avg_loss, <span class="string">&quot;epoch&quot;</span>: epoch&#125;)           </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wandb.agent(sweep_id, train, count=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>使用Sweep Controller返回的随机生成的超参数值，启动一个运行训练5次的agents</p><p><img data-src="https://s2.loli.net/2023/12/29/2EruyQMvpSNoeRm.png" alt="image-20231229183908912" style="zoom:67%;" /></p><p>另外课程还讲了Tracer等,现在我用不上….主要还是上传loss和acc这些结果.</p><h2 id="How-Diffusion-Models-Work"><a href="#How-Diffusion-Models-Work" class="headerlink" title="How Diffusion Models Work"></a>How Diffusion Models Work</h2><p>Diffusion Models在前段时间非常火,也是现在prompt生成图像的主要模型.</p><p><img data-src="https://i.imgur.com/MkB11s7.png" alt="image-20231225195441214"></p><p><img data-src="https://i.imgur.com/lm5t6Jr.png" alt="image-20231225200851364"></p><p><img data-src="https://s2.loli.net/2023/12/28/LQaYoIzTJWd1yNs.png" alt="image-20231228224804703"></p><p><img data-src="https://s2.loli.net/2023/12/28/G7JTpmDg4zF1neq.png" alt="image-20231228232748110"></p><p>训练的预测噪声网络是Unet</p><p><img data-src="https://s2.loli.net/2023/12/29/8VAft1WN9L6lrmG.png" alt="image-20231229151559530"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContextUnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, n_feat=<span class="number">256</span>, n_cfeat=<span class="number">10</span>, height=<span class="number">28</span></span>):</span>  <span class="comment"># cfeat - context features</span></span><br><span class="line">        <span class="built_in">super</span>(ContextUnet, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># number of input channels, number of intermediate feature maps and number of classes</span></span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.n_feat = n_feat</span><br><span class="line">        self.n_cfeat = n_cfeat</span><br><span class="line">        self.h = height  <span class="comment">#assume h == w. must be divisible by 4, so 28,24,20,16...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the initial convolutional layer</span></span><br><span class="line">        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the down-sampling path of the U-Net with two levels</span></span><br><span class="line">        self.down1 = UnetDown(n_feat, n_feat)        <span class="comment"># down1 #[10, 256, 8, 8]</span></span><br><span class="line">        self.down2 = UnetDown(n_feat, <span class="number">2</span> * n_feat)    <span class="comment"># down2 #[10, 256, 4,  4]</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment"># original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())</span></span><br><span class="line">        self.to_vec = nn.Sequential(nn.AvgPool2d((<span class="number">4</span>)), nn.GELU())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embed the timestep and context labels with a one-layer fully connected neural network</span></span><br><span class="line">        self.timeembed1 = EmbedFC(<span class="number">1</span>, <span class="number">2</span>*n_feat)</span><br><span class="line">        self.timeembed2 = EmbedFC(<span class="number">1</span>, <span class="number">1</span>*n_feat)</span><br><span class="line">        self.contextembed1 = EmbedFC(n_cfeat, <span class="number">2</span>*n_feat)</span><br><span class="line">        self.contextembed2 = EmbedFC(n_cfeat, <span class="number">1</span>*n_feat)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the up-sampling path of the U-Net with three levels</span></span><br><span class="line">        self.up0 = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">2</span> * n_feat, <span class="number">2</span> * n_feat, self.h//<span class="number">4</span>, self.h//<span class="number">4</span>), <span class="comment"># up-sample  </span></span><br><span class="line">            nn.GroupNorm(<span class="number">8</span>, <span class="number">2</span> * n_feat), <span class="comment"># normalize                       </span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.up1 = UnetUp(<span class="number">4</span> * n_feat, n_feat)</span><br><span class="line">        self.up2 = UnetUp(<span class="number">2</span> * n_feat, n_feat)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the final convolutional layers to map to the same number of channels as the input image</span></span><br><span class="line">        self.out = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">2</span> * n_feat, n_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0</span></span><br><span class="line">            nn.GroupNorm(<span class="number">8</span>, n_feat), <span class="comment"># normalize</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(n_feat, self.in_channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="comment"># map to same number of channels as input</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t, c=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x : (batch, n_feat, h, w) : input image</span></span><br><span class="line"><span class="string">        t : (batch, n_cfeat)      : time step</span></span><br><span class="line"><span class="string">        c : (batch, n_classes)    : context label</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass the input image through the initial convolutional layer</span></span><br><span class="line">        x = self.init_conv(x)</span><br><span class="line">        <span class="comment"># pass the result through the down-sampling path</span></span><br><span class="line">        down1 = self.down1(x)       <span class="comment">#[10, 256, 8, 8]</span></span><br><span class="line">        down2 = self.down2(down1)   <span class="comment">#[10, 256, 4, 4]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># convert the feature maps to a vector and apply an activation</span></span><br><span class="line">        hiddenvec = self.to_vec(down2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># mask out context if context_mask == 1</span></span><br><span class="line">        <span class="keyword">if</span> c <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            c = torch.zeros(x.shape[<span class="number">0</span>], self.n_cfeat).to(x)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># embed context and timestep</span></span><br><span class="line">        cemb1 = self.contextembed1(c).view(-<span class="number">1</span>, self.n_feat * <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)     <span class="comment"># (batch, 2*n_feat, 1,1)</span></span><br><span class="line">        temb1 = self.timeembed1(t).view(-<span class="number">1</span>, self.n_feat * <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        cemb2 = self.contextembed2(c).view(-<span class="number">1</span>, self.n_feat, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        temb2 = self.timeembed2(t).view(-<span class="number">1</span>, self.n_feat, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#print(f&quot;uunet forward: cemb1 &#123;cemb1.shape&#125;. temb1 &#123;temb1.shape&#125;, cemb2 &#123;cemb2.shape&#125;. temb2 &#123;temb2.shape&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        up1 = self.up0(hiddenvec)</span><br><span class="line">        up2 = self.up1(cemb1*up1 + temb1, down2)  <span class="comment"># add and multiply embeddings</span></span><br><span class="line">        up3 = self.up2(cemb2*up2 + temb2, down1)</span><br><span class="line">        out = self.out(torch.cat((up3, x), <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练的时候是random一个timestamp得到噪声,sampling的时候是根据步数进行依次sampling</p><h3 id="control-and-speed-up"><a href="#control-and-speed-up" class="headerlink" title="control and speed up"></a>control and speed up</h3><p>加入context_vector进行控制输出,使用DDIM替换DDPM进行加速samping</p><p><img data-src="https://s2.loli.net/2023/12/29/MAifZF7OlmPtbWs.png" alt="image-20231229152240050"></p><p><img data-src="https://s2.loli.net/2023/12/29/hqbUoyE9iRpdgNA.png" alt="image-20231229152302772"></p><p><img data-src="https://s2.loli.net/2023/12/29/jLaNo19nuCwDJQG.png" alt="image-20231229152531542"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define sampling function for DDIM   </span></span><br><span class="line"><span class="comment"># removes the noise using ddim</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">denoise_ddim</span>(<span class="params">x, t, t_prev, pred_noise</span>):</span></span><br><span class="line">    ab = ab_t[t]</span><br><span class="line">    ab_prev = ab_t[t_prev]</span><br><span class="line">    </span><br><span class="line">    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (<span class="number">1</span> - ab).sqrt() * pred_noise)</span><br><span class="line">    dir_xt = (<span class="number">1</span> - ab_prev).sqrt() * pred_noise</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x0_pred + dir_xt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fast sampling algorithm with context</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_ddim_context</span>(<span class="params">n_sample, context, n=<span class="number">20</span></span>):</span></span><br><span class="line">    <span class="comment"># x_T ~ N(0, 1), sample initial noise</span></span><br><span class="line">    samples = torch.randn(n_sample, <span class="number">3</span>, height, height).to(device)  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># array to keep track of generated steps for plotting</span></span><br><span class="line">    intermediate = [] </span><br><span class="line">    step_size = timesteps // n</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(timesteps, <span class="number">0</span>, -step_size):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;sampling timestep <span class="subst">&#123;i:3d&#125;</span>&#x27;</span>, end=<span class="string">&#x27;\r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reshape time tensor</span></span><br><span class="line">        t = torch.tensor([i / timesteps])[:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>].to(device)</span><br><span class="line"></span><br><span class="line">        eps = nn_model(samples, t, c=context)    <span class="comment"># predict noise e_(x_t,t)</span></span><br><span class="line">        samples = denoise_ddim(samples, i, i - step_size, eps)</span><br><span class="line">        intermediate.append(samples.detach().cpu().numpy())</span><br><span class="line"></span><br><span class="line">    intermediate = np.stack(intermediate)</span><br><span class="line">    <span class="keyword">return</span> samples, intermediate</span><br></pre></td></tr></table></figure><h2 id="ChatGPT-Prompt-Engineering-for-Developers"><a href="#ChatGPT-Prompt-Engineering-for-Developers" class="headerlink" title="ChatGPT Prompt Engineering for Developers"></a>ChatGPT Prompt Engineering for Developers</h2><p>使用ChatGPT辅助,感觉已经成为现代社会一种普通工具了</p><p><img data-src="https://s2.loli.net/2023/12/28/PmOSaTEFH8pVuqe.png" alt="image-20231228232329050"></p><p><img data-src="https://s2.loli.net/2023/12/28/sh6MfqxQEUnuG5j.png" alt="image-20231228232417399"></p><h2 id="Finetuning-Large-Language-Models"><a href="#Finetuning-Large-Language-Models" class="headerlink" title="Finetuning Large Language Models"></a>Finetuning Large Language Models</h2><p>大模型的finetune,了解原理即可</p><p><img data-src="https://s2.loli.net/2023/12/28/zyAIapuUNi9EeSw.png" alt="image-20231228232508902"></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Andrew Ng的Deep Learning短课程&lt;a href=&quot;https://www.deeplearning.ai/short-courses/&quot;&gt;Short Courses | Learn Generative AI from DeepLearning.AI&lt;/a&gt;,此外还有Cousera上的课程.学的东西比较实用还比较新.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Github_bot创建</title>
    <link href="https://www.sekyoro.top/2023/12/24/Github-bot%E5%88%9B%E5%BB%BA/"/>
    <id>https://www.sekyoro.top/2023/12/24/Github-bot%E5%88%9B%E5%BB%BA/</id>
    <published>2023-12-24T06:50:24.000Z</published>
    <updated>2023-12-24T07:15:26.734Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在看一些开源项目时,会发现一些帮忙处理issue和PR的bot,这些bot都是基于Github的Apps<a href="https://docs.github.com/en/apps/overview">GitHub Apps overview - GitHub Docs</a></p><span id="more"></span><h2 id="GitHub-Apps"><a href="#GitHub-Apps" class="headerlink" title="GitHub Apps"></a>GitHub Apps</h2><blockquote><p>GitHub应用程序是扩展GitHub功能的工具。GitHub应用程序可以在GitHub上做一些事情，比如打开问题、评论拉取请求和管理项目。他们也可以根据GitHub上发生的事件在GitHub之外做事情。例如，当在GitHub上打开问题时，GitHub应用程序可以在Slack上发布。</p></blockquote><p>可以在<a href="https://github.com/marketplace">GitHub Marketplace</a>上查找Github Apps,然后进行安装,有些是需要付费的.</p><p>关于使用直接安装然后看文档进行配置就行了。</p><h3 id="如何开发"><a href="#如何开发" class="headerlink" title="如何开发"></a>如何开发</h3><p><a href="https://github.com/github/github-app-js-sample?tab=readme-ov-file">github/github-app-js-sample: Sample of a GitHub App that comments new pull requests</a></p><p><img data-src="https://i.imgur.com/trK7YAv.png" alt="image-20231224150632938"></p><p>由于本地开发涉及到需要接受github发来的东西,需要涉及到内网穿透啥的,推荐使用smee或者ngrok进行本地开发.建议搭配下面介绍的probot进行开发.<a href="https://probot.github.io/docs/development/#installing-the-app-on-a-repository">probot.github.io/docs/development/#installing-the-app-on-a-repository</a></p><h2 id="Probot"><a href="#Probot" class="headerlink" title="Probot"></a>Probot</h2><blockquote><p>Probot是一个在Node.js中构建GitHub应用程序的框架。它旨在消除所有的繁琐工作，比如接收和验证Webhook，以及进行身份验证倒立，这样你就可以专注于你想要构建的功能。Probet应用程序易于编写、部署和共享。许多最流行的Probet应用程序都是托管的，所以没有什么可供您部署和管理的。</p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = <span class="function">(<span class="params">app</span>) =&gt;</span> &#123;</span><br><span class="line">  app.on(<span class="string">&quot;issues.opened&quot;</span>, <span class="keyword">async</span> (context) =&gt; &#123;</span><br><span class="line">    <span class="keyword">const</span> issueComment = context.issue(&#123;</span><br><span class="line">      <span class="attr">body</span>: <span class="string">&quot;Thanks for opening this issue!&quot;</span>,</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="keyword">return</span> context.octokit.issues.createComment(issueComment);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  app.onAny(<span class="keyword">async</span> (context) =&gt; &#123;</span><br><span class="line">    context.log.info(&#123; <span class="attr">event</span>: context.name, <span class="attr">action</span>: context.payload.action &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  app.onError(<span class="keyword">async</span> (error) =&gt; &#123;</span><br><span class="line">    app.log.error(error);</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="Repo-Automation-Bots"><a href="#Repo-Automation-Bots" class="headerlink" title="Repo Automation Bots"></a>Repo Automation Bots</h2><p><a href="https://github.com/googleapis/repo-automation-bots?tab=readme-ov-file">googleapis/repo-automation-bots: A collection of bots, based on probot, for performing common maintenance tasks across the open-source repos managed by Google on GitHub.</a>一组基于probot的机器人,用于谷歌在GitHub上管理的开源转发中执行常见维护任务。下面是一些可用的bot</p><div class="table-container"><table><thead><tr><th><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/auto-approve">auto-approve</a></th><th>Automatically approves and merges PRs matching user-specified configs</th><th><a href="https://github.com/apps/auto-approve-bot">install</a></th></tr></thead><tbody><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/auto-label">auto-label</a></td><td>Automatically labels issues and PRs with product, language, or directory based labels</td><td><a href="https://github.com/apps/product-auto-label">install</a></td></tr><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/blunderbuss">blunderbuss</a></td><td>Assigns issues and PRs randomly to a specific list of users</td><td><a href="https://github.com/apps/blunderbuss-gcf">install</a></td></tr><tr><td><a href="https://github.com/googleapis/repo-automation-bots/tree/main/packages/cherry-pick-bot">cherry-pick-bot</a></td><td>Cherry-pick merged PRs between branches</td><td><a href="https://github.com/apps/gcp-cherry-pick-bot">install</a></td></tr></tbody></table></div><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://probot.github.io/docs/">probot.github.io/docs/</a></li><li><a href="https://dev.to/pragativerma18/github-bots-for-every-open-source-project-47hl">GitHub Bots for every open-source project - DEV Community</a></li><li><a href="https://github.com/googleapis/repo-automation-bots?tab=readme-ov-file">googleapis/repo-automation-bots: A collection of bots, based on probot, for performing common maintenance tasks across the open-source repos managed by Google on GitHub.</a></li><li><a href="https://smee.io/dSzRk0AnpSDDOf0T">smee.io | Webhook deliveries</a></li><li><a href="https://ngrok.com/">ngrok | Unified Application Delivery Platform for Developers</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;在看一些开源项目时,会发现一些帮忙处理issue和PR的bot,这些bot都是基于Github的Apps&lt;a href=&quot;https://docs.github.com/en/apps/overview&quot;&gt;GitHub Apps overview - GitHub Docs&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测综述</title>
    <link href="https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/"/>
    <id>https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/</id>
    <published>2023-12-22T11:38:36.000Z</published>
    <updated>2023-12-23T12:04:08.140Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>2023年的目标检测综述<strong>A comprehensive review of object detection with deep learning</strong>以及<strong>3D Object Detection for Autonomous Driving: A Comprehensive Survey</strong>,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下.</p><span id="more"></span><h2 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h2><p>本综述详细介绍了物体检测及其各个方面。随着用于检测物体的深度学习算法逐渐发展，物体检测模型的性能也有了显著提高。但是，这并不意味着在深度学习出现之前已经发展了几十年的传统物体检测方法已经过时。<strong>在某些情况下，具有全局特征的传统方法是更优的选择</strong>。<strong>本综述论文首先简要概述了物体检测，然后介绍了物体检测框架、骨干卷积神经网络、常见数据集概述以及评估指标</strong>。此外，还详细研究了物体检测问题和应用。还<strong>讨论了设计深度神经网络的一些未来研究挑战</strong>。最后，比较了对象检测模型在 PASCAL VOC 和 MS COCO 数据集上的性能，并得出结论。</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>物体检测的主要目的是检测特定类别的视觉物体，如电视/显示器、书籍、猫、人类等，并使用边界框定位它们，然后将它们归入特定物体的类别中。</p><p>通用对象检测还有其他几个术语，例如通用对象类别检测、对象类别检测、类别级对象检测和对象类别检测。它也侧重于识别一些预设类别的实例.</p><p>物体检测的发展通常分为两个历史阶段。<strong>2014 年之前是传统方法阶段，2014 年之后则是基于深度学习的方法阶段</strong>。本文将重点讨论基于深度学习的方法。由于 CNN 在物体检测算法的实施中发挥着重要作用，因此本文将利用 CNN 来获得最佳结果。这两个阶段的架构在精度、速度和硬件资源方面各不相同。将 CNN 与传统技术相比，CNN 具有更好的架构和更强的表现力。</p><p>在讨论基于深度学习的物体检测算法之前，重要的是要了解传统技术的工作原理，并知道为什么基于深度学习的方法要优越得多。这将有助于研究人员更好地理解现代物体检测方法。</p><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>传统目标检测方法分为三个阶段。这些阶段及其各自的缺点如下:</p><p><strong>区域选择</strong> – 由于对象具有不同的大小和纵横比，因此它们可能出现在图像的不同区域。因此，在第一阶段，必须确定物体的区域。因此，使用多尺度滑动窗口方法检查整个图像以检测物体。然而，这种方法的计算成本很高，并且还会导致大量非必要的选择。</p><p><strong>特征提取</strong> – 定位对象后，执行特征提取过程以提供可靠的表示。<strong>HOG、Haar-like 、SIFT</strong> 等方法用于提取特征以进行目标识别，以提供有意义的表示。然而，由于对比鲜明的背景、照明环境和透视差异，手动构建一个能够正确识别各种对象的综合特征描述符是极其困难的。</p><p><strong>分类</strong> – 在这个阶段，使用分类器（如 Adaboost ）来识别目标对象，并构建更有条理、更有意义的视觉感知模型。</p><p>从以上几点可以清楚地看出，<strong>在传统方法中，手工制作的特征并不总是足以正确表示对象</strong>。除此之外，用于生成边界框的滑动窗口方法在计算上成本高昂且效率低下。传统的技术包括HOG、SIFT 、Haar、VJ检测器和其他算法，如。在HOG 中，识别一个物体需要很长时间，因为它采用滑动窗口方法来提取特征。SIFT算法速度极慢，计算成本高，也不擅长光照变化。在VJ检测器中，训练持续时间非常长，仅限于二元分类。因此，深度学习技术正在被用于克服传统方法的问题</p><p>深度学习的出现有可能解决传统技术的一些局限性。最近，深度学习方法在自动从数据中学习特征表示方面变得突出。这些方法显著改善了目标检测。基于深度学习的方法有 <strong>Faster RCNN、SSD、YOLO</strong> 等等。</p><p><img data-src="https://i.imgur.com/lJA5oxG.png" alt="image-20231222203515697"></p><p>由于深度 CNN 具有很高的特征表示能力，因此它们被用于对象检测架构。有两种类型的探测器：两级和一级探测器.</p><p>两阶段目标检测框架<strong>将目标定位和目标分类任务分开</strong>。简单来说，<strong>首先在对象所处的地方生成区域建议，然后根据其特定类别对该区域进行分类</strong>。这就是为什么它被称为两阶段的原因。两级目标探测器的主要优点是检测精度高，缺点是检测速度慢。</p><h4 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h4><p>基于区域的卷积神经网络（RCNN）在使用深度学习方法检测目标方面进行了深入研究。其架构如图所示。RCNN的过程在下面分四个阶段进行解释</p><p><img data-src="https://i.imgur.com/tIKAWn2.png" alt="image-20231222204652846"></p><p>第 1 阶段 :使用<strong>选择性搜索方法提取区域建议</strong>。选择性搜索<strong>根据不同的比例、外壳、纹理和颜色模式来识别这些区域</strong>。它<strong>从每张图像中提取大约 2000 个区域</strong></p><p>第 2 阶段 – 由于全连接层需要固定长度的输入向量，因此<strong>所有这些区域建议都重新缩放为相同的图像大小以匹配 CNN 输入大小</strong>。<strong>使用 CNN 提取每个候选区域的特征</strong>。</p><p>第 3 阶段 – 提取特征后，<strong>使用 SVM 分类器检测对象是否存在于每个区域</strong>中。</p><p>第 4 阶段 – 最后，对于图像中的每个已识别对象，使用线性回归模型在其周围生成更紧密的边界框。尽管RCNN在目标检测方面取得了很大的进步，但仍然存在一些局限性，如目标检测速度慢、多阶段流水线训练和选择性搜索方法的僵化。</p><h4 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h4><p>由于 RCNN 为每张图像生成 2000 个区域建议，因此从这些区域提取 CNN 特征是主要障碍。<strong>固定输入大小的约束只是因为全连接层</strong>。因此，<strong>为了克服这一困难，引入了一种称为空间金字塔池化网络层（SPP-Net）的新技术</strong>。<strong>将 SPP 层添加到最终卷积层的顶部，以生成全连接层的固定长度特征</strong>，无论 RoI（感兴趣区域）的大小如何，并且不会重新缩放它，这可能会导致信息丢失(相当于替代RCNN中的warp操作).</p><p><img data-src="https://i.imgur.com/GyXhKPL.png" alt="image-20231222205054546"></p><p>通过使用SPPNet层，RCNN的速度有了很大的提高，而检测质量没有任何损失。这是因为卷积层<strong>只需要在完整的测试图像上运行一次，就可以为随机大小的区域建议创建固定长度的特征</strong>。这里 SPP 层的输出是 256×M-d 向量。256 是卷积滤波器的数量，M 是bin的数量。全连接层接收固定长度的维向量。</p><h4 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h4><p>尽管SPPNet在效率和准确性方面优于RCNN，但它仍然存在一些问题，例如它大致遵循与RCNN相同的过程，包括网络微调、特征提取和边界框回归。</p><p>Girshick， R. 在 RCNN 和 SPPNet 方面表现出进一步的改进，并提出了一种名为 Fast RCNN 的新探测器 。<strong>它允许对检测器进行端到端训练，同时学习 softmax 分类器和特定于类的边界框回归，同时进行多任务损失，而不是像在 RCNN 和 SPPNet 中那样单独训练它们</strong>。</p><p>在 Fast RCNN 中，它不是对每张图像执行 2000 次 CNN，<strong>而是只运行一次并获取所有感兴趣的区域。然后，在最终卷积层和初始全连接层之间添加RoI池化层，从而提取出所有区域建议的固定长度向量特征</strong>。</p><p>1st – Fast RCNN 获取完整的输入图像并将其传递给 CNN 以生成特征图。</p><p>第 2 个 – 感兴趣区域 （RoI） 是使用选择性搜索方法生成的。</p><p>第三 – 在提取的 RoI 上应用 RoI 池化层以生成固定长度的特征向量。它确保所有区域都具有相同的量级。</p><p>第 4 次 – 然后<strong>将提取的特征发送到全连接层，同时使用 softmax 层和线性回归层进行分类和定位</strong>。</p><p>Fast RCNN消耗的计算时间更少，检测精度更高。然而，<strong>它基于传统的区域建议方法，使用选择性搜索方法，使其非常耗时</strong>。</p><h4 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h4><p>尽管Fast RCNN在速度和准确性方面取得了长足的进步，<strong>但它使用选择性搜索方法生成了2000个region proposals，这是一个非常缓慢的过程</strong>。任，S.等人致力于这个问题，并开发了一种新的检测器，名为Faster RCNN，作为第一个端到端深度学习检测器。<strong>它还通过将传统的region proposal算法（如选择性搜索、多尺度组合分组或边缘框）替换为称为区域建议网络（RPN）的CNN</strong>，提高了Fast RCNN的检测速度。</p><p>a） CNN 将图像作为输入，并提供图像的特征图作为输出。</p><p>b） <strong>RPN 应用于生成的特征图，返回对象建议 （RoI） 及其对象性分数</strong>。</p><p>c） 提取 RoI 后，将 RoI 池化层应用于其，以将所有提案置于固定维度。</p><p>d） <strong>将派生的特征向量提供给连续的全连接层中，顶部有一层 softmax 和回归层，用于对对象的边界框进行分类和输出</strong></p><p><img data-src="https://i.imgur.com/D9dXGsH.png" alt="image-20231222205738677" style="zoom:67%;" /></p><p>RPN 的工作 – 区域提案网络是一个完全卷积网络，它连接到骨干网络的最后一个卷积层 。<strong>它接收特征图，并使用这些特征图上的滑动窗口输出多个对象建议</strong>。<strong>在每个窗口上，网络生成 k 个不同大小和纵横比的锚框</strong>（也称为参考框）。</p><p><strong>只有从锚点框获得的特征是特定于类的，而不是锚点的位置</strong>。</p><p>每个对象提案由 4 个坐标和一个分数组成，用于确定对象是否存在。<strong>每个锚点映射到一个低维向量，并传递给两个全连接层，一个是对象类别分类层，另一个是box回归层</strong></p><h4 id="Feature-pyramid-network"><a href="#Feature-pyramid-network" class="headerlink" title="Feature pyramid network"></a>Feature pyramid network</h4><p>Lin， T. Y. et al. 提出了特征金字塔网络 （FPN）</p><p>DCNN 固有的多尺度金字塔层次结构，以低成本构建特征金字塔。它将任何大小的图像作为输入，并在多个级别输出相同大小的特征图。这种方法在许多应用中显示出相当大的增强。</p><p><img data-src="https://i.imgur.com/or3OwoO.png" alt="image-20231222210741517"></p><p>FPN 不是对象检测器。它是一种特征提取器，与对象检测器结合使用。<strong>FPN的架构使用自上而下的通路和横向连接将语义上较强的低分辨率特征与语义较弱的高分辨率特征相结合。FPN使用CNN架构的序列，通过横向连接构建自下而上的路径和自上而下的路径。在自下而上的路径（红色）中，图像作为输入传递给 CNN，它使用池化层将特征图设置为相同的大小。对于FPN的每个阶段（即每个分辨率级别），定义了一个金字塔级别</strong></p><p>在自上而下的路径（以蓝色显示）中，<strong>通过将特征图上采样回与自下而上部分相同的大小来使用更高分辨率的特征。然后使用横向连接，这些特征通过自下而上途径的特征进行增强</strong>。每个横向连接都从自下而上和自上而下路径组合了相同大小的特征图</p><p>FPN 的过程为生成具有大量语义内容的多尺度特征图提供了广泛的解决方案。F<strong>PN 不依赖于 CNN 的架构，可以强制执行到对象检测的不相同阶段</strong>，例如 RPN、Fast RCNN.尽管DCNN具有强大的表征能力，但<strong>有必要通过金字塔表示来解决多尺度挑战</strong></p><h4 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask RCNN"></a>Mask RCNN</h4><p>He， K. et al.设计了一款名为Mask RCNN的目标检测器，这是对Faster RCNN的增强，用于解决进行目标检测和语义分割作业的实例分割问题。这两个任务是自力更生的过程。Mask RCNN 的目标是执行像素级分割。蒙版RCNN检查每个像素并估计它是否是对象的一部分。</p><p>Mask R-CNN 遵循 Faster R-CNN 的架构;两者都使用相同的RPN，<strong>但区别在于掩码RCNN对每个对象提案有三个输出，即.class标签，边界框偏移量和对象检测掩码</strong>。在 Mask RCNN 中，RoIAlign 层用于将提取的特征与对象的输入位置相关联。RoIAlign 层的目的是修复 RoI 池化层中的错位问题。它无需测量 RoI 阈值，而是使用双线性插值来评估每个采样点的实际特征值。Mask RCNN 在实例分割方面实现了最先进的性能</p><p><strong>基于区域提案的框架由各个阶段组成，这些阶段相互连接并分别进行训练。这些是区域建议生成、使用 CNN 提取特征、分类和边界框回归</strong>。尽管这些方法能够实现高精度，但仍存在一些与实时速度相关的问题。这个问题可以通过统一的阶段检测器来克服，<strong>方法是删除区域建议阶段，并在单个CNN中实现特征提取、建议回归和预测</strong></p><p>损失或成本函数，如Hinge损失、L1 和 L2 损失、对数损失 [52]是预期输出和预测输出之间差异的度量。建议读者参考相应的物体检测器论文以获取更多信息</p><h3 id="One-stage-object-detectors"><a href="#One-stage-object-detectors" class="headerlink" title="One-stage object detectors"></a>One-stage object detectors</h3><p>单阶段对象检测框架使用 DCNN <strong>同时进行定位和分类</strong>，而无需将它们划分为两个部分。</p><p>在这种情况下，只需要通过神经网络进行一次传递。它具有前馈神经网络，可<strong>以一次预测所有边界框。它们将图像像素直接映射到边界框坐标和类概率</strong>。</p><h4 id="DetectorNet"><a href="#DetectorNet" class="headerlink" title="DetectorNet"></a>DetectorNet</h4><p>Szegedy， C.等将DetectorNet框架实现为回归问题。</p><p>它能够学习特征进行分类并获取一些几何信息。它<strong>使用 AlexNet 作为骨干网络，并将 softmax 层替换为回归层</strong>。为了预测前景像素，DetectorNet 将输入图像分割成coarse grid。它的训练过程非常缓慢，因为网络要针对每种对象类型和掩码类型进行训练。此外，DetectorNet 无法处理类似类的多个对象。当它与多尺度从粗到细方法结合使用时，基于 DNN 的对象掩码回归会产生出色的结果</p><h4 id="Overfeat"><a href="#Overfeat" class="headerlink" title="Overfeat"></a>Overfeat</h4><p>Sermanet， P.等提出了一种统一的结构，即<strong>使用卷积网络通过多尺度滑动窗口方法进行定位、分类和检测</strong>。它是最强大的目标检测框架之一，应用于 ImageNet 大规模视觉识别挑战赛 2013 （ILSVRC），在检测和定位方面排名第一 。它是<strong>第一个基于全卷积深度网络的单级检测器，它通过全卷积层使用单次前向通道来检测物体</strong>。</p><p>OverFeat 充当后来出现的算法的基础模型，即 YOLO 及其版本、SSD 等。主要区别在于<strong>分类器和回归器的训练是在 OverFeat 中连续完成的</strong></p><h4 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png" alt="img"></p><p>You Only Look Once（YOLO）是由Redmon， J.等人设计的单级目标检测器，<strong>其中目标检测作为回归问题进行。它预测对象的边界框的坐标，并确定它所关联的类别的可能性</strong>。由于仅使用单个网络，因此可以实现端到端优化。它<strong>使用有限的候选区域选择直接预测检测。与基于区域的方法不同，这些方法使用来自特定区域的特征</strong>，而YOLO广泛使用来自整个图像的特征</p><p>在YOLO目标检测中，图像被划分为S×S网格;每个网格由五个元组（x、y、w、h 和置信度分数）组成。单个对象的置信度分数基于概率。这个分数是给每个类的，无论哪个类的概率很高，该类都会优先。</p><p>边界框的参数宽度 （W） 和高度 （H） 是根据对象的大小来预测的。从重叠的边界框中，选择具有最高 IOU 的框，并删除其余框。</p><p>YOLOv2是YOLOv1的增强版本，由Redmon， J.等人]给出。在这个版本中，应用了不同的思想<strong>，如批量归一化、卷积锚框、高分辨率分类器</strong>、<strong>细粒度特征和多尺度训练</strong>来提高 YOLO 的性能。它使用 Darknet-19 作为包含 19 个卷积层和 5 个最大池化层的骨干分类，这些层需要更少的过程来分析图像，同时实现最佳精度</p><p>YOLOv3 是 YOLOv2  的渐进形式，它使用<strong>逻辑回归来估计每个边界框的客观性分数。边界框中包含多个类，为了预测这些类，使用了多标签分类</strong>。它还使用二进制交叉熵损失、数据增强技术和批量归一化。YOLOv3 使用一个名为 Darknet-53 的健壮特征提取器</p><p>YOLOv4 是一种先进的目标检测器，比以前所有版本的YOLO更准确、更快。它包括一种称为“Bag of freebies”的方法，该方法在不影响推理时间的情况下增加了训练时间。该方法利用<strong>数据增强技术、自对抗训练、交叉小批量归一化 （CmBN）、CIoU 损失 、DropBlock 正则化、余弦退火调度器来改进训练。YOLOv4 还包含了那些只影响推理时间的方法，称为“Bag of specials”;它包括 Mish 激活、多输入加权残差连接 （MiWRC）、SPP 模块 [26]、PAN 路径聚合模块 [58]、跨级部分连接 （CSP）和空间注意力模块模块</strong>。YOLOv4 可以在单个 GPU 上训练，并使用遗传算法来选择超参数</p><p>在 YOLOv4 发布后不久，Ultralytics 公司推出了 YOLOv5 存储库，与以前的 YOLO 模型相比，它有相当大的增强,由于 YOLOv5 不是作为同行评议的研究发表的，因此它引起了许多关于其合法性的争论;但它仍然被用于各种应用，并在产生模型可靠性的同时提供有效的结果。它以 140 fps 的推理速度运行。YOLOv5使用PyTorch，这使得模型的部署更快、更容易、更准确[60]。虽然 YOLOv4 和 YOLOv5 框架相似，因此很难比较它们之间的区别，但后来，YOLOv5 在某些情况下获得了比 YOLOv4 更高的性能。YOLOv5 模型有五种类型：nano、small、medium、large、extralarge。根据数据集选择模型类型。此外，YOLOv5 模型的轻量级模型随 6.0 版本发布;推理速度提高至 1666 fps.</p><h4 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png" alt="img"></p><p>SSD是一种用于多个类别的快速单次多box检测器，由Liu， W.等人实现。它构建了一个统一的检测器框架</p><p>该框架与 YOLO 一样快，与 Faster-RCNN 一样准确。SSD的设计结合了YOLO模型的回归思想和Faster R-CNN算法的锚定过程。通过使用 YOLO 的回归，SSD 降低了神经网络的计算复杂性，以确保实时性能。通过锚点程序，SSD能够提取各种大小和纵横比的特征，以确保检测精度。SSD 使用 VGG-16 作为骨干检测器</p><p>SSD的过程基于前馈CNN，该CNN为这些框中是否存在对象类实例生成固定大小和对象性分数的边界框，然后应用NMS（非最大抑制）进行最终检测。它还使用RPN的概念来获得快速的检测速度，同时保持高检测质量。通过一些辅助数据增强和硬负挖掘方法，SSD 在各种基准数据集上实现了最先进的性能</p><h3 id="Backbone-networks"><a href="#Backbone-networks" class="headerlink" title="Backbone networks"></a>Backbone networks</h3><p>DCNN作为目标检测模型的骨干网络。为了改善特征表示行为，网络的结构变得更加复杂，这意味着网络层会变得更深，其参数也会增加。骨干CNN用于提取基于DCNN的目标检测系统的特征。</p><p><strong>骨干网络作为目标检测方法的主要特征提取器，将图像作为输入，并为每个输入图像生成特征图作为输出</strong>。根据精度和效率的需要;可以使用密集连接的骨干网，如ResNet 、ResNext等。当需要高精度和构建精确的应用程序时，需要复杂的主干网。</p><p>AlexNet是一种重要的CNN架构，由5个卷积层和3个全连接层组成。在为图像提供固定大小（224 × 224）的输入后，网络一遍又一遍地卷积并汇集激活，然后将结果传输到完全连接的层。该网络在ImageNet上进行训练，并结合了多种正则化方法，例如数据增强，dropout等。为了加速数据处理，提高收敛速度，首次使用了ReLu激活函数和GPU。</p><h3 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h3><p>在AlexNet取得成功之后，研究人员想知道卷积层可视化背后的机制，以了解CNN如何学习特征以及如何检查每层图像特征图的差异。</p><p>因此，Zeiler， M. D. et al. 设计了一种<strong>使用反卷积层、解池层和ReLU非线性来可视化特征图的方法。与 AlexNet 一样，第一层的滤波器大小为 11×11，步幅为 4，但在 ZFNet 中，它减少到 7×7，步幅设置为 2 而不是 4</strong>。这样做的原因是第一层的滤波器包含频率信息的变化;它可以是高的，也可以是低的，并且具有非常小的中频百分比。该方法的性能优于AlexNet，并证明了网络的深度会影响深度学习模型的性能</p><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><p>VGG 进一步将AlexNet的深度扩大到16-19层，从而细化了网络的特征表示。VGG16 和 VGG19 是两种流行的 VGG 网络架构。在每一层中，它采用大小为 3×3 的内核，步幅为 1。小内核和步幅更有利于提取图像中物体位置的细节。它的好处是通过合并额外的卷积层来扩展网络的深度。最小化参数可以提高网络的特征表示能力</p><h4 id="GoogLeNet-或-inception-v1"><a href="#GoogLeNet-或-inception-v1" class="headerlink" title="GoogLeNet 或 inception v1"></a>GoogLeNet 或 inception v1</h4><p>GoogleNet  的主要目的Inception v1 架构旨在通过降低计算成本来实现高精度。向网络添加 1×1 卷积层，其深度增加。这种滤波器大小首先用于名为Network-in-Network的技术，主要用作降维以消除计算瓶颈并增加网络的宽度和高度。</p><p>GoogleNet 是一个 22 层的深度架构，是 ILSVRC 2014 竞赛的获胜者。基于这一思路，作者开发了一个具有降维功能的初始模块。通过使用 inception 模块，GoogLeNet 参数的数量减少了。Inception 模块由 1x1、3x3 和 5x5 滤波器大小的卷积层和相互平行组装的最大池化层组成。Inception v2 系列是第一个提出批量归一化的网络 ，从而实现快速训练</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>随着网络深度的增加，可能会出现精度在达到饱和点后下降的情况。这被称为退化问题，为了解决这个问题，提出了一个残差学习（ResNet）模块。与早期设计的架构（如AlexNet 和VGGNet）相比，它的计算复杂度更低。通常使用层数为50和101层的ResNet骨干网络。在 ResNet50 中，使用跳过连接来保留更深层的梯度，并且精度有所提高。在 ResNet101 中，该模块的性能与 VGG 网络相同，但参数数量较少，遵循 GoogLeNet 中的全局平均池化和瓶颈</p><h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p>Huang， G. et al. 提出了由密集块组成的 DenseNet 架构，该架构以前馈方式将每一层与其他层连接起来，从而带来特征重用、参数有效性和隐式深度监督等好处。DenseNet 减少了梯度消失的问题</p><h3 id="Problems-of-object-detection-and-its-solutions"><a href="#Problems-of-object-detection-and-its-solutions" class="headerlink" title="Problems of object detection and its solutions"></a>Problems of object detection and its solutions</h3><h4 id="Small-object-detection"><a href="#Small-object-detection" class="headerlink" title="Small object detection"></a>Small object detection</h4><p>检测小尺寸物体是物体检测中最困难的问题之一。Faster RCNN 和 YOLO等目标检测算法在检测小尺寸物体方面不足。在深度卷积神经网络中，由于独立特征层在实际图像中仅占据很小的像素尺寸，因此缺乏足够的知识。由于低分辨率的小尺寸物体携带有限的上下文细节，因此很难检测到它们。为了克服这个问题，可以<strong>通过增强生成更多的数据，或者可以提高模型的输入分辨率</strong>等</p><h4 id="Multi-scale-object-detection"><a href="#Multi-scale-object-detection" class="headerlink" title="Multi-scale object detection"></a>Multi-scale object detection</h4><p>在目标检测领域，多尺度目标检测是一项具有挑战性的任务。<strong>深度CNN的每一层都会生成特征图，而这些特征图生成的信息是相互独立的。多尺度对象的判别细节可以出现在骨干网络的任一层中，而对于小尺度对象，它出现在初始层中，并在后面的层中消散</strong>。在目标检测算法（一级和两级）中，预测是从最顶层进行的，这给检测多尺度对象（通常是小对象）的方式造成了障碍。为了克服这个困难;该文提出<strong>信息融合与DCNNs分层结构相结合的多层检测和特征融合</strong></p><p>代表性方法包括多尺度深度CNN 、深度监督目标检测（DSOD）和SSD。为了提高多尺度目标检测的可靠性，可以合并多层特征融合和多层检测。这包括特征金字塔网络（FPN）、反卷积单次检测器（DSSD）、尺度可转移检测网络（STDN）、与对象先验网络的反向连接（RON）、自上而下的调制（TDM）等几个具有代表性的框架。</p><h4 id="Intraclass-variation"><a href="#Intraclass-variation" class="headerlink" title="Intraclass variation"></a>Intraclass variation</h4><p>类内variation是指<strong>同一类的不同图像之间发生的variation</strong>。<strong>它们的形状、大小、颜色、材料、质地等各不相同</strong>。对象实例看起来很灵活，可以在缩放和旋转方面轻松转换。这些被称为内在因素。外部因素也会产生一些明显的影响。<strong>它包括照明不当、天气条件、照明、低质量相机等。这种差异可能由多种因素引起</strong>，如遮挡、照明、位置、透视等。这个问题可以通过验证训练数据是否具有良好的多样性（包括上述所有因素）来克服</p><h4 id="Class-imbalance"><a href="#Class-imbalance" class="headerlink" title="Class imbalance"></a>Class imbalance</h4><p>类之间的不规则数据分布称为类不平衡。简单来说，<strong>可以说当类包含不成比例数量的实例时，即在一个数据集中比另一个数据集中的标本多</strong>。从对象检测的角度来看，类不平衡可以分为两种类型：前景-背景不平衡和前景-前景不平衡。前者发生在训练过程中，与数据集中的类别数量无关。后者是指在样本数量范围内批次水平的不平衡，涉及正类。<strong>一般来说，一级目标探测器的精度低于两级目标探测器，其背后的原因之一是类别不平衡。为了解决这个问题，可以对类进行上采样和下采样，或者使用合成少数过采样技术（SMOTE）等生成合成数据</strong></p><h4 id="Generalization-issues"><a href="#Generalization-issues" class="headerlink" title="Generalization issues"></a>Generalization issues</h4><p>当模型欠拟合或过拟合时，就会出现目标检测中的泛化问题。欠拟合可以在训练阶段的初始阶段识别出来，这个问题可以通过增加训练周期的数量或模型的复杂性来解决。对于过拟合，我们可以使用重要的方法，例如<strong>增加训练数据、提前停止、正则化方法（L1、L2）或丢弃层</strong></p><h2 id="3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey"><a href="#3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey" class="headerlink" title="3D Object Detection for Autonomous Driving: A Comprehensive Survey"></a>3D Object Detection for Autonomous Driving: A Comprehensive Survey</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>近年来，自动驾驶因其减轻驾驶员负担和提高驾驶安全性的潜力而受到越来越多的关注。在现代自动驾驶管道中，感知系统是不可或缺的组成部分，<strong>旨在准确估计周围环境的状态，并为预测和规划提供可靠的观测结果。3D 物体检测旨在预测自动驾驶汽车附近 3D 物体的位置、大小和类别</strong>，是感知系统的重要组成部分。本文综述了自动驾驶三维目标检测的研究进展。首先，我们<strong>介绍了3D目标检测的背景，并讨论了该任务的挑战。其次，我们从模型和感官输入方面对3D目标检测的进展进行了全面调查，包括基于激光雷达、基于相机和多模态检测方法。我们还对每类方法的潜力和挑战进行了深入分析</strong>。此外，我们还系统地研究了3D目标检测在驾驶系统中的应用。最后，对三维目标检测方法进行了性能分析，进一步总结了多年来的研究趋势，并展望了该领域的未来发展方向。</p><h3 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h3><p>自动驾驶技术已广泛应用于许多场景，包括自动驾驶卡车、机器人出租车、送货机器人等，能够减少人为错误并增强道路安全性。作为自动驾驶系统的核心组成部分，汽车感知帮助自动驾驶汽车通过感官输入了解周围环境。<strong>感知系统通常以多模态数据（摄像头图像、激光雷达扫描仪点云、高清地图等）为输入，预测道路上关键要素的几何和语义信息。高质量的感知结果可作为目标跟踪、轨迹预测和路径规划等后续步骤的可靠观测</strong></p><p><img data-src="https://i.imgur.com/02X39qA.png" alt="image-20231222223259144"></p><p>3D 对象检测旨在根据感官输入预测驾驶场景中 3D 对象的边界框。3D 目标检测的一般公式可以表示为</p><script type="math/tex; mode=display">\begin{equation}\mathcal{B}=f_{det}(\mathcal{I}_{sensor}),\end{equation}</script><p>f~det~ 是 3D 对象检测模型，I~sensor~ 是一个或多个感官输入,B = {B1， · · · ， BN } 是场景中 N 个 3D 对象的集合。</p><p>如何表示 3D 对象 Bi 是此任务中的一个关键问题，因为它决定了应为以下预测和规划步骤提供哪些 3D 信息。在大多数情况下，3D 对象表示为包含此对象的 3D 长方体，</p><script type="math/tex; mode=display">\begin{equation}B=[x_c,y_c,z_c,l,w,h,\theta,class],\end{equation}</script><p>其中 （x~c~， y~c~， z~c~） 是长方体的 3D 中心坐标，l、w、h 分别是长方体的长度、宽度和高度，θ 是长方体在地平面上的航向角，即偏航角，class 表示 3D 对象的类别，例如汽车、卡车、行人、骑自行车的人。此外也有其他模型使用了更多参数的.</p><p><strong>Sensory inputs</strong></p><p>有许多类型的传感器可以为 3D 物体检测提供原始数据。<strong>在传感器中，雷达、摄像头和LiDAR（光探测和测距）传感器是三种最广泛采用的传感类型</strong>。雷达具有较长的探测范围，并且对不同的天气条件具有鲁棒性。由于多普勒效应，雷达可以提供额外的速度测量。摄像头价格便宜且易于获取，对于理解语义（例如交通标志的类型）至关重要。尽管价格便宜，但相机在用于 3D 物体检测方面存在固有的局限性<strong>。相机只能捕获外观信息，无法直接获取场景的 3D 结构信息</strong>。另一方面，<strong>3D物体检测通常需要在3D空间中进行精确定位，而从图像中估计的3D信息（例如深度）通常具有较大的误差</strong>。图像的变形通常<strong>容易受到极端天气和时间条件的影响</strong>。在<strong>夜间或雾天从图像中检测物体比在晴天检测要困难得多</strong>，这导致了实现自动驾驶的足够鲁棒性的挑战。</p><p><img data-src="https://i.imgur.com/lS0F3vq.png" alt="image-20231222225107201"></p><p>作为替代解决方案，<strong>LiDAR 传感器可以通过发射激光束然后测量其反射信息来获得场景的细粒度 3D 结构。</strong>一个激光雷达传感器发射 m 束并在一个扫描周期内进行 n 次测量，可以产生 I~range~ ∈ R^m×n×3^ 的距离图像,其中,范围图像的<strong>每个像素都包含球面坐标系中的距离 r、方位角α和倾角φ以及反射强度</strong>。</p><p><strong>范围(Range)图像是LiDAR传感器获得的原始数据格式，可以通过将球面坐标转换为笛卡尔坐标来进一步转换为点云。</strong></p><p>点云可以表示为 I~point~ ∈ R^N×3^，其中 N 表示场景中的点数，每个点有 3 个 xyz 坐标通道。<strong>距离图像和点云都包含由LiDAR传感器直接获取的精确3D信息</strong>。</p><p>因此，<strong>与相机相比，LiDAR 传感器更适合检测 3D 空间中的物体，并且 LiDAR 传感器也不太容易受到时间和天气变化的影响</strong>。然而，LiDAR 传感器比摄像头贵得多，这可能会限制在驾驶场景中的应用</p><p>2D 目标检测旨在<strong>在图像上生成 2D 边界框</strong>，是计算机视觉中的一个基本问题。</p><p>3D 目标检测方法借鉴了 2D 对应物的许多设计范式：<strong>region proposals生成和细化、锚点、非最大抑制等</strong>。然而，从很多方面来看，3D目标检测并不是2D目标检测方法对3D空间的简单改变。</p><p>（1）3D目标检测方法必须处理异构数据表示<strong>。点云检测需要新型算子和网络来处理不规则的点数据</strong>，而点云和图像的检测需要特殊的融合机制。</p><p>（2） 3D 目标检测方法通常<strong>利用不同的投影视图来生成对象预测</strong>。</p><p>与从透视图检测对象的 2D 对象检测方法相反，<strong>3D 方法必须考虑不同的视图来检测 3D 对象，例如从鸟瞰图、点视图和圆柱视图</strong>。（3）3D物体检测对物体在3D空间中的精确定位有很高的要求。分米级的定位误差可能导致行人和骑自行车的人等小物体的检测失败，而在二维物体检测中，几个像素的定位误差仍可能在预测边界框和地面实况边界框之间保持较高的交并 （IoU）。因此，精确的 3D 几何信息对于从点云或图像进行 3D 物体检测是必不可少的。</p><p>（1） LiDAR 和 RGB-D 传感器的点云分布不同。在室内场景中，点相对均匀地分布在扫描表面上，大多数 3D 对象在其表面上接收到足够数量的点。然而，在驾驶场景中，大多数点都落在 LiDAR 传感器的附近，而那些远离传感器的 3D 物体只会获得几个点。因此，在驾驶场景中，<strong>特别需要处理各种点云密度的三维物体，并准确检测那些远距离和稀疏的物体。</strong></p><p>（2）驾驶场景下的检测对推理时延有特殊要求。<strong>驾驶场景中的感知必须是实时的</strong>，以避免事故发生。因此，这些方法必须具有计算效率，否则它们将无法应用于实际应用。</p><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p><img data-src="https://i.imgur.com/HiTj4TF.png" alt="image-20231222230257242"></p><h3 id="LiDAR-based-3D-Object-Detection"><a href="#LiDAR-based-3D-Object-Detection" class="headerlink" title="LiDAR-based 3D Object Detection"></a>LiDAR-based 3D Object Detection</h3><p>我们将介绍基于LiDAR数据的3D目标检测方法，即点云或距离图像。回顾和分析了基于不同数据表示的基于 LiDAR 的 3D 目标检测模型，包括<strong>基于点</strong>、<strong>基于网格</strong>、<strong>基于点体素</strong>和<strong>基于距离</strong>的方法。(point-based, grid-based, point-voxel based, and range-based)</p><p><img data-src="https://i.imgur.com/7LLjGj3.png" alt="image-20231222230749845"></p><h4 id="Data-representations-for-3D-object-detection"><a href="#Data-representations-for-3D-object-detection" class="headerlink" title="Data representations for 3D object detection"></a>Data representations for 3D object detection</h4><p><strong>与像素有规律地分布在图像平面上的图像相比，点云是一种稀疏且不规则的 3D 表示</strong>，需要专门设计的模型进行特征提取。<strong>范围图像是一种密集而紧凑的表示形式，但范围像素包含 3D 信息而不是 RGB 值</strong>。因此，在范围图像上直接应用传统的卷积网络可能不是最佳解决方案。另一方面，自动驾驶场景中的检测通常具有实时推理的要求。因此，如何开发一个既<strong>能有效处理点云或范围图像数据又能保持高效率的模型</strong>，仍然是研究界面临的一个公开挑战。</p><h4 id="Point-based-3D-object-detection"><a href="#Point-based-3D-object-detection" class="headerlink" title="Point-based 3D object detection"></a>Point-based 3D object detection</h4><p>基于点的3D目标检测方法通常继承了点云深度学习技术的成功，并<strong>提出了直接从原始点检测3D对象的多种架构</strong>。</p><p>点云首先通过基于<strong>点的骨干网络</strong>，在该网络中，点逐渐采样，点云操作学习特征。然后，根据下采样点和特征预测 3D 边界框。</p><p><img data-src="https://i.imgur.com/Vl1RhDd.png" alt="image-20231222233252147"></p><p>基于点的 3D 对象检测器有两个基本组件：<strong>点云采样</strong>和<strong>特征学习</strong>。</p><p><strong>Point Cloud Sampling</strong>:PointNet++中的<strong>最远点采样（FPS）已被广泛用于基于点的检测器中</strong>，其中最远的点是<strong>从原始点集中依次选择的</strong>。PointRCNN 是一项开创性的工作，<strong>它采用 FPS 逐步对输入点云进行下采样，并从下采样点生成 3D 建议</strong>。类似的设计范式也被用于后续的许多工作，并进行了改进，如分割引导滤波、特征空间采样、随机采样、基于体素的采样(voxel-based sampling)和坐标细化(coordinate refinement)。</p><p><strong>Point Cloud Feature Learning</strong>:具体来说，首先通过球查询(ball query)在预定义的半径内收集上下文点。然后，通过多层感知器和maxpooling对上下文点和特征进行聚合，得到新的特征。还有其他使用不同点云算子的工作，包括图算子，注意力算子和Transformer。</p><p>基于点的检测器的表示能力主要受两个因素的限制：<strong>特征学习中采用的上下文点数和上下文半径</strong>。增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。</p><p><strong>增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗</strong>。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。可并行进行随机均匀采样，效率高。然而，考虑到LiDAR扫描中的点不是均匀分布的，<strong>随机均匀采样可能倾向于对那些高点云密度的区域进行过度采样，而对那些稀疏区域进行采样不足，这通常会导致与最远点采样相比性能较差。</strong></p><p><strong>最远点采样及其变体可以通过从现有点集中依次选择最远的点来获得更均匀的采样结果。然而，最远点采样本质上是一种顺序算法，不能变得高度并行</strong>。因此，最远点采样通常很耗时，并且无法进行实时检测。</p><h4 id="Grid-based-3D-object-detection"><a href="#Grid-based-3D-object-detection" class="headerlink" title="Grid-based 3D object detection"></a>Grid-based 3D object detection</h4><p><img data-src="https://i.imgur.com/O8n1SeP.png" alt="image-20231223135445200"></p><p>基于网格的 3D 对象检测器首先<strong>将点云栅格化为离散的网格表示</strong>，即<strong>体素、柱子和鸟瞰图 （BEV） 特征图</strong>。然后，他们<strong>应用传统的 2D 卷积神经网络或 3D 稀疏神经网络</strong>从网格中提取特征。最后，可以从BEV网格单元中检测到3D物体。基于网格的 3D 目标检测图如图  所示。基于网格的检测器有两个基本组件：<strong>基于网格的表示</strong>和<strong>基于网格的神经网络</strong></p><p><strong>Grid-based representations</strong>:</p><p><strong>voxels</strong>。如果<strong>将检测空间栅格化为规则的 3D 格网</strong>，则体素就是格网像元。<strong>如果点云落入此格网像元中，则体素可以为非空。</strong>由于点云分布稀疏，因此 3D 空间中的大多数体素单元格都是空的，不包含任何点。<strong>在实际应用中，只有那些非空体素才会被存储并用于特征提取。</strong>VoxelNet是一项开创性的工作，它利用稀疏的体素网格，提出了一种新的体素特征编码（VFE）层，从体素单元内的点中提取特征。</p><p>随后的一系列工作采用了类似的体素编码策略。此外，还有两类方法试图改进 3D 目标检测的体素表示：（1） <strong>多视图体素</strong>。一些方法从不同的视角提出了动态体素化和融合方案，例如鸟瞰图和透视图、圆柱面和球面视角、距离视角。（2）<strong>多尺度体素</strong>。一些论文生成不同尺度的体素或使用可重构的体素</p><p><strong>Pillars</strong>:柱子可以看作是特殊的体素，<strong>其中体素大小在垂直方向上是无限的</strong>。<strong>支柱特征可以通过PointNet从点聚合，然后散射回来</strong>，构建2D BEV图像进行特征提取。PointPillars 是一部开创性的著作，它引入了Pillar表示</p><p><strong>BEV feature maps</strong>:鸟瞰图特征图是一种密集的 2D 表示，其中<strong>每个像素对应于一个特定区域</strong>，<strong>并对该区域中的点信息进行编码</strong>。BEV特征图可以通过将3D特征投影到鸟瞰图中，从体素和柱子中获取，也可以通过汇总像素区域内的点统计数据，直接从原始点云中获取。</p><p><strong>Grid-based neural networks</strong>:基于网格的网络主要有两种类型：<strong>用于 BEV 特征图</strong>和<strong>Pillar的 2D 卷积神经网络</strong>，以及<strong>用于体素的 3D 稀疏神经网络</strong>。</p><p>2D convolutional neural networks:传统的 2D 卷积神经网络可以应用于 BEV 特征图，以从鸟瞰图检测 3D 对象。在大多数作品中，2D网络架构通常都是从2D目标检测中的成功设计中改编而来的,比如ResNet，区域建议网络（RPN）和特征金字塔网络（FPN）</p><p>3D 稀疏神经网络。3D稀疏卷积神经网络基于两个专门的3D卷积算子：sparse convolutions和submanifold convolutions，<strong>它们只能在那些非空体素上有效地进行3D卷积。与在整个体素空间上执行标准3D卷积相比，稀疏卷积算子效率更高</strong>，可以获得实时推理速度。</p><p>SECOND是一项开创性的工作，它使用基于 GPU 的哈希表实现了这两个稀疏算子，并构建了一个稀疏卷积网络来提取 3D 体素特征。</p><p>这种网络架构已在许多工程中得到应用，并成为基于体素的探测器中使用最广泛的骨干网络。还有一系列工作试图改进稀疏算子，将扩展为两级检测器，并将Transformer架构引入基于体素的检测。</p><p>与 BEV 特征图和Pillar等 2D 表示相比，体素包含更结构化的 3D 信息。此外，<strong>深度体素特征可以通过 3D 稀疏网络学习</strong>。然而，<strong>3D 神经网络会带来额外的时间和内存成本</strong>。<strong>BEV 特征图是最有效的网格表示，可直接将点云投影到 2D 伪图像中，而无需专门的 3D 运算符</strong>，如稀疏卷积或柱状编码。<strong>2D检测技术也可以无缝应用于BEV特征图，无需太多修改。</strong>基于BEV的检测方法通常可以获得高效率和实时推理速度。但是，<strong>简单地汇总像素区域内的点统计数据会丢失太多的 3D 信息</strong>，与基于体素的检测相比，这会导致检测结果的准确性较低。基于Pillar的检测方法利用 PointNet 对Pillar内的 3D 点信息进行编码，然后将特征分散回 2D 伪图像中以实现高效检测，从而平衡了 3D 目标检测的有效性和效率</p><p><strong>challenges of the grid-based detection methods</strong>:所有基于网格的方法都必须面对的一个关键问题是选择适当大小的网格单元。<strong>网格表示本质上是点云的离散格式</strong>，通过将连续的点坐标转换为离散的网格索引。</p><p>量化过程不可避免地会丢失一些 3D 信息<strong>，其有效性很大程度上取决于网格单元的大小：网格尺寸越小，网格就越高分辨率，因此可以保留更细粒度的细节，这对于准确的 3D 对象检测至关重要</strong>,然而，减小网格单元的大小会导致 2D 网格表示（如 BEV 特征图或支柱）的内存消耗呈二次增加。至于像体素这样的 3D 网格表示，问题可能会变得更加严重。因此，<strong>如何平衡较小网格尺寸带来的功效和内存增加影响的效率仍然是所有基于网格的三维目标检测方法面临的一个悬而未决的挑战</strong>。</p><h4 id="Point-voxel-based-3D-object-detection"><a href="#Point-voxel-based-3D-object-detection" class="headerlink" title="Point-voxel based 3D object detection"></a><strong>Point-voxel based 3D object detection</strong></h4><p>基于点体素的方法采用混合架构，<strong>利用点和体素进行 3D 对象检测</strong>。这些方法可以分为两类：单阶段和两阶段检测框架。</p><p><strong>Single-stage point-voxel detection frameworks.</strong></p><p>基于单级点体素的 3D 目标检测器<strong>试图将点和体素的特征与骨干网络中的点到体素和体素到点变换联系起来</strong></p><p><strong>点包含细粒度的几何信息，体素的计算效率很高，在特征提取阶段将它们组合在一起自然会从这两种表示中受益</strong>。在骨干网中利用点-体素特征融合的思想已被许多著作探索，其贡献包括点-体素卷积，辅助点网络和多尺度特征融合.</p><p><strong>Two-stage point-voxel detection frameworks</strong></p><p>基于点体素的两级 3D 对象检测器<strong>针对不同的检测阶段采用不同的数据表示</strong>。</p><p>具体来说，在第一阶段，采用<strong>基于体素的检测框架来生成一组 3D 对象建议</strong>,在第二阶段，<strong>首先从输入点云中对关键点进行采样，然后通过新的点算子从关键点进一步细化3D建议</strong>。</p><p>基于点体素的方法自然可以受益于从点获得的细粒度 3D 形状和结构信息以及体素带来的计算效率。然而，这些方法仍然存在一些挑战。对于混合点-体素骨干网，点-体素特征的融合一般<strong>依赖于体素-点和点-体素变换机制，可以带来不可忽视的时间成本</strong>。对于两阶段点体素检测框架，一个关键的挑战是<strong>如何有效地聚合 3D 提案的点特征，因为现有的模块和运算符通常非常耗时</strong>。综上所述，与纯基于体素的检测方法相比，基于点体素的检测方法可以获得更好的检测精度，但代价是增加了推理时间。</p><h4 id="Range-based-3D-object-detection"><a href="#Range-based-3D-object-detection" class="headerlink" title="Range-based 3D object detection"></a>Range-based 3D object detection</h4><p>范围图像是一种密集而紧凑的 2D 表示，其中<strong>每个像素都包含 3D 距离信息，而不是 RGB 值</strong>。基于距离的方法从两个方面解决了检测问题：<strong>设计适合距离图像的新模型和算子</strong>，以及<strong>选择合适的视图进行检测</strong>。</p><p><img data-src="https://i.imgur.com/r7H1L1T.png" alt="image-20231223162320834" style="zoom:67%;" /></p><p><strong>Range-based detection models</strong></p><p>由于距离图像是与RGB图像一样的2D表示，因此基于范围的3D对象检测器可以自然地借用2D对象检测中的模型来处理范围图像。</p><p>LaserNet 是一项开创性的工作，它利用深层聚合网络 （DLA-Net）从距离图像中获取多尺度特征并检测 3D 对象。一些论文还采用了其他 2D 目标检测架构</p><p><strong>Range-based operators</strong></p><p>距离图像的像素包含 3D 距离信息而不是颜色值，<strong>因此传统 2D 网络架构中的标准卷积算子对于基于范围的检测来说不是最佳选择</strong>，因为滑动窗口中的像素在 3D 空间中可能彼此相距很远。一些工作采用新颖的算子来有效地从范围像素中提取特征，包括范围扩展卷积、图算子和元核卷积</p><p><strong>Views for range-based detection</strong></p><p>范围图像是从范围视图 （RV） 捕获的，理想情况下，范围视图是点云的球面投影。</p><p>然而，从距离视图进行检测时，<strong>不可避免地会受到球面投影带来的遮挡和尺度变化问题的影响</strong>。</p><p>为了规避这些问题，许多方法都在<strong>利用其他视图来预测3D物体，例如中采用的圆柱视图（CYV），在中采用的距离视图、鸟瞰视图（BEV）和/或点视图（PV）的组合</strong></p><p><strong>Analysis: potentials and challenges of the range-based methods</strong></p><p>范围图像是一种密集而紧凑的 2D 表示，因此<strong>传统或专用的 2D 卷积可以无缝地应用于范围图像</strong>，这使得特征提取过程非常高效.然而，<strong>与鸟瞰图检测相比，距离图检测容易受到遮挡和尺度变化的影响</strong>。因此，<strong>从距离视图中提取特征</strong>，<strong>从鸟瞰图进行目标检测</strong>，成为基于距离的3D目标检测最实用的解决方案。</p><h3 id="Learning-objectives-for-3D-object-detection"><a href="#Learning-objectives-for-3D-object-detection" class="headerlink" title="Learning objectives for 3D object detection"></a>Learning objectives for 3D object detection</h3><p>学习目标在对象检测中至关重要。<strong>由于 3D 物体相对于整个检测范围非常小，因此在 3D 检测中非常需要特殊的机制来增强小物体的定位</strong>。另一方面，<strong>考虑到点云稀疏且物体通常具有不完整的形状</strong>，准确估计 3D 物体的中心和大小是一项长期存在的挑战。</p><h4 id="Anchor-based-3D-object-detection"><a href="#Anchor-based-3D-object-detection" class="headerlink" title="Anchor-based 3D object detection"></a>Anchor-based 3D object detection</h4><p>锚点(anchors)是具有固定形状的预定义长方体,可以放置在 3D 空间中。</p><p><img data-src="https://i.imgur.com/jFknBKH.png" alt="image-20231223174714325"></p><p>3D 对象可以基于与真实值具有高交集 （IoU） 的正锚点进行预测。将从锚点配置和损失函数方面介绍基于锚点的三维目标检测方法。</p><p>Anchor configurations:基于锚点的 3D 对象检测方法通常<strong>从鸟瞰图检测 3D 对象</strong>,其中 3D 锚框放置在 BEV 特征图的每个网格单元上。3D 锚点通常对每个类别具有固定大小，因为同一类别的对象具有相似的大小</p><p>Loss functions:基于锚点的方法利用分类损失Lcls来学习正负锚点，利用回归损失Lreg来学习基于正锚点的物体的大小和位置。此外，L~θ~ 用于学习物体的航向角。</p><script type="math/tex; mode=display">\begin{equation}L_{det}=L_{cls}+L_{reg}+L_\theta.\end{equation}</script><p>回归目标可以进一步应用于这些正锚点,以学习 3D 对象的大小和位置.</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\Delta x&=\frac{x^g-x^a}{d^a},\Delta y=\frac{y^g-y^a}{d^a},\Delta z=\frac{z^g-z^a}{h^a},\\\Delta l&=\log(\frac{l^g}{l^a}),\Delta w=\log(\frac{w^g}{w^a}),\Delta h=\log(\frac{h^g}{h^a}),\end{aligned}\end{equation}</script><script type="math/tex; mode=display">\begin{equation}L_{cls}^{bce}=-[q\cdot\log(p)+(1-q)\cdot\log(1-p)]\end{equation}</script><p>p 是每个锚点的预测概率，如果锚点为正，则目标 q 为 1，否则为 0</p><script type="math/tex; mode=display">\begin{equation}d^a=\sqrt{(l^a)^2+(w^a)^2}\end{equation}</script><p>此外还有使用Focal Loss,</p><script type="math/tex; mode=display">\begin{equation}L_{cls}^{focal}=-\alpha(1-p)^\gamma\log(p),\end{equation}</script><p>使用SmoothL1 loss用于回归</p><script type="math/tex; mode=display">\begin{equation}L_{reg}=\sum_{\begin{array}{c}u\in\{x,y,z,l,w,h\},\\v\in\{\Delta x,\Delta y,\Delta z,\Delta l,\Delta w,\Delta h\}\end{array}}\text{SmoothL1}(u-v).\end{equation}</script><p>为了学习航向角 θ，弧度方向偏移量可以直接用 SmoothL1 损失回归</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}\Delta\theta&=\theta^g-\theta^a,\\L_\theta&=\text{SmoothL}1(\theta-\Delta\theta).\end{aligned}\end{equation}</script><p>然而，由于回归范围较大,直接回归弧度偏移通常很困难,另外<strong>,基于bin的航向估计</strong>是学习航向角的较好解，其中<strong>首先将角度空间划分为bin,并采用基于bin的分类L~dir~和残差回归</strong>.</p><script type="math/tex; mode=display">\begin{equation}L_\theta=L_{dir}+\text{SmoothL}1(\theta-\Delta\theta^{\prime}),\end{equation}</script><p>正弦函数也可用于对弧度偏移进行编码</p><script type="math/tex; mode=display">\begin{equation}\Delta\theta=\sin(\theta^g-\theta^a),\end{equation}</script><p>除了分别学习<strong>物体大小</strong>、<strong>位置</strong>和<strong>方向</strong>的损失函数外，将所有物体参数视为一个整体的交并（IoU）损失也可以应用于3D物体检测</p><script type="math/tex; mode=display">\begin{equation}L_{IoU}=1-IoU(b^g,b),\end{equation}</script><p>其中 C^G^ ~i~ 和 C~I~ 分别是地面实况和预测长方体的第 i 个角。</p><p>基于锚点的方法可以从同一类别的 3D 对象应该具有相似形状的先验知识中受益，因此它们可以在 <strong>3D 锚点的帮助下生成准确的对象预测</strong>。然而，<strong>由于3D物体相对于检测范围相对较小，因此需要大量的锚点来确保整个检测范围的完全覆盖</strong>，例如，在KITTI 数据集的中使用了大约70k个锚点。<strong>此外，对于那些非常小的物体，如行人和骑自行车的人，应用基于锚点的分配可能非常具有挑战性</strong>。考虑到锚点通常放置在每个网格单元的中心，如果网格单元较大而单元中的对象较小，则该单元的锚点可能与小对象具有较低的 IoU，这可能会阻碍训练过程。</p><h3 id="Anchor-free-3D-object-detection"><a href="#Anchor-free-3D-object-detection" class="headerlink" title="Anchor-free 3D object detection"></a>Anchor-free 3D object detection</h3><p>无anchor box方法消除了复杂的anchor box设计，可以灵活地应用于不同的视图，例如鸟瞰图、点视图和范围视图。</p><p><img data-src="https://i.imgur.com/6IcyJdX.png" alt="image-20231223190845168"></p><p>基于锚点和无锚点方法之间的<strong>主要区别在于正样本和负样本的选择</strong>。</p><p><strong>Grid-based assignment</strong>:与依赖于带有锚点的 IoU 来确定正负样本的基于锚点的方法相比，无锚点方法<strong>利用各种基于grid的分配策略来评估 BEV 网格单元、Pillar和体素</strong></p><p>PIXOR是一项开创性的工作，它<strong>利用地面实况 3D 物体内部的网格单元作为正例，而其他则作为负例。</strong>Pillar-based object detection for autonomous driving.采用了这种内部对象分配策略，并在中通过选择最接近对象中心的网格单元进一步改进。CenterPoint利用每个对象中心的高斯核来分配正标签。</p><p>损失函数上,分类损失基本不变.但回归损失改变如下</p><script type="math/tex; mode=display">\begin{equation}\Delta=[dx,dy,z^g,\log(l^g),\log(w^g),\log(h^g),\sin(\theta^g),\cos(\theta^g)],\end{equation}</script><p>DX 和 DY 是正grid cells和对象中心之间的偏移量。</p><p><strong>Point-based assignment.</strong></p><p>大多数基于点的检测方法<strong>采用无锚点和基于点的分配策略</strong>，其中<strong>首先对点进行分割，然后选择 3D 对象内部或附近的前景点作为正样本</strong>，最后从这些前景点中学习 3D 边界框。<strong>大多数基于点的检测器都采用了这种前景点分割策略，并进行了改进，例如增加了中心度分数</strong></p><p><strong>Range-based assignment</strong></p><p>无锚点分配也可以用于范围图像。<strong>一种常见的解决方案是选择 3D 对象内部的范围像素作为正样本</strong>。与其他回归目标基于全局三维坐标系的方法不同，<strong>基于范围的方法采用以对象为中心的坐标系进行回归</strong>。</p><p><strong>Set-to-set assignment.</strong>DETR是一种颇具影响力的 2D 检测方法，它引入了<strong>一种集到集的分配策略</strong>，通过匈牙利算法自动将预测分配给相应的地面实况</p><script type="math/tex; mode=display">\begin{equation}\mathcal{M}^*=\underset{\mathcal{M}}{\operatorname*{argmin}}\sum_{(i\to j)\in\mathcal{M}}L_{det}(b_i^g,b_j),\end{equation}</script><p>其中 M 是从每个阳性样本到 3D 对象的一对一映射。</p><h3 id="3D-object-detection-with-auxiliary-tasks"><a href="#3D-object-detection-with-auxiliary-tasks" class="headerlink" title="3D object detection with auxiliary tasks"></a>3D object detection with auxiliary tasks</h3><p>许多方法都采用辅助任务来增强空间特征，<strong>并为精确的 3D 目标检测提供隐式指导。常用的辅助任务包括语义分割、交集并集预测、对象形状补全和对象部分估计</strong>。</p><p><img data-src="https://i.imgur.com/TkWPitq.png" alt="image-20231223195729269"></p><p><strong>Semantic segmentation</strong>:语义分割可以从3个方面帮助3D目标检测：（1）前景分割可以提供对象位置的隐性信息。在大多数基于点的 3D 目标检测器 中，逐点前景分割已被广泛采用，用于生成提案。（2）空间特征可以通过分割来增强。在文献[347]中，利用语义上下文编码器来增强具有语义知识的空间特征。（3）语义分割可以作为预处理步骤，过滤掉背景样本，使3D目标检测更加高效。</p><p><strong>IoU prediction</strong>:交集并集 （IoU） 可以作为纠正对象置信度分数的有用监督信号。Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In: AAAI<strong>提出了一个辅助分支来预测每个检测到的 3D 对象的 IoU 分数 S~IoU~</strong>。</p><script type="math/tex; mode=display">\begin{equation}S_{conf}=S_{cls}\cdot(S_{IoU})^\beta,\end{equation}</script><p>在推理过程中,来自传统分类分支的原始置信度分数 S~con~f = S~cls~ 被 IoU 分数 SIoU 进一步校正.其中，超参数β控制抑制低 IoU 预测和增强高 IoU 预测的程度。<strong>通过 IoU 校正,更容易选择高质量的 3D 对象作为最终预测</strong>。</p><p><strong>Object shape completion</strong></p><p>由于LiDAR传感器的性质，<strong>远处物体通常只在其表面上接收到几个点，因此3D物体通常是稀疏和不完整的</strong>。提高检测性能的一种直接方法是从稀疏点云中完成物体形状。完整的形状可以为准确和稳健的检测提供更多有用的信息<strong>。在3D检测中已经提出了许多形状补全技术，包括形状解码器、形状特征和概率占用网格</strong></p><p><strong>Object part estimation.</strong></p><p>识别对象内部的零件信息有助于 3D 对象检测，因为它可以显示对象的更细粒度的 3D 结构信息。</p><p>3D 对象检测与许多其他 3D 感知和生成任务具有内在关联性。与独立训练 3D 目标检测器相比，3D 检测和分割的多任务学习更有利，形状补全也有助于 3D 目标检测。还有其他任务可以帮助提高 3D 对象检测器的性能。例如，场景流估计可以识别静态和移动对象，在点云序列中跟踪相同的 3D 对象可以更准确地估计该对象。因此，将更多的感知任务集成到现有的3D目标检测管道中将是有希望的。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;2023年的目标检测综述&lt;strong&gt;A comprehensive review of object detection with deep learning&lt;/strong&gt;以及&lt;strong&gt;3D Object Detection for Autonomous Driving: A Comprehensive Survey&lt;/strong&gt;,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下.&lt;/p&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>Python的工程化之路</title>
    <link href="https://www.sekyoro.top/2023/12/04/Python%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8B%E8%B7%AF/"/>
    <id>https://www.sekyoro.top/2023/12/04/Python%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8B%E8%B7%AF/</id>
    <published>2023-12-04T07:54:52.000Z</published>
    <updated>2023-12-23T12:54:53.563Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>在工程化上,Python相比于Java,C#这类语言还是差了不少,不过整个生态还是不错的.</p><span id="more"></span><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>一般有两种,一种称为flat另一种为src.</p><ol><li><blockquote><p>├── sample<br>│   ├── AUTHORS.rst<br>│   ├── docs<br>|   |   ├── conf.py<br>│   │   └── index.rst<br>│   ├── HISTORY.rst<br>│   ├── LICENSE<br>│   ├── makefile<br>│   ├── MANIFEST.in<br>│   ├── README.rst<br>│   ├── requirements.txt<br>│   ├── sample<br>|   |   ├── app.py<br>│   │   └── helper.py<br>|   ├── setup.cfg<br>|   ├── setup.py<br>│   └── tests<br><img data-src="https://s2.loli.net/2023/12/04/zSZesqwjb72IaUD.png" alt="image-20231204170021714"></p></blockquote></li></ol><ol><li><img data-src="https://s2.loli.net/2023/12/04/B5I7NEVSCRilscz.png" alt="image-20231204171740258"></li></ol><p>主要是使用poetry等工具打包的时候需要注意一下,因为<code>pyproject.toml</code>字段不完全相同.</p><h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><ol><li>ImportError: attempted relative import with no known parent package</li></ol><p>包中的模块使用相对导入时不能直接运行该模块,一般是其他包或顶级模块进行调用</p><h2 id="工具链"><a href="#工具链" class="headerlink" title="工具链"></a>工具链</h2><h3 id="版本管理工具"><a href="#版本管理工具" class="headerlink" title="版本管理工具"></a>版本管理工具</h3><p>Anaconda可以同时解决Python版本和包管理的问题,但如果只是想开发个包,没有必要使用conda,在linux上可以考虑pyenv+poetry,windows上有对应的pyenv-windows+poetry.</p><p>pyenv允许您轻松地在多个版本的Python之间切换。它简单、不引人注目，并且遵循了UNIX传统的单用途工具，可以很好地完成一件事。</p><p><a href="https://github.com/pyenv/pyenv">pyenv/pyenv: Simple Python version management (github.com)</a></p><h3 id="包管理工具"><a href="#包管理工具" class="headerlink" title="包管理工具"></a>包管理工具</h3><p>目前开发Python包我推荐Poetry或者PDM,如果是搞数据计算直接Anaconda.</p><h4 id="Poetry"><a href="#Poetry" class="headerlink" title="Poetry"></a>Poetry</h4><p><a href="https://python-poetry.org/docs/">Introduction | Documentation | Poetry - Python dependency management and packaging made easy (python-poetry.org)</a></p><p>某种程度上告别<code>setup.py</code>,除了一般的虚拟环境和包管理之外,打包和发布到PYPI等都支持,也是现在比较火的工具.</p><p>虚拟环境管理类似conda,会在某个目录下放所有的虚拟环境</p><p><img data-src="https://s2.loli.net/2023/12/04/zg1ChV4DXyoQeEt.png" alt="image-20231204163324338"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">poetry init</span><br><span class="line">poetry install </span><br><span class="line">poetry shell</span><br></pre></td></tr></table></figure><p>通过初始化一个新的Poetry项目，这将以交互方式生成一个文件pyproject.toml。该文件将具有所有包依赖项。这与requirements.txt文件类似。</p><p>当参数 <code>virtualenvs.create</code> 为 <code>true</code> 时，执行 <code>poetry install</code> 或 <code>poetry add</code> 时会检测当前项目是否有虚拟环境，没有就自动创建，默认为 <code>true</code>。</p><p>当参数 <code>virtualenvs.in-project</code> 为 <code>true</code> 时，虚拟环境的依赖将会放置于项目的文件夹内，而不是 poetry 默认的 <code>&#123;cache-dir&#125;/virtualenvs</code>，默认为 <code>false</code>。</p><p>我的配置如下:</p><p><img data-src="https://s2.loli.net/2023/12/04/Z1qGakLnrBwdejx.png" alt="image-20231204164441421"></p><blockquote><p>当Poetry完成安装后，它会将所有包及其下载的确切版本写入Poetry.lock文件，从而将项目锁定到这些特定版本。该锁定文件也应包含在您的项目repo中，以便在项目中工作的每个人都被锁定到相同版本的依赖项。</p></blockquote><p><img data-src="https://s2.loli.net/2023/12/04/9rkoJysmI8dFt5b.png" alt="image-20231204163627134"></p><h4 id="PDM"><a href="#PDM" class="headerlink" title="PDM"></a>PDM</h4><p><a href="https://github.com/pdm-project/pdm">pdm-project/pdm: A modern Python package and dependency manager supporting the latest PEP standards (github.com)</a></p><p>支持最新PEP标准的现代Python包和依赖项管理器. Poetry的<code>pyproject.toml</code>与PEP标准不完全符合.</p><p>PDM可以管理项目和集中位置的虚拟环境（venv），类似于Pipenv。它从标准化的pyproject.toml文件中读取项目元数据，并支持锁定文件。用户可以通过插件添加额外的功能，这些功能可以通过将其作为分发版上传来共享。与Poetry和Hatch不同，PDM不局限于特定的构建后端；用户可以自由选择他们喜欢的任何构建后端。</p><h3 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h3><p>代码格式化工具,一般用black就够了.</p><h4 id="Black"><a href="#Black" class="headerlink" title="Black"></a>Black</h4><p>Black是不折不扣的Python代码格式化程序。通过使用它，您同意放弃对手工格式化细节的控制。作为回报，Black为您提供了速度、决定论和自由，使您免受pycode风格对格式的唠叨。你会为更重要的事情节省时间和精力。无论您正在阅读的项目是什么，变黑的代码看起来都是一样的。一段时间后，格式将变得透明，您可以转而关注内容。Black通过产生尽可能小的差异来加快代码审查。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install black</span><br><span class="line">black &#123;source_file_or_directory&#125;</span><br></pre></td></tr></table></figure><h4 id="yapf"><a href="#yapf" class="headerlink" title="yapf"></a>yapf</h4><p>YAPF是一个基于clang格式的Python格式化程序（由Daniel Jasper开发）。本质上，该算法采用代码并计算符合配置样式的最佳格式。它省去了维护代码的许多繁琐工作。最终目标是YAPF生成的代码与程序员在遵循样式指南的情况下编写的代码一样好。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install yapf</span><br></pre></td></tr></table></figure><h4 id="autopep8"><a href="#autopep8" class="headerlink" title="autopep8"></a><strong>autopep8</strong></h4><p>autoep8自动格式化Python代码，以符合PEP8样式指南。它使用pycodestyle实用程序来确定需要格式化代码的哪些部分。autoep8能够修复pycodestyle可能报告的大多数格式问题。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade autopep8</span><br><span class="line">autopep8 --in-place --aggressive --aggressive &lt;filename&gt;</span><br></pre></td></tr></table></figure><h3 id="Linter"><a href="#Linter" class="headerlink" title="Linter"></a>Linter</h3><p>一般用pylint足矣,喜欢尝鲜的可以用用Ruff.</p><h4 id="PyLint"><a href="#PyLint" class="headerlink" title="PyLint"></a>PyLint</h4><p>Pylint是Python 2或3的静态代码分析器。最新版本支持Python 3.8.0及以上版本。Pylint在不实际运行代码的情况下分析代码。它检查错误，强制执行编码标准，寻找代码气味，并可以就如何重构代码提出建议。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pylint</span><br></pre></td></tr></table></figure><h4 id="flake8"><a href="#flake8" class="headerlink" title="flake8"></a>flake8</h4><p>Flake8是这些工具的包装：PyFlakespycode样式Ned Batchelder的McCabe脚本Flake8通过启动单个Flake8命令来运行所有工具。它在每个文件的合并输出中显示警告。<a href="https://github.com/PyCQA/flake8">PyCQA/flake8: flake8 is a python tool that glues together pycodestyle, pyflakes, mccabe, and third-party plugins to check the style and quality of some python code. (github.com)</a></p><h4 id="Ruff"><a href="#Ruff" class="headerlink" title="Ruff"></a>Ruff</h4><p>比较新的工具<a href="https://github.com/astral-sh/ruff">astral-sh/ruff: An extremely fast Python linter and code formatter, written in Rust. (github.com)</a>,不只是语法提示器,也可以用于格式化.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install ruff</span><br><span class="line"></span><br><span class="line">ruff path/to/code/to/check.py</span><br><span class="line">ruff path/to/code/</span><br><span class="line">ruff path/to/code/*.py</span><br></pre></td></tr></table></figure><h3 id="类型检查工具"><a href="#类型检查工具" class="headerlink" title="类型检查工具"></a>类型检查工具</h3><p>在Python中使用typing的检查工具,此外与Pydantic<a href="https://docs.pydantic.dev/latest/">Welcome to Pydantic - Pydantic</a>搭配使用效果更佳</p><blockquote><p>Pydantic是Python中使用最广泛的数据验证库。Pydantic快速且可扩展，可以很好地处理您的linters/IDE/brain。</p><p>定义数据应该如何使用纯规范的Python 3.7+；用Pydantic验证它。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Delivery</span>(<span class="params">BaseModel</span>):</span></span><br><span class="line">    timestamp: datetime</span><br><span class="line">    dimensions: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m = Delivery(timestamp=<span class="string">&#x27;2020-01-02T03:04:05Z&#x27;</span>, dimensions=[<span class="string">&#x27;10&#x27;</span>, <span class="string">&#x27;20&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">repr</span>(m.timestamp))</span><br><span class="line"><span class="comment">#&gt; datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))</span></span><br><span class="line"><span class="built_in">print</span>(m.dimensions)</span><br><span class="line"><span class="comment">#&gt; (10, 20)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Mypy"><a href="#Mypy" class="headerlink" title="Mypy"></a>Mypy</h4><p><a href="https://github.com/python/mypy">python/mypy: Optional static typing for Python (github.com)</a></p><p>Mypy是Python的静态类型检查器。类型检查器有助于确保您在代码中正确使用变量和函数。</p><p>使用mypy，将类型提示（PEP484）添加到Python程序中，当您错误地使用这些类型时，mypy会发出警告。Python是一种动态语言，所以通常只有当你试图运行它时，你才会在代码中看到错误。Mypy是一个静态检查器，所以它甚至不用运行就可以发现程序中的错误！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -U mypy</span><br><span class="line">mypy PROGRAM</span><br></pre></td></tr></table></figure><h4 id="Pyright"><a href="#Pyright" class="headerlink" title="Pyright"></a>Pyright</h4><p><a href="https://github.com/microsoft/pyright">microsoft/pyright: Static Type Checker for Python (github.com)</a></p><p>Pyright是一个功能齐全、基于标准的Python静态类型检查器。它是为高性能而设计的，可以与大型Python源代码库一起使用。</p><h3 id="Git-pre-commit-hook"><a href="#Git-pre-commit-hook" class="headerlink" title="Git pre-commit hook"></a>Git pre-commit hook</h3><p>在<code>git commit</code>之前设置hook进行代码检查</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>测试工具</p><h4 id="Pytest"><a href="#Pytest" class="headerlink" title="Pytest"></a>Pytest</h4><p>pytest框架使编写小型可读测试变得容易，并且可以扩展以支持应用程序和库的复杂功能测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U pytest</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.csdn.net/Dontla/article/details/131538693">python项目结构示例（python代码结构、python目录结构）与python部署结构、python部署目录、flask项目结构、flask目录_python项目结构目录结构-CSDN博客</a></li><li><a href="https://blog.csdn.net/captain5339/article/details/128017400">各类Python项目的项目结构及代码组织最佳实践<em>python项目结构__</em>弯弓__的博客-CSDN博客</a></li><li><a href="https://www.cnblogs.com/cuiyubo/p/11756771.html">Python最佳工程实践，建立一个完美的工程项目 - cuiyubo - 博客园 (cnblogs.com)</a></li><li><a href="https://www.hatica.io/blog/pre-commit-git-hooks/">8 Pre-commit Git Hooks You Must Know for Improved Productivity - Hatica</a></li><li><a href="https://pythonguidecn.readthedocs.io/zh/latest/writing/structure.html">结构化您的工程 — The Hitchhiker’s Guide to Python (pythonguidecn.readthedocs.io)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;在工程化上,Python相比于Java,C#这类语言还是差了不少,不过整个生态还是不错的.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>设计模式与重构</title>
    <link href="https://www.sekyoro.top/2023/12/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%87%8D%E6%9E%84/"/>
    <id>https://www.sekyoro.top/2023/12/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%87%8D%E6%9E%84/</id>
    <published>2023-12-03T05:08:27.000Z</published>
    <updated>2023-12-23T14:04:25.837Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>软件开发相关知识,主要是一些设计思想.<br><span id="more"></span></p><h2 id="基本设计思想"><a href="#基本设计思想" class="headerlink" title="基本设计思想"></a>基本设计思想</h2><h3 id="开闭原则"><a href="#开闭原则" class="headerlink" title="开闭原则"></a>开闭原则</h3><p>由Bertrand Meyer提出的开闭原则（Open Closed Principle）是指，软件应该对扩展开放，而对修改关闭。这里的意思是在增加新功能的时候，能不改代码就尽量不要改，如果只增加代码就完成了新功能，那是最好的。</p><h3 id="里氏替换原则"><a href="#里氏替换原则" class="headerlink" title="里氏替换原则"></a>里氏替换原则</h3><p>里氏替换原则是Barbara Liskov提出的，这是一种面向对象的设计原则，即如果我们调用一个父类的方法可以成功，那么替换成子类调用也应该完全可以运行。</p><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h3 id="创建型模式"><a href="#创建型模式" class="headerlink" title="创建型模式"></a>创建型模式</h3><p>这类模式提供创建对象的机制， 能够提升已有代码的灵活性和可复用性。</p><h4 id="工厂方法模式"><a href="#工厂方法模式" class="headerlink" title="工厂方法模式"></a>工厂方法模式</h4><blockquote><p>定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。</p></blockquote><p>工厂方法即Factory Method，是一种对象创建型模式。</p><h4 id="抽象工厂"><a href="#抽象工厂" class="headerlink" title="抽象工厂"></a>抽象工厂</h4><blockquote><p>提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。</p></blockquote><p>抽象工厂模式（Abstract Factory）是一个比较复杂的创建型模式</p><p>抽象工厂模式和工厂方法不太一样，它要解决的问题比较复杂，不但工厂是抽象的，产品是抽象的，而且有多个产品需要创建，因此，这个抽象工厂会对应到多个实际工厂，每个实际工厂负责创建多个实际产品</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><blockquote><p>将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。</p></blockquote><p>生成器模式（Builder）是使用多个“小型”工厂来最终创建出一个完整对象。</p><p>当我们使用Builder的时候，一般来说，是因为创建这个对象的步骤比较多，每个步骤都需要一个零部件，最终组合成一个完整的对象。</p><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><blockquote><p>用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。</p></blockquote><p>原型模式，即Prototype，是指创建新对象的时候，根据现有的一个原型来创建。</p><h4 id="单例"><a href="#单例" class="headerlink" title="单例"></a>单例</h4><blockquote><p>保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p></blockquote><p>单例模式（Singleton）的目的是为了保证在一个进程中，某个类有且仅有一个实例。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 静态字段引用唯一实例:</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton INSTANCE = <span class="keyword">new</span> Singleton();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// private构造方法保证外部无法实例化:</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结构性模式"><a href="#结构性模式" class="headerlink" title="结构性模式"></a>结构性模式</h3><p>这类模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。</p><h4 id="适配器"><a href="#适配器" class="headerlink" title="适配器"></a>适配器</h4><blockquote><p>将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p></blockquote><p>适配器模式是Adapter，也称Wrapper</p><h4 id="桥接"><a href="#桥接" class="headerlink" title="桥接"></a>桥接</h4><blockquote><p>将抽象部分与它的实现部分分离，使它们都可以独立地变化。</p></blockquote><h4 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h4><blockquote><p>将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。</p></blockquote><p>组合模式（Composite）经常用于树形结构，为了简化代码，使用Composite可以把一个叶子节点与一个父节点统一起来处理。</p><p>在XML或HTML中，从根节点开始，每个节点都可能包含任意个其他节点，这些层层嵌套的节点就构成了一颗树。</p><h4 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h4><blockquote><p>动态地给一个对象添加一些额外的职责。就增加功能来说，相比生成子类更为灵活。</p></blockquote><p>装饰器（Decorator）模式，是一种在运行期动态给某个对象的实例增加功能的方法</p><h4 id="外观"><a href="#外观" class="headerlink" title="外观"></a>外观</h4><blockquote><p>为子系统中的一组接口提供一个一致的界面。Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。</p></blockquote><p>外观模式，即Facade，是一个比较简单的模式。它的基本思想如下：</p><p>如果客户端要跟许多子系统打交道，那么客户端需要了解各个子系统的接口，比较麻烦。如果有一个统一的“中介”，让客户端只跟中介打交道，中介再去跟各个子系统打交道，对客户端来说就比较简单。所以Facade就相当于搞了一个中介。</p><h4 id="享元"><a href="#享元" class="headerlink" title="享元"></a>享元</h4><blockquote><p>运用共享技术有效地支持大量细粒度的对象。</p></blockquote><p>享元（Flyweight）的核心思想很简单：如果一个对象实例一经创建就不可变，那么反复创建相同的实例就没有必要，直接向调用方返回一个共享的实例就行，这样即节省内存，又可以减少创建对象的过程，提高运行速度。</p><h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><blockquote><p>为其他对象提供一种代理以控制对这个对象的访问。</p></blockquote><p>代理模式，即Proxy，它和Adapter模式很类似</p><h3 id="行为模式"><a href="#行为模式" class="headerlink" title="行为模式"></a>行为模式</h3><p>这类模式负责对象间的高效沟通和职责委派。</p><h4 id="责任链"><a href="#责任链" class="headerlink" title="责任链"></a>责任链</h4><blockquote><p>使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。</p></blockquote><p>责任链模式（Chain of Responsibility）是一种处理请求的模式，它让多个处理器都有机会处理该请求，直到其中某个处理成功为止</p><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><p>命令模式（Command）是指，把请求封装成一个命令，然后执行该命令。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TextEditor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> StringBuilder buffer = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">paste</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String text = getFromClipBoard();</span><br><span class="line">        add(text);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        buffer.append(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            buffer.deleteCharAt(buffer.length() - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> buffer.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CopyCommand</span> <span class="keyword">implements</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 持有执行者对象:</span></span><br><span class="line">    <span class="keyword">private</span> TextEditor receiver;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CopyCommand</span><span class="params">(TextEditor receiver)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.receiver = receiver;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        receiver.copy();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PasteCommand</span> <span class="keyword">implements</span> <span class="title">Command</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> TextEditor receiver;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PasteCommand</span><span class="params">(TextEditor receiver)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.receiver = receiver;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        receiver.paste();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="解释器"><a href="#解释器" class="headerlink" title="解释器"></a>解释器</h4><p>给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。</p><h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><p>提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。迭代器模式（Iterator）实际上在Java的集合类中已经广泛使用了。我们以<code>List</code>为例，要遍历<code>ArrayList</code>，即使我们知道它的内部存储了一个<code>Object[]</code>数组，也不应该直接使用数组索引去遍历，因为这样需要了解集合内部的存储结构</p><h4 id="中介模式"><a href="#中介模式" class="headerlink" title="中介模式"></a>中介模式</h4><blockquote><p>用一个中介对象来封装一系列的对象交互。中介者使各个对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。</p></blockquote><p>中介模式（Mediator）又称调停者模式，它的目的是把多方会谈变成双方会谈，从而实现多方的松耦合。</p><p>Mediator模式经常用在有众多交互组件的UI上。为了简化UI程序，MVC模式以及MVVM模式都可以看作是Mediator模式的扩展。</p><h4 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h4><blockquote><p>在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。</p></blockquote><p>备忘录模式（Memento），主要用于捕获一个对象的内部状态，以便在将来的某个时候恢复此状态。</p><p>其实我们使用的几乎所有软件都用到了备忘录模式。最简单的备忘录模式就是保存到文件，打开文件。对于文本编辑器来说，保存就是把<code>TextEditor</code>类的字符串存储到文件，打开就是恢复<code>TextEditor</code>类的状态。对于图像编辑器来说，原理是一样的，只是保存和恢复的数据格式比较复杂而已。Java的序列化也可以看作是备忘录模式。</p><h4 id="观察者"><a href="#观察者" class="headerlink" title="观察者"></a>观察者</h4><blockquote><p>定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p></blockquote><p>观察者模式（Observer）又称发布-订阅模式（Publish-Subscribe：Pub/Sub）。它是一种通知机制，让发送通知的一方（被观察方）和接收通知的一方（观察者）能彼此分离，互不影响。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Store</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;ProductObserver&gt; observers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, Product&gt; products = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册观察者:</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addObserver</span><span class="params">(ProductObserver observer)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.observers.add(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 取消注册:</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeObserver</span><span class="params">(ProductObserver observer)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.observers.remove(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addNewProduct</span><span class="params">(String name, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">        Product p = <span class="keyword">new</span> Product(name, price);</span><br><span class="line">        products.put(p.getName(), p);</span><br><span class="line">        <span class="comment">// 通知观察者:</span></span><br><span class="line">        observers.forEach(o -&gt; o.onPublished(p));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProductPrice</span><span class="params">(String name, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">        Product p = products.get(name);</span><br><span class="line">        p.setPrice(price);</span><br><span class="line">        <span class="comment">// 通知观察者:</span></span><br><span class="line">        observers.forEach(o -&gt; o.onPriceChanged(p));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h4><blockquote><p>允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。</p></blockquote><p>状态模式（State）经常用在带有状态的对象中。</p><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><blockquote><p>定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。</p></blockquote><p>策略模式：Strategy，是指，定义一组算法，并把其封装到一个对象中。然后在运行时，可以灵活的使用其中的一个算法</p><h4 id="模板方法"><a href="#模板方法" class="headerlink" title="模板方法"></a>模板方法</h4><blockquote><p>定义一个操作中的算法的骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。</p></blockquote><p>模板方法（Template Method）是一个比较简单的模式。它的主要思想是，定义一个操作的一系列步骤，对于某些暂时确定不下来的步骤，就留给子类去实现好了，这样不同的子类就可以定义出不同的步骤。</p><h4 id="访问者"><a href="#访问者" class="headerlink" title="访问者"></a>访问者</h4><blockquote><p>表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。</p></blockquote><p>访问者模式（Visitor）是一种操作一组对象的操作，它的目的是不改变对象的定义，但允许新增不同的访问者，来定义新的操作。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol><li><a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1264742167474528">设计模式 - 廖雪峰的官方网站 (liaoxuefeng.com)</a></li><li><a href="https://refactoringguru.cn/design-patterns/catalog">设计模式目录：22种设计模式 (refactoringguru.cn)</a></li><li><a href="https://www.runoob.com/design-pattern/design-pattern-intro.html">设计模式简介 | 菜鸟教程 (runoob.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;软件开发相关知识,主要是一些设计思想.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>图像特征协同感知融合算法</title>
    <link href="https://www.sekyoro.top/2023/11/30/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95/"/>
    <id>https://www.sekyoro.top/2023/11/30/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95/</id>
    <published>2023-11-30T02:58:03.000Z</published>
    <updated>2024-01-17T01:52:30.616Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>除了3D<strong>目标检测</strong>算法外,自动驾驶还需要将获取到的图像数据或者处理后的特征进行<strong>通信</strong>和<strong>融合</strong>,这里介绍相关论文.</p><span id="more"></span><p>重要的几个仓库<a href="https://github.com/coperception/coperception">coperception/coperception: An SDK for multi-agent collaborative perception. (github.com)</a>,<a href="https://github.com/DerrickXuNu/OpenCOOD和[ucla-mobility/V2V4Real">https://github.com/DerrickXuNu/OpenCOOD和[ucla-mobility/V2V4Real</a>: <a href="https://github.com/ucla-mobility/v2v4real">CVPR2023 Highlight] The official codebase for paper “V2V4Real: A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception” (github.com)</a></p><p>此外<a href="https://github.com/Little-Podi/Collaborative_Perception">Little-Podi/Collaborative_Perception: This repository is a paper digest of recent advances in collaborative / cooperative / multi-agent perception for V2I / V2V / V2X autonomous driving scenario. (github.com)</a>会汇总相关领域最新的论文.</p><h2 id="OPV2V-An-Open-Benchmark-Dataset-and-Fusion-Pipeline-for-Perception-with-Vehicle-to-Vehicle-Communication"><a href="#OPV2V-An-Open-Benchmark-Dataset-and-Fusion-Pipeline-for-Perception-with-Vehicle-to-Vehicle-Communication" class="headerlink" title="OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication"></a>OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>利用车对车通信提高自动驾驶技术的感知性能近年来引起了广泛关注;然而，由于缺乏合适的开放数据集来对算法进行基准测试，因此很难开发和评估协同感知技术。</p><h3 id="融合方法介绍"><a href="#融合方法介绍" class="headerlink" title="融合方法介绍"></a>融合方法介绍</h3><p>论文中融合算法介绍了多种,这里按时间线写一下.</p><p>早期融合有最早的Cooper.</p><p>主要介绍中期融合</p><h4 id="F-cooper"><a href="#F-cooper" class="headerlink" title="F-cooper"></a>F-cooper</h4><p>2019论文中提出了VFF和SFF.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatialFusion</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SpatialFusion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">regroup</span>(<span class="params">self, x, record_len</span>):</span></span><br><span class="line">        cum_sum_len = torch.cumsum(record_len, dim=<span class="number">0</span>)</span><br><span class="line">        split_x = torch.tensor_split(x, cum_sum_len[:-<span class="number">1</span>].cpu())</span><br><span class="line">        <span class="keyword">return</span> split_x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, record_len</span>):</span></span><br><span class="line">        <span class="comment"># x: B, C, H, W, split x:[(B1, C, W, H), (B2, C, W, H)]</span></span><br><span class="line">        split_x = self.regroup(x, record_len)</span><br><span class="line">        out = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> xx <span class="keyword">in</span> split_x:</span><br><span class="line">            xx = torch.<span class="built_in">max</span>(xx, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">            out.append(xx)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(out, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>最后利用融合后的特征传给cls_head和reg_head</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">psm = self.cls_head(fused_feature)</span><br><span class="line">rm = self.reg_head(fused_feature)</span><br><span class="line">output_dict = &#123;<span class="string">&#x27;psm&#x27;</span>: psm,</span><br><span class="line">               <span class="string">&#x27;rm&#x27;</span>: rm&#125;</span><br></pre></td></tr></table></figure><h4 id="V2VNet-Vehicle-to-Vehicle-Communication-for-Joint-Perception-and-Prediction-2020"><a href="#V2VNet-Vehicle-to-Vehicle-Communication-for-Joint-Perception-and-Prediction-2020" class="headerlink" title="V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction 2020"></a>V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction 2020</h4><h5 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h5><p>探讨了使用<strong>车对车（V2V）通信来改善自动驾驶汽车的感知和运动预测性能</strong>。通过智能地聚合从附近多辆车接收到的信息，我们可以从不同的视角观察同一场景。这使我们能够<strong>穿透遮挡物并远距离检测参与者，其中观察结果非常稀疏或不存在</strong>。</p><p>SDV 需要对场景进行 3D 推理，识别其他智能体，并预测他们的未来可能如何发展。这些任务通常称为感知和运动预测。强大的感知和运动预测对于 SDV 规划和操纵交通以安全地从一个点到另一个点都至关重要.</p><p>尽管取得了这些进展，但挑战依然存在。例如，<strong>被严重遮挡或距离较远的物体会导致观测稀疏，并对现代计算机视觉系统构成挑战</strong>。</p><p>在本文中，考虑了车对车 （V2V） 通信设置，其中每辆车都可以向附近的车辆（半径 70m 以内）广播和接收信息。请注意，基于现有的通信协议，这个广播范围是现实的。为了在满足现有硬件传输带宽能力的同时，实现具有强大感知和运动预测性能的最佳折衷方案，我们应该发送P&amp;P神经网络的压缩中间表示。因此，我们推导出了一种名为V2VNet的新颖的P&amp;P模型，该模型<strong>利用空间感知图神经网络（GNN）来聚合从附近所有SDV接收到的信息</strong>，使我们能够智能地组合来自不同时间点和场景中视点的信息。</p><p><img data-src="https://s2.loli.net/2024/01/07/MXuJrkjUeywFPf1.png" alt="image-20240107145123567"></p><p><img data-src="https://s2.loli.net/2024/01/07/JMEKzxTutGrfW8h.png" alt="image-20240107145440332"></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>较早的一篇使用全面神经网络探讨V2V协同的目标感知与检测,贡献了V2V-sim仿真数据集.</p><h4 id="DiscoNet"><a href="#DiscoNet" class="headerlink" title="DiscoNet"></a>DiscoNet</h4><p><img data-src="https://s2.loli.net/2024/01/10/kU47aJnKuABtjS3.png" alt="image-20240110151834154"></p><p>涉及到了图和蒸馏的方法,我了解不多.</p><p><img data-src="https://s2.loli.net/2024/01/10/CnGtDVSgHJWEjLd.png" alt="image-20240110151903360"></p><h4 id="CoBEVT-2022"><a href="#CoBEVT-2022" class="headerlink" title="CoBEVT 2022"></a>CoBEVT 2022</h4><p><strong>Abstract</strong></p><p>在本文中，提出了CoBEVT，这是第一个可以<strong>协同生成BEV地图预测的通用多智能体多相机感知框架</strong>。</p><p>为了在底层 Transformer 架构中有效地融合来自多视图和多智能体数据的相机特征，我们设计了一个融合轴向注意力模块 （FAX），该模块可以捕获跨视图和智能体的稀疏局部和全局空间交互.</p><p>将多摄像头视图投射到整体纯电动汽车空间中，在空间和时间上保持道路元素的位置和比例方面具有明显的优势，这对于各种自动驾驶任务（包括场景理解和规划）至关重要</p><p>地图视图（或 BEV）语义分割是一项基本任务,旨在根据单个或多个校准的摄像头输入预测路段。在基于摄像头的精确BEV语义分割方面已经做出了重大努力。最流行的技术之一是利用<strong>深度信息来推断相机视图和规范地图之间的对应关系</strong>。另一个系列使用<strong>基于注意力的模型直接学习摄像头到BEV的空间转换</strong>，无论是隐式还是显式</p><p>尽管取得了令人鼓舞的结果，<strong>但基于视觉的感知系统具有固有的局限性——众所周知，相机传感器对物体遮挡和景深有限很敏感，这可能导致在严重遮挡或远离相机镜头的区域性能较差</strong></p><p><img data-src="https://s2.loli.net/2024/01/02/I1xRkviLBmbnqfQ.png" alt="image-20240102164321954">`</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>最近，V2VNet提出将从3D骨干中提取的中间特征（即中间融合）循环，然后利用空间感知图神经网络进行多智能体特征聚合。遵循类似的传输范式，<strong>OPV2V 采用简单的智能体单头注意力来融合所有特征</strong>。<strong>F-Cooper使用简单的 maxout 操作来融合特征</strong>。</p><p><strong>DiscoNet通过约束中间特征图来匹配早期融合教师模型中的对应关系来探索知识蒸馏。</strong></p><p>与之前的多智能体算法相比,我 CoBEVT 率先<strong>采用稀疏transformer来高效</strong>、详尽地探索车辆之间的相关性。</p><p>此外，以往的方法主要侧重于与激光雷达的协同感知，而我们的目标是提出一种低成本的<strong>基于摄像头</strong>的、<strong>没有激光雷达设备的协同感知解决方案</strong>。</p><blockquote><p>Transformer 最初是为自然语言处理而提出的 。ViT  首次证明，一个纯粹的 Transformer 只是将图像块视为视觉词，通过大规模预训练就足以完成视觉任务。Swin Transformer通过限制局部（移位）窗口中的注意力场，进一步提高了纯 Transformer 的通用性和灵活性。对于高维数据，Video Swin Transformer 将 Swin 方法扩展到移动的 3D 时空窗口，从而实现高性能和低复杂性。</p><p>最近的工作主要集中在<strong>改进注意力模型的结构</strong>上，包括稀疏注意力，扩大的感受野，金字塔设计，有效的替代方案]等。</p></blockquote><h4 id="Fused-Axial-Attention"><a href="#Fused-Axial-Attention" class="headerlink" title="Fused Axial Attention"></a>Fused Axial Attention</h4><p><strong>融合来自多个智能体的 BEV 特征需要跨所有智能体的空间位置进行局部和全局交互</strong>。一方面,相邻的自动驾驶汽车通常对同一物体具有不同的遮挡级别;因此,更关心细节的局部注意力可以帮助在该对象上构建像素到像素的对应关系。</p><p><strong>ego应汇总附近自动驾驶汽车每个位置的所有BEV特征</strong>，以获得可靠的估计。另一方面，<strong>长期的全局情境感知也有助于理解道路拓扑语义或交通状态</strong>——车辆前方的道路拓扑和交通密度通常与后方的道路拓扑和交通密度高度相关。这种全局推理也有利于多相机视图的理解。</p><p>同一辆车被分成多个视图,global attention能够将它们连接起来以进行语义推理</p><p><img data-src="https://s2.loli.net/2024/01/03/w5AtSc4o2gsTzB9.png" alt="image-20240103202226517"></p><p>提出了一种称为融合轴向注意力的新颖 3D 注意力机制,作为 SinBEVT 和 FuseBEVT 的核心组件,可以有效地聚合本地和全局范围内跨代理或相机视图的特征。</p><p><img data-src="https://s2.loli.net/2024/01/05/Sel1ad8xyFHqLfJ.png" alt="image-20240105103533757"></p><p><strong>设 X∈ R^N×H×W×C^ 是来自 N 智能体的空间维度为 H × W 的堆叠 BEV 特征</strong></p><p>在局部特征上<strong>,将特征图划分为 3D 非重叠窗口</strong>,每个窗口的大小为 N × P × P,然后将形状为(H/P× W/P,N × P^2^,C) 的分割张量 输入到自注意力模型中</p><p>在全局特征中,使用统一的 3D 网格 N ×G×G 将特征 X 划分为形状 (N × G^2^,H/G×W /G,C)</p><p>将注意力集中在该张量的第一个轴上,该张量表示关注稀疏采样的标记</p><p><img data-src="https://s2.loli.net/2024/01/03/NVMak8mhlgxeKwo.png" alt="image-20240103203511174"></p><p>将这种 3D 局部和全局注意力与 Transformer 的典型设计相结合 ，包括层归一化 （LN）、MLP  和跳跃连接，形成了提出的 FAX 注意力块</p><h4 id="SinBEVT-for-Single-agent-BEV-Feature-Computation"><a href="#SinBEVT-for-Single-agent-BEV-Feature-Computation" class="headerlink" title="SinBEVT for Single-agent BEV Feature Computation"></a>SinBEVT for Single-agent BEV Feature Computation</h4><p>CVT 使用低分辨率的 BEV 查询，该查询完全交叉关注图像特征，这会导致小物体的性能下降，尽管效率很高。因此，<strong>CoBEVT 学习高分辨率的 BEV 嵌入，然后使用分层结构来优化分辨率降低的 BEV 特征</strong>。为了在高分辨率下有效地查询来自相机编码器的特征，FAX-SA模块进一步扩展为构建FAX交叉关注（FAX-CA）模块，其中查询向量使用BEV嵌入获得，而键/值向量由多视图相机特征投影。</p><h5 id="FuseBEVT-for-Multi-agent-BEV-Feature-Fusion"><a href="#FuseBEVT-for-Multi-agent-BEV-Feature-Fusion" class="headerlink" title="FuseBEVT for Multi-agent BEV Feature Fusion"></a>FuseBEVT for Multi-agent BEV Feature Fusion</h5><p>一旦接收到包含中间BEV表示的广播消息和发送者的姿态,自我车辆就会应用可微分的空间变换算子Γξ,将接收到的特征几何扭曲到ego的坐标系上:H~i~ = Γ~ξ~ (F~i~)∈ R^H×W×C^</p><p>3D FAX-SA 可以处理从多个代理抽取的同一估计区域（红框），以得出最终的聚合表示。此外，稀疏采样token（蓝框）可以全局交互，以获得对地图语义（如道路、交通等）的上下文理解。</p><h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><p>使用swim transfomer类似架构进行多层级的特征融合.设计了agent_size和window_size.</p><h4 id="Robust-Collaborative-3D-Object-Detection-in-Presence-of-Pose-Errors-CoAlign-2022"><a href="#Robust-Collaborative-3D-Object-Detection-in-Presence-of-Pose-Errors-CoAlign-2022" class="headerlink" title="Robust Collaborative 3D Object Detection in Presence of Pose Errors (CoAlign) 2022"></a>Robust Collaborative 3D Object Detection in Presence of Pose Errors (CoAlign) 2022</h4><h5 id="Abs-1"><a href="#Abs-1" class="headerlink" title="Abs"></a>Abs</h5><p>协作式 3D 目标检测利用多个智能体之间的信息交换，在存在传感器损伤（如遮挡）的情况下提高目标检测的准确性,然而，在实践中，由于定位不完善导致的姿态估计误差会导致空间信息错位，并显著降低协作性能。</p><p>为了减轻姿势错误的不利影响，我们提出了CoAlign，这是一种新颖的混合协作框架，对<strong>未知的姿势错误具有鲁棒性</strong>。所提出的解决方案依赖于一种<strong>新颖的智能体-对象姿态图建模来增强协作智能体之间的姿态一致性</strong>。此外，<strong>采用多尺度数据融合策略，在多个空间分辨率下聚合中间特征</strong>。</p><p>尽管大规模数据集和强大模型的发展速度很快，<strong>但单个智能体的 3D 对象检测仍存在固有的局限性，例如遮挡和远处的物体。通过利用智能体之间的通信，例如驾驶场景中的车联网（V2X），多个智能体可以相互共享互补的感知信息，从而促进更全面的接受领域。</strong></p><p>为了实现这种协作式3D检测，最近的工作<strong>贡献了高质量的数据集</strong>和<strong>有效的协作方法</strong>。但在这个新兴领域仍然存在许多挑战，例如通<strong>信带宽限制</strong>、<strong>延迟</strong>和<strong>对抗性攻击</strong>。这项工作的重点是减轻姿势误差的负面影响。</p><p>由于我们的<strong>智能体-对象姿态图</strong>在优化过程中<strong>没有使用任何训练参数</strong>，因此该方法具有<strong>很强的泛化能力，可以适应任意级别的姿态误差</strong>。为了有效缓解姿态误差的影响，我们进一步考虑了一种<strong>多尺度中间融合策略，该策略在多个空间尺度上全面聚合协作信息。</strong></p><p>我们在仿真和真实数据集上对基于LiDAR的3D目标检测任务进行了广泛的实验，包括<strong>OPV2V 、V2X-Sim 2.0和DAIR-V2X</strong>.所提出的CoAlign在存在姿态误差的协同3D目标检测任务中实现了<strong>至少12%的性能提升</strong>。</p><p>考虑场景中的 N 个代理。每个智能体都具有感知、沟通和检测的能力。目标是通过分布式协作来达到每个智能体更好的 3D 检测能力。</p><p>在以前的文献中，有三种类型的协作：早期协作，传输原始观测数据，中间协作，，传输中间特征，后期协作，传输检测输出。</p><p>设 O~i~ 和 B~i~ 分别是第 i 个智能体的感知观察和检测输出。对于第 i 个代理，基于中间协作的标准 3D 对象检测的工作原理如下</p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{F}_{i}=f_{\mathrm{encoder}}\left(\mathbf{O}_{i}\right), \\&\mathbf{M}_{j\to i}=f_{\mathbf{transform}}\left(\xi_i,(\mathbf{F}_j,\xi_j)\right), \\&\mathbf{F}_{i}^{\prime}=f_{\mathrm{fusion}}\left(\mathbf{F}_{i},\{\mathbf{M}_{j\to i}\}_{j=1,2,\cdots,N}\right), \\&\mathbf{B}_{i}=f_{\mathrm{decoder}}\left(\mathbf{F}_{i}^{\prime}\right),\end{aligned}</script><blockquote><p>在实践中，定位模块估计的每个 6DoF 姿态 ξi 通常是有噪声的。然后，在步骤2中进行姿态变换后，每个消息Mj→i将具有不同的固有坐标系，导致步骤3中的融合错位和步骤4中的检测输出不好。</p><p>这项工作的目标<strong>是通过在步骤2之前引入额外的姿势校正来最大限度地减少姿势错误的影响</strong></p></blockquote><p>CoAlign<strong>结合了中间和后期协作策略。与中间融合相比，这种混合协作可以利用智能体检测到的边界框作为场景地标，并校正智能体之间的相对姿态</strong>。</p><script type="math/tex; mode=display">\begin{gathered}\mathbf{F}_{i},\mathbf{B}_{i} =\quad f_\text{detection}\left(\mathbf{O}_i\right), \text{(2a)} \\\{\xi_{j\rightarrow i}^{\prime}\}_j =\quad f_{\mathrm{correction}}\left(\{\mathbf{B}_{j},\xi_{j}\}_{j=1,2,\cdots,N}\right), (2\mathbf{b}) \\\mathrm{M}_{j\rightarrow i} =\quad f_{\mathrm{transform}}\left(\mathbf{F}_j,\xi_{j\to i}^{\prime}\right), \text{(2c)} \\\mathbf{F}_{i}^{\prime} =\quad f_{\mathrm{fusion}}\left(\{\mathrm{M}_{j\to i}\}_{j=1,2,\cdots,N}\right), \text{(2d)} \\\mathbf{B}_{i}^{\prime} =\quad f_\text{decoder}\left(\mathbf{F}_i^{\prime}\right), \text{(2c)} \end{gathered}</script><p><img data-src="https://s2.loli.net/2024/01/07/Q8ywAZ2N7IcCBSG.png" alt="image-20240107164141678"></p><p>为了实现单智能体 3D 目标检测器 fdetection(·),可以利用现成的设计，例如 PointPillars ,为第 i 个智能体生成中间特征 F~i~ 和估计边界框 B~i~。请注意，对<strong>于每个边界框，我们还估计其不确定性</strong>。</p><p>由于我们后来依靠这些框来纠正姿势错误，因此混乱的检测可能会导致更糟糕的相对姿势。每个框的估计不确定度可以提供有益的置信度信息，以排除不需要的检测。</p><p>每个框的估计不确定度可以提供有益的置信度信息，以排除不需要的检测。</p><h5 id="Agent-Object-Pose-Graph-Optimization"><a href="#Agent-Object-Pose-Graph-Optimization" class="headerlink" title="Agent-Object Pose Graph Optimization"></a>Agent-Object Pose Graph Optimization</h5><p>单智能体检测后，第 i 个智能体共享三类消息，包括 </p><p>i） 其姿态 ξ~i~ 由其自身定位模块估计;</p><p>ii） 第 i 个agent检测到的边界框;</p><p>iii） 其特征图 F~i~</p><p>为了可靠地融合来自其他智能体的特征图,每个智能体都需要校正相对姿态。</p><h5 id="Multiscale-Feature-Fusion"><a href="#Multiscale-Feature-Fusion" class="headerlink" title="Multiscale Feature Fusion"></a>Multiscale Feature Fusion</h5><p>空间对齐后，每个智能体聚合其他智能体的协作信息，并获得信息量更大的特征。但是，即使在相对姿态校正之后，特征图之间的错位可能仍然存在。更精细尺度的特征可以提供更详细的几何和语义信息，而粗尺度的特征对位姿误差的敏感度较低。多尺度结构可以同时发挥优势，并产生信息丰富和强大的功能。</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{F}_{j\to i}^{(1)}& =\quad\mathbf{M}_{j\to i},j=1,2,\cdots N,  \\\mathbf{F}_{j\to i}^{(\ell+1)}& =\quad g_\ell(\mathbf{F}_{j\to i}^{(\ell)}),\ell=1,2,\cdots,L,  \\\mathbf{F}_{i}^{(\ell)}& =\quad\text{Fuse}(\{\mathbf{F}_{j\to i}^{(\ell)}\}_{j=1,2,\cdots,N}),  \\\mathbf{F}_{i}^{\prime}& =\quad\mathrm{Cat}([\mathbf{F}_i^{(1)},u_2(\mathbf{F}_i^{(2)}),\cdots,u_L(\mathbf{F}_i^{(L)})]), \end{aligned}</script><p><strong>总结</strong></p><p>结合了单车的late和intermediate的输出,提出了agent-pose map用于修正pose.使用多尺度融合.CoAlign在融合层面主要使用self-attention和multi-scale(backbone使用了多尺度的ResNet).在代码中使用resnet并下采样得到多尺度的特征,将这些多尺度特征通过融合模块.融合模块种通过warp_affine将特征转换到(H,W)整个感知范围内,再进行self-attention.然后将融合后的多尺度特征通过多级decoder最后concat在一起.</p><p>其中affine操作利用车之间转换矩阵以及affine_grid和affine_sample得到,也就是将特征仿射变换到lidar range.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_pairwise_tfm</span>(<span class="params">pairwise_t_matrix, H, W, discrete_ratio, downsample_rate=<span class="number">1</span></span>):</span></span><br><span class="line">    pairwise_t_matrix = pairwise_t_matrix[:,:,:,[<span class="number">0</span>, <span class="number">1</span>],:][:,:,:,:,[<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>]] <span class="comment"># [B, L, L, 2, 3]</span></span><br><span class="line">    pairwise_t_matrix[...,<span class="number">0</span>,<span class="number">1</span>] = pairwise_t_matrix[...,<span class="number">0</span>,<span class="number">1</span>] * H / W</span><br><span class="line">    pairwise_t_matrix[...,<span class="number">1</span>,<span class="number">0</span>] = pairwise_t_matrix[...,<span class="number">1</span>,<span class="number">0</span>] * W / H</span><br><span class="line">    pairwise_t_matrix[...,<span class="number">0</span>,<span class="number">2</span>] = pairwise_t_matrix[...,<span class="number">0</span>,<span class="number">2</span>] / (downsample_rate * discrete_ratio * W) * <span class="number">2</span></span><br><span class="line">    pairwise_t_matrix[...,<span class="number">1</span>,<span class="number">2</span>] = pairwise_t_matrix[...,<span class="number">1</span>,<span class="number">2</span>] / (downsample_rate * discrete_ratio * H) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    normalized_affine_matrix = pairwise_t_matrix</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> normalized_affine_matrix</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warp_affine_simple</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        src, M, dsize,</span></span></span><br><span class="line"><span class="params"><span class="function">        mode=<span class="string">&#x27;bilinear&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        padding_mode=<span class="string">&#x27;zeros&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        align_corners=<span class="literal">False</span></span>):</span></span><br><span class="line">    B, C, H, W = src.size()</span><br><span class="line">    grid = F.affine_grid(M, [B, C, dsize[<span class="number">0</span>], dsize[<span class="number">1</span>]],</span><br><span class="line">                         align_corners=align_corners).to(src)</span><br><span class="line">    <span class="keyword">return</span> F.grid_sample(src, grid, align_corners=align_corners)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Att_w_Warp</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, feature_dims</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Att_w_Warp, self).__init__()</span><br><span class="line">        self.att = ScaledDotProductAttention(feature_dims)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, xx, record_len, normalized_affine_matrix</span>):</span></span><br><span class="line">        _, C, H, W = xx.shape</span><br><span class="line">        B, L = normalized_affine_matrix.shape[:<span class="number">2</span>]</span><br><span class="line">        split_x = regroup(xx, record_len)</span><br><span class="line">        batch_node_features = split_x</span><br><span class="line">        out = []</span><br><span class="line">        <span class="comment"># iterate each batch</span></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(B):</span><br><span class="line">            N = record_len[b]</span><br><span class="line">            t_matrix = normalized_affine_matrix[b][:N, :N, :, :]</span><br><span class="line">            <span class="comment"># update each node i</span></span><br><span class="line">            i = <span class="number">0</span> <span class="comment"># ego</span></span><br><span class="line">            x = warp_affine_simple(batch_node_features[b], t_matrix[i, :, :, :], (H, W))</span><br><span class="line">            cav_num = x.shape[<span class="number">0</span>]</span><br><span class="line">            x = x.view(cav_num, C, -<span class="number">1</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>) <span class="comment">#  (H*W, cav_num, C), perform self attention on each pixel.</span></span><br><span class="line">            h = self.att(x, x, x)</span><br><span class="line">            h = h.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).view(cav_num, C, H, W)[<span class="number">0</span>, ...]  <span class="comment"># C, W, H before</span></span><br><span class="line">            out.append(h)</span><br><span class="line"></span><br><span class="line">        out = torch.stack(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">        self.sqrt_dim = np.sqrt(dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value</span>):</span></span><br><span class="line">        score = torch.bmm(query, key.transpose(<span class="number">1</span>, <span class="number">2</span>)) / self.sqrt_dim</span><br><span class="line">        attn = F.softmax(score, -<span class="number">1</span>)</span><br><span class="line">        context = torch.bmm(attn, value)</span><br><span class="line">        <span class="keyword">return</span> context</span><br></pre></td></tr></table></figure><h4 id="Learning-for-Vehicle-to-Vehicle-Cooperative-Perception-under-Lossy-Communication-V2VAM-2023"><a href="#Learning-for-Vehicle-to-Vehicle-Cooperative-Perception-under-Lossy-Communication-V2VAM-2023" class="headerlink" title="Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication (V2VAM) 2023"></a>Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication (V2VAM) 2023</h4><h5 id="Abs-2"><a href="#Abs-2" class="headerlink" title="Abs"></a>Abs</h5><p>深度学习已广泛应用于智能车辆驾驶感知系统，如3D目标检测。一种很有前途的技术是协同感知，它利用车对车（V2V）通信在车辆之间共享基于深度学习的特征。然而，大<strong>多数协作感知算法都假设了理想的通信，而没有考虑现实世界中非常常见的有损通信（LC）对特征共享的影响</strong>。</p><p>在本文中探讨了LC对合作感知的影响，并提出了一种新的方法来减轻这些影响。我们的方法包括LC感知维修网络（LCRN）和V2V注意力模块（V2VAM），具有车内注意力和不确定性感知车辆间注意力。</p><p>V2V通信中的信息共享有三种方式：（1）将原始传感器数据共享为早期融合，（2）将基于深度学习的检测网络的中间特征共享为中间融合，（3）将检测结果共享为后期融合。</p><p><strong>最近的研究]表明，中间融合是检测精度和带宽要求之间的最佳权衡</strong>。</p><p>最近提出了许多用于V2V协同感知的中间融合方法;但是，它们都假定了理想的通信。<strong>唯一一项考虑非理想通信的V2V协同感知研究仅关注通信延迟</strong>。迄今为止，尚无研究探讨有损通信（LC）对复杂真实驾驶环境中V2V协同感知的影响</p><p>在城市交通场景中，V2V通信容易受到一系列可能导致有损通信的因素的影响，例如<strong>来自障碍物</strong>（例如建筑物和车辆）的多径效应(电磁波经不同路径传播后，各分量场到达接收端时间不同，按各自相位相互叠加而造成干扰，使得原来的信号失真，或者产生错误)、<strong>快速移动的车辆引入的多普勒频移</strong>、<strong>其他通信网络产生的干扰</strong>.</p><p>该文首先研究了<strong>有损通信在V2V协同感知中的负面影响</strong>，<strong>然后提出了一种新的中间LC感知特征融合方法</strong>。具体而言，所提方法<strong>包括LC感知修复网络</strong>（LCRN）和<strong>专门设计的V2V注意力模块</strong>（V2VAM），以增强自我车辆与其他车辆之间的交互.</p><p>V2VAM包括<strong>ego车辆的车内注意力</strong>和<strong>不确定性感知的车辆间注意力</strong>。在真实驾驶中收集具有有损通信的真实CAV感知数据具有挑战性.</p><blockquote><p>基于LiDAR的检测方法，这些方法通常将LiDAR点转换为体素或柱子，在基于体素的、或基于Pillar的目标检测方法中占主导地位。PointRCNN提出了一种基于原始点云的两阶段策略，即先学习粗略估计，然后用语义属性对其进行细化。一些方法[]建议将空间分割成体素，并为每个体素生成特征。但是，3D 体素的处理成本通常很高。为了解决这个问题，PointPillars建议将沿z轴的所有体素压缩为一个柱子，然后在鸟瞰空间中预测3D框。此外，最近的一些方法结合了基于体素和基于点的方法，以联合检测3D物体。</p></blockquote><p>3D 感知方法的性能很大程度上取决于 3D 点云的精度。然而，LiDAR 摄像头存在折射、遮挡和远距离等问题，因此单车系统在一些具有挑战性的情况下可能会变得不可靠 。近年来，车对车（V2V）/车对基础设施（V2X）协同系统被提出，以克服单车系统使用多辆车的弊端。不同车辆之间的协作使3D感知网络能够融合来自不同来源的信息。</p><p>为了在数据负载和准确性之间找到平衡，<strong>最近的方法侧重于通过共享中间表示来进行中间融合。F-cooper应用体素特征融合和空间特征融合来自两辆车。V2VNet 采用图神经网络来聚合 LiDAR 从每辆车中提取的特征。V2X-ViT 提出了一种视觉 Transformer 架构，用于融合车辆和基础设施的功能。Cui等提出了一种基于点的Transformer进行点云处理，该Transformer可以将协同感知与控制决策相结合</strong></p><p>Tu等提出了一种基于<strong>中间表示的多智能体深度学习系统中高效实用的在线攻击网络</strong>。Luo等利用<strong>注意力模块融合中间特征，增强特征互补性</strong>。Lei等提出了一种<strong>延迟补偿模块，以实现中间特征级同步</strong>。胡(where2comm)等提出了一种空间置信度感知沟通策略，通过关注感知关键领域来使用较少的communication来提高绩效.</p><p><strong>OPV2V利用自注意力模块来融合接收到的中间特征。CoBEVT 提出了局部全局稀疏注意力，可以捕获视图和智能体之间的复杂空间交互，以提高协作感知的性能</strong></p><p>然而，这些融合方法都是以理想通信为前提的，在现实世界中，<strong>有损通信会使性能急剧下降。为了解决这个问题，我们设计了一种特殊的V2V注意力模块（V2VAM），包括ego车辆的车内注意力和不确定性感知的车辆间注意力,以增强V2V交互</strong>。</p><p><img data-src="https://s2.loli.net/2024/01/07/dc81urwk6RBAfoX.png" alt="image-20240107223439462"></p><p>本文主要关注数据传输过程中的有损通信挑战 所以<strong>假设V2V系统中不存在通信延迟或定位错误</strong>。<strong>共享数据也可能在到达目的地之前受到其他信号的干扰或被攻击者修改</strong>，从而导致有损数据。在这项工作中，旨在通过<strong>提出LC感知修复网络</strong>和<strong>提高V2V感知网络的鲁棒性来消除有损通信</strong>。</p><p>该文提出了一种新的中间LCaware特征融合框架。包括五个组件：1）V2V元数据共享，2）激光雷达特征提取，3）特征共享，4）LC感知修复网络和V2V注意力模块，5）分类和回归头。</p><p>LC感知修复网络和V2V注意力模块.从周围其他CAV聚合的中间特征被输入到我们框架的主要组件中,即<strong>LC-Aware修复网络,用于使用张量滤波在有损通信中恢复中间特征图,</strong>以及<strong>V2V注意力模块,用于利用注意力机制进行迭代的车辆间和车辆内特征融合</strong>.</p><p><strong>LC-aware Repair Network</strong></p><p>LC感知修复网络的框架如图3所示，该网络是一种具有跳跃连接的编码器-解码器架构。该网络生成一个特定的每个张量过滤器内核，以共同对齐和恢复输入损坏的特征，以生成输出特征的恢复版本。LC感知修复网络的输入特征为S ∈ R^c×h×w^，然后生成一个张量核K并应用于S，以产生恢复的输出特征ˆ S ∈ R^c×h×w^。</p><p><img data-src="https://s2.loli.net/2024/01/07/zpJc21GkmUEadLO.png" alt="image-20240107231955750"></p><p>LC感知修复网络的输入特征为S ∈ R^c×h×w^，然后生成一个张量核K并应用于S，以产生恢复的输出特征ˆ S ∈ R^c×h×w^。</p><p>LC 感知修复损失函数 L~LC~（ ˆ S， ˆ S^g^） 是遭受有损通信之前的真值原始特征 ˆ S^g^ 与修复特征 ˆ S 之间的张量 L1 距离。</p><p><strong>V2V Attention Module</strong></p><p>自注意力的关键思想是将某个位置的响应计算为所有位置的特征的加权总和，特征之间的相互作用由特征本身决定，而不是像卷积那样由它们的相对位置决定。</p><p>通过考虑有损通信情况，设计了一种定制化的车内和车间注意力融合方法，以增强自我CAV与其他CAV之间的交互.此外,我们在提出的V2V注意力方法中采用了<strong>纵横交错的注意力模块</strong>,可以利用该模块更有效地从全功能依赖关系中捕获上下文信息。</p><p>仅对于ego车辆，车内注意力模块可以使任何位置的特征进行全局感知，从而享受全图像上下文信息，以更好地捕捉代表性特征。从形式上讲，让 H^e^ ∈ R^C×H×W^ 成为自我车辆的输入特征图，它是自我车辆生成的完美数据，不会遭受任何有损通信。</p><script type="math/tex; mode=display">\mathbf{A}^{in\boldsymbol{l}ra}=\mathrm{softmax}\left(\frac{\mathbf{Q}^{e}\mathbf{K}^{e\mathbf{T}}}{\sqrt{d_{k}^{e}}}\right)\mathbf{V}^{e},</script><p>在车内注意力模块中，<strong>特征图H^e^将由三个1×1卷积层计算，分别产生三个特征向量Qe、Ke和Ve</strong>，其中所有特征向量都具有相同的大小，{Q^e^，K^e^，V^e^}∈R^C×H×W^。按照中的缩放点积注意力，我们计算了Q^e^和K^e^的点积，然后使用比例因子（即特征向量的维度）将它们除以，并应用softmax函数来获得V^e^上的权重。</p><p>在V2V协同感知中，从其他CAV聚合而来的中间特征图H^s^∈R^C×H×W^被共享给ego车辆。具有有损通信的共享特征图Hs将被LC感知修复网络恢复，但它们在一定程度上仍然有噪声，而ego特征图H^s^是完美的.</p><p>针对这一问题，该文考虑恢复特征图的不确定性，提出一种不确定性感知的车间注意力融合方法。</p><p>在该模块中,共享特征图将由两个 1 × 1 卷积层计算，分别产生两个特征向量 K^s^ 和 V^s^，其中它们都具有相同的大小，{Ks， Vs} ∈ R^C×H×W^，另一个特征向量 Q^e^ 直接从自我车辆而不是其他车辆获得.</p><script type="math/tex; mode=display">\mathbf{A}^{inter}=\sum_i^N\mathrm{softmax}\left(\frac{\mathbf{Q}^e\mathbf{K}_i^{s\text{T}} }{ \sqrt { d _ k ^ s }}\right)\mathbf{V}_i^s,</script><p><strong>Efficient Implementation</strong></p><p>采用两个连续的纵横交错（CC）注意力模块来实现点云数据中的V2V注意力,而不是缩放的点积注意力,减小计算量.</p><p><strong>在获得车内注意力和车间注意力后，将它们分别输入到最大池化层和平均池化层，以获得丰富的空间信息，然后将它们串联为具有ReLU激活函数的二维卷积层的输入</strong>。</p><p><strong>总结</strong></p><p>感觉融合了cobev和coalign两篇文章.在代码上主要使用了ego的q和其他车的k,v计算注意力,而且计算注意力使用Criss-Cross Attention,然后使用max和mean再通过一个卷积.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">V2V_AttFusion</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, feature_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(V2V_AttFusion, self).__init__()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        self.cov_att = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=feature_dim, out_channels=feature_dim, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(feature_dim,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.CCNet = CrissCrossAttention(feature_dim)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, record_len</span>):</span></span><br><span class="line"></span><br><span class="line">        split_x = self.regroup(x, record_len)  <span class="comment">#x =[5, 64, 100, 352], record_len=[3,2]</span></span><br><span class="line"></span><br><span class="line">        out = []</span><br><span class="line">        att = []</span><br><span class="line">        <span class="keyword">for</span> xx <span class="keyword">in</span> split_x:<span class="comment">#split_x[0] [num_car, C, W, H]</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27; CCNet: Criss-Cross Attention Module: attention for ego vehicle feature + cav feature &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">            ego_q, ego_k, ego_v = xx[<span class="number">0</span>:<span class="number">1</span>], xx[<span class="number">0</span>:<span class="number">1</span>], xx[<span class="number">0</span>:<span class="number">1</span>] </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(xx[:,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])):</span><br><span class="line">                att_vehicle = self.CCNet(ego_q, xx[i:i+<span class="number">1</span>], xx[i:i+<span class="number">1</span>])</span><br><span class="line">                att.append(att_vehicle)</span><br><span class="line"></span><br><span class="line">            pooling_max = torch.<span class="built_in">max</span>(torch.cat(att, dim=<span class="number">0</span>), dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">            pooling_ave = torch.mean(torch.cat(att, dim=<span class="number">0</span>), dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            fuse_fea = pooling_max + pooling_ave</span><br><span class="line"></span><br><span class="line">            fuse_att = fuse_fea</span><br><span class="line">            fuse_att = self.cov_att(fuse_att)</span><br><span class="line"></span><br><span class="line">            out.append(fuse_att) <span class="comment">#[[1, 64, 100, 352], [1, 64, 100, 352]]</span></span><br><span class="line">            <span class="comment"># torch.cuda.empty_cache()</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out, dim=<span class="number">0</span>) <span class="comment">#[2, 64, 100, 352]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">regroup</span>(<span class="params">self, x, record_len</span>):</span></span><br><span class="line">        cum_sum_len = torch.cumsum(record_len, dim=<span class="number">0</span>)</span><br><span class="line">        split_x = torch.tensor_split(x, cum_sum_len[:-<span class="number">1</span>].cpu())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> split_x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrissCrossAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Criss-Cross Attention Module</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    reference: https://github.com/speedinghzl/CCNet</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CrissCrossAttention,self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.query_conv = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(in_dim,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            )</span><br><span class="line">        self.key_conv = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(in_dim,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            )</span><br><span class="line">        self.value_conv = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(in_dim,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>),</span><br><span class="line">                nn.ReLU()</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.softmax = Softmax(dim=<span class="number">3</span>)</span><br><span class="line">        self.INF = INF</span><br><span class="line">        self.gamma = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value</span>):</span></span><br><span class="line">        m_batchsize, _, height, width = query.size()</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        proj_query = self.query_conv(query)</span><br><span class="line">        proj_query_H = proj_query.permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(m_batchsize*width,-<span class="number">1</span>,height).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        proj_query_W = proj_query.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>).contiguous().view(m_batchsize*height,-<span class="number">1</span>,width).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        proj_key = self.key_conv(key)</span><br><span class="line">        proj_key_H = proj_key.permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(m_batchsize*width,-<span class="number">1</span>,height)</span><br><span class="line">        proj_key_W = proj_key.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>).contiguous().view(m_batchsize*height,-<span class="number">1</span>,width)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        proj_value = self.value_conv(value)</span><br><span class="line">        proj_value_H = proj_value.permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>).contiguous().view(m_batchsize*width,-<span class="number">1</span>,height)</span><br><span class="line">        proj_value_W = proj_value.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>).contiguous().view(m_batchsize*height,-<span class="number">1</span>,width)</span><br><span class="line">        energy_H = (torch.bmm(proj_query_H, proj_key_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize,height,width,width)</span><br><span class="line">        concate = self.softmax(torch.cat([energy_H, energy_W], <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        att_H = concate[:,:,:,<span class="number">0</span>:height].permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>).contiguous().view(m_batchsize*width,height,height)</span><br><span class="line">        att_W = concate[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)</span><br><span class="line">        out_H = torch.bmm(proj_value_H, att_H.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)).view(m_batchsize,width,-<span class="number">1</span>,height).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">        out_W = torch.bmm(proj_value_W, att_W.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)).view(m_batchsize,height,-<span class="number">1</span>,width).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.gamma*(out_H + out_W) + value</span><br></pre></td></tr></table></figure><h4 id="Adaptive-Feature-Fusion-for-Cooperative-Perception-using-LiDAR-Point-Clouds-2023"><a href="#Adaptive-Feature-Fusion-for-Cooperative-Perception-using-LiDAR-Point-Clouds-2023" class="headerlink" title="Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds 2023"></a>Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds 2023</h4><h5 id="Abs-3"><a href="#Abs-3" class="headerlink" title="Abs"></a>Abs</h5><p>提出了具有<strong>可训练特征选择模块的自适应特征融合模型</strong>。我们提出的一个模型空间自适应特征融合（S-AdaFusion）在OPV2V数据集的两个子集上优于所有其他最先进的（SOTA）:用于车辆检测的默认CARLA Towns和用于域适应的Culver City.</p><blockquote><p>此外，以前的研究只测试了车辆检测的协同感知。然而，行人在交通事故中受重伤的可能性要大得多。</p><p>我们使用 CODD 数据集评估了车辆和行人检测的协同感知性能。我们的架构在 CODD 数据集上实现了比其他现有模型更高的车辆和行人检测平均精度 （AP）。实验表明，与传统的单车感知过程相比，协同感知也提高了行人检测的准确率。</p></blockquote><p>LiDAR 可以生成包含准确深度信息的点云数据，并且受外部照明条件的影响较小。然而，<strong>远离LiDAR的点云非常稀疏，这使得探测其他物体变得更加困难。</strong></p><p>感知到的信息可以包括 GPS 和各种传感器数据，包括<strong>雷达、摄像头和激光雷达数据</strong>。协同感知有助于弥补当前视觉感知技术的局限性，例如分辨率有限、天气效应和盲点。</p><p>首先，4个CAV处理其LiDAR点云，并在其本地系统中并行提取中间特征图。接下来，其他三个CAV将其提取的特征图连同LiDAR姿态信息一起广播到CAV1。然后，CAV1 将三个特征图投影到自己的坐标系，并将信息与自己的感知信息聚合在一起，用于 3D 目标检测。</p><p>根据CAV之间共享的数据类型，现有文献中发现了三种数据融合方法：1）早期融合聚合了来自其他CAV的原始输入传感器数据;</p><p>2）中间融合聚合了来自其他CAV的处理后的特征图;</p><p>3）后期融合聚合了来自其他CAV的目标检测的预测输出。</p><blockquote><p>在最近的研究中，与早期和晚期融合方法相比，中间特征融合已被证明是最有效的融合方法。我们假设通过实现有效的特征选择和融合模块，降低计算成本，可以进一步改进中间融合方法，以实现实时感知和更高的准确性.</p></blockquote><p>我们提出了几种具有可训练神经网络的特征融合模型，这些模型可以从多个CAV中自适应地选择特征</p><p>这项工作的贡献如下：1）我们创建了一个具有中间融合的轻量级协作感知架构;2）利用3D CNN和自适应特征融合进行协同感知，提出3种可训练的特征融合模型进行协同感知;3）我们使用两个公共合作感知基准数据集（OPV2V数据集[27]和CODD数据集[2]）验证了所提出的模型，用于多个任务，包括a）车辆检测，b）行人检测和c）领域适应;4）我们尝试了不同数量的CAVs，以观察其对合作感知的影响</p><p>从形式上讲，合作感知的问题可以描述如下。我们将来自一组周围 CAV 的原始输入数据（相机数据和 LiDAR 数据）表示为 I = {I1， I2， . . . ， In}，表示为 V = {v1， v2， . . . ， vn}。输入数据的相应提取中间特征集表示为 F = {F1， F2， . . . ， Fn}，其中目标检测器预测的输出集表示为 O = {O1， O2， . . . ， On}。在传统的视觉感知过程中，AV vi 接收来自摄像头和激光雷达等传感器的原始数据 Ii。然后对这些数据进行处理以提取特征图 Fi，以用于计算模型中用于预测对象作为输出 Oi。在协同感知中，应用额外的数据融合步骤来聚合来自其他车辆的数据，以改善感知。</p><p>中间融合是利用已处理的中间特征表示 F 的折衷方案。因此，准确和优化地集成和处理从不同位置获得的信息对于有效的特征融合以实现准确的目标检测至关重要。</p><p>Marvasti等将3D LiDAR点云扭曲为鸟瞰图（BEV），并应用2D CNN提取每个连接的AV中的中间特征。来自 CAV 的特征图被投影到自我车辆的坐标系上。然后将这些与自我车辆的特征图聚合在一起。在中，只使用了两个CAV，求和使重叠具有更大的权重，而在现实生活中，CAV的数量各不相同。我们计算重叠处的平均值。</p><p>Chen等提出了特征级融合方案，选择重叠处的最大值来表示中间特征。</p><p>上面提到的模型使用简单的约简算子，例如求和、最大池化或平均池化。这些算子能够处理重叠处的信息，并以可以忽略不计的计算成本融合特征图。然而，由于缺乏信息选择和数据相关性的识别，所选择的特征不一定是最好的。</p><p>在V2VNe中，应用图神经网络（GNN）基于地质坐标表示CAV地图，以促进数据融合。GNN 将从多辆车接收到的信息与车辆的内部状态（根据其自己的传感器数据计算）聚合在一起，以计算更新的中间表示。</p><p>Xu等提出了AttFuse，并<strong>利用自注意力模型融合了中间特征图</strong>。<strong>V2X-Vit 和 CoBEVT  中使用了 Transformer，用于与中间特征融合的协同感知</strong>。</p><p>我们探索了特征融合模型，这些模型可以有效和高效地利用CAV的多个特征图。</p><p><strong>Feature Learning and Feature Fusion</strong></p><p>注意力机制在解决计算机视觉任务中已经证明了它的实用性。通过在神经网络中加入一个小模块，该模型可以利用通道和/或空间信息，并增强提取的表示。</p><p>在协同感知中，通过特征通道连接中间特征图会随着CAV数量的增加而大大增加计算成本。<strong>因此,与其连接特征图并创建超大型特征提取网络，不如将特征图与几何和地理信息聚合起来更有效</strong>。</p><p><strong>Overview of the Proposed Framework</strong></p><p>整个网络以点云为输入，分5个步骤对数据进行处理：</p><p>1）特征编码利用支柱特征网络（PFN）将点云转换为伪图像;</p><p>2）中间特征提取利用<strong>二维金字塔网络从伪图像中提取多尺度特征</strong>;</p><p>3）<strong>特征投影将CAV的中间特征图投射到与LiDAR姿态信息协调的ego车辆上</strong>;</p><p>4）中间特征融合<strong>生成具有特征融合网络的组合特征图</strong>;</p><p>5）3D目标检测<strong>使用单次检测器（SSD）对边界框进行回归，并预测检测到的对象的类别</strong>。</p><p><strong>Feature Encoding</strong></p><p>维度为（n×4）的输入点云由n个点组成，每个点具有属性（x、y、z）坐标和强度。</p><p>点云被编码为柱子，其高度等于z轴上的点云高度。每个柱子中的点都增加了 5 个额外的属性，包括柱子中所有点的算术平均值的偏移量和柱子中心的偏移量。将点云数据转换为P柱，每个柱子具有N个点和D个特征然后将 PointNet 应用于柱子上以提取特征并生成大小的张量 （C~in~ × P ）。具有 C~in~ 特征的柱子被投射回原始位置，以生成大小的伪像 （C~in~ × 2H × 2W）。我们在这里使用 2H 和 2W，因为我们在下一个特征提取步骤中将特征图下采样为 （C × H × W ）。</p><p><strong>Feature Extraction</strong></p><p>FPN 包含三个用于多分辨率特征提取的下采样模块。每个模块包含一个 2D 卷积层、一个批量归一化层和一个 ReLU 激活函数。</p><p>然后，对从三个下采样模块获得的三个特征图进行上采样和连接。由CNN对得到的多尺度特征图进行细化，以生成大小为F ′ ∈ RC×H×W的特征图。</p><p><img data-src="https://s2.loli.net/2024/01/08/R7C6MQ8PWzI13JK.png" alt="image-20240108181057303"></p><p><strong>Feature Projection</strong></p><p>从<strong>不同CAV中提取的特征图具有不同的地质位置和方向。因此，需要将它们转换为接收机的坐标系，以进行特征融合和目标检测</strong>。CAV 传播特征图及其 LiDAR 姿态信息，其中包含（X、Y、Z、滚动、偏航、俯仰）。一旦自我车辆从相邻的CAV接收到数据，它就会被投射到自我车辆的坐标系和时间戳中。</p><p><strong>Feature Fusion</strong></p><p>来自不同 CAV 的投影中间特征图被扩展为 4D 张量并连接起来进行进一步处理。在特征通道上连接特征图会生成具有 nC 通道的 3D 张量，这将增加计算复杂性和特征融合和细化的成本。因此，<strong>我们将特征图聚合为一个 4D 张量 F ∈ R^n×C×H×W^，其中 n 是 CAV 的最大数量。为了融合地质坐标系中的重叠特征图，我们提出了空间和通道特征融合模型</strong></p><p>我们提出的特征融合模型分为空间特征融合和通道特征融合。</p><p><strong>Spatial-wise Feature Fusion</strong>。为了融合特征图，将直接的约简算子（如Max或Mean）应用于重叠的特征，如图3a所示。本文将这两种融合方法称为MaxFusion和AvgFusion，它们分别在通道轴上计算最大池化和平均池化，得到融合特征图Ffusion ∈ R^1×C×H×W^</p><p>受信道注意力模块<strong>SENet</strong>的启发,提出了一种信道自适应特征融合（C-AdaFusion）模块，该模块可以利用<strong>信道信息选择和融合中间特征图</strong>。</p><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20240108204441345.png" alt="image-20240108204441345"></p><p>首先，通过计算沿第一通道轴的最大池化和平均池化，将输入特征图F ∈ R^n×C×H×W^分解为Smax ∈ R^1×C×H×W^和Savg ∈ R^1×C×H×W^。</p><p>将两个特征图连接在一起，得到一个四维张量F~spatial~ ∈ R^2×C×H×W^，其中包含来自原始级联中间特征图的两种空间信息。然后,利用<strong>具有ReLU激活函数的三维卷积进行进一步的特征选择和降维</strong>,输入通道数和输出通道数分别等于2和1.</p><p><strong>Channel-wise Feature Fusion</strong>CNN 在从表示中提取特征并减小其维度方面表现非常出色。对于输入的 4D 张量 F ∈ R^n×C×H×W^，可以使用 3D CNN 提取通道特征并减少输入特征通道的数量。3D CNN 的输入通道数将等于单个输出通道表示组合特征集的最大 CAV 数。</p><p><img data-src="https://s2.loli.net/2024/01/08/W5qG8EdRyZwz6lM.png" alt="image-20240108204452386"></p><p>根据这两种不同的attention提出了c-AdaFusion.利用 3D 自适应最大池化和平均池化来提取两个通道描述符 C~max~ ∈ R^n×1×1×1^ 和 C~avg~ ∈ R^n×1×1×1^。然后，将两个向量连接起来，分别通过两个具有ReLU和Sigmoid激活函数的线性层。输入特征图F ∈ R^n×C×H×W^ 通道相乘学习的通道权重 F ′ 通道∈ R^n×1×1×1^，以生成新的特征表示 F ′ ∈ R^n×C×H×W^。融合特征图F~fusion~ ∈ R^1×C×H×W^是利用通道缩减3D CNN得到的。</p><p><img data-src="https://s2.loli.net/2024/01/08/9eTsclkiWp3Z4fV.png" alt="image-20240108185354500"></p><p><strong>总结</strong></p><p>主要是利用了通道以及空间的fusion.我是打算再结合transformer做的.</p><h4 id="where2comm-2022"><a href="#where2comm-2022" class="headerlink" title="where2comm 2022"></a>where2comm 2022</h4><h5 id="abs-1"><a href="#abs-1" class="headerlink" title="abs"></a>abs</h5><p>多智能体协同感知可以使智能体通过通信相互共享互补信息，从而显著提升感知性能。</p><p>这不可避免地导致感知性能和通信带宽之间的基本权衡。为了解决这一瓶颈问题，<strong>我们提出了一种空间置信度图，该图反映了感知信息的空间异质性。它使智能体能够只共享空间稀疏但感知上至关重要的信息，从而有助于沟通的位置</strong>。</p><p>基于这种新颖的空间置信度图，我们提出了一种通信效率高的协作感知框架Where2comm。有两个明显的优势：</p><p>i）它考虑了语用压缩，并通过专注于感知关键区域来使用更少的通信来实现更高的感知性能;</p><p>ii）通过动态调整通信所涉及的空间区域，可以处理不同的通信带宽。为了评估 Where2comm，我们考虑了在真实世界和模拟场景中的 3D 目标检测，在四个数据集上使用两种模态（摄像头/激光雷达）和两种代理类型（汽车/无人机）：OPV2V、V2X-Sim、DAIR-V2X 和我们原始的 CoPerception-UAV。</p><h5 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h5><p>协作感知使多个智能体能够相互共享互补的感知信息，从而促进更全面的感知。它提供了一个新的方向，从根本上克服了单智能体感知的一些不可避免的局限性，如遮挡和远距离问题。在广泛的实际应用中迫切需要相关的方法和系统，例如<strong>车联网通信辅助自动驾驶</strong>、<strong>多机器人仓库自动化系统</strong>和用于<strong>搜救</strong>的多无人机。为了实现协作感知，最近的工作贡献了高质量的数据集和有效的协同方法。</p><p>在这个新兴领域,目前最大的挑战是如何优化感知性能和通信带宽之间的权衡。现实世界中的<strong>通信系统总是受到限制,以至于它们几乎无法承受巨大的实时通信消耗</strong>,例如通过完整的原始观测或大量特征。</p><p>以前的工作都做出了一个合理的假设：<strong>一旦两个智能体合作，他们就有义务平等地共享所有空间区域的感知信息。这种不必要的假设会极大地浪费带宽，因为很大一部分空间区域可能包含与感知任务无关的信息</strong>。为了填补这一空白，我们考虑了一种<strong>新颖的空间置信度感知沟通策略</strong>。其<strong>核心思想是为每个智能体启用空间置信度图，其中每个元素都反映了相应空间区域的感知临界水平</strong>。根据此地图，智能体决定要通信的空间区域（位置）。</p><p>Where2comm 包括三个关键模块：</p><p>i） 空间置信度生成器,它<strong>生成空间置信度图以指示感知关键区域</strong>;</p><p>ii）空间置信度感知通信模块,<strong>利用空间置信度图通过新型消息打包决定在何处进行通信</strong>，以及<strong>通过新型通信图构建来向谁进行通信</strong>;</p><p>iii）空间置信感知消息融合模块,<strong>该模块使用新颖的置信感知多头注意力来融合从其他智能体接收到的所有消息,从而升级每个智能体的特征图</strong>。</p><p>Where2comm有两个明显的优势。首先在特征层面促进了语用压缩，并通过专注于感知关键区域，<strong>使用更少的通信来实现更高的感知性能</strong>。其次，<strong>它适应各种通信带宽和通信轮数</strong>，而以前的型号只能处理一个预定义的通信带宽和固定数量的通信轮数。</p><p><strong>问题定义</strong></p><p>考虑场景中的 N 个代理。让 X~i~ 和 Y~i~ 分别是第 i 个智能体的观察和感知监督。协作感知的目标是实现所有智能体的感知性能最大化，作为总通信点 B 和通信轮 K 的函数</p><script type="math/tex; mode=display">\xi_{\Phi}(B,K)=\arg\max_{\theta,\mathcal{P}}\sum_{i=1}^{N}g\left(\Phi_{\theta}\left(\mathcal{X}_{i},\{\mathcal{P}_{i\rightarrow j}^{(K)}\}_{j=1}^{N}\right),\mathcal{Y}_{i}\right),\mathrm{~s.t.~}\sum_{k=1}^{K}\sum_{i=1}^{N}|\mathcal{P}_{i\rightarrow j}^{(k)}|\leq B,</script><p>在这项工作中，我们考虑了3D目标检测的感知任务，并提出了三个贡献：</p><p>i）我们通过设计紧凑的消息和稀疏的通信图使通信更加高效;</p><p>ii）我们通过实施更全面的信息融合来提高感知性能;</p><p>iii） 我们通过动态调整沟通地点和人员，使整个系统能够适应不同的沟通条件。</p><p><img data-src="https://s2.loli.net/2024/01/09/1fg5jPZS6lUhmxz.png" alt="image-20240109171737063"></p><p><strong>observation encoder</strong>从传感器数据中提取特征图。Where2comm 接受单模态/多模态输入，例如 RGB 图像和 3D 点云。这项工作采用鸟瞰图（BEV）中的特征表示，其中所有智能体将其个人感知信息投射到同一个全局坐标系，避免了复杂的坐标变换，并支持更好的共享跨智能体协作。</p><p><strong>Spatial confidence generator</strong>空间置信度图<strong>反映了各个空间区域的感知临界水平</strong>。直观地说,对于物体检测任务,包含<strong>物体的区域比背景区域更关键</strong>。在协作过程中,由于视野有限,有对象的区域可以帮助恢复误检对象;可以<strong>省略背景区域以节省宝贵的带宽。因此,我们用检测置信度图表示空间置信度图，其中感知临界水平高的区域是包含置信度得分高的对象的区域。</strong></p><p>传感器位置编码表示每个智能体的传感器与其观察点之间的物理距离。<strong>它采用以感应距离和特征尺寸为条件的标准位置编码功能</strong>。在输入到transformer之前，将这些特征与每个位置的位置编码相加。</p><p>与现有不使用注意力机制或仅使用智能体级注意力的融合模块相比，所提出的融合所采用的<strong>每位置注意力机制强调位置特异性特征交互。它使特征融合更具针对性</strong>。与同样使用基于每个位置注意力的融合模块的方法相比，所提出的融合模块利用了具有两个额外先验的多头注意力，<strong>包括空间置信度图和感知距离</strong>。两者都有助于注意力学习，以偏爱高质量和关键功能</p><p>空间置信度感知消息融合的目标是通过聚合从其他代理接收到的消息来增强每个代理的功能。为了实现这一点，我们采用了 transformer 架构，该架构利用多头注意力来融合来自每个单独空间位置的多个智能体的相应特征</p><p><strong>Spatial confidence-aware message fusion</strong></p><script type="math/tex; mode=display">\mathbf{C}_i^{(k)}=\Phi_{\mathrm{generator}}(\mathcal{F}_i^{(k)})\in[0,1]^{H\times W}</script><script type="math/tex; mode=display">\mathbf{W}_{j\rightarrow i}^{(k)}=\mathrm{MHA}_{\mathrm{W}}\left(\mathcal{F}_{i}^{(k)},\mathcal{Z}_{j\rightarrow i}^{(k)},\mathcal{Z}_{j\rightarrow i}^{(k)}\right)\odot\mathbf{C}_{j}^{(k)}\in\mathbb{R}^{H\times W},</script><script type="math/tex; mode=display">\mathcal{F}_{i}^{(k+1)}=\mathrm{FFN}\left(\sum_{j\in\mathcal{N}_{i}\bigcup\{i\}}\mathbf{W}_{j\rightarrow i}^{(k)}\odot\mathcal{Z}_{j\rightarrow i}^{(k)}\right)\in\mathbb{R}^{H\times W\times D},</script><p><strong>总结</strong>很好的一篇论文,利用了其他车的输出作为confidence map再结合多头注意力进行协同感知.</p><h4 id="V2X-ViT-Vehicle-to-Everything-Cooperative-Perception-with-Vision-Transformer-2022"><a href="#V2X-ViT-Vehicle-to-Everything-Cooperative-Perception-with-Vision-Transformer-2022" class="headerlink" title="V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer 2022"></a>V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer 2022</h4><p>这篇文章主要强调车与异构设备之间进行协同感知,提出新的改进的注意力机制,所以这里就不着重讲了.</p><ul><li>我们提出了第一个用于V2X感知的统一Transformer架构（V2X-ViT），该架构可以捕获V2X系统的异构性，对各种噪声具有很强的鲁棒性。</li><li>此外，所提模型在具有挑战性的协同检测任务中取得了最先进的性能。– 我们提出了一种新型的异构多智能体注意力模块（HMSA），用于异构智能体之间的自适应信息融合。</li><li>我们提出了一种新的多尺度窗口注意力模块 （MSwin），该模块可同时并行捕获局部和全局空间特征交互。– 我们构建了 V2XSet，这是一个用于 V2X 感知的新的大规模开放仿真数据集，它明确地解释了不完美的现实世界条件。</li></ul><p><strong>OPV2V</strong>本文提出了Attentive Fusion</p><p>由于来自不同联网车辆的传感器观测结果可能带有不同的噪声水平（例如，由于车辆之间的距离），因此，一种<strong>既能关注重要观测值又能忽略中断观测值的方法对于稳健的检测至关重要</strong>。因此，我们提出了一个中间融合管道来捕捉相邻连接车辆特征之间的相互作用，帮助网络关注关键观测。</p><p>首先广播每个CAV的相对姿态和外在性,以构建一个空间图,其中每个节点都是通信范围内的一个CAV,每个边代表一对节点之间的通信通道。</p><p>构建图形后，将在组中选择一辆 ego 车辆. 所有相邻的 CAV 将自己的点云投射到自我车辆的 LiDAR 框架上，并根据投影的点云提取特征。这里的特征提取器可以是现有 3D 对象检测器的backbone.</p><p>采用自注意力模型来融合这些解压缩特征。同一特征图中的每个特征向量（对应于原始点云中的某些空间区域。因此<strong>，简单地展平特征图并计算特征的加权总和将破坏空间相关性。取而代之的是，我们为特征图中的每个特征向量构建一个局部图，其中边是为来自不同连接车辆的相同空间位置的特征向量构建的。</strong></p><h3 id="Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds"><a href="#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds" class="headerlink" title="Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds"></a>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</h3><h4 id="Abs-4"><a href="#Abs-4" class="headerlink" title="Abs"></a>Abs</h4><p>自动驾驶汽车可能会因为检测和识别不准确而做出错误的决定。因此，<strong>智能车辆可以将自身数据与其他车辆的数据相结合，增强感知能力，从而提高探测精度和驾驶安全性</strong>。然而，<strong>多车协同感知需要整合真实世界的场景，而原始传感器数据交换的流量远远超过了现有车辆网络的带宽。</strong>据我们所知，我们是<strong>第一个对原始数据层面的合作感知进行研究，以提高自动驾驶系统的检测能力</strong>。</p><p>在这项工作中，我们<strong>以激光雷达三维点云为依托，融合从联网车辆的不同位置和角度收集的传感器数据</strong>。我们提出了一种基于点云的三维物体检测方法，可用于多种对齐点。</p><p>然而，到目前为止，人类驾驶的汽车与自动驾驶汽车之间的大多数比较都是不平衡的，包含各种不公平的因素。自动驾驶汽车不会感到疲劳、情绪衰弱，如愤怒或沮丧。但是，它们无法像细心和经验丰富的人类驾驶员那样，在不确定和模糊的情况下做出熟练的反应或预测</p><h3 id="Proposed-Solution"><a href="#Proposed-Solution" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h3><p>为了解决这个问题，我们研究了其中一个基础类别，即原始数据的低级融合。<strong>原始传感数据是自动驾驶汽车上所有传感器的组成部分，因此非常适合在不同制造商的不同汽车之间传输。因此，不同数据处理算法的异质性不会影响车辆之间共享数据的准确性</strong>。由于自动驾驶本身就是一项至关重要的任务，与车辆的集成度如此之高，即使是一个小小的检测错误也可能导致灾难性的事故。因此，我们需要自动驾驶汽车尽可能清晰地感知环境。为了实现这一最终目标，它们需要一个强大而可靠的感知系统。</p><p>在此过程中，我们要解决以下两个主要问题：(1) 我们需要在车辆之间共享的数据类型，以及 (2) 需要传输的数据量与接收车辆实际需要的数据量。</p><p>第一个问题是<strong>汽车数据集中的可共享数据</strong>。第二个问题<strong>是每辆车产生的数据量巨大。由于每辆自动驾驶汽车每天都会收集超过 1000GB 的数据</strong>，因此只收集区域数据变得更加困难。同样，重建附近感知系统从不同位置和角度收集的共享数据也是另一大挑战。</p><p>在不同类型的原始数据中，我们建议使用激光雷达（LiDAR）点云作为解决方案，原因如下：</p><ul><li>与二维图像和视频相比，<strong>激光雷达点云具有空间维度的优势</strong>。</li><li>在保留感知对象的准确模型的同时，对实体或私人数据（如人脸和车牌号码）进行本机混淆。</li><li>由于数据是由点而不是像素组成的，因此在图像和视频融合过程中具有多功能性。<strong>对于图像或视频融合来说，要求有一个清晰的重叠区，而点云数据则不需要重叠区，这使得点云数据成为一种更稳健的选择，尤其是在考虑到汽车的不同视角时</strong>。</li></ul><h5 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h5><p>不准确的物体检测和识别是实现强大而有效的感知系统的主要障碍。自动驾驶汽车最终会屈服于这种能力，无法实现预期结果，这对自动驾驶是不安全的。为了解决这些问题，我们提出了一种解决方案，即<strong>自动驾驶车辆将自身的感知数据与其他联网车辆的感知数据相结合，以帮助增强感知能力</strong>。我们还认为，如前所述，<strong>数据冗余是解决这一问题的方法，我们可以通过自动驾驶车辆之间的数据共享和组合来实现</strong>。我们提出的 Cooper 系统可以提高探测性能和驾驶体验，从而提供保护和安全。具体来说，我们的贡献如下:</p><ul><li>我们提出了稀疏点云物体检测（SPOD）方法，用于<strong>检测低密度点云数据中的物体</strong>。虽然 SPOD 是针对低密度点云设计的，但它也适用于高密度激光雷达数据。</li><li>我们展示了所提出的 Cooper 系统如何通过扩大感知区域和提高检测精度来超越单个感知。</li><li>我们证明，可以利用现有的车载网络技术来促进车辆之间兴趣区域 (ROI) 激光雷达数据的传输，从而实现合作感知。</li></ul><p>鉴于当前自动驾驶汽车数据融合领域的前景和工作，我们需要更进一步，定义我们眼中的合作传感。我们认为，自动驾驶汽车的合作传感将带来一系列挑战和益处，这将是发展过程中不可避免的一部分。</p><h3 id="F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds"><a href="#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds" class="headerlink" title="F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds"></a>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</h3><h4 id="Abs-5"><a href="#Abs-5" class="headerlink" title="Abs"></a>Abs</h4><p>自动驾驶汽车在很大程度上依赖于传感器来完善对周围环境的感知，然而，就目前的技术水平而言，汽车所使用的数据仅限于来自自身传感器的数据.车辆和/或边缘服务器之间的数据共享受到可用网络带宽和自动驾驶应用严格的实时性限制。</p><p>为了解决这些问题，我们为联网自动驾驶汽车提出了<strong>基于点云特征的合作感知框架</strong>（F-Cooper），以实现更高的目标检测精度。</p><p>基于特征的数据不仅足以满足训练过程的需要，我们还利用特征的固有小尺寸来实现实时边缘计算，而不会面临网络拥塞的风险。</p><p>我们的实验结果表明，通过融合特征，我们能够获得更好的物体检测结果，20 米内的检测结果提高了约 10%，更远距离的检测结果提高了 30%，同时还能以较低的通信延迟实现更快的边缘计算，在某些特征选择中只需 71 毫秒。</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>互联自动驾驶汽车（CAV）为改善道路安全提供了一个前景广阔的解决方案。这有赖于车辆能够实时感知路况并精确探测物体。</p><p>然而，准确和实时的感知在现场具有挑战性。它需要处理来自各种传感器的大量连续数据流，并有严格的时间要求。此外，车辆的感知精度往往会受到传感器有限视角和范围的影响。</p><h3 id="Proposed-Solution-1"><a href="#Proposed-Solution-1" class="headerlink" title="Proposed Solution"></a>Proposed Solution</h3><p>我们提出的方法可以提高自动驾驶车辆的检测精度，同时不会带来太多的计算开销。一个有益的启示是，现代自动驾驶车辆的物体检测技术，<strong>无论是基于图像的还是基于三维激光雷达数据的，通常都采用卷积神经网络（CNN）来处理原始数据，并利用区域建议网络（RPN）来检测物体</strong>。我们认为，特征图的能力尚未得到充分挖掘，特别是对于自动驾驶车辆上生成的 3D LiDAR 数据，因为特征图仅用于单个车辆的物体检测。</p><p>为此，我们引入了基于特征的协同感知（FCooper）框架，利用特征级融合实现端到端的三维物体检测，从而提高检测精度。我们的 F-Cooper 框架支持两种不同的融合方案：<strong>体素特征融合和空间特征融合</strong>。</p><p>与原始数据级融合解决方案[3]相比，<strong>前者实现了几乎相同的检测精度提升，而后者则提供了动态调整待传输特征图大小的能力</strong>。F-Cooper 的独特之处在于它可以在车载和路边边缘系统上部署和执行。</p><p>除了能够<strong>提高检测精度</strong>外，<strong>特征融合所需的数据大小仅为原始数据的百分之一</strong>。对于一个典型的激光雷达传感器来说，每个激光雷达帧包含约 100,000 个点，约为 4 MB。对于任何现有的无线网络基础设施来说，如此庞大的数据量都将成为沉重的负担。</p><p>要确认特征对融合的有用性，我们必须回答以下三个基本问题。</p><p>1) 特征是否具备融合的必要手段？<br>2) 我们能否通过特征在自动驾驶车辆之间有效地交流数据？<br>3) 如果特征满足前面两个要求，那么我们从自动驾驶车辆中获取特征图的难度有多大？</p><h4 id="Fusion-Characteristics"><a href="#Fusion-Characteristics" class="headerlink" title="Fusion Characteristics"></a>Fusion Characteristics</h4><p>受致力于融合不同层生成的特征图的研究成果（如特征金字塔网络（FPN） 和级联 R-CNN [2]）的启发，我们发现<strong>在不同的特征图中检测物体是可能的。例如，FPN 采用自上而下的金字塔结构特征图进行检测。这些网络非常善于复合特征融合的效率</strong>。</p><p>从这些著作中汲取灵感，我们假设兼容融合的汽车<strong>将使用相同的检测模型</strong>。这一点非常重要，因为我们看到只有最可靠的检测模型才会被用于自动驾驶。有了这个假设，我们现在来看看融合的特点</p><h4 id="Compression-and-Transmission"><a href="#Compression-and-Transmission" class="headerlink" title="Compression and Transmission"></a>Compression and Transmission</h4><p>与原始数据相比，特征地图的另一个优势在于车辆之间的传输过程。原始数据可能有多种不同的格式，但它们都能达到一个目的，那就是保留所捕获数据的原始状态。例如，从驾驶过程中获取的激光雷达数据将存储驾驶过程中沿途的所有点云。不过，这种存储格式<strong>会将不必要的数据与基本数据一起记录下来</strong>；而特征地图则避免了这一问题.</p><p>在 CNN 网络处理原始数据的过程中，所有无关数据都会被网络过滤掉，只留下可能被网络用于物体检测的信息。<strong>这些特征图存储在稀疏矩阵中，只存储被认为有用的数据，任何被过滤掉的数据在矩阵中都存储为 0。</strong></p><p>通过<strong>无损压缩（如 gzip 压缩方法），数据大小的优势会进一步扩大，如文献[14]所示。再加上稀疏矩阵的特性，我们就能将二者结合起来，实现压缩后的特征数据不超过 1 MB</strong>，使特征数据成为部署 On-Edge 融合的最佳选择。</p><h4 id="Generic-and-Inherent-Properties"><a href="#Generic-and-Inherent-Properties" class="headerlink" title="Generic and Inherent Properties"></a>Generic and Inherent Properties</h4><p>所有自动驾驶车辆都必须根据传感器生成的数据做出决策。<strong>原始数据由车辆上的物理传感器生成，然后传送到车载计算设备。在那里，原始数据通过基于 CNN 的深度学习网络进行处理，最终做出驾驶决策。</strong>在此过程中，<strong>我们可以提取提取的特征进行共享。这样，我们就能有效地获得原始数据的特征图，而无需额外的计算时间或车载计算设备的功率</strong>。迄今为止，几乎所有已知的自动驾驶车辆都使用了基于 CNN 的网络，因此特征提取是通用的，在融合之前无需进一步处理。</p><p>得益于自动驾驶车辆处理数据的方式，我们能够直接从原始激光雷达点云数据中提取处理后的特征图进行融合，因为这本身就提供了位置数据。<strong>只要激光雷达传感器已经按照自动驾驶所需的标准进行了校准，那么我们就能获得能够保留所有物体与车辆相对位置的特征地图</strong>。</p><p>为了融合两辆汽车的三维特征，设计了两种融合范式：体素特征融合和空间特征融合。在范式 I 中，首先融合两组体素特征，然后生成空间特征图。</p><h3 id="Voxel-Features-Fusion"><a href="#Voxel-Features-Fusion" class="headerlink" title="Voxel Features Fusion"></a>Voxel Features Fusion</h3><p>与位图中的像素一样，体素代表三维空间中规则立方体上的一个数值。在一个体素内，可能有零个或多个由激光雷达传感器生成的点云。对于至少包含一个点的任何体素，VoxelNet 的 VFE 层可以生成一个体素特征</p><p>假设原始激光雷达检测区域被划分为一个体素网格。</p><p>在这些体素中，我们将获得绝大多数空体素，而剩余的体素则包含关键信息。所有非空的体素都会通过一系列全连接层进行转换，并转化为长度为 128 的固定大小的矢量。固定大小的向量通常被称为特征图。</p><blockquote><p>为了提高内存/计算效率，我们将非空体素的特征保存到哈希表中，并将体素坐标作为哈希键。由于我们的重点主要是自动驾驶，因此我们只将非空体素存储到哈希表中。</p></blockquote><p>在 VFF 中，我们明确地将来自两个输入的所有体素的特征结合起来。具体来说，来自汽车 1 的体素 3 和来自汽车 2 的体素 5 共享相同的校准位置。</p><blockquote><p>虽然两辆车的物理位置不同，但它们共享同一个配准的三维空间，不同的偏移量表示每辆车在所述三维标定空间中的相对物理位置。为此，我们采用了element-wise maxout来融合体素 3 和体素 5。</p></blockquote><p><img data-src="https://s2.loli.net/2023/11/30/pKu3lxqZ489NEk2.png" alt="image-20231129091921213"></p><p>受卷积神经网络的启发，使用 maxout 进行潜在规模选择，提取明显的特征，同时抑制对三维空间检测无益的特征，从而实现更小的数据量。在我们的实验中，我们使用 maxout 来决定在比较车辆间的数据时哪个特征最突出。</p><h3 id="Spatial-Feature-Fusion"><a href="#Spatial-Feature-Fusion" class="headerlink" title="Spatial Feature Fusion"></a>Spatial Feature Fusion</h3><p>VFF 需要考虑两辆车所有体素的特征，这涉及车辆之间的大量数据交换。<strong>为了进一步减少网络流量，同时保持基于特征融合的优势，我们设计了一种空间特征融合（SFF）方案</strong>。与 VFF 相比，SFF 融合的是空间特征图，与体素特征图相比，空间特征图更为稀疏，因此更容易压缩以进行通信。</p><p>与 VFF 不同，我们对每辆车上的体素特征进行预处理，以获得空间特征。接下来，将两个源空间特征融合在一起，并将融合后的空间特征转发给 RPN，以进行区域建议和目标检测。</p><p><img data-src="https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png" alt="image-20231129210539499"></p><p>特征学习网络的输出是一个稀疏张量，其形状为 128 × 10 × 400 × 352。为了整合所有体素特征，我们采用了三个三维卷积层，依次获得语义信息更多的较小特征图，大小为 64 × 2 × 400 × 352。然而，生成的特征无法满足传统区域建议网络的形状要求。为此，必须将输出重新塑造为 128 × 400 × 352 大小的三维特征图，然后才能将其转发给 RPN。</p><p>对于 SFF，我们生成一个更大的检测范围，大小为 W × H，其中 W &gt; W1，H &gt; H1。接下来，对重叠区域进行融合，同时保留非重叠区域的原始特征。假设 GPS 将汽车 1 的实际位置记录为 (x1，y1)，汽车 2 的实际位置记录为 (x2，y2)，如果 x2 + H1, y2 - W1 2 属于 2 号车的特征图，而左上角代表 1 号车的特征图，那么我们就可以得到左上角的位置。那么我们就很容易确定重叠区域。与 VFF 采用 maxout 策略类似，我们在 SFF 中也采用了 maxout 来融合重叠的空间特征。</p><p><img data-src="https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png" alt="image-20231129215351930" style="zoom:50%;" /></p><p>最后，我们采用区域建议网络在融合特征图上提出潜在区域。</p><blockquote><p>SENet 等最新研究表明，不同的通道具有不同的权重。也就是说，特征图中的某些通道对分类/检测的贡献更大，而其他通道则是多余或不需要的。</p></blockquote><p>选择从全部 128 个通道中选择部分通道进行传输。我们假定自动驾驶汽车装配了与实际应用中相同的训练有素的检测模型。</p><h3 id="使用融合特征进行目标检测"><a href="#使用融合特征进行目标检测" class="headerlink" title="使用融合特征进行目标检测"></a>使用融合特征进行目标检测</h3><p>为了检测车辆，我们将合成特征图输入区域建议网络（RPN）进行对象建议。然后应用损失函数进行网络训练。</p><h4 id="区域建议网络"><a href="#区域建议网络" class="headerlink" title="区域建议网络"></a>区域建议网络</h4><p>RPN:区域建议网络。不管是采用体素融合范式还是空间融合范式，当我们得到空间特征图后，都会将其送入区域提议网络（RPN）。通过 RPN 网络后，我们将得到两个损失函数的输出结果：</p><p>(1) 提议感兴趣区域的概率分数 p∈ [0, 1]；</p><p>(2) 提议区域的位置 P = (Px , Pw , Pz , Pl , Pw , Ph, Pθ ) ，其中 Px , Py , Pz 表示提议区域的中心，(Pl , Pw , Ph, Pθ ) 分别表示长度、宽度、高度和旋转角度。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数由两部分组成：分类损失 Lcls 和回归损失 Lreg。</p><p><img data-src="https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70" alt="img"></p><p>ground-truth bounding box,即gt-box表示为G = Gx , Gy, Gz, Gl , Gw , Gh, Gθ 其中，Gx , Gy , Gz 表示方框的中心点，（Gl , Gw , Gh, Gθ ）分别表示长度、宽度、高度和偏航旋转角</p><p>输出的值包括</p><p><img data-src="https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png" alt="image-20231129221138541"></p><p><img data-src="https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png" alt="image-20231129225123172"></p><p>损失可以表示为</p><p><img data-src="https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png" alt="image-20231129225143741"></p><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p><strong>KITTI</strong></p><p>由于我们的重点是三维物体检测，因此我们使用了 KITTI 数据集提供的三维 Velodyne 点云数据。</p><p>云点数据<strong>每帧提供 100K 个点</strong>，并存储在二进制浮点矩阵中。<strong>数据包括每个点的三维位置和相关的反射率信息</strong>。<strong>但是，由于 KITTI 数据是由单个车辆记录的，我们必须利用同一记录中的不同时间段来模拟由两辆车生成的数据</strong>。因此，KITTI 数据只适用于某些测试场景。</p><p>为了解决这个问题，我们在两辆名为汤姆和杰瑞（T&amp;J）的车辆上安装了必要的传感器，如激光雷达（Velodyne VLP-16）、摄像头（Allied Vision Mako G-319C）、雷达（Delphi ESR 2.5）、IMU&amp;GPS（Xsens MTi-G-710 套件）和边缘计算设备（Nvidia Drive PX2），以便在我们学校的校园内收集所需的数据。我们的车辆配有 16 波束 Velodyn 激光雷达传感器，以二进制原始以太网数据包的形式存储数据。由于我们的车辆可以相互独立移动，因此我们能够用两辆车在真实环境中测试各种场景。</p><h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>在停车场环境中，我们将距离车辆 20 米以内的物体视为高优先级物体，20 米以外的物体视为低优先级物体。</p><p>由于我们的激光雷达传感器只有 16 个光束，因此与更高端的激光雷达传感器相比，得到的点云数据相对稀疏。为了减轻稀疏数据带来的负面影响，我们将探测范围限制在沿 <strong>X、Y 和 Z 轴[0,70.4]X[-40,40] X [-3,1]</strong> 。我们不使用超出探测范围的数据。除了车辆的检测范围外，我们还<strong>将体素大小设置为 vD = 0.4 米、vH = 0.2 米、vW = 0.2 米，因此 D1 = 10、H1 = 400、W1 = 352</strong>。在我们的实验中，F-Cooper 框架在配备 GeForce GTX 1080 Ti GPU 的计算机上运行。</p><p>为了评估 F-Cooper，我们在实验中收集并测试了 200 多组数据。根据处理激光雷达数据的方法，我们将测试分为四类，方法（1）到（3）均来自相同的检测模型：（1）作为基线的非融合，（2）带有 VFF 的 F-Cooper，（3）带有 SFF 的 F-Cooper，以及（4）原始点云融合方法 - Cooper。特征融合在上述四种情况中随机进行，重点放在繁忙的校园停车场，因为由于遮挡物较多，这是最困难的情况。</p><p>后面结合OpenCood这个项目代码学习学习 <a href="https://github.com/DerrickXuNu/OpenCOOD">ICRA 2022] An opensource framework for cooperative detection. Official implementation for OPV2V. (github.com)</a></p><h2 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h2><p>目前找到的数据集还是不少的.</p><h3 id="OPV2V"><a href="#OPV2V" class="headerlink" title="OPV2V"></a>OPV2V</h3><h3 id="V2XSet"><a href="#V2XSet" class="headerlink" title="V2XSet"></a>V2XSet</h3><h3 id="V2V4Real"><a href="#V2V4Real" class="headerlink" title="V2V4Real"></a>V2V4Real</h3><h3 id="DAIR-V2X"><a href="#DAIR-V2X" class="headerlink" title="DAIR-V2X"></a>DAIR-V2X</h3><h3 id="V2X-Sim"><a href="#V2X-Sim" class="headerlink" title="V2X-Sim"></a>V2X-Sim</h3><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;除了3D&lt;strong&gt;目标检测&lt;/strong&gt;算法外,自动驾驶还需要将获取到的图像数据或者处理后的特征进行&lt;strong&gt;通信&lt;/strong&gt;和&lt;strong&gt;融合&lt;/strong&gt;,这里介绍相关论文.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>type hints in Python</title>
    <link href="https://www.sekyoro.top/2023/11/24/tip-types-in-python/"/>
    <id>https://www.sekyoro.top/2023/11/24/tip-types-in-python/</id>
    <published>2023-11-24T13:38:14.000Z</published>
    <updated>2023-11-29T11:47:54.547Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Python中的类型系统,使用type hints使得整个开发过程更加顺畅.类似typescript的目的.<br><span id="more"></span></p><h2 id="Type-Theory"><a href="#Type-Theory" class="headerlink" title="Type Theory"></a>Type Theory</h2><p>值得一提的是python目前还在蒸蒸日上,所以一些东西后面可能会有些改变,不过答题的东西是不变的,可以使用mypy<a href="https://github.com/python/mypy">python/mypy: Optional static typing for Python (github.com)</a>(或者pyright<a href="https://github.com/microsoft/pyright">microsoft/pyright: Static Type Checker for Python (github.com)</a>)进行检查,可以使用<a href="https://docs.pydantic.dev/latest/">Welcome to Pydantic - Pydantic</a>作为数据验证,大多数IDE本身也对这个默认支持.</p><p><a href="https://www.python.org/dev/peps/pep-0483/">PEP 483</a> 是这一切的起点.</p><h3 id="Subtypes"><a href="#Subtypes" class="headerlink" title="Subtypes"></a>Subtypes</h3><p>一个重要的概念是subtypes(亚型)。</p><p>形式上，如果以下两个条件成立，我们说T型是U的subtypes：</p><ul><li>来自T的每个值也在U类型的值集合中。</li><li>来自U型的每个函数也在T型函数的集合中。</li></ul><p>这两个条件保证了即使类型T与U不同，类型T的变量也可以总是假装为U。</p><blockquote><p>举个具体的例子，考虑T=bool和U=int。bool类型只取两个值。通常这些名称表示为True和False，但这些名称分别只是整数值1和0的别名：</p></blockquote><h3 id="Covariant-Contravariant-and-Invariant"><a href="#Covariant-Contravariant-and-Invariant" class="headerlink" title="Covariant, Contravariant, and Invariant"></a>Covariant, Contravariant, and Invariant</h3><p>在复合类型中使用子类型时会发生什么？例如，Tuple[bool]是Tuple[int]的一个子类型吗？答案取决于复合类型，以及该类型是协变(Covariant)的、反变(Contravariant)的还是不变(Invariant)的。</p><ul><li>元组是协变(Covariant)的。这意味着它保留了其项类型的类型层次结构：Tuple[bool]是Tuple[int]的子类型，因为bool是int的子类型。</li><li>列表是不变(Invariant)的。不变类型不能保证子类型。虽然List[bool]的所有值都是List[int]的值，但您可以将int附加到List[int]，而不是List[bool。换句话说，子类型的第二个条件不成立，并且List[bool]不是List[int]的子类型。</li><li>Callable在其参数中是反变(Contravariant)的。这意味着它颠倒了类型层次结构。若Callable[[T]，…]作为一个函数，它唯一的参数是T类型。Callable的一个例子[[int]，…]是double()函数。反变意味着，如果期望一个在布尔上操作的函数，那么一个在int上操作的功能是可以接受的。</li></ul><h3 id="内置类型"><a href="#内置类型" class="headerlink" title="内置类型"></a>内置类型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">x: <span class="built_in">float</span> = <span class="number">1.0</span></span><br><span class="line">x: <span class="built_in">bool</span> = <span class="literal">True</span></span><br><span class="line">x: <span class="built_in">str</span> = <span class="string">&quot;test&quot;</span></span><br><span class="line">x: <span class="built_in">bytes</span> = <span class="string">b&quot;test&quot;</span></span><br></pre></td></tr></table></figure><p>在3.8及之前,使用<code>from typing import List,Dict,Set,Tuple</code>  来使用collections,之后可以直接使用list,dict这种.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">list</span>[<span class="built_in">int</span>] = []</span><br><span class="line">x: <span class="built_in">tuple</span>[<span class="built_in">int</span>,...] = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">x: <span class="built_in">set</span>[<span class="built_in">int</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>&#125;</span><br><span class="line">x: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="built_in">float</span>] = &#123;<span class="string">&quot;field&quot;</span>: <span class="number">2.0</span>, <span class="string">&quot;field2&quot;</span>: <span class="string">&quot;a&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>在3.10+,可以直接使用<code>|</code>代替Union</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="built_in">list</span>[<span class="built_in">int</span>|<span class="built_in">str</span>] = [<span class="number">1</span>, <span class="number">2</span>, <span class="string">&quot;a&quot;</span>]</span><br><span class="line">x: <span class="type">Optional</span>[<span class="built_in">str</span>]</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x: <span class="type">Callable</span>[[<span class="built_in">int</span>], <span class="built_in">str</span>] = stringify</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; Iterator[<span class="built_in">int</span>]:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_email</span>(<span class="params">address: <span class="type">Union</span>[<span class="built_in">str</span>,<span class="built_in">list</span>[<span class="built_in">str</span>],<span class="literal">None</span>]</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    ...</span><br><span class="line"><span class="comment"># This says each positional arg and each keyword arg is a &quot;str&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, *args: <span class="built_in">str</span>, **kwargs: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    reveal_type(args)  <span class="comment"># Revealed type is &quot;tuple[str, ...]&quot;</span></span><br><span class="line">    reveal_type(kwargs)  <span class="comment"># Revealed type is &quot;dict[str, str]&quot;</span></span><br><span class="line">    request = make_request(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> self.do_api_query(request)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quux</span>(<span class="params">x: <span class="built_in">int</span>,/, y: <span class="built_in">str</span>, z: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">quux(<span class="number">1</span>, <span class="string">&#x27;2&#x27;</span>, z=<span class="number">3.0</span>)</span><br></pre></td></tr></table></figure><blockquote><p>如果你想要函数的调用者在某个参数位置只能使用位置参数而不能使用关键字参数传参，那么你只需要在所需位置后面放置一个/。</p><p>如果你希望强迫调用者使用某些参数，且必须以关键字参数的形式传参，那么你只需要在所需位置的前一个位置放置一个*。</p></blockquote><h3 id="类"><a href="#类" class="headerlink" title="类"></a>类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> ClassVar</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BankAccount</span>:</span></span><br><span class="line">    account_name: <span class="built_in">str</span></span><br><span class="line">    balance: <span class="built_in">float</span></span><br><span class="line">    </span><br><span class="line">    count: ClassVar</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, account_name: <span class="built_in">str</span>, initial_balance: <span class="built_in">float</span> = <span class="number">0.0</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.account_name = account_name</span><br><span class="line">        self.balance = initial_balance</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deposit</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.balance += amount</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">withdraw</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.balance -= amount</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuditedBankAccount</span>(<span class="params">BankAccount</span>):</span></span><br><span class="line">    audit_log: <span class="built_in">list</span>[<span class="built_in">str</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, account_name: <span class="built_in">str</span>, initial_balance: <span class="built_in">float</span> = <span class="number">0.0</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(account_name, initial_balance)</span><br><span class="line">        self.audit_log = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deposit</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.audit_log.append(<span class="string">f&quot;Deposited <span class="subst">&#123;amount&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">withdraw</span>(<span class="params">self, amount: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.audit_log.append(<span class="string">f&quot;Withdrew <span class="subst">&#123;amount&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can use the ClassVar annotation to declare a class variable</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span>:</span></span><br><span class="line">    seats: ClassVar[<span class="built_in">int</span>] = <span class="number">4</span></span><br><span class="line">    passengers: ClassVar[<span class="built_in">list</span>[<span class="built_in">str</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Setting&quot;</span>, key, <span class="string">&quot;to&quot;</span>, value)</span><br><span class="line">        self.__dict__[key] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self, key</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Getting&quot;</span>, key)</span><br><span class="line">        <span class="keyword">return</span> self.__dict__[key]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">A</span>):</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line">    weight: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name: <span class="built_in">str</span>, age: <span class="built_in">int</span>, weight: <span class="built_in">float</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.weight = weight</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = Person(<span class="string">&quot;John&quot;</span>, <span class="number">30</span>, <span class="number">80.0</span>)</span><br><span class="line"><span class="built_in">print</span>(p.name)</span><br></pre></td></tr></table></figure><h3 id="Forward-references"><a href="#Forward-references" class="headerlink" title="Forward references"></a>Forward references</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># You may want to reference a class before it is defined.</span></span><br><span class="line"><span class="comment"># This is known as a &quot;forward reference&quot;.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: A</span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># This will fail at runtime with &#x27;A&#x27; is not defined</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># However, if you add the following special import:</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> annotations</span><br><span class="line"><span class="comment"># It will work at runtime and type checking will succeed as long as there</span></span><br><span class="line"><span class="comment"># is a class of that name later on in the file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: A</span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># Ok</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another option is to just put the type in quotes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">foo: <span class="string">&#x27;A&#x27;</span></span>) -&gt; <span class="built_in">int</span>:</span>  <span class="comment"># Also ok</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="comment"># This can also come up if you need to reference a class in a type</span></span><br><span class="line">    <span class="comment"># annotation inside the definition of that class</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span>(<span class="params">cls</span>) -&gt; A:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><h3 id="Decorators"><a href="#Decorators" class="headerlink" title="Decorators"></a>Decorators</h3><p>decorator通常是将一个函数作为参数并返回另一个函数的函数。</p><p>用类型来描述这种行为可能有点棘手；我们将展示如何使用TypeVar和一种称为参数规范的特殊类型变量来实现这一点。</p><p>假设我们有装饰器，尚未进行类型注释，它保留了原始函数的签名，只打印装饰函数的名称：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwds</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>给这个装饰器类型注释</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Callable</span>, cast, <span class="type">Any</span></span><br><span class="line">F = TypeVar(<span class="string">&quot;F&quot;</span>, bound=<span class="type">Callable</span>[..., <span class="type">Any</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: F</span>) -&gt; F:</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: <span class="type">Any</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">Any</span>:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> cast(F, wrapper)</span><br></pre></td></tr></table></figure><p>这仍然存在一些不足。首先，我们需要使用不安全的cast()来说服mypy wrapper()与func具有相同的签名。其次，wrapper()函数没有经过严格的类型检查，尽管wrapper函数通常足够小，所以这不是什么大问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, TypeVar</span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> ParamSpec</span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[P, T]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwds: P.kwargs</span>) -&gt; T:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>可以使用参数规范（ParamSpec）来获得更好的类型注释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Callable</span>, <span class="type">Any</span>,ParamSpec</span><br><span class="line">P = ParamSpec(<span class="string">&quot;P&quot;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P,T]</span>) -&gt; <span class="type">Callable</span>[P,T]:</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwargs: P.kwargs</span>) -&gt; <span class="type">Any</span>:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func.__name__)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure><p>参数规范还允许描述更改输入函数签名的装饰器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, TypeVar</span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> Concatenate, ParamSpec</span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># We reuse &#x27;P&#x27; in the return type, but replace &#x27;T&#x27; with &#x27;str&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stringify</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[P, <span class="built_in">str</span>]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args: P.args, **kwds: P.kwargs</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(func(*args, **kwds))</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta"> @stringify</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">add_forty_two</span>(<span class="params">value: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">     <span class="keyword">return</span> value + <span class="number">42</span></span><br><span class="line"></span><br><span class="line"> a = add_forty_two(<span class="number">3</span>)</span><br><span class="line"> reveal_type(a)      <span class="comment"># Revealed type is &quot;builtins.str&quot;</span></span><br><span class="line"> add_forty_two(<span class="string">&#x27;x&#x27;</span>)  <span class="comment"># error: Argument 1 to &quot;add_forty_two&quot; has incompatible type &quot;str&quot;; expected &quot;int&quot;</span></span><br><span class="line"></span><br><span class="line">P = ParamSpec(<span class="string">&#x27;P&#x27;</span>)</span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printing_decorator</span>(<span class="params">func: <span class="type">Callable</span>[P, T]</span>) -&gt; <span class="type">Callable</span>[Concatenate[<span class="built_in">str</span>, P], T]:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">msg: <span class="built_in">str</span>, /, *args: P.args, **kwds: P.kwargs</span>) -&gt; T:</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Calling&quot;</span>, func, <span class="string">&quot;with&quot;</span>, msg)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@printing_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_forty_two</span>(<span class="params">value: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value + <span class="number">42</span></span><br><span class="line"></span><br><span class="line">a = add_forty_two(<span class="string">&#x27;three&#x27;</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Callable</span>, TypeVar</span><br><span class="line"></span><br><span class="line">F = TypeVar(<span class="string">&#x27;F&#x27;</span>, bound=<span class="type">Callable</span>[..., <span class="type">Any</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bare_decorator</span>(<span class="params">func: F</span>) -&gt; F:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorator_args</span>(<span class="params">url: <span class="built_in">str</span></span>) -&gt; <span class="type">Callable</span>[[F], F]:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><h3 id="Generics"><a href="#Generics" class="headerlink" title="Generics"></a>Generics</h3><p>内置集合类是泛型类。泛型类型有一个或多个类型参数，这些参数可以是任意类型。例如，dict[int，str]具有类型参数int和str，list[int]具有类型形参int。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypeVar, <span class="type">Generic</span></span><br><span class="line"></span><br><span class="line">T = TypeVar(<span class="string">&#x27;T&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>(<span class="params"><span class="type">Generic</span>[T]</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="comment"># Create an empty list with items of type T</span></span><br><span class="line">        self.items: <span class="built_in">list</span>[T] = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span>(<span class="params">self, item: T</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        self.items.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span>(<span class="params">self</span>) -&gt; T:</span></span><br><span class="line">        <span class="keyword">return</span> self.items.pop()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> self.items</span><br></pre></td></tr></table></figure><blockquote><p>类ClassName（Protocol[T]）被允许作为类ClassName的简写class ClassName(Protocol, Generic[T])</p></blockquote><h3 id="TypedDict"><a href="#TypedDict" class="headerlink" title="TypedDict"></a>TypedDict</h3><p>Python程序经常使用带有字符串键的字典来表示对象。TypedDict允许您为表示具有固定架构的对象的字典提供精确的类型，例如｛’id’：1，’items’：〔’x’〕｝。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line">Movie = TypedDict(<span class="string">&#x27;Movie&#x27;</span>, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="built_in">str</span>, <span class="string">&#x27;year&#x27;</span>: <span class="built_in">int</span>&#125;)</span><br><span class="line"></span><br><span class="line">movie: Movie = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Blade Runner&#x27;</span>, <span class="string">&#x27;year&#x27;</span>: <span class="number">1982</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Movie</span>(<span class="params">TypedDict</span>):</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    year: <span class="built_in">int</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookBasedMovie</span>(<span class="params">Movie</span>):</span></span><br><span class="line">    based_on: <span class="built_in">str</span></span><br></pre></td></tr></table></figure><h3 id="Literal"><a href="#Literal" class="headerlink" title="Literal"></a>Literal</h3><p>Literal类型可以指示表达式等于某个特定的primitive 值。</p><p>例如，如果我们用Literal[“foo”]类型注释一个变量，mypy将理解该变量不仅是str类型的，而且具体地等于字符串“foo”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, <span class="type">Literal</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expects_literal</span>(<span class="params">x: <span class="type">Literal</span>[<span class="number">19</span>]</span>) -&gt; <span class="literal">None</span>:</span> <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">reveal_type(<span class="number">19</span>)</span><br><span class="line">expects_literal(<span class="number">19</span>)</span><br></pre></td></tr></table></figure><h3 id="更多类型"><a href="#更多类型" class="headerlink" title="更多类型"></a>更多类型</h3><ul><li>NoReturn可以告诉mypy函数永远不会正常返回。</li><li>NewType允许您定义类型的变体，该变体被mypy视为单独的类型，但在运行时与原始类型相同。例如，您可以将UserId作为int的一个变体，它在运行时只是一个int。</li><li>@overload允许您定义一个可以接受多个不同签名的函数。如果您需要对难以正常表达的参数和返回类型之间的关系进行编码，这将非常有用。</li><li>Async 类型允许您使用异步和等待来键入检查程序。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> NoReturn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span>() -&gt; NoReturn:</span></span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">&#x27;no way&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> NewType</span><br><span class="line"></span><br><span class="line">UserId = NewType(<span class="string">&#x27;UserId&#x27;</span>, <span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">name_by_id</span>(<span class="params">user_id: UserId</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">UserId(<span class="string">&#x27;user&#x27;</span>)          <span class="comment"># Fails type check</span></span><br><span class="line"></span><br><span class="line">name_by_id(<span class="number">42</span>)          <span class="comment"># Fails type check</span></span><br><span class="line">name_by_id(UserId(<span class="number">42</span>))  <span class="comment"># OK</span></span><br><span class="line"></span><br><span class="line">num: <span class="built_in">int</span> = UserId(<span class="number">5</span>) + <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://realpython.com/python-type-checking/#type-theory">Python Type Checking (Guide) – Real Python</a></li><li><a href="https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html">Type hints cheat sheet - mypy 1.7.1 documentation</a></li><li><a href="https://python-type-challenges.zeabur.app/">https://python-type-challenges.zeabur.app/</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;Python中的类型系统,使用type hints使得整个开发过程更加顺畅.类似typescript的目的.&lt;br&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://www.sekyoro.top/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>图像读取与显示的问题</title>
    <link href="https://www.sekyoro.top/2023/11/16/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://www.sekyoro.top/2023/11/16/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2023-11-16T11:01:33.000Z</published>
    <updated>2023-11-16T14:26:23.920Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最近在用opencv和matplotlib展示图片,但是遇到了一些问题,这里展开说说<br><span id="more"></span></p><p>首先需要明确的是,opencv和matplotlib读取图片都是通道在最后,而前者默认可见光图像是BGR,后者是RGB.此外还有PIL以及imageio等读取图像的工具,这里不一一赘述.</p><h2 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h2><p>对于opencv,使用<code>cv2.imshow</code>,<code>cv2.imread</code>以及<code>cv2.imwrite</code>来读写以及显示.</p><h3 id="imshow"><a href="#imshow" class="headerlink" title="imshow"></a>imshow</h3><blockquote><p>显示图像的缩放取决于图像深度：<br>对 8 位无符号图像，按原样显示；<br>对 16 位无符号或 32 位整数图像，将像素值范围 [0,255*255] 映射到 [0,255] 显示；<br>对 32 位浮点图像，将像素值范围 [0,1] 映射到 [0,255] 显示；</p></blockquote><p>当cv2.imshow()处理图像深度为CV_8U（默认范围为[0,255]）时，按原数据显示；</p><p>当处理图像深度为CV_16U（默认范围为[0,65535]）时，除以256,映射到[0,255]；</p><p>当图像深度为CV_32F和CV_64F时（默认范围为[0,1]），乘以255映射到[0,255],超过255直接饱和；</p><p><strong>当输入负数时，当作0来处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = -<span class="number">1</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = -<span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>由于numpy默认类型float64,浮点数会乘以255,所以只有最上面有一条白线.负值直接黑色</p><p><img data-src="https://i.imgur.com/80ELhGP.png" alt="image-20231116201141606" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 新建numpy数组，注意np.zero()创建的数据类型为float64</span></span><br><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231116201615437.png" alt="image-20231116201615437" style="zoom:50%;" /></p><p>而如果是大于1的浮点数,也是直接饱和.</p><p>如果是uint8,如果超出255,则会被numpy截取,也就是取模</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>),dtype=np.uint8)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">20</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">30</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/tL7jHo0.png" alt="image-20231116201823217" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>),dtype=np.uint8)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] =  <span class="number">512</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>打印img[250:270, 150:350]的值发现是0</p><p><img data-src="https://i.imgur.com/B39iVFz.png" alt="image-20231116202123379" style="zoom:50%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">10</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] =  <span class="number">512</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/QT0dMuS.png" alt="image-20231116202222468" style="zoom:50%;" /></p><p>所以这涉及两个问题,一个是本身numpy的截取另一个是opencv的截取机制.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">img = np.zeros((<span class="number">500</span>, <span class="number">500</span>, <span class="number">1</span>), dtype=np.uint16)</span><br><span class="line"><span class="built_in">print</span>(img.dtype)</span><br><span class="line">img[<span class="number">150</span>:<span class="number">170</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">2</span></span><br><span class="line">img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span>*<span class="number">255</span></span><br><span class="line">img[<span class="number">350</span>:<span class="number">370</span>, <span class="number">150</span>:<span class="number">350</span>] = <span class="number">255</span>*<span class="number">100</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;img&#x27;</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">250</span>:<span class="number">270</span>, <span class="number">150</span>:<span class="number">350</span>])</span><br><span class="line">cv2.imwrite(<span class="string">&quot;test.png&quot;</span>, img)</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/92aVQEA.png" alt="image-20231116202916344" style="zoom:50%;" /></p><p>如果是16位无符号整数,会除以255.</p><p>最后注意,如果是int32可能会报错</p><h3 id="imwrite"><a href="#imwrite" class="headerlink" title="imwrite"></a>imwrite</h3><p>机制与imshow类似,不过会根据保存文件的后缀进行编码参数.</p><p>cv2.imwrite() 能保存 BGR 3通道图像，或 8 位单通道图像、或 PNG/JPEG/TIFF 16位无符号单通道图像</p><p><strong>注意</strong>:如果保存float32的图像值超过了1,此时会与imshow机制不同,表现为值被归到0-255</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones([<span class="number">255</span>,<span class="number">255</span>,<span class="number">1</span>],dtype=np.float32)</span><br><span class="line">a[<span class="number">0</span>:<span class="number">255</span>,<span class="number">0</span>:<span class="number">255</span>] = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">cv2.imshow(<span class="string">&quot;img&quot;</span>,a)</span><br><span class="line">cv2.waitKey()</span><br><span class="line">cv2.imwrite(<span class="string">&quot;test.png&quot;</span>,a)</span><br></pre></td></tr></table></figure><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231116222050856.png" alt="image-20231116222050856"></p><p><img data-src="https://i.imgur.com/kwtCm9Z.png" alt="image-20231116222107874"></p><p>上面有两张图,分别是imwrite的图片与imshow的图片,由于是浮点数,imshow展示时乘了255导致饱和白色.所以会说imwrite对浮点数不友好,不符合imshow的道理,</p><h3 id="imread"><a href="#imread" class="headerlink" title="imread"></a>imread</h3><p>注意如果有通道则通道在最后,可以设置</p><blockquote><p>IMREAD_UNCHANGED            = -1, //如果设置，则返回的数据带有alpha通道（R,G,B,A 四个通道），否则没有alpha通道<br>      IMREAD_GRAYSCALE            = 0,  //如果设置，则将图像转换为单通道灰度图像<br>      IMREAD_COLOR                = 1,  //如果设置，则将图像转换成3通道BGR彩色图像<br>      IMREAD_ANYDEPTH             = 2,  //如果设置，则在输入具有相应深度时返回16位/32位图像，否则将其转换为8位<br>      IMREAD_ANYCOLOR             = 4,  //如果设置，则图像可能以任何颜色格式读取<br>      IMREAD_LOAD_GDAL            = 8,  //如果设置，使用gdal驱动程序加载图像<br>      IMREAD_REDUCED_GRAYSCALE_2  = 16, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/2<br>      IMREAD_REDUCED_COLOR_2      = 17, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/2<br>      IMREAD_REDUCED_GRAYSCALE_4  = 32, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/4<br>      IMREAD_REDUCED_COLOR_4      = 33, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/4<br>      IMREAD_REDUCED_GRAYSCALE_8  = 64, //如果设置，总是将图像转换为单通道灰度图像且图像大小减少1/8<br>      IMREAD_REDUCED_COLOR_8      = 65, //如果设置，总是将图像转换为3通道BGR彩色图像且图像大小减少1/8<br>      IMREAD_IGNORE_ORIENTATION   = 128 //如果设置，不会根据EXIF的方向标志旋转图像</p></blockquote><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><h3 id="imshow-1"><a href="#imshow-1" class="headerlink" title="imshow"></a>imshow</h3><p>主要讲讲matplotlib的imshow</p><blockquote><p>matplotlib在imshow时，如果接收到的是二维矩阵，会自动归一化，映射到彩色。如果输入的矩阵里面值都是一样的，归一化会把他们全部变为255，也就是呈现黑色。</p></blockquote><p>用于在使用 cmap 映射到颜色之前将标量数据缩放到 [0, 1] 范围的归一化方法。默认情况下，使用线性缩放，将最低值映射到 0，将最高值映射到 1。</p><p>imshow的输入</p><blockquote><p>图像数据。支持的数组形状有：(M,N)：具有标量数据的图像。使用归一化和颜色图将值映射到颜色。请参阅参数norm、cmap、vmin、vmax。</p><p>(M, N, 3)：具有 RGB 值（0-1 float 或 0-255 int）的图像。</p><p>(M, N, 4)：具有 RGBA 值（0-1 float 或 0-255 int）的图像，即包括透明度。前两个维度（M、N）定义图像的行和列。超出范围的 RGB(A) 值将被剪裁。</p></blockquote><p>所以如果使用单通道的数据,会默认norm,而这种norm是根据输入值的min-max进行norm,并不是norm到0-255</p><p><img data-src="https://i.imgur.com/1zK0cLv.png" alt="image-20231116210309620"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = torch.ones(<span class="number">152</span>,<span class="number">152</span>,<span class="number">1</span>,dtype=torch.uint8)*<span class="number">220</span></span><br><span class="line">img = img.numpy()</span><br><span class="line">plt.imshow(img,cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>解决办法是设置vmin=0,vmax=255,当然使用三通道也可以</p><p><img data-src="https://i.imgur.com/4gWVsnC.png" alt="image-20231116210559535"></p><h3 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h3><ol><li><a href="https://blog.csdn.net/zjh12312311/article/details/116209353">matplotlib 可视化图像明明255，结果出来全为黑色的问题<em>plt.imshow 不加vmin和vmax参数是全黑的</em>佳hong的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/weixin_42216109/article/details/89707220">有关函数cv2.imshow()处理不同图像深度时的数据转化问题_cv2.cv_8u图像深度-CSDN博客</a>这篇文章有点问题,目前opencv将负值作为0处理</li><li><a href="https://www.cnblogs.com/siren27/p/12738571.html">opencv中imwrite对float的处理 - siren27 - 博客园 (cnblogs.com)</a></li><li><a href="https://blog.csdn.net/m0_37579176/article/details/105460265">【精选】使用 tiff/png 文件类型对 uint16_t/float 数据类型存取的无聊实验_float存储方式和uint16-CSDN博客</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在用opencv和matplotlib展示图片,但是遇到了一些问题,这里展开说说&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>transformer family(一)</title>
    <link href="https://www.sekyoro.top/2023/11/08/transformer-family/"/>
    <id>https://www.sekyoro.top/2023/11/08/transformer-family/</id>
    <published>2023-11-08T03:14:00.000Z</published>
    <updated>2024-01-11T08:11:39.545Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><del>流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.</del>主要介绍attention和transformer系列.</p><span id="more"></span><h2 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>主流的序列转换模型基于<strong>复杂的递归或卷积神经网络</strong>，其中<strong>包括一个编码器和一个解码器</strong>。性能最好的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构—“transformer”，它<strong>完全基于注意力机制，无需递归和卷积</strong>。</p><h3 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h3><p>递归神经网络，特别是长短期记忆和门控递归神经网络，已被牢固确立为语言建模和机器翻译等序列建模和转译问题的最先进方法。自此以后，许多人继续努力推动递归语言模型和编码器-解码器架构的发展。</p><p>递归模型通常按照输入和输出序列的符号位置进行计算。将位置与计算时间的步长对齐，它们会生成隐藏状态 h~t~ 的序列，作为前一个隐藏状态 h~t-1~ 和位置 t 的输入的函数。<strong>这种固有的序列性质排除了训练实例内的并行化，而在序列长度较长时，这一点变得至关重要，因为内存约束限制了跨实例的批处理。</strong>最近的研究通过因式分解技巧和条件计算显著提高了计算效率，同时也改善了后者的模型性能。然而，顺序计算的基本限制仍然存在。</p><p>在各种任务中，注意力机制已成为引人注目的序列建模和转导模型的一个组成部分，它可以对依赖关系进行建模，而无需考虑其在输入或输出序列中的距离。然而，除了少数情况，这种注意机制都是与递归网络结合使用的。</p><blockquote><p>比如下图,利用一个双向RNN得到每个token的状态,利用一个简单的ffn聚合这些状态作为输出token的上一个状态</p></blockquote><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png" alt="Image showing an encoder/decoder model with an additive attention layer"></p><p>在这项工作中，我们提出了 Transformer 模型架构，<strong>它摒弃了递归</strong>，<strong>而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong>。Transformer <strong>可以大大提高并行化程度</strong>，在 8 个 P100 GPU 上只需训练 12 个小时，翻译质量就能达到新的水平。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>减少顺序计算的目标也是Extended Neural GPU、ByteNet和 ConvS2S的基础，它们都使用<strong>卷积神经网络</strong>作为基本构建模块<strong>，并行计算</strong>所有输入和输出位置的隐藏表示。在这些模型中，将两个任意输入或输出位置的信号联系起来所需的运算次数随位置间距离的增加而增加，<strong>ConvS2S 是线性增加，ByteNet 是对数增加。这就增加了学习远距离位置之间依赖关系的难度</strong>。在 Transformer 中，这将被减少到一个恒定的操作数(O(1))，尽管<strong>代价是由于平均注意力加权位置而降低了有效分辨率</strong>。</p><p>自我注意（有时也称为内部注意）是一种注意机制，它将单个序列的不同位置联系起来，以计算序列的表征。自我注意已成功应用于多种任务中，包括阅读理解、抽象概括、文本引申和学习与任务无关的句子表征。</p><h3 id="model-architecture"><a href="#model-architecture" class="headerlink" title="model architecture"></a>model architecture</h3><p>大多数转导模型都具有编码器-解码器结构.在这里,编码器将输入的符号表示序列 (x1, …, xn) 映射为连续表示序列 z = (z1, …, zn)。 在给定 z 的情况下，解码器会逐个元素生成一个符号输出序列（y1, …, ym）。在每一步中，模型都是自动回归的，在生成下一步时，会消耗之前生成的符号作为额外输入。</p><p><img data-src="https://i.imgur.com/Y7S2I1W.png" alt="image-20231108115505228" style="zoom: 67%;" /></p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>编码器由 N = 6 层相同的层堆叠组成。每一层都有两个子层。第一个是多头自注意机制，第二个是简单的位置全连接前馈网络。我们在两个子层的每个周围都采用了残差连接，然后进行层归一化。也就是说，每个子层的输出都是 LayerNorm(x + Sublayer(x))，其中 Sublayer(x) 是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生维数为 dmodel = 512 的输出。</p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>解码器也由 N = 6 层相同的层堆叠组成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，对编码器堆栈的输出执行多头关注。</p><p>与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。我们还<strong>修改了解码器堆栈中的自我关注子层，以防止位置关注到后续位置</strong>。这种屏蔽，再加上输出嵌入偏移一个位置的事实，确保了对位置 i 的预测只能依赖于小于 i 的位置的已知输出。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>注意力函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。<strong>输出结果以值的加权和的形式计算</strong>，<strong>其中分配给每个值的权重是通过查询与相应密钥的兼容性函数计算得出的</strong>。</p><p><img data-src="https://i.imgur.com/DAhCPHu.png" alt="image-20231108120527812"></p><blockquote><p>上图就是一般用的q与k的计算方式,说白了就是矩阵相乘,而其中的mask是为了把其中用不上的token置为-∞,这样后面做softmax权重就是0了. 因为tensor维度都是相同的,q与k,</p></blockquote><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>输入包括维度为 d~k~的query和key,以及维度为 d~v~的value。我们计算query与所有keys的点积，将每个点积除以 √dk，然后应用软最大函数获得值的权重。</p><script type="math/tex; mode=display">\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>最常用的两种注意力函数是<strong>加法注意力</strong>(additive attention)和<strong>点积</strong>(dot production)注意力。点积注意力与我们的算法相同，只是缩放因子为 1 √dk。</p><p>加法注意使用单隐层前馈网络计算相容函数(相当于用一个全连接网络得到一个输出)。虽然两者的理论复杂度相似，但点积注意力在实际应用中速度更快，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>虽然在 dk 值较小的情况下，这两种机制的表现类似，但在 d~k~ 值较大的情况下，加法注意比点积注意更胜一筹。我们猜测，对于较大的 d~k~ 值，点积的幅度会越来越大，从而将软最大函数推向梯度极小的区域</p><blockquote><p>也就是说除以d~k~原因是使得梯度更大,效果更好</p></blockquote><h4 id="Multi-Head-attention"><a href="#Multi-Head-attention" class="headerlink" title="Multi-Head attention"></a>Multi-Head attention</h4><p>我们发现，将查询、键值和值分别线性投影到 d~k~、d~k~ 和 d~v~ 维度，而不是对 d~model~ 维度的键、值和查询执行单一的注意函数，这样做是有益的。</p><p>多头注意力允许模型<strong>在不同位置共同关注来自不同表征子空间的信息</strong>。而在单注意头的情况下，平均化会抑制这一点。</p><script type="math/tex; mode=display">MultiHead( Q, K, V) = Concat( \mathrm{head}_1, ..., \mathrm{head}_\mathrm{h} ) W^O \\ where \ head¡=Attention( QW_i^Q, KW_i^K, VW_i^V)</script><p>其中，投影是参数矩阵 W^Q^~i~∈R^dmodel×dk^ , W^K^ ~i~∈R^dmodel×dk^ , W^V^~i~∈R^dmodel×dv^ 和 W O∈R^hdv×dmodel^</p><h3 id="Transformer的应用"><a href="#Transformer的应用" class="headerlink" title="Transformer的应用"></a>Transformer的应用</h3><p>在 “encoder-decoder 注意 “层中,query来自前一个decoder层，而memory keys和memory values则来自encoder的输出。这使得decoder中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的encoder-decoder注意机制。</p><p>encoder包含自注意层。在自注意层中，所有的键、值和查询都来自同一个地方，在这种情况下，就是encoder中上一层的输出。encoder中的每个位置都可以关注encoder上一层的所有位置。</p><p>同样，解码器中的自关注层允许解码器中的每个位置关注解码器中包括该位置在内的所有位置。<strong>我们需要防止decoder中的信息向左流动，以保持自动回归特性</strong>。</p><p>通过点乘注意力中的mask实现,也就是在输出序列中,把后面的token得到的value设置为-∞,</p><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>除了注意力子层外，我们的编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别对每个位置进行相同的处理。这包括两个线性变换，中间有一个 ReLU 激活。</p><script type="math/tex; mode=display">\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2</script><p>虽然不同位置的线性变换相同，但各层使用的参数不同。</p><p>另一种描述方法是<strong>两个内核大小为 1 的卷积</strong>(全卷积)。输入和输出的维度为 d~model~ = 512，内层的维度为 d~ff~= 2048。</p><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>与其他序列转换模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度为 d~model~的向量。我们还使用通常学习到的线性变换和softmax，将解码器输出转换为预测的下一个标记词概率，在模型中，我们在两个嵌入层和pre-softmax linear transformation之间共享相同的权重矩阵。</p><p>在嵌入层中，我们将这些权重乘以 √dmodel。</p><h3 id="衍生"><a href="#衍生" class="headerlink" title="衍生"></a>衍生</h3><h4 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h4><p><img data-src="https://i.imgur.com/PcgRBVU.png" alt="image-20231125145939351"></p><p>BERT（来自 Transformers 的双向编码器表示）是一个非常大的多层 Transformer 网络,BERT-base 有 12 层，BERT-large 有 24 层,其旨在通过在所有层中联合调节左右上下文来预训练未标记文本的深度双向表示。因此，预训练的 BERT 模型只需一个额外的输出层即可进行微调，从而为各种任务（例如问答和语言推理）创建最先进的模型，而无需进行大量任务特定的架构修改。</p><h4 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h4><p><img data-src="https://i.imgur.com/bmErBND.png" alt="image-20231125145906882"></p><p>在CV领域,注意力要么与卷积网络结合使用,要么用来替换卷积网络的某些组件,整体结构保持不变。本文证明了CV领域不一定依赖CNN,使用纯粹的Transformer用于图片块序列，也可以很好的完成图像分类任务</p><h2 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2><p><img data-src="https://s2.loli.net/2024/01/10/SImfCJajrYsKyqO.png" alt="image-20240110092327560"></p><script type="math/tex; mode=display">\begin{aligned}&\hat{\mathbf{z}}^l=\text{W-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l-1}\right)\right)+\mathbf{z}^{l-1}, \\&\mathbf{z}^{l}=\mathbf{MLP}\left(\mathbf{LN}\left(\hat{\mathbf{z}}^{l}\right)\right)+\hat{\mathbf{z}}^{l}, \\&\hat{\mathbf{z}}^{l+1}=\text{SW-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l}\right)\right)+\mathbf{z}^{l}, \\&\mathbf{z}^{l+1}=\mathbf{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^{l+1}\right)\right)+\hat{\mathbf{z}}^{l+1},\end{aligned}</script><p><img data-src="https://s2.loli.net/2024/01/10/Kxgkua1MC7PnrSR.png" alt="image-20240110093609755"></p><h2 id="Squeeze-and-Excitation-Networks"><a href="#Squeeze-and-Excitation-Networks" class="headerlink" title="Squeeze-and-Excitation Networks"></a>Squeeze-and-Excitation Networks</h2><p><img data-src="https://s2.loli.net/2024/01/10/Lq6VRkQuoUpYSmc.png" alt="image-20240110094833740"></p><blockquote><ol><li>SENet通过学习channel之间的相关性，筛选出了针对通道的注意力，稍微增加了一点计算量，但是效果提升较明显</li><li>Squeeze-and-Excitation(SE) block是一个子结构，可以有效地嵌到其他分类或检测模型中。</li><li>SENet的核心思想在于通过网络根据loss去学习feature map的特征权重来使模型达到更好的结果</li><li>SE模块本质上是一种attention机制</li></ol></blockquote><p><img data-src="https://s2.loli.net/2024/01/10/ZrhKeEALunoDxpF.png" alt="image-20240110095007870"></p><h2 id="CBAM-Convolutional-Block-Attention-Module"><a href="#CBAM-Convolutional-Block-Attention-Module" class="headerlink" title="CBAM: Convolutional Block Attention Module"></a>CBAM: Convolutional Block Attention Module</h2><p><img data-src="https://s2.loli.net/2024/01/10/uPRhgXEveC9JbFS.png" alt="image-20240110104503985"></p><p><img data-src="https://s2.loli.net/2024/01/10/wxDGep963Qzs5tq.png" alt="image-20240110104513785"></p><p><img data-src="https://s2.loli.net/2024/01/10/hyUFPErbDKB3TwI.png" alt="image-20240110104523806"></p><script type="math/tex; mode=display">\begin{aligned}\mathbf{F^{\prime}=M_c(F)\otimes F,}\\\mathbf{F^{\prime\prime}=M_s(F^{\prime})\otimes F^{\prime},}\end{aligned}</script><script type="math/tex; mode=display">\begin{gathered}\mathbf{M_{c}}(\mathbf{F}) =\sigma(MLP(AvgPool(\mathbf{F}))+MLP(MaxPool(\mathbf{F}))) \\=\sigma(\mathbf{W_1}(\mathbf{W_0}(\mathbf{F_{avg}^c}))+\mathbf{W_1}(\mathbf{W_0}(\mathbf{F_{max}^c}))), \end{gathered}</script><script type="math/tex; mode=display">\begin{aligned}\mathbf{M_{s}}(\mathbf{F})& =\sigma(f^{7\times7}([AvgPool(\mathbf{F});MaxPool(\mathbf{F})]))  \\&=\sigma(f^{7\times7}([\mathbf{F_{avg}^{s}};\mathbf{F_{max}^{s}}])),\end{aligned}</script><p><img data-src="https://s2.loli.net/2024/01/10/MQNWLjZP2yH8cwd.png" alt="image-20240110142553774"></p><h2 id="CC-Net和Axial-Attention"><a href="#CC-Net和Axial-Attention" class="headerlink" title="CC-Net和Axial Attention"></a>CC-Net和Axial Attention</h2><p>看论文时提到了CC-Net使用了交叉注意了.</p><p>参考<a href="https://www.codenong.com/cs106760382/">Axial Attention 和 Criss-Cross Attention及其代码实现 | 码农家园 (codenong.com)</a>这篇blog,写的不错.</p><h2 id="Non-Local"><a href="#Non-Local" class="headerlink" title="Non-Local"></a>Non-Local</h2><p><img data-src="https://s2.loli.net/2024/01/11/IW6cKRTQN178kp4.png" alt="image-20240111161108109"></p><h2 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h2><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;del&gt;流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.&lt;/del&gt;主要介绍attention和transformer系列.&lt;/p&gt;</summary>
    
    
    
    <category term="deep learning" scheme="https://www.sekyoro.top/categories/deep-learning/"/>
    
    
    <category term="deep learning" scheme="https://www.sekyoro.top/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>图像融合论文阅读</title>
    <link href="https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2023-11-02T14:24:07.000Z</published>
    <updated>2023-11-18T08:43:46.828Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>课程作业<br><span id="more"></span></p><h1 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h1><p>介绍图像融合概念，回顾sota模型，其中包括数字摄像图像融合，多模态图像融合，</p><p>接着评估一些代表方法</p><p>介绍一些常见应用，比如RGBT目标跟踪，医学图像检查，遥感监测</p><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>动机：</p><p>由于硬件设备的理论和技术限制，单一传感器或单一拍摄设置所拍摄的图像无法有效、全面地描述成像场景</p><p>图像融合：图像融合能够将不同源图像中的有意义信息结合起来，生成单一图像，该图像包含更丰富的信息，更有利于后续应用</p><p>由于融合图像的优异特性，图像融合作为一种图像增强方法已被广泛应用于许多领域，例如摄影可视化</p><h2 id="传统融合方法："><a href="#传统融合方法：" class="headerlink" title="传统融合方法："></a>传统融合方法：</h2><p>在深度学习盛行之前，图像融合已经得到了深入的研究。早期实现图像融合的方法采用相关的数学变换，在<strong>空间域或变换域</strong>人工分析活动水平并设计融合规则，称为传统融合方法。</p><p>典型的传统融合方法包括基于<strong>多尺度变换</strong>的方法、基于<strong>稀疏表示</strong>的方法、<strong>基于子空间</strong>的方法、基于<strong>显著性</strong>的方法、基于total-variance的方法等。</p><p>传统图像融合方法的缺点：</p><ol><li>为了保证后续图像融合的可行性，传统方法会对不同源的图像采用相同变换来提取特征。这种方法没有考虑到源图像的特征差异，可能导致提取的特征表现力较差。</li><li>融合策略粗糙，表现较差。</li></ol><p>引入深度学习的优势：</p><ol><li><p>可以利用不同的网络实现差异化的特征提取</p></li><li><p>在良好设计的损失函数下，融合策略可以学到更合理的特征</p></li></ol><p>现有的深度学习方法致力于解决图像融合中的三个主要问题：“feature extraction, feature fusion and image reconstruction.” (Zhang 等, 2021, p. 323)</p><p>现有方法可以分为 AutoEncoder-based，CNN- based，GAN-based。</p><h3 id="1-AE-based"><a href="#1-AE-based" class="headerlink" title="1.AE-based"></a>1.AE-based</h3><p>AE 方法通常会预先训练一个自动编码器。然后利用训练好的自编码器实现特征提取和图像重建，中间的特征融合则根据传统的融合规则实现。</p><p>DenseFuse</p><h3 id="2-CNN-based"><a href="#2-CNN-based" class="headerlink" title="2.CNN-based"></a>2.CNN-based</h3><p>他们通常以两种不同的形式将卷积神经网络引入图像融合。一种是通过使用精心设计的损失函数和网络结构，实现端到端的特征提取、特征融合和图像重建</p><p>PMGI。它提出了梯度和强度的比例维护损失，引导网络直接生成融合图像。</p><p>另外还有使用CNN得到融合规则，而使用传统的特征提取和重建方法</p><h3 id="3-GAN-baesd"><a href="#3-GAN-baesd" class="headerlink" title="3.GAN-baesd"></a>3.GAN-baesd</h3><p>GAN 方法依靠生成器和判别器之间的对抗博弈来估计目标的概率分布，从而以隐含的方式共同完成特征提取、特征融合和图像重构</p><p>比如FusionGAN 是基于 GAN 的图像融合的先驱，它在融合图像和可见图像之间建立对抗博弈，以进一步丰富融合图像的纹理细节。由于各种图像融合任务存在显著差异，这些方法在不同融合场景中的实现方式也不尽相同。</p><h2 id="图像融合场景"><a href="#图像融合场景" class="headerlink" title="图像融合场景"></a>图像融合场景</h2><p>digital photography image fusion</p><p>由于数字成像设备的性能限制，传感器无法在单一设置完全表征成像场景中的信息</p><p>例如，数码摄影产生的图像只能承受有限的光照变化，并具有预定的景深。</p><p>多曝光度图像融合和多聚焦图像融合</p><p>以产生高动态范围和完全清晰的效果。</p><p>人们使用摄像机拍摄时,希望可以获得同一场景中所有景物都清晰的图像。但是<strong>摄像机镜头受景深的限制,无法同时聚焦所有目标,因此拍摄的照片中部分区域清晰,部分区域模糊。多聚焦图像融合技术可以将多幅同一场景下聚焦区域不同的图像融合成一幅全清晰的图像</strong>,从而有效地解决这个问题,提高图像的信息利用率。</p><p>multi-modal image fusion</p><p>由于成像原理的限制，单个传感器只能捕捉到部分场景信息。多模态图像融合将多个传感器获取的图像中最重要的信息结合起来，从而实现对场景的有效描述。</p><p>典型的多模态图像融合包括红外和可见光图像融合</p><p>sharpening fusion</p><p>在保证信噪比的前提下，光谱/滤镜与瞬时视场（IFOV）之间存在一定的矛盾。</p><p>换句话说，没有任何传感器能同时捕捉高空间分辨率和高光谱分辨率的图像。</p><p>锐化融合专门用于融合不同空间/光谱分辨率的图像，以生成所需的结果，这些结果不仅具有高空间分辨率，还具有高光谱分辨率。</p><p>典型的锐化融合包括多光谱（MS）锐化和高光谱锐化。从源图像成像的角度来看，锐化融合也属于多模态图像融合。不过，就融合目标而言，锐化融合比上述多模态图像融合要求更高的光谱/空间保真度，能直接提高分辨率。因此，锐化融合将作为一个单独的类别进行讨论。</p><p>多光谱锐化是将低空间分辨率（LRMS）的多光谱图像与全色（PAN）图像融合，生成高空间分辨率的多光谱图像。</p><p>与多光谱图像相比，高光谱图像具有更高的光谱分辨率和更低的空间分辨率。</p><blockquote><p>多光谱: 谱段有多个,可以看做是高光谱的一种情况，即成像的波段数量比高光谱图像少，一般只有几个到十几个。由于光谱信息其实也就对应了色彩信息，所以多波段遥感图像可以得到地物的色彩信息，但是空间分辨率较低。更进一步，光谱通道越多，其分辨物体的能力就越强，即光谱分辨率越高。</p><p>高光谱:高光谱由更窄的波段（10-20 nm）组成，具有较高的光谱分辨率，可以检测物体的光谱特效，可提供更多无形的数据,图像可能有数百或数千个波段</p><p>全色图:全色图像是单通道的，其中全色是指全部可见光波段0.38~0.76um，全色图像为这一波段范围的混合图像。因为是单波段，所以在图上显示为灰度图片。全色遥感图像一般空间分辨率高，但无法显示地物色彩，也就是图像的光谱信息少。</p></blockquote><h3 id="digital-photography-image-fusion"><a href="#digital-photography-image-fusion" class="headerlink" title="digital photography image fusion"></a>digital photography image fusion</h3><p>数字成像设备利用光学镜头捕捉反射的可见光然后采用CCD和CMOS等书子模块记录场景信息。另一方面，由于动态范围有限，这些数字模块无法承受过大的成像曝光差异。</p><p>一方面，由于光学镜头受景深限制，通常无法同时聚焦所有物体。</p><h3 id="Infrared-and-visible-image-fusion”"><a href="#Infrared-and-visible-image-fusion”" class="headerlink" title="Infrared and visible image fusion”"></a>Infrared and visible image fusion”</h3><p>红外图像具有<strong>明显的对比度</strong>，即使在恶劣天气下也能从背景中有效地突出目标。可见光图像包含丰富的纹理细节，更符合人类的视觉感知。红外和可见光图像融合就是要将这两种特性结合起来，生成对比度高、纹理丰富的图像。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。</p><p><strong>高对比度，恶劣条件下也能有效突出目标</strong>。</p><p><strong>可见光图像包含丰富的纹理信息</strong>，更符合人类视觉感知。红外和可见光图像融合就是要<strong>将这两种特性结合起来，生成对比度高、纹理丰富的图像</strong>。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。</p><p>AE方法</p><p>首先使用数据集训练一个autoencoder，训练好的自动编码器自然可以用来解决图像融合中的两个子问题：特征提取和图像重建</p><p>图像融合的关键在于<strong>特征融合策略</strong>的设计。目前，在红外和可见光图像融合中，特征融合的策略仍然是手工计算的，无法学习，如加法、l1-norm [19]、注意力加权等。这种手工计算的融合策略比较粗糙，限制了红外图像和可见光图像融合的进一步改进。</p><p>一种用于红外和可见光图像融合的 CNN 方法是端对端地实现三个子问题。这种CNN结构通常需要残差连接，全连接以及双端结构。</p><p>由于红外图像和可见光图像融合没有ground truth，因此损失函数的设计在于确定<strong>融合结果和源图像之间对比度和纹理的相似性</strong>。</p><p>参与红外和可见光图像融合的另一种 CNN 形式是使用预先训练好的网络（如 VGGNet）从源图像中提取特征，并根据这些特征生成融合权重图。</p><p>从这个角度看，卷积神经网络只实现了融合，而不考虑特征提取和图像重建，带来的融合性能非常有限。</p><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p>GAN 方法是目前红外和可见光图像融合领域最流行的方法，它能够以<strong>隐含的方式完成特征提取、特征融合和图像重建</strong>。</p><p>一般来说，GAN 方法依赖于两种损失函数，即内容损失和对抗损失。内容损失与 CNN 方法类似，用于初步融合源图像，而对抗损失则进一步限制信息融合的趋势。</p><p>早期GAN方法 fusionGAN，只是在融合后的图像和可见光图像之间建立对抗博弈，以进一步增强对可见光图像丰富细节的保留。</p><p>为了更好地平衡红外信息和可见光信息，随后的方法 [25,66-69] 开始使用具有<strong>多个分类约束条件的单一判别器或双判别器来同时估计源图像的两种概率分布。</strong></p><p>一般来说，GAN 方法可以产生很好的融合结果。然而，在训练过程中保持生成器和判别器之间的平衡并非易事。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>评估指标包括:EN,SSIM,PSNR,SF,SD,CC,SF,VIF以及融合运行时间等等. </p><h4 id="EN"><a href="#EN" class="headerlink" title="EN"></a>EN</h4><p>熵值</p><p><img data-src="https://i.imgur.com/czqa4Qz.png" alt="image-20231103165404222"></p><p>p~l~是融合图像中相应灰度级的归一化直方图</p><p>熵越大，融合图像包含的信息就越多，融合方法的性能就越好。</p><h4 id="SD"><a href="#SD" class="headerlink" title="SD"></a>SD</h4><p>标准差</p><p><img data-src="https://i.imgur.com/nP1CzPO.png" alt="image-20231103165557350"></p><p>对比度高的区域总是能吸引人的注意力，而对比度高的融合图像往往能产生较大的标清值，这意味着融合图像能达到更好的视觉效果。</p><h4 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h4><p>结构相似性,取值[-1,1],<strong>数值越接近1表示结构相似性越高</strong></p><p>SSIM 用于建立图像损失和失真的模型，<strong>衡量源图像和融合图像之间的结构相似性</strong>。SSIM 主要由三部分组成：<strong>相关性损失、亮度失真和对比度失真</strong>。</p><p><img data-src="https://pic4.zhimg.com/80/v2-13bb3c60b27920c3ec834e045ec8756f_720w.webp" alt="img"></p><p>在融合任务中,计算两张源图与融合后图像的SSIM和</p><p><img data-src="https://i.imgur.com/xSMcHqm.png" alt="image-20231103171546346"></p><h4 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h4><p>峰值信噪比,衡量图像有效信息与噪声之间的比率,能够反映图像是否失真.<strong>数值越大表示失真越小</strong></p><p><img data-src="https://i.imgur.com/5OAM3nA.png" alt="image-20231103164139843"></p><p>Z表示理想参考图像灰度最大值与最小值之差，通常为255。PSNR的值越大，表示融合图像的质量越好。</p><blockquote><p>PSNR值的范围通常在<strong>0到100之间</strong>，单位为分贝（dB）。 通常情况下，PSNR值越高，表示原始图像与重建图像之间的差异越小，图像质量越接近原始图像。 一般来说，PSNR值在30到40dB之间被认为是可以接受的</p></blockquote><h4 id="CC"><a href="#CC" class="headerlink" title="CC"></a>CC</h4><p>CC 衡量融合图像与源图像的线性相关程度,CC 越大，融合后的图像与源图像越相似，融合效果越好</p><p><img data-src="https://i.imgur.com/EnD6Jys.png" alt="image-20231103172617252"></p><p><img data-src="https://i.imgur.com/FW1COsX.png" alt="image-20231103172623697"></p><h4 id="SF-空间频率"><a href="#SF-空间频率" class="headerlink" title="SF 空间频率"></a>SF 空间频率</h4><p><img data-src="https://i.imgur.com/qczz1P9.png" alt="image-20231103173521300"></p><p>测量图像的梯度分布</p><p><img data-src="https://i.imgur.com/Hqdxy5j.png" alt="image-20231103173745980"></p><p><img data-src="https://i.imgur.com/DmKSSiy.png" alt="image-20231103173758017"></p><p>SF 越大，融合图像的边缘和纹理就越丰富</p><p>​    </p><h4 id="VIF-空间信息保真度"><a href="#VIF-空间信息保真度" class="headerlink" title="VIF 空间信息保真度"></a>VIF 空间信息保真度</h4><p>VIF 衡量融合图像的信息保真度，其计算方法分为四个步骤：首先，将源图像和融合图像划分为不同的区块；然后，评估每个区块在失真和未失真情况下的视觉信息；接着，评估每个子波段的 VIF；最后，根据 VIF 计算总体指标。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>TNO TNO影像融合数据集包含不同军事相关场景的单光谱（增强视觉、近红外和长波红外或热）夜间影像，在不同的多波段camnera系统中注册。</p><p> INO RoadScene MSRS LLVIP MFD</p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>主要关注红外图像与可见光图像融合以及多焦点图像融合,从这些出发,最后到一个统一的图像融合框架.</p><h3 id="FusionGAN"><a href="#FusionGAN" class="headerlink" title="FusionGAN"></a>FusionGAN</h3><p>2019年较早的使用GAN作为图像融合算法融合红外和可见光</p><p><img data-src="https://i.imgur.com/OP6inkz.png" alt="image-20231104161851282"></p><h4 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h4><p><img data-src="https://i.imgur.com/gcNlg22.png" alt="image-20231104161928602"></p><p>注意损失函数设计</p><script type="math/tex; mode=display">\mathcal{L}_G=V_\text{FusionGAN}(G)+\lambda\mathcal{L}_{\mathrm{content}},</script><p>使用了一个对于GAN对抗的融合损失以及一个内容损失,对抗损失,这种想法来源LSGAN,a 和 b 分别表示虚假数据和真实数据的标签，c表示生成器希望鉴别器相信的虚假数据值。</p><script type="math/tex; mode=display">\begin{aligned}\min_DV_{\mathrm{LSGAN}}(D)&=~\frac12\mathbb{E}_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-a)^2],\\\min_GV_{\mathrm{LSGAN}}(G)&=~\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-c)^2],\end{aligned}</script><p>有两种方法可以确定公式中的 a、b 和 c 值。第一种是设置 b - c = 1 和 b - a = 2，从而最小化公式 ，使 P~data~ +P~g~ 与 P~g~ 之间的 Pearson χ2 最小化</p><p>第二种是设置 c = b，使生成器生成的样本尽可能真实。上述两种方法通常能获得相似的性能。</p><script type="math/tex; mode=display">V_{\text{FusionGAN}} ( G ) = \frac 1 N \sum _ { n = 1 }^{N}\left(D_{\theta_D}(I_f^n)-c\right)^2,</script><p>第二项 L~content~代表内容损失，λ 用于在 V~FusionGAN~(G) 和 L~content~之间取得平衡。由于红外图像的热辐射信息由其像素强度表征，而可见光图像的纹理细节信息可部分由其梯度表征. 当然可以有其他用于表征图片的某些特性的指标,比如上面介绍的SSIM等.</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{content}}=\frac1{HW}(\|I_f-I_r\|_F^2+\xi\|\nabla I_f-\nabla I_v\|_F^2),</script><p>实际上，如果没有 D~θ~，我们也可以得到融合图像，它可以保留红外图像中的热辐射信息和可见光图像中的梯度信息。</p><p>但这往往还不够，因为仅使用梯度信息无法完全表现可见图像中的纹理细节。因此，我们在生成器 G~θG~和判别器 D~θD~ 之间建立了一个对抗博弈，以调整基于可见光图像 IIv 的融合图像 If。</p><h4 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h4><p>从第一层到第四层，我们在卷积层中使用 3 × 3 滤波器，并将stride设为 2，不带填充。这与生成器网络不同。其根本原因在于，鉴别器是一个分类器，它首先从输入图像中提取特征图，然后进行分类。因此，它的工作方式与池化层相同，将stride设置为 2。</p><p><img data-src="https://i.imgur.com/XZhUCg2.png" alt="image-20231104163811787"></p><script type="math/tex; mode=display">\mathcal{L}_D=\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_v)-b\right)^2+\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_f)-a\right)^2,</script><p>我使用了这个模型</p><h3 id="TarDAL"><a href="#TarDAL" class="headerlink" title="TarDAL"></a>TarDAL</h3><p>面向检测的融合</p><p>我们采用双层优化公式同时进行图像融合和物体检测，不仅检测精度高，而且融合后的图像视觉效果更好。</p><p>我们设计了一种参数较少的目标感知双对抗学习网络（TarDAL），用于面向检测的融合。这种 “求同存异 “的单生成器双判别器网络可保留红外目标信息和可见光纹理细节。</p><p>我们从双层表述中推导出一种合作训练方案，为快速推理（融合和检测）提供最佳网络参数。</p><p>与以往追求高视觉质量的方法不同，我们认为，IVIF 必须生成既有利于视觉检测又有利于计算机感知的图像，即面向检测的融合。</p><h3 id="问题建模"><a href="#问题建模" class="headerlink" title="问题建模"></a>问题建模</h3><p><img data-src="https://i.imgur.com/Qwko06N.png" alt="image-20231104153245210"></p><p>假设红外图像、可见光图像和融合图像都是大小为 m×n 的灰度图像，分别用列向量 x、y 和 u∈R^mn×1^ 表示。</p><p>L~d~ 是目标检测相关的训练损失,Ψ是一个目标检测网络,f () 是一个基于能量的保真度项，包含融合图像 u 以及源图像 x 和 y，而 g~T~ () 和 g~D~ () 则是两个可行性约束条件,分别定义在红外图像和可见光图像上。</p><p><img data-src="https://i.imgur.com/lv9Mg1o.png" alt="image-20231104154941474"></p><p>引入一个带有学习参数 ω~f~的融合网络 Φ，并将双级优化转换为单级优化. </p><p>因此，我们将优化分解为两个学习网络 Φ 和 Ψ。我们采用 YOLOv53 作为检测网络 Ψ 的主干，其中 L~d~也沿用其设置，并精心设计了融合网络 Φ 。</p><blockquote><p>典型的深度融合方法致力于学习两种不同成像模式的共同特征。相反，我们的融合网络在<strong>学习这两种成像方式互补特征的差异的同时，也在寻求共性</strong>。通常情况下<strong>，红外图像能突出显示目标的独特结构，而可见光图像则能提供背景的纹理细节</strong>。</p></blockquote><h4 id="Target-aware-dual-adversarial-network"><a href="#Target-aware-dual-adversarial-network" class="headerlink" title="Target-aware dual adversarial network"></a>Target-aware dual adversarial network</h4><p><img data-src="https://i.imgur.com/vuIYIRj.png" alt="image-20231107114627598"></p><p>生成器G生成一张逼真的融合图像,目标判别器D~T~使用强度一致性评估红外图像中的目标与G提供的融合图像中被mask的目标.细节判别器 D~D~判别的是可见光梯度分布与融合图像的梯度分布</p><h4 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h4><p>生成器的作用是生成能保留整体结构并保持与源图像相似强度分布的融合图像。常用的结构相似性指数(SSIM)作为损失函数.</p><p>为了平衡源图像的像素强度分布，引入了基于突出度权重（SDW）的像素损失。</p><p>另外提出了一个基于显著性pixel loss</p><script type="math/tex; mode=display">S_{\mathbf{x}(k)}=\sum_{i=0}^{2\text{55}} \boldsymbol { H _ { \mathbf{x}}}(i)|\mathbf{x}(k)-i|,</script><p>其中H~x~(i)表示直方图中i的值,x(k)表示第k个值,因为x为一个大小为mn的vector</p><script type="math/tex; mode=display">\mathscr{L}_{\mathrm{pixe}1}=\|\mathrm{u}-\omega_1\mathrm{x}\|_1+\|\mathrm{u}-\omega_2\mathrm{y}\|_1,</script><p>pixel loss如上,其中</p><script type="math/tex; mode=display">\boldsymbol{\omega}_1=S_\mathbf{x}(k)/[S_\mathbf{x}(k)-S_\mathbf{y}(k)],\boldsymbol{\omega}_\mathbf{2}=1-\boldsymbol{\omega}_\mathbf{1}.</script><p>使用 5 层密集块作为 G 来提取共同特征，然后使用包含三个卷积层的合并块进行特征聚合。每个卷积层由一个卷积运算、批处理归一化和 ReLU 激活函数组成。生成的融合图像 u 与源图像大小相同。</p><h4 id="目标鉴别器与细节鉴别器"><a href="#目标鉴别器与细节鉴别器" class="headerlink" title="目标鉴别器与细节鉴别器"></a>目标鉴别器与细节鉴别器</h4><p>目标判别器 D~T~ 用于将融合结果的前景热目标与红外目标区分开来。而细节判别器 D~D~ 的作用是将融合结果的背景细节与可见光图像的细节区分开来。</p><p>采用了预训练的显著性检测网络从红外图像中计算出目标掩码 m，这样两个判别器就能在各自的区域（目标和背景）进行判别(也就是将图像中的目标与背景分割)</p><p>对抗损失如下,R(x)表示红外图像中的目标,R(u)表示融合后图像中的目标,R^^^则表示背景</p><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}_{D_T}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\mathcal{R}(\mathbf{x}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\mathcal{R}(\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{D_D}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{y}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{\mathbf{f}}^{\mathrm{adv}}=\mathcal{L}_{D_T}^\mathbf{f}+\mathcal{L}_{D_D}^\mathbf{f},\end{gathered}</script><p>R = x*m ,R^^^= 1 − R. m表示使用预训练模型得到mask.</p><p>对于鉴别器,损失分别是</p><script type="math/tex; mode=display">\begin{gathered}\mathcal{L}_{D_T}=\mathcal{L}_{D_T}^\mathbf{f}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\mathcal{R}(\mathbf{x}))}[(\|\nabla D_T(\tilde{x})\|)^p], \\\mathcal{L}_{D_{D}}=\mathcal{L}_{D_{D}}^{\mathbf{f}}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\hat{\mathcal{R}}(\nabla\mathbf{x}))}[(\|\nabla D_{D}(\tilde{x})\|)^{p}], \end{gathered}</script><p>两个鉴别器 D~T~ 和 D~D~ 具有相同的网络结构，即四个卷积层和一个全连接层。</p><p><img data-src="https://i.imgur.com/A3gJRD2.png" alt="image-20231107131708716"></p><h4 id="合作训练策略"><a href="#合作训练策略" class="headerlink" title="合作训练策略"></a>合作训练策略</h4><script type="math/tex; mode=display">\begin{aligned}\min_{\boldsymbol{\omega}_{\mathbf{d}},\boldsymbol{\omega}_{\mathbf{f}}}\mathcal{L}^{\mathbf{d}}(\Psi(\mathbf{u}^*;\boldsymbol{\omega}_{\mathbf{d}}))+\lambda\mathcal{L}^{\mathbf{f}}\big(\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}})\big),\\s.t.\mathbf{~u}^*=\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}}),\end{aligned}</script><p>双层优化自然会衍生出一种合作训练策略，以获得最佳网络参数 ω = (ω~d~, ω~f~)</p><p>引入了一个融合正则因子 L^f^，将受融合约束的检测优化转换为相互优化</p><p>损失函数包含目标检测的损失函数以及融合的损失函数.</p><h3 id="红外与可见光图像的融合结果"><a href="#红外与可见光图像的融合结果" class="headerlink" title="红外与可见光图像的融合结果"></a>红外与可见光图像的融合结果</h3><h4 id="定性比较"><a href="#定性比较" class="headerlink" title="定性比较"></a>定性比较</h4><p>首先，可以很好地保留红外图像中的分辨目标。如图 6（第二组的绿色缠结）所示，我们的方法中的人表现出高对比度和独特的突出轮廓，因此有利于视觉观察.</p><p>其次，我们的结果可以保留可见光图像中丰富的纹理细节（第一组和第三组的绿色缠结），这更符合人类的视觉系统。</p><h4 id="定量比较"><a href="#定量比较" class="headerlink" title="定量比较"></a>定量比较</h4><p>在 400 个图像对（20 个来自 TNO 的图像对、40 个来自 RoadScene 的图像对和 340 个来自 M3FD 的图像对）上对我们的 TarDAL 和上述竞争对手进行了定量比较。</p><p>使用了MI,EN和SD作为评估指标.</p><h3 id="红外与可见光目标检测结果"><a href="#红外与可见光目标检测结果" class="headerlink" title="红外与可见光目标检测结果"></a>红外与可见光目标检测结果</h3><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h3 id="多聚焦图像融合"><a href="#多聚焦图像融合" class="headerlink" title="多聚焦图像融合"></a>多聚焦图像融合</h3><h3 id="MFIF-GAN"><a href="#MFIF-GAN" class="headerlink" title="MFIF-GAN"></a>MFIF-GAN</h3><p>针对多焦点图像融合</p><p>在数码摄影领域，有限的景深（DOF）导致单一场景中可能存在多种图像焦点区域，并产生散焦效应（DSE）[1]。作为一种图像增强技术，多焦点图像融合（MFIF）被用来融合图 1(a) 和图 1(b) 所示的多焦点图像，使融合结果（如图 1(c) 所示）能够清晰地保留来源信息。这一操作是各类计算机视觉（CV）任务的前提条件，例如物体检测和定位、识别和分割</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>MFIF-GAN 中的生成器将源彩色图像 IA 和 IB 作为输入，旨在生成焦点图 ̂ F。判别器的输入是 IA、IB 和（真实或生成的）焦点图的连接。生成器的目的是尽可能精确地重建焦点图，而鉴别器的目的是将生成的焦点图与真实的焦点图区分开来。</p><p>G 包括一个编码器、一个张量连接模块和一个解码器。<strong>为了有效处理彩色图像，编码器被设计成六个并行子网络分支，共享源图像每个通道的参数</strong>。</p><h3 id="FuseGAN"><a href="#FuseGAN" class="headerlink" title="FuseGAN"></a>FuseGAN</h3><p>我们的目标是通过构建基于 cGAN 的网络 FuseGAN，学习从源图像到置信度图的映射，从而为融合任务提供重点信息。我们首先详细介绍了该架构，然后分析了其目标函数；最后阐述了融合方案.</p><p>生成器 G：如图所示，生成器 G 由三个部分组成：编码器、张量并合器和解码器。<strong>它将一对多焦点图像作为输入，并输出置信度图</strong>。具体来说，编码器有两个分支，每个分支包含 12 个块。为简单起见，我们将卷积层、批规范层和转置卷积层分别称为 Conv、BN 和 Decov。其中，第一块是 Conv-BN-ReLu，滤波器尺寸较大，为 7×7，步长为 1，目的是粗略提取特征。</p><p><img data-src="https://i.imgur.com/Fgpy1VL.png" alt="image-20231108184311430" style="zoom: 80%;" /></p><p>我们利用 中的 PatchGAN 作为判别器 D。从概念上讲，它试图辨别图像中每个大小为 K×K 的patch是真是假。鉴别器对图像中的所有响应进行卷积平均，最后生成输出。。</p><p><img data-src="https://i.imgur.com/oI8nYfO.png" alt="image-20231108190809183"></p><p>因此，我们利用自适应权重块设计的特定内容损失函数可以自适应地引导融合图像在像素级逼近源图像中重点区域的强度分布和梯度分布</p><p>此外,由于我们的优化目标是<strong>基于每个像素</strong>,<strong>为了避免融合后的图像出现色差,保证整体的自然度,我们增加了 SSIM 损失项</strong>。根据统计学原理,计算每个源图像片段中较大分数的平均值,作为相应 SSIM 损失项的权重。</p><h3 id="MFFGAN"><a href="#MFFGAN" class="headerlink" title="MFFGAN"></a>MFFGAN</h3><p>图像融合的理念是从源图像中提取并组合最有意义的信息。<strong>对于多焦点图像融合来说，最有意义的信息是源图像中的锐利区域，这些区域反映在强度分布和纹理细节上</strong>。自然，在信息提取过程中，应保留锐利区域的这些信息，而摒弃模糊区域的这些信息。</p><p>当然，在信息提取过程中，尖锐区域的信息应该保留，模糊区域的信息应该舍弃。因此，有必要在优化过程中<strong>引入损失函数的调整机制</strong>，以约束网络有选择地提取和重构信息。</p><p>首先，我们设计了一个自适应决策块，它可以根据重复模糊原理评估每个像素的清晰度,也就是说，清晰度较高的图像，经过模糊处理后，像素值变化较大。根据这一观察结果，生成screening map来描述有效信息的位置。screening map作用于我们构建的特定内容损失函数，从而在像素尺度上调整优化目标。</p><p>判定块可以自适应地引导融合结果在像素尺度上逼近清晰源图像的强度分布和梯度分布</p><p>.我们的具体方法是选择分数较大的像素（放弃较小的分数）作为两个源图像相应像素位置的优化目标。在决策块和内容损失的共同作用下，生成器可以得到相对清晰自然的结果。</p><p>我们将联合梯度图定义为真实数据，将融合图像的梯度图定义为假数据。持续的对抗性学习可以引导生成器更专注于纹理的保留。因此，我们可以获得更高质量的融合结果，其中包含更丰富的纹理细节。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损耗函数由生成器损耗L~G~和鉴别器损耗L~D~组成。</p><h4 id="生成器-1"><a href="#生成器-1" class="headerlink" title="生成器"></a>生成器</h4><p>生成器的损失有两部分，即用于提取和重构信息的内容损失L~Gcon~，以及用于增强纹理细节的对抗性损失L~Gadv~。</p><script type="math/tex; mode=display">\mathcal{L}_{G}=\mathcal{L}_{G_{\mathrm{adv}}}+\alpha L_{G_{\mathrm{con}}}</script><script type="math/tex; mode=display">\mathcal{L}_{G_{\mathrm{adv}}}=\frac{1}{N}\sum_{n=1}^{N}(D(\nabla(I_{\mathrm{fused}}^{n}))-a)^2</script><p>其中 N 是训练期间批次中融合图像的数量，a 是生成器期望判别器确定融合图像的概率标签</p><script type="math/tex; mode=display">L_{G_{\mathrm{con}}}=\beta_{1}\mathcal{L}_{\mathrm{int}}+\beta_{2}\mathcal{L}_{\mathrm{grad}}</script><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{int}}=\frac{1}{HW}\sum_{\cdot}\sum_{\cdot}S_{1_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{1_{i,j}})^{2}+S_{2_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{2_{i,j}})^{2}</script><script type="math/tex; mode=display">\begin{aligned}S_{1_{i,j}}&=\operatorname{sign}(RB(I_{1_{i,j}})-\min(RB(I_{1_{i,j}}),RB(I_{2_{i,j}}))),\\S_{2_{i,j}}&=1-S_{1_{i,j}},\end{aligned}</script><p>重复模糊函数</p><script type="math/tex; mode=display">RB(\cdot)~=~abs(I_{i,j}-LP(I_{i,j}))</script><p>LP （⋅） 表示低通滤波器函数。值得注意的是，S（⋅）的大小也是H × W。</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\mathrm{grad}}&=\frac1{HW}\sum_i\sum_jS_{\mathbf{1}_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{\mathbf{1}_{i,j}})^2\\&+S_{2_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{2_{i,j}})^2.\end{aligned}</script><h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><p>判别器的损失功能使判别器能够准确识别真假数据。在我们的方法中，假数据是融合图像的梯度图。真实数据是我们构建的联合梯度图。</p><script type="math/tex; mode=display">Grad_{\mathrm{fused}}=\mathrm{abs}(\nabla I_{\mathrm{fused}}) \\Grad_{\mathrm{joint}}=\max(\mathrm{abs}(\nabla I_1),\mathrm{abs}(\nabla I_2)),</script><script type="math/tex; mode=display">\mathcal{L}_{D_{\mathrm{adv}}}=\frac1N\sum_{n=1}^{N}[D(Grad_{\mathrm{fused}}^{n})-b]^{2}+[D(Grad_{\mathrm{joint}}^{n})-c]^{2}</script><p>其中 b 是融合图像梯度图的标签，应设置为 0。c 是联合梯度图的标签，应设置为 1。</p><p>也就是说，判别器期望准确地将联合梯度图识别为真实数据，将融合图像的梯度图识别为假数据。在这种约束下，判别器可以引导生成器在信息维护方面的倾向，即有利于强纹理保存.</p><h4 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h4><p><img data-src="https://i.imgur.com/WPHLLvv.png" alt="image-20231108222112489"></p><h4 id="生成器架构"><a href="#生成器架构" class="headerlink" title="生成器架构"></a>生成器架构</h4><p>我们将生成器拆分为两条路径来提取信息，对应于两个源图像。生成器网络的设计灵感来自pseudo-Siamese，它擅长处理两种相对不同的输入。由于多焦点图像对在相应的像素位置清晰或模糊，因此pseudo-Siamese网络适用于此类图像</p><p><img data-src="https://i.imgur.com/l2ONRey.png" alt="image-20231108215808309"></p><p>在这两条路径中，都有四个卷积层来提取特征。第一个卷积层使用 5 × 5 卷积核，其余三个卷积层使用 3 × 3 卷积核。它们都使用 Leaky ReLU 作为激活函数。为了防止卷积过程中的信息丢失，我们根据 DenseNet 的思想重用了这些特征.</p><p>同时，为了提取更充分的信息，我们在两条路径之间交换信息。具体来说，交换的信息是通过连接和卷积的方法生成的。然后，交换的信息与所有前一个卷积层的输出连接在一起，作为下一个卷积层的输入。</p><p>最后，我们将两条路径中所有卷积层的输出串联起来，然后通过卷积层生成融合图像。卷积层的核大小为 1 × 1，激活函数为 tanh。值得注意的是，在所有卷积层中，填充模式设置为“SAME”，即特征图的大小在整个卷积过程中没有变化，这与源图像的大小相同。</p><h4 id="判别器架构"><a href="#判别器架构" class="headerlink" title="判别器架构"></a>判别器架构</h4><p><img data-src="https://i.imgur.com/veuN5Um.png" alt="image-20231108222051637"></p><p>判别器中的输入有两种类型，一种是<strong>基于源图像的联合梯度图和融合图像的梯度图</strong>。鉴别器由四个卷积层和一个线性层组成。四个卷积层的卷积核大小为 3 × 3，它们都使用了LeakyReLU 激活函数。这些卷积层的步幅设置为 2。最后一层是用于查找分类概率的线性层。</p><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>我们的实验是在两个数据集上进行的，比如Lytro数据集[34]和我们基于公共数据库构建的MFI-WHU数据集。</p><p>在 Lytro 数据集和 MFI-WHU 数据集上，用于测试的图像对数分别为 10 和 30。对于训练，为了获得更多的训练数据，我们采用了剪裁分解的扩展策略。具体来说，对于 Lytro 数据集，我们将其余图像裁剪为 22,090 个大小为 60 × 60 的图像图块对进行训练;对于 MFI-WHU 数据集，我们将其余图像裁剪为 202,246 个大小为 60 × 60 的图像patch对进行训练。</p><p>batch_size=32,epochs=20,训练一个epoch需要m步数,将一张图片分为多个patch,m设置为所有patch数除以b. 一般考虑训练更多的判别器,训练判别器次数是生成器的p倍.</p><p><img data-src="https://i.imgur.com/TYR6p1y.png" alt="image-20231109104901606"></p><p>我们将图像从 RGB 转换为 YCbCr 色彩空间。由于 Y 通道（亮度通道）可以表示结构细节和亮度变化，因此我们只致力于融合 Y 通道值。对于 Cb 和 Cr 通道（色度通道），我们以传统方式融合它们。然后，将这些通道的融合分量转移到RGB以获得最终结果。</p><h3 id="一些结果"><a href="#一些结果" class="headerlink" title="一些结果"></a>一些结果</h3><h4 id="多焦图像融合"><a href="#多焦图像融合" class="headerlink" title="多焦图像融合"></a>多焦图像融合</h4><p><img data-src="https://i.imgur.com/YrdNz1J.png" alt="image-20231118163720106"></p><p><img data-src="https://i.imgur.com/8y3MF5I.png" alt="image-20231118164233372"></p><h4 id="红外可见光图像融合"><a href="#红外可见光图像融合" class="headerlink" title="红外可见光图像融合"></a>红外可见光图像融合</h4><p><img data-src="https://i.imgur.com/6H8J7CW.png" alt="image-20231118164258767"></p><h3 id="代码链接"><a href="#代码链接" class="headerlink" title="代码链接"></a>代码链接</h3><p><a href="https://github.com/drowning-in-codes/UFGAN">drowning-in-codes/UFGAN: GAN for Image Fusion which is inspired by FusionGAN and U-net (github.com)</a></p><p><a href="https://github.com/drowning-in-codes/MFF-GAN">drowning-in-codes/MFF-GAN: Code of MFF-GAN: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion. (github.com)</a></p><p>colab 链接<a href="https://colab.research.google.com/drive/1wcb28gzgF62GphVdx42XkoZ68GxDaRpo#scrollTo=K8FjqBFQxmnR">UFGAN.ipynb - Colaboratory (google.com)</a></p><h3 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h3><p>利用预训练模型提供内容和风格 transfer learning?</p><p>利用cGAN思想? 此外损失函数的设计有必要换成神经网络而不是人工设计的一些值了.可以看看一篇CVPR的TARDAL<a href="http://arxiv.org/abs/2203.16220">http://arxiv.org/abs/2203.16220</a></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><p><a href="https://blog.csdn.net/Chaolei3/article/details/79404806">详细理解RGB图像、全色图像、多光谱图像、高光谱图像-CSDN博客</a></p></li><li><p><a href="https://github.com/Linfeng-Tang/Image-Fusion">Linfeng-Tang/Image-Fusion: Deep Learning-based Image Fusion: A Survey (github.com)</a></p><p><strong>综述</strong></p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253521001342">Image fusion meets deep learning: A survey and perspective - ScienceDirect</a></p></li><li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522001518">Current advances and future perspectives of image fusion: A comprehensive review - ScienceDirect</a></p></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;课程作业&lt;br&gt;</summary>
    
    
    
    
    <category term="image fusion" scheme="https://www.sekyoro.top/tags/image-fusion/"/>
    
  </entry>
  
  <entry>
    <title>目标检测学习_P3</title>
    <link href="https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/"/>
    <id>https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/</id>
    <published>2023-11-01T13:29:52.000Z</published>
    <updated>2023-11-30T02:45:02.311Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO.<br><span id="more"></span></p><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>YOLO模型是构建快速实时物体探测器的第一次尝试。因为YOLO不经历区域建议步骤，并且只在有限数量的边界框上进行预测，所以它能够超快速地进行推理。</p><ol><li><p>残差块</p><p>首先，将图像划分为不同的网格。每个网格的尺寸为S x S。将输入图像转换为网格的过程如下图所示。每个网格单元将检测其中出现的对象。</p></li><li><p>边界框线性回归</p></li></ol><p>边界框是高亮显示图像中具有某些属性（如宽度（bw）、高度（bh）和类别（如人、汽车、红绿灯等）的对象的轮廓，由字母C表示。边界框的中心（bx）。YOLO使用单边界框回归来预测对象的高度、宽度、中心和类别。</p><ol><li><p>IOU</p><p>并集交集（IOU）是一种用于对象检测的工具，用于解释方框如何重叠。YOLO使用IOU完美地围绕对象的完美输出框。网格中的每个单元负责预测边界框及其置信度得分。如果预测的边界框与实际框相同，则IOU等于1。此技术可以消除与实际框不相等的边界框。</p></li></ol><p>YOLOv2:YOLOv2于2017年发布，其架构对YOLO进行了几次迭代改进，包括BatchNorm、更高分辨率和锚盒。</p><p>YOLOv3：于2018年发布，YOLOv3在以前的模型的基础上，为边界框预测添加客观性分数，为主干层添加连接性，并在三个不同的级别进行预测，以提高对较小对象的性能。</p><p>YOLOv4:YOLOv4由Alexey Bochkovskiy于2020年4月发布，其中引入了改进的功能聚合、“免费包”（带增强）、漏洞激活等改进。</p><p>YOLOv5:由Glenn Jocher于2020年6月发布，YOLOv5与之前的所有版本不同，因为它是PyTorch实现，而不是原始暗网的分支。与YOLO v4一样，YOLO v5具有CSP脊椎和PA-NET颈部。主要改进包括马赛克数据扩展和自动学习边界框锚定。</p><p>PP-YOLO：百度基于YOLO v3于2020年8月发布。PP-YOLO的主要目标是实现一种具有相对平衡的效率和有效性的对象检测器，该检测器可以直接用于当前的应用场景，而不是设计新的检测模型。</p><p>Scaled YOLOv4:发布于2020年11月，作者：王、博奇科夫斯基和廖。该模型使用跨阶段部分网络来增加网络大小，同时保持YOLOv4的准确性和速度。</p><p>PP-YOLOv2：再次由百度团队撰写并于2021年4月发布，它对PP-YOLO进行了小修改，以获得更好的性能，包括添加错误激活功能和路径聚合网络。</p><p>流程:</p><ol><li>预训练一个CNN用于图像分类任务</li><li>将输入图像分为SxS的块,如果一个物体的中心落入一个块cell中，该块“负责”检测该物体的存在.包括预测<strong>每个块预测碰撞盒的位置</strong>,<strong>置信度</strong>以及<strong>包含物体的概率</strong></li><li>位置就是(x,y,w,h),x,y是相对于cell的offset,w,h被归一化</li><li>置信度是<code>Pr(containing an object) x IoU(pred, truth)</code>; 其中<code>Pr</code> = 概率</li><li>如果一个cell包含物体,它会预测一个概率,表示这个物体属于每一类的概率Pr(the object belongs to the class C_i | containing an object),在该阶段模型仅预测每个cell的一组类概率,而与bbox无关</li><li>最终,一张图像包含SXSXB个bbox,每个bbox包含四个预测位置以及置信度和K个条件概率.所以预测的值shape是SXSX(5B+K)</li></ol><h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo.png" alt="img">Network Architecture</h3><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png" alt="img"></p><blockquote><p>作为一个单级对象检测器，YOLO速度极快，但由于候选边界框的数量有限，它不善于识别形状不规则的对象或一组小对象。</p></blockquote><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img data-src="https://s2.loli.net/2023/11/29/63TqhgSLFPbIKki.png" alt="image-20231129233153863"></p><p>损失由两部分组成，边界框偏移预测的定位损失和条件类概率的分类损失。这两部分都计算为误差平方和。</p><p><img data-src="https://s2.loli.net/2023/11/30/AGzmK4HVNhW95rq.png" alt="image-20231130101219411"></p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png" alt="img"></p><h3 id="YOLOV2改进"><a href="#YOLOV2改进" class="headerlink" title="YOLOV2改进"></a>YOLOV2改进</h3><p>应用了多种修改以使YOLO预测更准确、更快，包括：</p><p>1.BatchNorm有助于：在所有卷积层上添加批次范数，从而显著提高收敛性。</p><p>2.图像分辨率很重要：用高分辨率图像微调基本模型可以提高检测性能。</p><p>3.卷积锚盒检测：YOLOv2不是在整个特征图上预测具有完全连接层的边界盒位置，而是使用卷积层来预测锚盒的位置，就像在更快的R-CNN中一样。空间位置的预测和类概率是解耦的。总体而言，这一变化导致mAP略有下降，但召回率有所上升。</p><p>4.框维度的K-均值聚类：与使用手工挑选的锚框大小的更快的R-CNN不同，YOLOv2对训练数据进行K-均值集群，以在锚框维度上找到良好的先验。距离度量是根据IoU分数设计的：</p><p><img data-src="https://s2.loli.net/2023/11/30/nKCZulO46oeVdJb.png" alt="image-20231130103908962"></p><p>通过聚类生成的锚框在固定数量的框的条件下提供更好的平均IoU。</p><p>5.直接位置预测：YOLOv2以一种不会与中心位置偏离太多的方式来制定边界框预测。如果盒子位置预测可以将盒子放置在图像的任何部分，就像在区域提案网络中一样，那么模型训练可能会变得不稳定。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png" alt="img" style="zoom:50%;" /></p><p>6.添加细粒度特性：YOLOv2添加了一个直通层，将细粒度特性从早期层带到最后一个输出层。该穿透层的机制类似于ResNet中的身份映射，以从以前的层中提取更高维度的特征。这将使性能提高1%。</p><p>7.多尺度训练：为了训练模型对不同大小的输入图像具有鲁棒性，每10个批次随机采样一个新大小的输入维度。由于YOLOv2的conv层将输入维度下采样因子为32，因此新采样的大小是32的倍数。</p><p>8.轻量级基础模型：为了更快地进行预测，YOLOv2采用了轻量级基础模型DarkNet-19，该模型有19个conv层和5个最大池化层。关键是在3x3 conv层之间插入平均池和1x1 conv滤波器。</p><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p>Single Shot Detector（SSD；Liu等人，2016）是<strong>首次尝试使用卷积神经网络的金字塔特征层次来有效检测各种大小的对象之一</strong>。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png" alt="img"></p><p>该模型以图像作为输入，<strong>该图像通过具有不同大小滤波器（10x10、5x5和3x3）的多个卷积层。使用来自网络不同位置的卷积层的特征图来预测边界框</strong>。它们由具有3x3滤波器的特定卷积层处理，称为额外特征层，以产生一组类似于快速R-CNN的锚框的边界框。</p><p>与需要对象建议的方法相比，SSD 非常简单，因为它<strong>完全省去了建议生成和随后的像素或特征重采样阶段</strong>，并将所有计算封装在一个网络中。</p><p><img data-src="https://miro.medium.com/v2/resize:fit:770/1*f0p4it3vSVV_qeTJq5Jv1Q.png" alt="img"></p><p>此模型<strong>主要由基础网络组成，其后是几个多尺度特征块</strong>。 <strong>基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络</strong>。</p><p> 单发多框检测论文中选用了在分类层之前截断的VGG (<a href="http://zh.d2l.ai/chapter_references/zreferences.html#id98">Liu <em>et al.</em>, 2016</a>)，现在也常用ResNet替代。 我们可以设计基础网络，使它输出的高和宽较大。 这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。</p><p><strong>接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小</strong>（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。</p><p>通过深度神经网络分层表示图像的多尺度目标检测的设计。 由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。 简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。</p><h4 id="default-box的生成"><a href="#default-box的生成" class="headerlink" title="default box的生成"></a>default box的生成</h4><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-box-scales.png" alt="img"></p><script type="math/tex; mode=display">\begin{aligned}\text{level index: } &\ell = 1, \dots, L \\\text{scale of boxes: } &s_\ell = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{L - 1} (\ell - 1) \\\text{aspect ratio: } &r \in \{1, 2, 3, 1/2, 1/3\}\\\text{additional scale: } & s'_\ell = \sqrt{s_\ell s_{\ell + 1}} \text{ when } r = 1 \text{thus, 6 boxes in total.}\\\text{width: } &w_\ell^r = s_\ell \sqrt{r} \\\text{height: } &h_\ell^r = s_\ell / \sqrt{r} \\\text{center location: } & (x^i_\ell, y^j_\ell) = (\frac{i+0.5}{m}, \frac{j+0.5}{n})\end{aligned}</script><script type="math/tex; mode=display">\mathcal{L}_\text{cls} = -\sum_{i \in \text{pos}} \mathbb{1}_{ij}^k \log(\hat{c}_i^k) - \sum_{i \in \text{neg}} \log(\hat{c}_i^0)\text{, where }\hat{c}_i^k = \text{softmax}(c_i^k)</script><p>其中1表示对于k类bbox与gt-box是否match</p><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_\text{loc} &= \sum_{i,j} \sum_{m\in\{x, y, w, h\}} \mathbb{1}_{ij}^\text{match} L_1^\text{smooth}(d_m^i - t_m^j)^2\\L_1^\text{smooth}(x) &= \begin{cases}    0.5 x^2             & \text{if } \vert x \vert < 1\\    \vert x \vert - 0.5 & \text{otherwise}\end{cases} \\t^j_x &= (g^j_x - p^i_x) / p^i_w \\t^j_y &= (g^j_y - p^i_y) / p^i_h \\t^j_w &= \log(g^j_w / p^i_w) \\t^j_h &= \log(g^j_h / p^i_h)\end{aligned}</script><p>此外SSD使用了NMS和HHM优化训练过程.</p><blockquote><p>NMS:非最大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。跳过具有高IoU（即大于0.5）的剩余框，使用之前选择的框</strong>。</p><p>HNM:有些负类很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。</p></blockquote><h3 id="连结多尺度的预测"><a href="#连结多尺度的预测" class="headerlink" title="连结多尺度的预测"></a>连结多尺度的预测</h3><p><strong>单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量</strong>。</p><p>在不同的尺度下,特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。 因此，不同尺度下预测输出的形状可能会有所不同。</p><p>除了批量大小这一维度外，其他三个维度都具有不同的尺寸。 为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cls_predictor</span>(<span class="params">num_inputs, num_anchors, num_classes</span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class="number">1</span>),</span><br><span class="line">                     kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x, block</span>):</span></span><br><span class="line">    <span class="keyword">return</span> block(x)</span><br><span class="line"></span><br><span class="line">Y1 = forward(torch.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>)), cls_predictor(<span class="number">8</span>, <span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">Y2 = forward(torch.zeros((<span class="number">2</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>)), cls_predictor(<span class="number">16</span>, <span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_pred</span>(<span class="params">pred</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.flatten(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_preds</span>(<span class="params">preds</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat([flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br><span class="line">concat_preds([Y1, Y2]).shape</span><br></pre></td></tr></table></figure><h4 id="高和宽减半块"><a href="#高和宽减半块" class="headerlink" title="高和宽减半块"></a>高和宽减半块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">down_sample_blk</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        blk.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                             kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    blk.append(nn.MaxPool2d(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure><h4 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h4><p>Feature Pyramid Networks for Object Detection</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/featurized-image-pyramid.png" alt="img"></p><p>在看多尺度特征的时候注意到了这篇文章.提出了一个利用深度卷积神经网络固有的多尺度金字塔结构来以极小的计算量构建特征金字塔的网络结构</p><p><img data-src="https://s2.loli.net/2023/11/29/SaJdTtXjVy5qx7f.png" alt="image-20231129231413439"></p><p><img data-src="https://upload-images.jianshu.io/upload_images/18299912-aa79ebef839e6772.png?imageMogr2/auto-orient/strip|imageView2/2/w/611/format/webp" alt="img"></p><ul><li>自下而上的路径是正常的前馈计算。</li><li>自上而下的路径朝着相反的方向发展，通过横向连接将粗糙但语义更强的特征图添加回更大尺寸的先前金字塔级别。</li></ul><p>首先，更高级别的特征在空间上更粗糙地上采样，使其大2倍。对于图像放大，本文使用了最近邻上采样。虽然有许多图像放大算法，例如使用deconv，但<strong>采用另一种图像缩放方法可能会也可能不会提高RetinaNet的性能</strong>。</p><p>较大的特征图<strong>经过1x1 conv层以减小通道尺寸</strong>。</p><p>最后，通过<strong>元素相加</strong>将这两个特征图合并。</p><p>根据消融研究，特征化图像金字塔设计的组件的重要性等级如下：1x1横向连接&gt;跨多层检测对象&gt;自上而下的富集&gt;金字塔表示（与仅使用最底层相比）。</p><p>与SSD中一样，<strong>通过对每个合并的特征图进行预测，可以在所有金字塔级别中进行检测</strong>。因为预测共享相同的分类器和框回归器，所以它们都形成为具有相同的通道维度d=256。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/retina-net.png" alt="img"></p><h4 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h4><p>[<a href="https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf">overfeat</a>]</p><p>Overfeat是将目标检测、定位和分类任务集成到一个卷积神经网络中的先驱模型。主要思想是（i）<strong>以滑动窗口的方式在图像的多个尺度的区域上的不同位置进行图像分类</strong>，以及（ii）使用<strong>在相同卷积层上训练的回归器来预测边界框位置</strong>。</p><p>（1）用一个共享的CNN（ConvNet）来同时处理图像分类，定位，检测三个任务，可以提升三个任务的表现。</p><p>（2）用CNN有效地实现了一个多尺度的，滑动窗口的方法，来处理任务。</p><p>（3）提出了一种方法，通过累积预测来求bounding boxes（而不是传统的非极大值抑制）</p><p><a href="https://blog.csdn.net/Gentleman_Qin/article/details/84836122">OverFeat——全卷积首次用于检测问题 (目标检测)(深度学习)(ICLR 2014）_overfeat是做什么的-CSDN博客</a></p><p><img data-src="https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/overfeat-training.png" alt="img"></p><h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><h3 id="Focal-Loss-for-Dense-Object-Detection"><a href="#Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="Focal Loss for Dense Object Detection"></a>Focal Loss for Dense Object Detection</h3><p>在损失函数上进行改进.对象检测模型训练的一<strong>个问题是不包含对象的背景和包含感兴趣对象的前景之间的极端不平衡。焦点损失被设计为在硬的、容易被错误分类的例子（即具有噪声纹理或部分对象的背景）上分配更多的权重，并对容易被加权的例子（例如明显为空的背景）进行加权。</strong></p><h3 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h3><p>迄今为止，准确率最高的物体检测器都是基于 R-CNN 推广的两阶段方法，即对稀疏的候选物体位置集进行分类。<strong>相比之下，应用于对可能的物体位置进行规则、密集采样的单阶段检测器有可能更快、更简单，但迄今为止，其准确性仍落后于两阶段检测器。在本文中，我们将探讨出现这种情况的原因。</strong></p><p>我们发现，dense detectors训练过程中遇到的前景-背景类别极度不平衡是主要原因。我们建议通过重塑标准交叉熵损失来解决这种类别不平衡问题，从而降低分类良好示例的损失权重。</p><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>焦点损失（Focal Loss）的设计目<strong>的是解决在训练过程中前景类和背景类之间极度不平衡（例如 1:1000）的单阶段物体检测问题</strong>。我们从用于二元分类的交叉熵（CE）损失开始引入焦点损失</p><p><img data-src="https://s2.loli.net/2023/11/29/nHdKf2JCcxV6sry.png" alt="image-20231129231823763"></p><p>在上述公式中，y∈{±1} 表示地面实况类别，p∈[0, 1]是模型对标签 y = 1 的类别的估计概率。</p><p>我们定义 p~t~</p><p><img data-src="https://s2.loli.net/2023/11/29/b1BWorKMJTxf2R4.png" alt="image-20231129231934979"></p><p>重写 CE(p, y) = CE(p~t~) = - log(pt)。</p><p>我们建议在交叉熵损失中加入一个调制因子 (1 - p~t~)γ ，可调聚焦参数 γ ≥ 0。</p><p><img data-src="https://s2.loli.net/2023/11/29/3DqjYcTbewiCrE6.png" alt="image-20231129231711234"></p><p>我们注意到焦点损失的两个特性。(1) 当一个例子被错误分类且 p~t~ 较小时，调制因子接近 1，损失不受影响。</p><p><img data-src="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/focal-loss.png" alt="img" style="zoom:67%;" /></p><h4 id="BackBone"><a href="#BackBone" class="headerlink" title="BackBone"></a>BackBone</h4><p>我们采用特征金字塔网络（FPN）作为 RetinaNet 的骨干网络。</p><p>简而言之，FPN 利用自上而下的路径和横向连接增强了标准卷积网络，因此该网络能从单一分辨率的输入图像中有效构建丰富的多尺度特征金字塔。金字塔的每一层都可用于检测不同尺度的物体。<strong>FPN 可以改进全卷积网络 (FCN) [23] 的多尺度预测，这体现在它对 RPN [28] 和 DeepMask 式提案 [24] 以及快速 R-CNN [10] 或 Mask R-CNN [14] 等两阶段检测器的增益上</strong>。继 [20] 之后，我们在 ResNet 架构 [16] 的基础上构建了 FPN。我们构建了一个 P3 到 P7 级的金字塔，其中 l 表示金字塔级别（Pl 的分辨率比输入低 2l）。与文献 [20] 一样，所有金字塔层级都有 C = 256 个通道。虽然许多设计选择并不重要，但我们<strong>强调使用 FPN 主干网才是关键；使用仅来自最后 ResNet 层的特征进行的初步实验得出的 AP 值较低。</strong></p><h4 id="anchors"><a href="#anchors" class="headerlink" title="anchors"></a>anchors</h4><p>我们使用了与中 RPN 变体类似的平移不变锚点框。锚点在金字塔 P3 到 P7 层的面积分别为 32^2^ 到 512^2^。与文献[20]一样，我们在每个金字塔层使用了三种纵横比的锚点{1:2, 1:1, 2:1}。为了获得比[20]更密集的比例覆盖，我们在每个层级添加了尺寸为{2^0^, 2^1/3^, 2^2/3^}的锚点，这些锚点是原始的 3 种宽高比锚点的集合。这改进了我们的 AP 设置。每个级别总共有 A = 9 个锚点，相对于网络的输入图像，这些锚点覆盖了 32-813 个像素的范围。</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO.&lt;br&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>3D Object Detection Learning</title>
    <link href="https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/"/>
    <id>https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/</id>
    <published>2023-10-30T08:19:04.000Z</published>
    <updated>2024-01-01T14:14:03.337Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知.</p><span id="more"></span><p>先看几篇论文.</p><p>anchor-free detection,脱离了SSD,RetinaNet以及YOLO的anchor-based的工作。</p><h2 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet"></a>PointNet</h2><h3 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h3><p>点云是一种重要的几何数据结构。<strong>由于其不规则的格式，大多数研究人员将此类数据转换为规则的 3D 体素网格或图像集合</strong>。但是<strong>，这会使数据变得不必要地宽松(voluminous)并导致问题</strong>。在本文中，我们设计了一种新型的神经网络，它<strong>直接使用点云，它很好地尊重了输入中点的排列不变性</strong>。</p><h3 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h3><p>典型的卷积架构需要高度规则的输入数据格式，如图像网格或 3D 体素格式，以便执行权重共享和其他内核优化。由于点云或网格不是常规格式，因此大多数研究人员通常会将此类数据转换为常规的 3D 体素网格或图像集合（例如视图），然后再将它们馈送到深度网络架构。</p><p>然而，这种<strong>数据表示转换使生成的数据变得不必要地大量，同时还引入了量化伪影，这些伪影可能会掩盖数据的自然不变性</strong>(renders the resulting data unnecessarily voluminous — while also introducing quantization artifacts that can obscure natural invariances of the data.)。</p><p>点云是简单而统一的结构，避免了网格的组合不规则性和复杂性，因此更容易学习。</p><p> PointNet 是一个统一的架构，<strong>它直接将点云作为输入,并输出整个输入的类标签或输入的每个点段/部分标签</strong>。在基本设置中<strong>，每个点仅由其三个坐标（x、y、z）表示。可以通过计算法线和其他局部或全局特征来添加其他维度。</strong></p><p>我们方法的<strong>关键是使用单个对称函数，即最大池化</strong>。实际上，网络学习了一组优化函数/标准，这些函数/标准选择点云中信息丰富的点，并对其选择的原因进行编码。</p><p>网络的<strong>最终全连接层将这些学习到的最优值聚合到整个形状的全局描述符中</strong>，</p><p>输入格式很容易应用刚性或仿射变换，因为每个点都是独立变换的。因此，我们可以添加一个依赖于数据的<strong>空间转换器网络</strong>，<strong>该网络在PointNet处理数据之前尝试对数据进行规范化</strong>，从而进一步改善结果。</p><p>我们的网络学<strong>习通过一组稀疏的关键点来总结输入点云，这大致对应于根据可视化对象的骨架</strong>。</p><p>点云的大多数现有功能都是针对特定任务手工制作的。<strong>点特征通常对点的某些统计属性进行编码，并被设计为对某些变换不变，这些变换通常被归类为内在或外在</strong> 。它们还可以<strong>分为局部要素和全局要素</strong>。对于特定任务，找到最佳特征组合并非易事。</p><p>3D 数据具有多种流行的表示形式，从而产生了各种学习方法。体积 CNN是将 3D 卷积神经网络应用于体素化形状的先驱。然而，由于数据稀疏性和三维卷积的计算成本，体积表示受到其分辨率的限制。</p><p>我们设计了一个深度学习框架，直接使用无序点集作为输入。点云表示为一组 3D 点 {Pi| i = 1， …， n}，其中<strong>每个点 Pi 是其 （x， y， z） 坐标加上额外的特征通道（如颜色、法线等）的向量。为了简单明了起见，除非另有说明，否则我们仅使用 （x， y， z） 坐标作为点的通道</strong></p><p>对于对象分类任务，<strong>输入点云要么直接从形状中采样，要么从场景点云中预先分割</strong>。</p><p>对于语义分割，<strong>输入可以是用于部分区域分割的单个对象</strong>，也可以是用<strong>于对象区域分割的 3D 场景中的子体积</strong></p><p><img data-src="https://i.imgur.com/f2UTqxK.png" alt="image-20231225102852445"></p><p>我们的网络有三个关键模块：<strong>最大池化层作为聚合所有点信息的对称函数</strong>,<strong>局部和全局信息组合结构</strong>,以及<strong>两个对齐输入点和点特征的联合对齐网络</strong>。</p><p><strong>Symmetry Function for Unordered Input</strong></p><p>为了使模型对输入排列不变，存在三种策略：1）将输入排序为规范顺序;</p><p>2）将输入视为训练RNN的序列，但通过各种排列来增强训练数据;</p><p>3）使用简单的对称函数来聚合每个点的信息。</p><p>对称函数将 n 个向量作为输入，并输出一个与输入顺序不变的新向量。</p><p>虽然排序听起来像是一个简单的解决方案，但在高维空间中，实际上并不存在一般意义</p><p>上的稳定的点扰动排序。</p><p>我们的想法是通过<strong>对集合中的变换元素应用对称函数来近似在点集上定义的一般函数</strong></p><script type="math/tex; mode=display">\begin{equation}f(\{x_1,\ldots,x_n\})\approx g(h(x_1)\ldots,h(x_n)),\end{equation}</script><p>从经验上讲，我们的基本模块非常简单：<strong>通过多层感知器网络来近似 h</strong>,<strong>通过单个变量函数和最大池化函数的组合来近似 g</strong>.实验发现这效果很好。通过 h 的集合,我们可以学习多个 f 来捕获集合的不同性质.</p><p><strong>Local and Global Information Aggregation</strong></p><p>上一节的输出形成一个向量 [f1， . . . ， fK ]，它是输入集的全局签名。我们可以轻松地在形状全局特征上训练 SVM 或多层感知器分类器进行分类。但是，点<strong>分割需要结合本地和全局知识。</strong></p><p>在计算出全局点云特征向量后，我们通过将<strong>全局特征与每个点特征连接起来，将其反馈给每个点的特征。然后，我们根据组合的点特征提取新的每点特征</strong> - 这一次，每点特征同时识别局部和全局信息。</p><p><strong>Joint Alignment Network</strong></p><p>如果点云经历某些几何变换（例如刚性变换），则点云的语义标记必须是不变的。因此，我们期望点集的学习表示对于这些变换是不变的。<strong>一个自然的解决方案是在特征提取之前将所有输入集对齐到规范空间</strong>。Jaderberg等介绍了空间变换器的概念，通过采样和插值来对齐2D图像，这是通过在GPU上实现的专门定制层实现的。</p><blockquote><p>点云数据所代表的<strong>目标</strong>对某些空间转换应该具有不变性，如旋转和平移等刚体变换</p></blockquote><p>与相比,我们的点云输入形式使我们能够以更简单的方式实现这一目标。我们不需要发明任何新图层，也没有像图像案例那样引入别名。</p><p>通过一个微型网络<strong>预测一个仿射变换矩阵，并将该变换直接应用于输入点的坐标</strong>。小网络本身类似于大网络，由点无关特征提取、最大池化和全连接层等基础模块组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">STN3d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channel</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(STN3d, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(channel, <span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">128</span>, <span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">256</span>, <span class="number">9</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn4 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn5 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        batchsize = x.size()[<span class="number">0</span>] <span class="comment"># shape (batch_size,3,point_nums)</span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x))) <span class="comment"># shape (batch_size,64,point_nums)</span></span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x))) <span class="comment"># shape (batch_size,128,point_nums)</span></span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x))) <span class="comment"># shape (batch_size,1024,point_nums)</span></span><br><span class="line">        x = torch.<span class="built_in">max</span>(x, <span class="number">2</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>] <span class="comment"># shape (batch_size,1024,1)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">1024</span>) <span class="comment"># shape (batch_size,1024)</span></span><br><span class="line"></span><br><span class="line">        x = F.relu(self.bn4(self.fc1(x))) <span class="comment"># shape (batch_size,512)</span></span><br><span class="line">        x = F.relu(self.bn5(self.fc2(x))) <span class="comment"># shape (batch_size,256)</span></span><br><span class="line">        x = self.fc3(x) <span class="comment"># shape (batch_size,9)</span></span><br><span class="line"></span><br><span class="line">        iden = Variable(torch.from_numpy(np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]).astype(np.float32))).view(<span class="number">1</span>, <span class="number">9</span>).repeat(</span><br><span class="line">            batchsize, <span class="number">1</span>) <span class="comment"># # shape (batch_size,9)</span></span><br><span class="line">        <span class="keyword">if</span> x.is_cuda:</span><br><span class="line">            iden = iden.cuda()</span><br><span class="line">        <span class="comment"># that&#x27;s the same thing as adding a diagonal matrix(full 1)</span></span><br><span class="line">        x = x + iden <span class="comment"># iden means that add the input-self</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>) <span class="comment"># shape (batch_size,3,3)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space"><a href="#PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space" class="headerlink" title="PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"></a>PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</h2><p>之前很少有研究点集上的深度学习。PointNet 是这个方向的先驱。然而，根据设计，<strong>PointNet 不会捕获由度量空间点所在的局部结构引起的局部结构，这限制了其识别细粒度模式的能力和对复杂场景的泛化性。</strong></p><p>在这项工作中，我们引入了一个分层神经网络，该神经网络将PointNet递归应用于输入点集的嵌套分区。通过利用度量空间距离，我们的网络能够学习具有越来越大的上下文尺度的局部特征。随着进一步观察<strong>点集通常以不同的密度进行采样，这导致在均匀密度下训练的网络的性能大大降低</strong>，<strong>我们提出了新的集合学习层来自适应地组合来自多个尺度的特征</strong>。</p><p><img data-src="https://i.imgur.com/Q6l2kcO.png" alt="image-20231225105720739"></p><p>虽然 PointNet 使用单个最大池化操作来聚合整个点集，但我们的新架构<strong>构建了点的分层分组，并沿着层次结构逐步抽象出越来越大的局部区域</strong>。</p><p>在每个级别上，都会对一组点进行处理和抽象，以生成具有较少元素的新集合。集合抽象层由三个关键层组成：<strong>采样层、分组层和 PointNet 层</strong>。</p><p><strong>采样层从输入点中选择一组点，用于定义局部区域的质心</strong>。然后<strong>，分组层通过查找质心周围的“相邻”点来构建局部区域集</strong>。PointNet 层使用微型 PointNet 将局部区域模式编码为特征向量。</p><p><strong>Sampling layer.</strong>给定输入点 {x1， x2， …， xn}，<strong>使用迭代最远点采样</strong> （FPS） 来选择点 {xi1 ， xi2 ， …， xim } 的子集，<strong>使得 xij 是相对于其余点与集合 {xi1 ， xi2 ， …， xij−1 } 最远的点</strong>）。<strong>与随机采样相比，在质心数量相同的情况下，它对整个点集的覆盖率更高</strong>。<strong>与扫描数据分布的向量空间无关的 CNN 相比，我们的采样策略以数据依赖的方式生成感受野。</strong></p><p><strong>Grouping layer.</strong>该层的输入是大小为 N × （d + C） 的点集和大小为 N ′ × d 的一组质心的坐标。<strong>输出是大小为 N ′ × K × （d + C） 的点集组</strong>，其中<strong>每组对应一个局部区域，K 是质心点邻域中的点数</strong>。请注意，K 因组而异，<strong>后续的 PointNet 层能够将灵活数量的点转换为固定长度的局部区域特征向量。</strong></p><p><strong>PointNet layer：</strong>在该层中,输入是数据大小为 N ′×K ×（d+C） 的点的 N ′ 局部区域。输出中的<strong>每个局部区域都由其质心和编码质心邻域的局部特征抽象</strong>。输出数据大小为 N ′ × （d + C′）。</p><h3 id="Robust-Feature-Learning-under-Non-Uniform-Sampling-Density"><a href="#Robust-Feature-Learning-under-Non-Uniform-Sampling-Density" class="headerlink" title="Robust Feature Learning under Non-Uniform Sampling Density"></a>Robust Feature Learning under Non-Uniform Sampling Density</h3><p><strong>点集在不同区域具有不均匀的密度是很常见的。这种不均匀性给点集特征学习带来了重大挑战。在密集数据中学习的特征可能无法泛化到稀疏采样区域</strong>。因此，针对稀疏点云训练的模型可能无法识别细粒度的局部结构。</p><p>理想情况下，我们希望尽可能仔细地检查到一个点集，<strong>以捕获密集采样区域中最精细的细节</strong>。但是，在低密度区域禁止进行这种仔细检查，因为局部模式可能会因采样缺陷而损坏。在这种情况下，我们应该在更近的地方寻找更大尺度的模式。为了实现这一目标，我们提出了密度自适应PointNet层，<strong>当输入采样密度发生变化时，该层可以学习组合来自不同尺度区域的特征</strong>。</p><p>提出了MSG和MRG.</p><blockquote><p>对方法MSG而言，是对<strong>不同半径的子区域</strong>进行特征提取后进行<strong>特征堆叠</strong>，特征提取过程还是采用了PointNet</p><p>作者是考虑到上述的MSG方法<strong>计算量太大</strong>，提出来备选方案MRG。MRG用两个Pointnet对连续的两层分别做<strong>特征提取与聚合</strong>，然后再进行特征拼接</p></blockquote><p><img data-src="https://i.imgur.com/fFcXqBb.png" alt="image-20231225115753620"></p><h2 id="Objects-as-Points-2019"><a href="#Objects-as-Points-2019" class="headerlink" title="Objects as Points  2019"></a>Objects as Points  2019</h2><h3 id="Abs"><a href="#Abs" class="headerlink" title="Abs"></a>Abs</h3><p>检测是将图像中的物体识别为轴对齐的方框。大多数成功的物体检测器都会枚举几乎所有潜在的物体位置，并对每个位置进行分类。这不仅浪费资源、效率低下，还需要额外的后期处理。在本文中，我们采用了一种不同的方法。我们将物体建模为一个点—其边界框的中心点。</p><p>我们的检测器<strong>使用关键点估算来寻找中心点</strong>，并<strong>对所有其他物体属性进行回归，如大小、三维位置、方向甚至姿态</strong>。与相应的基于边界框的检测器相比，我们基于中心点的方法 CenterNet 是端到端可微分的，更简单、更快速、更准确。</p><h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><p><img data-src="https://i.imgur.com/JWpC9qY.png" alt="image-20231222110245995" style="zoom: 67%;" /></p><p>使用中心点作为预测结果,输出是一个热力图.假设 I∈ R^W×H×3^ 是宽度为 W、高度为 H 的输入图像，我们的目标是生成一个关键点热图 ˆ Y∈ [0, 1] ^W/R×H/R×C^,R 是输出跨度，C 是关键点类型的数量(就是类别).使用R = 4 的默认输出跨度。输出步长对输出预测进行下采样.预测值 ˆ Y = 1 对应于检测到的关键点，而 ˆ Y= 0 则是背景。</p><blockquote><p>论文中使用几种不同的全卷积编码器-解码器网络来预测图像 I 中的ˆY：堆叠沙漏网络、上卷积残差网络（ResNet）和深层聚合（DLA）。</p></blockquote><p>对于gtbox中的每个中心(也就是keypoint)会计算出一个低分辨率等效点<img data-src="https://s2.loli.net/2023/12/01/OZL5Fz6Vi7mEcvK.png" alt="image-20231201151345184"></p><p>因为预测的输出坐标是经过四倍下采样的,然后利用这个真值通过一个高斯核函数拼接到热图上.我们知道预测的输出是在0-1之间的,而且大小是W/R×H/R×C,利用这个核函数计算每个下采样后的关键点在热力图上的值.其中，σ 是与物体大小相适应的标准偏差，如果同一类别的两个高斯重叠，我们取元素最大值</p><p><img data-src="https://s2.loli.net/2023/12/01/t5BC7l1IqMNKDbf.png" alt="image-20231201151511596"></p><p>损失使用RetinaNet提出的Focal损失变型.主要是得到预测的中心位置,ground truth没有直接使用,而是使用一个高斯核将不是中心的点的值设置为(0-1),相当于更好地优化了.从简单的0-1到离散值.</p><p><img data-src="https://s2.loli.net/2023/12/01/q59OprywgN2Ehsk.png" alt="image-20231201151714656"></p><p>此外,因为需要恢复输出跨距造成的离散化误差,还添加了损失.</p><p>为每个中心点预测一个局部偏移量 ˆ O∈ R^W/R×H/RX2^。所有类别 c 共享相同的偏移预测,由于输入是一张图像,通过backbone(论文中的是ResNet和DLA)得到downsampling之后的feature map(原文叫heat map)</p><p><img data-src="https://s2.loli.net/2023/12/01/GTviaIdYJ9gFA6Z.png" alt="image-20231201152014024"></p><p>由此得到了物体的中心点,接下来需要回归得到尺寸.</p><blockquote><p>我们使用关键点估计器 ˆ Y 来预测所有中心点。此外，我们对每个对象 k 的对象尺寸 sk = (x(k) 2 - x(k) 1 , y(k) 2 - y(k) 1 ) 进行回归。</p></blockquote><script type="math/tex; mode=display">\begin{equation}L_{size}=\frac1N\sum_{k=1}^N\left|\hat{S}_{p_k}-s_k\right|.\end{equation}</script><script type="math/tex; mode=display">L_{det}=L_k+\lambda_{size}L_{size}+\lambda_{off}L_{off}.</script><p>对于3D目标检测,还需要得到深度、三维空间和方向。会为每个输出添加一个单独的头部。</p><p>深度:深度 d 是每个中心点的<strong>单一标量</strong>。然而，深度很难直接回归。我们使用 Eigen 等人的输出变换和 d = 1/σ( ˆ d) - 1，其中 σ 是 sigmoid 函数。我们将深度作为关键点估计器的附加输出通道 ˆ D∈[0, 1] W R ×H R 来计算。</p><p>物体的三维尺寸是三个标量。使用单独的头 </p><script type="math/tex; mode=display">\begin{equation}\hat{\Gamma}\in\mathcal{R}^{\frac WR\times\frac HR\times3}\end{equation}</script><p> 和 L1 损失直接回归到它们的绝对值（以米为单位）。</p><p>默认情况下，方向是一个单一标量。但是，很难对其进行回归。效仿 Mousavian 等人的研究，将方向表示为两个bins，并进行bins内回归。具体来说，bin使用 8 个标量编码，每个bin有 4 个标量。对于一个bins，两个标量用于softmax，其余两个标量在每个分区内回归到一个angle。</p><h2 id="Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation"><a href="#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation" class="headerlink" title="Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation"></a>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</h2><p><a href="http://arxiv.org/abs/2111.09515">http://arxiv.org/abs/2111.09515</a></p><h3 id="Abs-1"><a href="#Abs-1" class="headerlink" title="Abs"></a>Abs</h3><p>近年来，用于自动驾驶的激光雷达数据三维物体检测技术取得了长足进步,在最先进的方法中，将<strong>点云编码成鸟瞰图</strong>（BEV,bird’s eye view）已被证明是既有效又高效的方法。<strong>与透视图(perspective views)不同，鸟瞰图保留了物体之间丰富的空间和距离信息</strong>。然而,在 BEV 中,虽然<strong>同类型的远距离物体看起来并不更小</strong>,但它们<strong>包含的点云特征却更稀疏</strong>。这一事实<strong>削弱了使用共享权重卷积神经网络（CNN）提取 BEV 特征的能力</strong>.</p><p>为了应对这一挑战,我们提出了范围感知注意力网络 (RAANet),它能提取有效的 BEV 特征并生成出色的 3D  object detection 输出.</p><p>范围感知注意力（RAA）卷积显著改善了<strong>对远近物体的特征提取</strong>。</p><p>此外，我们还提出了一<strong>种用于点密度估计</strong>(point density estimation)的新型辅助损失，以进一步<strong>提高 RAANet 对遮挡物体的检测精</strong>度。值得注意的是，我们提出的 RAA 卷积是轻量级的,可以集成到任何用于检测 BEV 的 CNN 架构中.</p><p>在 <strong>nuScenes 和 KITTI 数据集上</strong>进行的大量实验表明，在基于激光雷达(LiDAR-based 3D object detection)的三维物体检测方面，我们提出的方法优于最先进的方法，在 nuScenes 激光雷达帧上进行的测试中，完整版的实时推理速度为 16 Hz，精简版为 22 Hz。</p><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h3><p>随着处理单元的快速改进，得益于深度神经网络的成功，自动驾驶的感知能力近年来得到了蓬勃发展。通过<strong>激光雷达传感器进行 3D 物体检测</strong>是自动驾驶的重要功能之一。</p><p>早期的研究采用了三维卷积神经网络（CNN），这种网络处理速度慢，内存需求大。</p><p>为了降低内存要求并提供实时处理，最近的方法利用了体素化(voxelization)和鸟瞰投影（BEV）。</p><p>体素化(Voxelization)作为三维点云(3D point clouds)的一种预处理方法得到了广泛应用，因为<strong>结构更合理的数据可提高计算效率和性能精度</strong>。</p><p>一般来说，体素化将点云划分为均匀分布的体素网格，然后将三维激光雷达点分配到各自的体素上。输出空间保留了物体之间的欧氏距离，并避免了边界框的重叠。</p><p>这些特点使得<strong>无论物体与激光雷达的距离如何，都能将物体的尺寸变化控制在一个相对较小的范围内</strong>，从而有<strong>利于在训练过程中进行形状回归</strong>。</p><p>在本文中，我们提出了距离感知注意力网络（RAANet），其中包含新型的范围感知注意力卷积层（RAAConv），设计<strong>用于LiDAR BEV的目标检测</strong>。RAAConv 由两个独立的卷积分支和注意力图组成,对输入特征图的位置信息敏感.</p><p>我们的方法受到BEV图像特性的启发，<strong>随着物体和自我车辆之间距离的增加，点变得越来越稀疏</strong>。<strong>理想情况下，对于BEV特征图，不同位置的元素应由不同的卷积核处理</strong>。但是，应用不同的内核会显着增加计算费用。</p><p>为了在BEV特征提取过程中<strong>利用位置信息，在避免繁重计算的同时，将BEV特征图视为稀疏特征和密集特征的组合</strong>。我们应用两个不同的卷积核来同时提取稀疏和密集特征。</p><p>每个提取的特征图的通道大小都是最终输出的一半。同时，根据输入形状生成范围和位置编码。然后，根<strong>据相应的特征图以及范围和位置编码计算每个范围感知注意力热图</strong>。最后，将<strong>注意力热图应用于特征图以增强特征表示</strong>。从两个分支生成的特征图按通道concat为 RAAConv 输出。</p><p>此外,遮挡的影响也不容忽视,因为同一物体在不同的遮挡量下可能具有不同的点分布。因此，我们提出了一个高效的辅助分支，称为<strong>辅助密度水平估计模块</strong>（ADLE），允许RAANet考虑遮挡。由于注释各种遮挡是一项耗时且昂贵的任务，因此我们<strong>设计了ADLE来估计每个对象的点密度水平。如果没有遮挡，则近处物体的点密度水平高于远处物体的点密度水平</strong>。</p><p>但是，<strong>如果附近的物体被遮挡，则其点密度水平会降低。因此，通过结合距离信息和密度水平信息，我们能够估计遮挡信息的存在</strong>。ADLE仅用于训练阶段，用于提供密度信息指导，在推理状态下可以删除，以提高计算效率。</p><p>主要贡献:</p><ol><li>我们提出了RAAConv层，它允许基于LiDAR的探测器提取更具代表性的BEV特征。此外，RAAConv 层可以集成到任何用于 LiDAR BEV 的 CNN 架构中。</li><li>我们提出了一种新的用于点密度估计的辅助损失，以帮助主网络学习与遮挡相关的特征。该密度水平估计器进一步提高了RAANet对被遮挡物体的检测精度。</li><li>我们提出了范围感知注意力网络（RAANet），它集成了前面提到的RAA和ADLE模块。RAANet通过基于ground truth生成各向异性(anistropic)高斯热图，进一步优化，</li></ol><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>大多数目标检测工作可以分为两大类：有锚点和无锚点的目标检测。此外，在早期阶段存在对点云数据进行编码的工作]，但它们超出了目标检测网络重构的范围。</p><h4 id="Object-detection-with-anchors"><a href="#Object-detection-with-anchors" class="headerlink" title="Object detection with anchors"></a>Object detection with anchors</h4><p>固定形状的锚回归方法，以便可以提取中间特征</p><p>two-stage:RCNN家族</p><p>one-stage:YOLO,Retinanet,SSD</p><p>YOLO:将目标检测重新定义为单一回归问题，该问题采用端到端神经网络进行单次前向传播来检测目标</p><p>SSD:Liu等开发了一种多分辨率锚点技术，用于检测尺度混合物的物体，并在一定程度上学习偏移量，而不是学习锚点。</p><p>RetinaNet:Lin等提出了一种焦点损失，以解决密集和小目标检测问题，同时处理类不平衡和不一致。</p><p>Zhou和Tuzel(VoxelNet)以及Lang等(PointPillars)提出了用于点云的神经网络，这为3D检测任务开辟了新的可能性。</p><h4 id="Object-detection-without-anchors"><a href="#Object-detection-without-anchors" class="headerlink" title="Object detection without anchors"></a>Object detection without anchors</h4><p>为了解决锚点回归带来的计算开销和超参数冗余问题，并有效地处理点云编码，无锚点目标检测已在许多工作中得到应用。无锚点目标检测可分为两大类，即<strong>基于中心的方法和基于关键点的方法</strong>。</p><p>基于中心的方法:在这种方法中，对象的中心点用于定义正样本和负样本，而不是IoU。该方法通过预测从正样本到物体边界的四个距离来生成边界框，从而大大降低了计算成本。</p><p>基于关键点的方法:通过几个预定义的方法或自学习模型定位关键点，然后生成边界框来对对象进行分类。</p><p>为了提取具有代表性的特征，我们重点关注两个主要组成部分：<strong>范围感知特征提取</strong>和<strong>遮挡监督</strong>。</p><p>我们提出的范围感知注意力网络（RAANet）的主要架构如图所示</p><p><img data-src="https://i.imgur.com/GERZSZ7.png" alt="image-20231119161935219"></p><p>我们结合了CenterNet的思想来构建一个无锚探测器，并引入了两个新颖的模块：距离感知注意力卷积层（RAAConv）和辅助密度级估计模块（ADLE）。</p><p>区域建议网络 （RPN） 将该 BEV 特征图作为输入，并使用多个下采样和上采样模块来生成高维特征图。</p><p>除了主要任务中的检测头外，我们还提出了一个辅助任务，用于点密度水平估计，以实现更好的检测性能。</p><p>RAAConv 首先利用两组卷积核来提取每个分支的中间特征图。</p><p>然后，将热图 fa 和 fb 分别乘以可学习标量 γa 和 γb。γa 和 γb 初始化为 1.0，并在训练过程中逐渐学习它们的值</p><h2 id="VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017"><a href="#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017" class="headerlink" title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017"></a>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017</h2><h3 id="abs-1"><a href="#abs-1" class="headerlink" title="abs"></a>abs</h3><p>准确检测三维点云中的物体是自主导航、看家机器人和增强/虚拟现实等许多应用中的核心问题。点云数据 高度稀疏</p><p>为了将高度稀疏的激光雷达点云与区域建议网络（RPN）连接起来，现有的大部分工作都集中在手工制作的特征表示上，例如鸟瞰投影。</p><p>在这项工作中，我们<strong>不再需要对三维点云进行人工特征工程，而是提出了一种通用的三维检测网络—VoxelNet，它将特征提取和边界框预测统一为一个单一阶段、端到端可训练的深度网络</strong>。</p><h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><p>3D传感器技术的快速发展促使研究人员开发有效的表示来<strong>检测和定位点云中的物体</strong>,当有丰富而详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。</p><p>然而，它们<strong>无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性</strong>，导致自主导航等不受控制的场景的成功有限。</p><p>鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框,然而，基于图像的三维检测方法的精度受深度估计精度的限制。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>所提出的VoxelNet由三个功能块组成：（1）特征学习网络(Feature learning network)，（2）卷积中间层(Convolutional middle layers)，（3）区域建议网络(Region proposal network)。</p><h3 id="Feature-learning-network"><a href="#Feature-learning-network" class="headerlink" title="Feature learning network"></a>Feature learning network</h3><p>Voxel Partition</p><p><img data-src="https://i.imgur.com/dhgI1JD.png" alt="image-20231119112120958"></p><p>Stacked Voxel Feature Encoding</p><p>用V表示一个体素(Voxel),</p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p><img data-src="https://i.imgur.com/wvQa3jb.png" alt="image-20231119112203016"></p><p>RPN层有两个分支，一个用来输出类别的概率分布（通常叫做Score Map），一个用来输出Anchor到真实框的变化过程（通常叫做 Regression Map）</p><blockquote><p>注意这里论文是直接输出预测的anchor box的坐标而不是修正值.</p></blockquote><h4 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h4><p>我们初始化一个 K × T × 7 维张量结构来<strong>存储体素输入特征缓冲区</strong>，其中 <strong>K 是非空体素的最大数量，T 是每个体素的最大点数，7 是每个点的输入编码维度</strong>。</p><p>这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><img data-src="https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png" alt="image-20231129221227758"></p><p>da = √(la)2 + (wa)2 是anchor box的对角线。</p><p><img data-src="https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png" alt="image-20231129221345941"></p><p>ui ∈ R^7^ 和 u∗ i ∈ R^7^ 分别是正锚点 a^pos^ ~i~ 的回归输出和地面实况。</p><h2 id="Center-based-3D-Object-Detection-and-Tracking"><a href="#Center-based-3D-Object-Detection-and-Tracking" class="headerlink" title="Center-based 3D Object Detection and Tracking"></a>Center-based 3D Object Detection and Tracking</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>与研究透彻的二维检测问题相比，点云上的三维检测提出了一系列有趣的挑战.点云稀疏，<strong>三维空间的大部分区域都没有测量值</strong>,其次，<strong>输出结果是一个三维方框，通常无法与任何全局坐标框架很好地对齐</strong>。第三，<strong>三维物体有多种尺寸、形状和长宽比</strong>，例如，在交通领域，自行车接近平面，公共汽车和豪华轿车细长，行人高大。</p><p><strong>二维和三维检测之间的这些显著差异使得这两个领域之间的理念转换变得更加困难。问题的关键在于，轴对齐的二维方框  并不能代表自由形态的三维物体</strong></p><p>一种解决方案可能是为每个物体方向分类不同的模板（锚，<strong>但这不必要地增加了计算负担，并可能带来大量潜在的假阳性检测</strong>。我们认为，<strong>将二维和三维领域连接起来的主要挑战在于物体的这种表现形式</strong>。</p><p>然后，它将这一表示法<strong>扁平化为俯视地图视图，并使用标准的基于图像的关键点检测器来查找对象中心</strong>，对于每个检测到的中心点，它会<strong>根据中心点位置的点特征回归到所有其他物体属性，如三维尺寸、方向和速度</strong>。</p><p>​        基于中心的表示法有几个主要优点：首先，<strong>与边界框不同，点没有固有方向。这大大缩小了物体检测器的搜索空间</strong>，同时<strong>允许骨干学习物体的旋转不变性和相对旋转的旋转等差性</strong>。其次，基于中心的表示法<strong>简化了追踪等下游任务</strong>。如果物体是点，小轨迹就是空间和时间中的路径。中<strong>心点可以预测连续帧之间物体的相对偏移（速度），然后将其贪婪地连接起来</strong>。第三，基于<strong>点的特征提取使我们能够设计一个有效的两阶段细化模块，其速度比以往的方法快得多</strong></p><p><img data-src="https://i.imgur.com/tS9TBGi.png" alt="image-20231222175607203"></p><p>CenterPoint 的第一阶段预测特定类别的热图、物体大小、子象素位置细化、旋转和速度。所有输出均为密集预测。</p><h3 id="Center-heatmap-head"><a href="#Center-heatmap-head" class="headerlink" title="Center heatmap head."></a>Center heatmap head.</h3><p>中心头的目标是在检测到的任何物体的中心位置生成一个热图峰值。该头会生成 K 个通道的热图 ˆ Y，K 个类别中的每个类别都有一个通道。</p><p>在训练过程中，它的目标是将注释边界框的三维中心投影到地图视图中产生的二维高斯。我们使用focal损耗</p><blockquote><p>自上而下地图视图中的物体比图像中的要稀疏。在地图视图中，距离是绝对的，而在图像视图中，距离会因透视而扭曲。以道路场景为例，在地图视图中，车辆所占的面积很小，但在图像视图中，几个大物体可能占据了屏幕的大部分区域</p></blockquote><p>采用 CenterNet的标准监督方式会导致监督信号非常稀疏，大多数位置都被视为背景。为了解决这个问题，我们通过<strong>扩大每个地面实况对象中心的高斯峰值，来增加目标热图 Y 的正向监督</strong>。</p><h3 id="Regression-heads"><a href="#Regression-heads" class="headerlink" title="Regression heads"></a>Regression heads</h3><p>在物体的中心特征处存储了几个物体属性：<strong>子象素位置细化</strong>(sub-voxel) o∈R2、<strong>离地高度</strong> hg∈R、<strong>三维尺寸</strong>(3D dimension)s∈R3，<strong>以及偏航旋转角度</strong>（sin(α), cos(α)）∈R2。</p><p>子体素位置细化 o 可减少主干网络体素化和跨距造成的量化误差</p><p>地面高度 hg 可帮助定位三维物体，并补充地图视图投影中缺失的高程信息。</p><p>方位预测使用偏航角的正弦和余弦作为连续回归目标。</p><p>结合方框大小，这些回归头可提供三维边界框的全部状态信息。每个输出都使用自己的回归头。在训练时，只使用 L1 回归损失对地面实况中心进行监督。</p><h3 id="Two-Stage-CenterPoint"><a href="#Two-Stage-CenterPoint" class="headerlink" title="Two-Stage CenterPoint"></a>Two-Stage CenterPoint</h3><p>第二阶段从骨干网的输出中提取额外的点特征。</p><p>我们从预测边界框的每个面的三维中心提取一个点特征。请注意，边界框中心、顶面和底面中心在地图视图中都投影到同一个点。</p><p>因此，我们只考虑四个朝外的方框面和预测的物体中心。对于每个点，我们使用双线性插值法从骨干地图视图输出 M 中提取特征。</p><p>第二阶段在单阶段 CenterPoint 预测结果的基础上，预测与类别无关的置信度得分和box refinement。</p><p>对于不区分类别的置信度得分预测，遵循的方法，使用得分目标 I，该目标由方框的 3D IoU 和相应的地面实况边界方框引导</p><script type="math/tex; mode=display">\begin{equation}I=\min(1,\max(0,2\times IoU_t-0.5))\end{equation}</script><p>IoUt 是第 t 个建议框与gt bbox之间的 IoU</p><script type="math/tex; mode=display">\begin{equation}L_{score}=-I_t\log(\hat{I}_t)-(1-I_t)\log(1-\hat{I}_t)\end{equation}</script><p>在推理过程中，<strong>直接使用单阶段中心点的类别预测，并以两个分数的几何平均值计算最终置信度分数</strong></p><script type="math/tex; mode=display">\begin{equation}\hat{Q_t}=\sqrt{\hat{Y_t}*\hat{I_t}}\end{equation}</script><p>其中 ˆ Qt 是对象 t 的最终预测置信度，ˆ Yt = max0≤k≤K ˆ Yp,k 和 ˆ It 分别是对象 t 的第一阶段和第二阶段置信度。</p><p>对于<strong>bbox回归</strong>，模型<strong>在第一阶段建议的基础上预测细化，用 L1 损失来训练模</strong>型。</p><h2 id="SECOND-Sparsely-Embedded-Convolutional-Detection-2018"><a href="#SECOND-Sparsely-Embedded-Convolutional-Detection-2018" class="headerlink" title="SECOND: Sparsely Embedded Convolutional Detection 2018"></a>SECOND: Sparsely Embedded Convolutional Detection 2018</h2><h1 id="PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018"><a href="#PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018" class="headerlink" title="PointPillars: Fast Encoders for Object Detection from Point Clouds 2018"></a>PointPillars: Fast Encoders for Object Detection from Point Clouds 2018</h1><h1 id="PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019"><a href="#PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019" class="headerlink" title="PIXOR: Real-time 3D Object Detection from Point Clouds 2019"></a>PIXOR: Real-time 3D Object Detection from Point Clouds 2019</h1><h2 id="Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021"><a href="#Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021" class="headerlink" title="Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021"></a>Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021</h2><h2 id="CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021"><a href="#CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021" class="headerlink" title="CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021"></a>CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021</h2><h2 id="协同感知-3D检测任务"><a href="#协同感知-3D检测任务" class="headerlink" title="协同感知 3D检测任务"></a>协同感知 3D检测任务</h2><p>综述</p><h3 id="Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges"><a href="#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges" class="headerlink" title="Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges"></a>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</h3><p>协作感知对于解决自动驾驶中的遮挡和传感器故障问题至关重要。</p><p>自动驾驶感知可分为<strong>个体感知和协作感知</strong>。虽然个体感知随着深度学习的发展取得了长足的进步，但一些问题也限制了其发展。首先，<strong>个体感知在感知相对全面的环境时经常会遇到遮挡</strong>。其次，<strong>车载传感器在感知远处物体时存在物理限制</strong>。此外，<strong>传感器噪音也会降低感知系统的性能</strong>。为了弥补个体感知的不足，协作或合作感知利用了多个代理之间的互动，受到了广泛关注。</p><p>协同感知是一种多agent系统，其中agent共享感知信息，以克服自我视听的视觉局限。在单个感知场景中，自我视听只能检测到附近物体的部分遮挡和远处稀疏的点云。在协作感知场景中，ego AV通过接收其他agent的信息来扩大视野。<strong>通过这种协作方式，ego AV不仅能检测到远处和被遮挡的物体，还能提高在密集区域的检测精度</strong>。</p><p><img data-src="https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png" alt="image-20231122203934187"></p><p>长期以来，协作感知一直是人们关注的焦点。之前的工作专注于构建协作感知系统，以评估该技术的可行性。然而，由<strong>于缺乏大型公共数据集，它没有得到有效的推进</strong>。近年来，随着深度学习的发展和大规模协作感知数据集的公众关注和研究激增。</p><p><strong>考虑到通信中的带宽限制，大多数研究人员致力于设计新颖的协作模块，以实现精度和带宽之间的权衡</strong>。</p><p>在协作感知场景中，自我 AV 通过接收来自其他智能体的信息来扩展视野。通过这种协作方式，自我AV不仅可以检测远处和被遮挡的物体，还可以提高密集区域的检测精度。</p><p>为了总结这些技术和问题，我们回顾了自动驾驶中的协同感知方法，并从方法、数据集和挑战方面对近年来的进展进行了全面综述。我们还注意到近年来发表了一些关于协作感知的综述。</p><h3 id="Collaboration-scheme"><a href="#Collaboration-scheme" class="headerlink" title="Collaboration scheme"></a>Collaboration scheme</h3><h4 id="早期融合"><a href="#早期融合" class="headerlink" title="早期融合"></a>早期融合</h4><p>早期协作在网络输入端采用原始数据融合，也称为数据级或低级融合</p><p>因此，早期协作可以从根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效。</p><p>在自动驾驶场景中，自我车辆接收并转换来自其他智能体的原始传感器数据，然后聚合车载转换后的数据。原始数据包含最全面的信息和实质性的代理描述。因此，早期协作可以从<strong>根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效</strong>。</p><p>考虑到早期协作的高带宽，一些工作提出了中间协作感知方法来平衡性能-带宽的权衡。在中间协作中，其他智能体通常会将深层语义特征转移到自我载体。自我车辆融合特征以做出最终预测。中间协作已成为最流行的多智能体协作感知灵活性选择。然而，特征提取往往会造成信息丢失和不必要的信息冗余，这促使人们探索合适的特征选择和融合策略。</p><h4 id="中期"><a href="#中期" class="headerlink" title="中期"></a>中期</h4><p>考虑到早期协作的高带宽，一些研究提出了中间协作感知方法，以平衡性能与带宽之间的权衡。在中间协作中，其他代理通常会将深层语义特征传输给自我车辆。</p><h3 id="晚期"><a href="#晚期" class="headerlink" title="晚期"></a>晚期</h3><p>后期或对象级协作在网络输出端采用预测融合。每个代理单独训练网络并相互共享输出。自我车辆在空间上转换输出，并在后处理后合并所有输出。后期协作比早期和中期协作更节省带宽，也更简单。然而，后期的合作也有局限性。由于<strong>单个输出可能是嘈杂和不完整的，因此后期协作总是具有最差的感知性能</strong>。</p><h3 id="原始数据融合-Raw-Data-Fusion"><a href="#原始数据融合-Raw-Data-Fusion" class="headerlink" title="原始数据融合(Raw Data Fusion)"></a>原始数据融合(Raw Data Fusion)</h3><p>早期协作在<strong>输入阶段采用原始数据融合。由于点云是不规则的，可以直接汇总</strong>，因此早期的协同工作通常采用<strong>点云融合策略</strong>。</p><p>第一个早期的协同感知系统 Cooper<strong>选择激光雷达数据</strong>作为融合目标。只需提取位置坐标和反射值，就能将点云压缩成较小的尺寸。在代理之间进行交互后，Cooper 利用变换矩阵重构接收到的点云，然后将自我点云集concat起来，进行最终预测。</p><p>受 Cooper 的启发，Coop3D 还探索了早期的协作，并引入了一种新的点云融合方法。具体来说，Coop3D 系统没有采用串联，而是利用空间变换来融合传感器数据。此外，与Cooper在车上共享车对车信息不同，Coop3D提出了一个中央系统来合并多个传感器数据，从而可以协同摊销传感器和处理成本。</p><h3 id="customized-communication-mechanism"><a href="#customized-communication-mechanism" class="headerlink" title="customized communication mechanism"></a>customized communication mechanism</h3><p>早期协作中的原始数据融合拓宽了自我飞行器的视野，也<strong>造成了高带宽压力</strong>。为了缓解上述问题，<strong>越来越多的工作 发展了中间协作</strong>。</p><p>最初的中间协作方法<strong>遵循一种贪婪的通信机制，以获取尽可能多的信息。一般来说，它们会与通信范围内的所有代理共享信息，并将压缩后的完整特征图放入集体感知信息（CPM,collective perception message）中</strong>。然而，由于特征稀疏和代理冗余，贪婪通信可能会极大地浪费带宽。</p><p>Who2com 建立了首个带宽限制下的通信机制，通过三阶段握手实现。具体来说，<strong>Who2com 使用一般注意力函数计算代理之间的匹配分数，并选择最需要的代理，从而有效减少带宽</strong>。</p><p>在 Who2com 的基础上，When2com<strong>引入了缩放一般注意力来决定何时与他人交流</strong>。这样，自我代理只有在信息不足时才会与他人交流，从而有效地节省了协作资源。</p><p>除了选择合适的通信代理外，<strong>通信内容对于减少带宽压力也很重要</strong>。FPVRCNN 中提出了初始特征选择策略.具体来说，FPV-RCNN 采用检测头生成proposals，并只选择proposals中的特征点。</p><p><strong>关键点选择模块减少了共享深度特征的冗余，为初始proposals提供了有价值的补充信息。</strong></p><p>Where2comm 也提出了一种新颖的空间信心感知通信机制。其核心思想是<strong>利用空间置信度图来决定共享特征和通信目标</strong>。<strong>在特征选择阶段</strong>，<strong>Where2comm 选择并传输满足高置信度和其他agent请求的空间元素</strong>。在<strong>agent选择阶段，自我代理只与能提供所需特征的代理通信。通过发送和接收感知关键区域的特征，Where2comm 节省了大量带宽，并显著提高了协作效率</strong>。</p><h3 id="Feature-Fusion"><a href="#Feature-Fusion" class="headerlink" title="Feature Fusion"></a>Feature Fusion</h3><blockquote><p>Feature fusion module is crucial in intermediate collaboration. After receiving CPMs from other agents, the ego vehicle can <strong>leverage different strategies to aggregate these features</strong>.</p></blockquote><p>可行的融合策略能够捕捉特征之间的潜在关系，提高感知网络的性能。根据基于特征融合的思想，我们将现有的特征融合方法分为传统融合、基于图的融合和基于注意力的融合。</p><h4 id="传统融合"><a href="#传统融合" class="headerlink" title="传统融合"></a>传统融合</h4><p>在协同感知研究的早期阶段，研究人员倾向于使用传统的策略来融合特征，如concat、求和和线性加权。中级协作将这些不变的置换操作应用于深度特征，因其简单性而实现了快速推理。</p><p>第一个中间协同感知框架 FCooper<strong>提取了低级体素和深度空间特征</strong>。基于这两级特征，F-Cooper 提出了两种特征融合策略：<strong>体素特征融合</strong>（VFF）和<strong>空间特征融合</strong>（SFF）。</p><p>这两种方法都采用<strong>元素最大值（element-wise maxout）来融合重叠区域的特征</strong>。由于<strong>体素特征更接近原始数据，因此 VFF 与原始数据融合方法一样能够进行近距离物体检测</strong>。同时，SFF 也有其优势。</p><p>受 SENet的启发，SFF 选择选择部分信道来减少传输时间消耗，同时保持可比的检测精度</p><p>考虑到 F-Coope<strong>r忽略了低置信度特征的重要性</strong>，Guo 等人提出了 <strong>CoFF</strong> 来改进 F-Cooper。<strong>CoFF 通过测量重叠特征的相似度和重叠面积对其进行加权。相似度越小，距离越大，邻近特征提供的补充信息就越直观。</strong></p><p>此外，还添加了一个增强参数，以提高弱特征的值。</p><p>实验表明，简单而高效的设计使 CoFF 大大提高了 F-Cooper 的性能。</p><p>传统的融合方法虽然简单,但并没有被最近的方法所抛弃。Hu 等人提出了协作式纯相机三维检测（CoCa3D）,证明了协作在增强基于相机的三维检测方面的潜力。由于深度估计是基于相机的 3D 检测的瓶颈，因此 CoCa3D 包含协作深度估计 （Co-Depth）,但协作特征学习 （Co-FL） 除外。</p><h4 id="图融合"><a href="#图融合" class="headerlink" title="图融合"></a>图融合</h4><p>基于图的融合：尽管传统的中间融合很简单，但它们忽略了多方agent之间的潜在关系，无法推理从发送方到接收方的信息。图神经网络（GNN）能够传播和聚合来自邻居的信息，最近的研究表明，图神经网络在感知和自动驾驶方面非常有效。</p><p>V2VNet 首先利用空间感知图神经网络（GNN）对代理之间的通信进行建模,在 GNN 信息传递阶段，V2VNet 利用变分图像压缩算法来压缩特征。在跨车辆聚合阶段，V2VNet 首先补偿时间延迟，为每个节点创建初始状态，然后对从邻近代理到自我车辆的压缩特征进行扭曲和空间变换，所有这些操作都在重叠视场中(overlapping fields of view)进行。在特征融合阶段，V2VNet 采用平均运算来聚合特征，并利用卷积门控递归单元（ConvGRU）更新节点状态。虽然 V2VNet与 GNN 相比性能有所提高，但标量值协作权重无法反映不同空间区域的重要性。受此启发，DiscoNet 提出使用矩阵值边缘权重来捕捉高分辨率的代理间注意力。在信息传递过程中，DiscoNet 将特征串联起来，并为特征图中的每个元素应用矩阵值边缘权重。此外，DiscoNet 还将早期融合和中期融合结合在一起，通过对特征图中的每个元素应用矩阵值边缘权重。zhou 等人提出了另一种基于 GNN 的广义感知框架 MP-Pose。在信息传递阶段，MP-Pose 利用空间编码网络编码相对空间关系，而不是直接扭曲特征。受图形注意网络（GAT）的启发，MP-Pose 进一步使用动态交叉注意编码网络来捕捉代理之间的关系，并像 GAT 一样聚合多个特征。</p><h4 id="Attention-based"><a href="#Attention-based" class="headerlink" title="Attention-based"></a>Attention-based</h4><p>除了图形学习，注意力机制也已成为探索特征关系的有力工具.注意机制可根据数据域分为<strong>通道注意、空间注意和通道与空间注意</strong></p><p>在过去的十年中，<strong>注意力机制在计算机视觉领域发挥了越来越重要的作用 ，并激发了协作感知研究</strong>。</p><p>为了捕捉特征图中特定区域之间的相互作用，Xu 等人提出了 AttFusion，并首先在准确的空间位置采用自注意操作。具体来说，<strong>AttFusion 引入了单头自注意融合模块，与传统方法 F-Cooper和基于图的方法 DiscoNet相比，实现了性能和推理速度之间的平衡</strong>。</p><p>除了传统的基于注意力的方法，基于transformer的方法也能激发协作感知。Cui 等人提出了基于点transformer的 COOPERNAUT，这是一种用于点云处理的自注意力网络。</p><p>接收到信息后，ego agent会使用下采样块和点transformer block来聚合点特征。这两种操作<strong>都保持了信息的排列不变性</strong>。更重要的是，COOPERNAUT <strong>将协同感知与控制决策相结合，这对自动驾驶的模块联动具有重要意义</strong></p><p>与 V2V 协作相比，<strong>V2I 可以利用大量基础设施提供更稳定的协作信息，但目前很少有研究关注这一场景</strong>。</p><p>Xu 等人提出了首个统一转换器架构（V2X-ViT），它同时涵盖了 V2V 和 V2I。为了在不同类型的agent之间建立互动模块，V2X-ViT 提出了一个新颖的异构多代理关注模块（HMSA）来学习 V2V 和 V2I 之间的不同关系。此外，还引入了多尺度窗口注意模块（MSwin），以捕捉高分辨率检测中的长距离空间交互。</p><p>定制损失函数：虽然 V2V 通信为自我车辆提供了相对丰富的感知视野，但共享信息的冗余性和不确定性带来了新的挑战。</p><p>以往的协作感知研究大多侧重于<strong>协作效率和感知性能</strong>，但所有这些方法都假设了完美的条件。在现实世界的自动驾驶场景中，通信系统可能存在以下问题</p><p>1) 定位错误；2) 通信延迟和中断；3) 模型或任务差异；4) 隐私和安全问题</p><h4 id="协同感知数据集"><a href="#协同感知数据集" class="headerlink" title="协同感知数据集"></a>协同感知数据集</h4><ul><li><p>V2X-Sim是一个<strong>全面的模拟多代理感知数据集</strong>。它<strong>由交通模拟 SUMO  和 CARLA 模拟器生成</strong>，数据格式遵循 nuScenes 。V2X-Sim 配备了 RGB 摄像头、激光雷达、GPS 和 IMU，收集了 100 个场景共 10,000 个帧，每个场景包含 2-5 辆车。V2X-Sim 中的帧分为 8,000/1,000/1,000 帧，用于训练/验证/测试。V2X-Sim 的基准支持三个关键的感知任务：检测、跟踪和分割，需要注意的是，所有任务都采用鸟瞰（BEV）表示法，并以二维 BEV 生成结果。</p></li><li><p>OPV2V：O<strong>PV2V是另一个针对V2V通信的模拟协同感知数据集</strong>，它是<strong>通过协同模拟框架OpenCDA和CARLA模拟器收集的</strong>。整个数据集可通过提供的配置文件进行重现。OPV2V 包含 11,464 帧激光雷达点和 RGB 摄像机。OPV2V 的一个显著特点是提供了一个名为 “卡尔弗城 “的仿真测试集，可用于评估模型的泛化能力。其基准支持三维物体检测和 BEV 语义分割，目前只包含一种类型的物体（车辆）。</p></li><li><strong>V2XSet 是一个大规模的 V2X 感知开放模拟数据集</strong>。该数据集格式与 OPV2V类似，共有 11,447 个帧。与 V2X 协作数据集 V2X-Sim和 V2I 协作数据集 DAIR-V2X相比，V2XSet 包含更多场景，并且该基准考虑了不完美的真实世界条件。该基准支持 3D 物体检测和 BEV 分割，有两种测试设置（完美和嘈杂）供评估。</li><li>DAIR-V2X：作为<strong>第一个来自真实场景的大规模 V2I 协同感知数据集</strong>，DAIR-V2X [对自动驾驶的协同感知意义重大。DAIR-V2X-C 集可用于研究 V2I 协作，VIC3D 基准可用于探索 V2I 物体检测任务。与主要关注激光雷达点的 V2X-Sim和 V2XSet不同，VIC3D 物体检测基准同时提供了基于图像和基于激光雷达点的协作方法。</li><li>V2V4Real：V2V4Real 是<strong>首个大规模真实世界多模式 V2V 感知数据集</strong>，由俄亥俄州哥伦布市的一辆特斯拉汽车和一辆福特 Fusion 汽车收集而成，覆盖 410 公里的道路。该数据集包含 20,000 个 LiDAR 帧和超过 240,000 个三维边界框注释，涉及五个不同的车辆类别。此外，V2V4Real 还提供了三个合作感知任务的基准，包括三维物体检测、物体跟踪和域适应。</li></ul><h2 id="VoxelNet"><a href="#VoxelNet" class="headerlink" title="VoxelNet"></a>VoxelNet</h2><p>在这项工作中,我们消除了对 3D 点云进行手动特征工程的需要，并提出了 VoxelNet，这是一种通用的 3D 检测网络，将特征提取和边界框预测统一到一个单阶段、端到端可训练的深度网络中。具体来说，VoxelNet 将点云划分为等间距的 3D 体素，并<strong>通过新引入的体素特征编码 （VFE） 层将每个体素内的一组点转换为统一的特征表示</strong>。通过这种方式,<strong>点云被编码为描述性体积表示,然后将其连接到RPN以生成检测</strong>。</p><p><img data-src="https://s2.loli.net/2024/01/01/McwI9AFzmXhLC6K.png" alt="image-20240101215637846"></p><p><img data-src="https://s2.loli.net/2024/01/01/BotrdTR2xQ9UuVj.png" alt="image-20240101215613643"></p><p><img data-src="https://s2.loli.net/2024/01/01/toNv8YDawXhpMqS.png" alt="image-20240101215656769"></p><h2 id="Point-Pillar"><a href="#Point-Pillar" class="headerlink" title="Point Pillar"></a>Point Pillar</h2><p>点云中的物体检测是许多机器人应用（如自动驾驶）的一个重要方面。在本文中，我们考虑了将点云编码为适合下游检测管道的格式的问题。最近的文献提出了两种类型的编码器;固定编码器往往速度快，但会牺牲准确性，而从数据中学习的编码器更准确，但速度较慢。在这项工作中，我们<strong>提出了PointPillars，这是一种新颖的编码器，它利用PointNets来学习以垂直列（柱子）组织的点云的表示</strong>。虽然编码特征可以与任何标准的 2D 卷积检测架构一起使用，但我们进一步提出了一个精益的下游网络。广泛的实验表明，PointPillars 在速度和精度方面都远远优于以前的编码器。尽管只使用激光雷达，但我们的完整检测流程在3D和鸟瞰图KITTI基准测试方面都明显优于最先进的技术，即使在融合方法中也是如此。这种检测性能是在 62 Hz 下运行时实现的：运行时间提高了 2 - 4 倍。我们方法的更快版本与105 Hz的最新技术相匹配。这些基准测试表明，PointPillars 是用于点云中对象检测的合适编码。</p><p><img data-src="https://s2.loli.net/2024/01/01/pgidY1Gzk3vDPAa.png" alt="image-20240101220949018"></p><h2 id="SECOND"><a href="#SECOND" class="headerlink" title="SECOND"></a>SECOND</h2><p>近年来，基于卷积神经网络（CNN）的目标检测[、实例分割[3]和关键点检测[4]取得了长足的进步。这种检测形式可用于基于单目或立体图像的自动驾驶。但是，用于处理图像的方法不能直接应用于LiDAR数据。这对于自动驾驶和机器人视觉等应用来说是一个重大限制。最先进的方法可以实现 90% 的 2D 汽车检测的平均精度 （AP），但对于基于 3D 图像的汽车检测，AP 仅为 15%。</p><p>为了克服仅靠图像提供的空间信息不足的问题，点云数据在 3D 应用中变得越来越重要。点云数据包含准确的深度信息，可以由LiDAR或RGB-D相机生成。</p><p>我们在基于LiDAR的目标检测中应用了稀疏卷积，从而大大提高了训练和推理的速度。• 我们提出了一种改进的稀疏卷积方法，使其运行得更快。</p><p>• 我们提出了一种新的角度损失回归方法，该方法显示出比其他方法更好的方向回归性能。</p><p> • 我们针对仅激光雷达的学习问题引入了一种新的数据增强方法，大大提高了收敛速度和性能。</p><p><img data-src="https://s2.loli.net/2024/01/01/DuRbMClOPc6a9UL.png" alt="image-20240101221307223"></p><p><img data-src="https://s2.loli.net/2024/01/01/sySo9kjwteduihm.png" alt="image-20240101221332774"></p><h2 id="PIXOR"><a href="#PIXOR" class="headerlink" title="PIXOR"></a>PIXOR</h2><h3 id="abs-2"><a href="#abs-2" class="headerlink" title="abs"></a>abs</h3><p>由于点云的高维数，现有方法的计算成本很高。我们通过鸟瞰图（BEV）表示场景，更有效地利用3D数据，并提出了PIXOR，这是一种无需提案的单级检测器，可输出从像素级神经网络预测解码的定向3D对象估计值。</p><h3 id="intro-1"><a href="#intro-1" class="headerlink" title="intro"></a>intro</h3><p>处理激光雷达数据的主要困难在于，传感器以点云的形式生成非结构化数据，通常每 360 度扫描包含大约 105 个 3D 点。</p><p>现有的表示主要分为两种类型：<strong>3D 体素网格</strong>和 <strong>2D 投影</strong>。</p><p>3D 体素网格<strong>将点云转换为规则间隔的 3D 网格</strong>，其中每个体素单元可以包含标量值（例如，占用率）或矢量数据（例如，根据该体素单元内的点计算出的手工统计数据）。然而，<strong>由于点云本质上是稀疏的，因此体素网格非常稀疏，因此很大一部分计算是冗余和不必要的</strong>。因此，使用此表示 的典型系统只能以 1-2 FPS 的速度运行</p><p>另一种方法是将点云投影到平面上，然后将其离散化为基于2D图像的表示，其中应用了2D卷积,在离散化过程中，手工制作的要素（或统计数据）将计算为 2D 图像的像素值.这些基于 2D 投影的表示更紧凑，但它们<strong>会在投影和离散化过程中带来信息丢失</strong>。例如，<strong>范围视图投影将具有扭曲的对象大小和形状</strong>.为了减轻信息损失，MV3D [3] 建议将 2D 投影与相机图像融合以带来更多信息。</p><p>我们选择 BEV 表示，因为<strong>它在计算上比 3D 体素网格更友好，并且还保留了度量空间</strong>，这使我们的模型能够探索有关对象类别的大小和形状的先验.</p><p>我们的探测器在鸟瞰图中以真实世界的尺寸输出精确定向的边界框。请注意，这些是 3D 估计值，因为我们假设物体在地面上。在自动驾驶场景中，这是一个合理的假设，因为车辆不会飞行。</p><p><img data-src="https://i.imgur.com/gPMcBei.png" alt="image-20231225154926867"></p><p>我们的边界框估<strong>计不仅包含3D空间中的位置，还包含航向角</strong>，因为准确预测对于自动驾驶非常重要。我们利用了激光雷达点云的 2D 表示，因为与 3D 体素网格表示相比，它更紧凑，因此更适合实时推理。</p><p><strong>Input Representation</strong></p><p>标准卷积神经网络执行离散卷积，因此假设输入位于网格上。然而，3D点云是非结构化的，因此不能直接应用标准卷积。</p><p>一种选择是使用体素化形成 3D 体素格网，其中每个体素单元格包含该体素内点的某些统计数据。为了从此 3D 体素网格中提取特征表示，通常使用 3D 卷积。然而，这在计算中可能非常昂贵，因为我们必须沿着三维滑动 3D 卷积核。这也是不必要的，因为激光雷达点云非常稀疏，以至于大多数体素像元都是空的。</p><p>相反，<strong>我们可以仅从鸟瞰图 （BEV） 来表示场景</strong>。</p><p>通过将自由度从 3 减少到 2，我们不会丢失点云中的信息，因为我们仍然可以将高度信息保留为沿第三维的通道（如 2D 图像的 RGB 通道）。</p><p>在自动驾驶的背景下，<strong>这种降维是合理的，因为感兴趣的对象位于同一地面上</strong>。除了计算效率外，BEV表示还具有其他优势。<strong>它缓解了对象检测的问题，因为对象彼此不重叠（与前视图表示相比）。它还保留了度量空间，因此网络可以利用有关对象物理尺寸的先验。</strong></p><p>体素化激光雷达表示的<strong>常用特征是占用率、强度（反射率）、密度和高度特征</strong>。在PIXOR中，为了简单起见，我们只使用<strong>占用率和强度</strong>作为特征。在实践中，我们首先定义我们感兴趣的场景的 3D 物理尺寸 L × W × H。</p><p>然后，我们计算网格分辨率为 d~L~ × d~W~ × d~H~ 的占用特征图，并计算网格分辨率为 d~L~ × d~W~ × H 的强度特征图。请注意，<strong>在占用特征图中添加了两个额外的通道，以覆盖超出范围的点</strong>。</p><p>最终表示的形状为 L /d~L~ × W/d~W~ × （ H/d~H~ + 3）。</p><p><strong>Network Architecture</strong></p><p>PIXOR 使用<strong>全卷积神经网络</strong>，专为密集定向 3D 目标检测而设计。我们不采用常用的提案生成分支.取而代之的是，该网络<strong>在单个阶段输出像素级预测，每个预测对应于 3D 对象估计</strong>。</p><p>由于采用了全卷积架构，可以非常有效地计算出如此密集的预测。</p><p>在网络预测中 3D 对象的编码方面，我们使用直接编码，而不求助于预定义的对象锚点.</p><p>所有这些设计使PIXOR变得非常简单，并且由于网络架构中的零超参数，可以很好地泛化。具体来说，不需要设计对象锚点，也不需要调整从第一阶段传递到第二阶段的提案数量以及相应的非 Non-Maximum Suppression 阈值。</p><p>整个架构可以分为两个子网：backbone和header。</p><p><strong>backbone网络用于以卷积特征图的形式提取输入的一般表示。它具有很高的表示能力，可以学习鲁棒的特征表示。</strong></p><p><strong>header网络用于进行特定于任务的预测，在我们的示例中，它有多任务输出的单分支结构：表示对象类概率的分数图，以及编码定向 3D 对象的大小和形状的几何图。</strong></p><h4 id="Backbone-Network"><a href="#Backbone-Network" class="headerlink" title="Backbone Network"></a>Backbone Network</h4><p>卷积神经网络通常由卷积层和池化层组成.</p><p>许多基于图像的物体检测器中的骨干网络通常具有 16 的下采样因子，并且通常设计为高分辨率层数较少，低分辨率层数较多。它适用于图像，因为对象的像素大小通常很大。但是，在我们的例子中，<strong>这将导致一个问题，因为对象可能非常小。使用 0.1m 的离散化分辨率时，典型车辆的尺寸为 18×40 。经过 16× 的下采样后，它仅覆盖大约 3 个像素。</strong>一个直接的<strong>解决方案是使用更少的池化层</strong>。然而，这将减小最终特征图中每个像素的感受野的大小，从而限制了表示能力。然而<strong>，这会导致高级特征图中出现棋盘伪影</strong>。我们的解决方案很简单，我们使用 16× 的下采样因子，但进行了两次修改。</p><p>首先，<strong>我们在较低的级别中添加更多具有较小通道数的层，以提取更精细的细节信息</strong>。其次，<strong>我们采用类似于FPN的自上而下的分支，将高分辨率特征图与低分辨率特征图相结合，从而对最终的特征表示进行上采样</strong></p><p><img data-src="https://i.imgur.com/7ILcxnF.png" alt="image-20231225165719123" style="zoom:67%;" /></p><p>第二至第五块由残余层组成,每个残差块的第一个卷积的步幅为 2，以便对特征图进行下采样。总的来说，我们的下采样因子为 16。为了对特征图进行上采样，我们添加了一个自上而下的路径，每次对特征图进行上采样 2。</p><p>然后通过像素求和将其与相应分辨率的自下而上的特征图相结合。使用两个上采样层，从而得到最终的特征图，相对于输入，下采样因子为 4×。</p><h4 id="Header-Network"><a href="#Header-Network" class="headerlink" title="Header Network"></a>Header Network</h4><p>Header 网络是一个多任务网络，可处理对象识别和定位。</p><p><strong>分类分支输出 1 通道特征图，后跟 sigmoid 激活功能</strong>。<strong>回归分支输出无非线性的 6 通道特征图。</strong></p><p>在两个分支之间共享权重的层数存在权衡。一方面，我们希望更有效地利用权重。另一方面，由于它们是不同的子任务，我们希望它们更加独立和专业化。</p><p>我们将<strong>每个对象参数化为定向边界框 b，即 {θ， x~c~， y~c~， w， l}，每个元素对应于航向角度（在 [−π， π] 范围内）、对象的中心位置和对象的大小。</strong></p><p>与基于长方体的 3D 目标检测相比，我们省略了沿 Z 轴的位置和大小，因为在自动驾驶等应用中，感兴趣的对象被限制在同一接地平面上，因此我们只关心如何在该平面上定位它（此设置在一些文献中也称为 3D 定位 ）。</p><p>请注意，航向角度被分解为两个相关值，以强制执行角度范围约束。学习目标是 {cos(θ),sin(θ),dx, dy,log(w), log(l)}，在训练集上预先归一化为零均值和单位方差。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h4 id="KITTI"><a href="#KITTI" class="headerlink" title="KITTI"></a>KITTI</h4><p>我们利用我们的自动驾驶平台Anniway开发了具有挑战性的新型现实世界计算机视觉基准。我们感兴趣的任务是：立体、光流、视觉里程计、三维物体检测和三维跟踪。为此，我们为一辆标准旅行车配备了两台高分辨率彩色和灰度摄像机。Velodyne激光扫描仪和GPS定位系统提供了准确的地面实况。我们的数据集是通过在中型城市卡尔斯鲁厄、农村地区和高速公路上行驶来获取的<strong>。每张图片最多可看到15辆汽车和30名行人</strong>。除了以原始格式提供所有数据外，我们还为每个任务提取基准。对于我们的每个基准，我们还提供了一个评估指标和这个评估网站。初步实验表明，在Middlebury等既定基准上排名靠前的方法在从实验室转移到现实世界时，表现低于平均水平。我们的目标是减少这种偏见，并通过向社区提供具有新困难的现实世界基准来补充现有基准。</p><h4 id="nuScenes"><a href="#nuScenes" class="headerlink" title="nuScenes"></a>nuScenes</h4><p>nuScenes数据集（发音为/nuõsiõnz/）是Motional（前身为nuTonomy）团队开发的一个用于<strong>自动驾驶的公共大规模数据集</strong>。Motional正在使无人驾驶汽车成为一种安全、可靠和可访问的现实。通过向公众发布我们的一部分数据，Motional旨在支持公众对计算机视觉和自动驾驶的研究。</p><p>为此，收集了波士顿和新加坡的1000个驾驶场景，这两个城市以交通密集和极具挑战性的驾驶环境而闻名。20秒长的场景是手动选择的，以展示一组多样而有趣的驾驶动作、交通状况和意外行为。nuScenes的丰富复杂性将鼓励开发能够在每个场景有几十个物体的城市地区安全驾驶的方法。收集不同大陆的数据进一步使我们能够研究计算机视觉算法在不同地点、天气条件、车辆类型、植被、道路标记以及左右交通中的通用性。</p><p>所有检测结果均按照平均精度 (mAP)、平均平移误差 (mATE)、平均比例误差 (mASE)、平均方向误差 (mAOE)、平均速度误差 (AVE)、平均属性误差 (AAE) 和 nuScenes 检测得分 (NDS) 进行评估。</p><h4 id="The-Waymo-opendataset"><a href="#The-Waymo-opendataset" class="headerlink" title="The Waymo opendataset"></a>The Waymo opendataset</h4><p>机器学习领域正在迅速变化。Waymo通过创建和共享一些最大、最多样化的自动驾驶数据集，在为研究界做出贡献方面处于独特的地位。查看我们最新发布的感知对象资产数据集，其中包括31k个具有传感器数据的独特感知对象实例，用于生成建模！</p><p>从多传感器数据中提取感知对象：所有5个摄像头和顶级激光雷达 激光雷达特征包括支持三维物体形状重建的三维点云序列。</p><p>我们还通过点云形状注册为车辆类别中的所有对象提供了精细的长方体姿态 相机功能包括most_visible_Camera的相机补丁序列、相应相机上投影的激光雷达返回、每个像素的相机光线信息，以及支持对象NeRF重建的自动标记2D全景分割，本文对此进行了详细介绍 来自31K个独特对象实例的120万个对象帧，涵盖2类（车辆和行人）</p><p>2030个片段，每个片段20秒，在不同的地理位置和条件下以10Hz（390000帧）采集,1个中距离激光雷达4个短程激光雷达5个摄像头（正面和侧面）同步激光雷达和相机数据激光雷达到摄像机的投影传感器校准和车辆姿态</p><h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><ol><li><a href="https://paperswithcode.com/task/3d-object-detection">3D Object Detection | Papers With Code</a></li><li><a href="https://github.com/patrick-llgc/Learning-Deep-Learning">patrick-llgc/Learning-Deep-Learning: Paper reading notes on Deep Learning and Machine Learning (github.com)</a></li><li><a href="https://www.stereolabs.com/docs/object-detection/">3D Object Detection Overview | Stereolabs</a></li><li><a href="https://zhuanlan.zhihu.com/p/591349104">系列二：3D Detection目标检测系列论文总结（2023年更） - 知乎 (zhihu.com)</a></li><li>3D点云<a href="https://github.com/HuangCongQing/3D-Point-Clouds">HuangCongQing/3D-Point-Clouds: 🔥3D点云目标检测&amp;语义分割(深度学习)-SOTA方法,代码,论文,数据集等 (github.com)</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知.&lt;/p&gt;</summary>
    
    
    
    
    <category term="Deep Learning" scheme="https://www.sekyoro.top/tags/Deep-Learning/"/>
    
    <category term="3D Object Detection" scheme="https://www.sekyoro.top/tags/3D-Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>后台执行:从nohup到tmux</title>
    <link href="https://www.sekyoro.top/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/"/>
    <id>https://www.sekyoro.top/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/</id>
    <published>2023-10-26T10:18:34.000Z</published>
    <updated>2023-10-26T12:36:29.012Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>远程连接到Linux服务器运行命令后,后面如果直接关掉终端会影响程序运行.这时可以使用nohup等命令用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行.<br><span id="more"></span></p><h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h3><blockquote><p><strong>nohup</strong> 英文全称 no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。</p><p><strong>nohup</strong> 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 <strong>$HOME/nohup.out</strong> 文件中</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python myprogramm.py &gt; output.txt</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep <span class="string">&quot;python&quot;</span> </span><br><span class="line"><span class="comment"># 查看相关进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 &lt;pid&gt;</span><br><span class="line"><span class="comment"># 如果要关闭该进程</span></span><br></pre></td></tr></table></figure><blockquote><p>使用nohup启动的程序会忽略hangup信号，hangup只是终止信号的一种，但是在关闭终端时，还会有其他的终止的信号，所以这时候往往需要配合 &amp; 一起使用，这样就可以做到不管是我们主动或者意外断开终端，程序依然能够继续运行。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python test.py &gt; output.txt &amp;</span><br></pre></td></tr></table></figure><h3 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h3><p>光是nohup功能太少了</p><blockquote><p>screen命令用于多重视窗管理程序。</p><p>screen为多重视窗管理程序。此处所谓的视窗，是指一个全屏幕的文字模式画面。通常只有在使用telnet登入主机或是使用老式的终端机时，才有可能用到screen程序</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install screen </span><br></pre></td></tr></table></figure><p>常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">screen -S <span class="comment">#创建新会话</span></span><br><span class="line">screen -ls <span class="comment">#显示所有会话</span></span><br><span class="line">screen -r <span class="comment">#进入该会话</span></span><br><span class="line">screen -d <span class="comment">#退出该会话 或者ctrl+a d</span></span><br><span class="line">screen -S screen_name -X quit <span class="comment">#删除会话</span></span><br><span class="line">screen -R 　<span class="comment">#先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业</span></span><br></pre></td></tr></table></figure><h3 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h3><p>tmux应该是用得比较多的工具了,不过操作还要复杂一些,支持分屏,分窗口以及类似screen得分session.</p><blockquote><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次”会话”（session）</strong> </p><p>会话的一个重要特点是，窗口与其中启动的进程是<a href="https://www.ruanyifeng.com/blog/2016/02/linux-daemon.html">连在一起</a>的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p><p>一个典型的例子就是，<a href="https://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html">SSH 登录</a>远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了</p></blockquote><p>tmux是一个 terminal multiplexer（终端复用器），它可以启动一系列终端会话。它解绑了会话和终端窗口。关闭终端窗口再打开，会话并不终止，而是继续运行在执行。将会话与终端窗后彻底分离。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure><h4 id="会话管理"><a href="#会话管理" class="headerlink" title="会话管理"></a>会话管理</h4><p>第一个启动的 Tmux 窗口，默认编号是<code>0</code>，第二个窗口的编号是<code>1</code>，以此类推。这些窗口对应的会话，就是 0 号会话、1 号会话。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s &lt;session-name&gt;</span><br><span class="line">tmux detach <span class="comment"># 分离会话 或者ctrl+b d</span></span><br><span class="line">tmux ls<span class="comment">#查看会话</span></span><br><span class="line">tmux attach -t [0|&lt;session_name&gt;] <span class="comment">#使用会话</span></span><br><span class="line">tmux kill-session -t [0|&lt;session_name&gt;] <span class="comment"># 删除会话</span></span><br><span class="line">tmux switch -t [0|&lt;session_name&gt;] <span class="comment"># 切换会话</span></span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b d</code>：分离当前会话。</li><li><code>Ctrl+b s</code>：列出所有会话。</li><li><code>Ctrl+b $</code>：重命名当前会话。</li></ul><h4 id="窗口管理"><a href="#窗口管理" class="headerlink" title="窗口管理"></a>窗口管理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tmux new-window -n &lt;window-name&gt; <span class="comment">#新建一个指定名称的窗口</span></span><br><span class="line">tmux select-window -t [&lt;window-name&gt;|&lt;window-number&gt;] <span class="comment">#选择窗口</span></span><br><span class="line">tmux rename-window &lt;new-name&gt; <span class="comment">#重命名当前window</span></span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b c</code>：创建一个新窗口，状态栏会显示多个窗口的信息。</li><li><code>Ctrl+b p</code>：切换到上一个窗口（按照状态栏上的顺序）。</li><li><code>Ctrl+b n</code>：切换到下一个窗口。</li><li><code>Ctrl+b &lt;number&gt;</code>：切换到指定编号的窗口，其中的<code>&lt;number&gt;</code>是状态栏上的窗口编号。</li><li><code>Ctrl+b w</code>：从列表中选择窗口。</li><li><code>Ctrl+b ,</code>：窗口重命名。</li></ul><h4 id="窗格管理"><a href="#窗格管理" class="headerlink" title="窗格管理"></a>窗格管理</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分上下两个窗格</span></span><br><span class="line">tmux split-window</span><br><span class="line"><span class="comment"># 划分左右两个窗格</span></span><br><span class="line">tmux split-window -h</span><br><span class="line"><span class="comment"># 当前窗格上移</span></span><br><span class="line">tmux swap-pane -U</span><br><span class="line"><span class="comment"># 当前窗格下移</span></span><br><span class="line">tmux swap-pane -D</span><br><span class="line"><span class="comment"># 光标切换到上方窗格</span></span><br><span class="line">tmux select-pane -U</span><br><span class="line"><span class="comment"># 光标切换到下方窗格</span></span><br><span class="line">tmux select-pane -D</span><br><span class="line"><span class="comment"># 光标切换到左边窗格</span></span><br><span class="line">tmux select-pane -L</span><br><span class="line"><span class="comment"># 光标切换到右边窗格</span></span><br><span class="line">tmux select-pane -R</span><br></pre></td></tr></table></figure><ul><li><code>Ctrl+b %</code>：划分左右两个窗格。</li><li><code>Ctrl+b &quot;</code>：划分上下两个窗格。</li><li><code>Ctrl+b &lt;arrow key&gt;</code>：光标切换到其他窗格。<code>&lt;arrow key&gt;</code>是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键<code>↓</code>。</li><li><code>Ctrl+b ;</code>：光标切换到上一个窗格。</li><li><code>Ctrl+b o</code>：光标切换到下一个窗格。</li><li><code>Ctrl+b &#123;</code>：当前窗格与上一个窗格交换位置。</li><li><code>Ctrl+b &#125;</code>：当前窗格与下一个窗格交换位置。</li><li><code>Ctrl+b Ctrl+o</code>：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。</li><li><code>Ctrl+b Alt+o</code>：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。</li><li><code>Ctrl+b x</code>：关闭当前窗格。</li><li><code>Ctrl+b !</code>：将当前窗格拆分为一个独立窗口。</li><li><code>Ctrl+b z</code>：当前窗格全屏显示，再使用一次会变回原来大小。</li><li><code>Ctrl+b Ctrl+&lt;arrow key&gt;</code>：按箭头方向调整窗格大小。</li><li><code>Ctrl+b q</code>：显示窗格编号。</li></ul><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有快捷键，及其对应的 Tmux 命令</span></span><br><span class="line">$ tmux list-keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有 Tmux 命令及其参数</span></span><br><span class="line">$ tmux list-commands</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出当前所有 Tmux 会话的信息</span></span><br><span class="line">$ tmux info</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载当前的 Tmux 配置</span></span><br><span class="line">$ tmux source-file ~/.tmux.conf</span><br></pre></td></tr></table></figure><p>我是推荐用screen或者tmux的,有条件用tmux.</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://blog.csdn.net/afterlake/article/details/105826424">nohup · VS · screen_screen nohup-CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/107802400">使用 screen 代替 nohup - 知乎 (zhihu.com)</a></li><li><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">Tmux 使用教程 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://zhuanlan.zhihu.com/p/98384704">tmux使用教程 - 知乎 (zhihu.com)</a></li><li><a href="https://blog.csdn.net/NSJim/article/details/127754413">Tmux使用教程_tmux 新建窗口-CSDN博客</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;远程连接到Linux服务器运行命令后,后面如果直接关掉终端会影响程序运行.这时可以使用nohup等命令用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行.&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>DDNLP:深入NLP</title>
    <link href="https://www.sekyoro.top/2023/10/23/DDNLP-%E6%B7%B1%E5%85%A5NLP/"/>
    <id>https://www.sekyoro.top/2023/10/23/DDNLP-%E6%B7%B1%E5%85%A5NLP/</id>
    <published>2023-10-23T02:31:33.000Z</published>
    <updated>2023-11-19T05:49:44.922Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前学过一段时间NLP,因为其中涉及到一些深度学习常用的知识或者框架,但苦于不系统以及没有任务focus不能长久.这里借助微软的教程写点东西.<br><span id="more"></span></p><h2 id="tokenization-amp-amp-representation"><a href="#tokenization-amp-amp-representation" class="headerlink" title="tokenization&amp;&amp;representation"></a>tokenization&amp;&amp;representation</h2><p>将一句话中的单词分割就是分词(tokenization),英文分词比较简单.中文就比较麻烦了.需要把握分词的粒度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tokenizer = torchtext.data.utils.get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">tokenizer(<span class="string">&#x27;He said: hello&#x27;</span>)</span><br></pre></td></tr></table></figure><p>分词之后就需要表示每个分词的含义了,<strong>需要某种方式将文本表示为张量</strong>.可以分为</p><ul><li>字符级表示(<strong>Character-level representation</strong>),当我们通过将每个字符视为一个数字来表示文本时。鉴于我们的文本语料库中有 C (如果是英语也就26个字符)不同的字符，单词 Hello 将由 5xC 张量表示。每个字母将对应于一个独热编码中的张量列。</li><li>单词级表示(<strong>Word-level representation</strong>),其中我们创建文本中所有单词的词汇表(<strong>vocabulary</strong> )，然后使用独热编码表示单词。这种方法在某种程度上更好，因为每个字母本身没有太多意义，因此通过使用更高层次的语义概念 - 单词 - 我们简化了神经网络的任务。但是，鉴于字典大小较大，我们需要处理高维稀疏张量。</li></ul><blockquote><p>无论表示方式如何，我们首先需要将文本转换为一系列标记(<strong>tokens</strong>)，一个标记是字符、单词，有时甚至是单词的一部分(也即是上面说的分词)</p><p>然后，我们将token转换为一个数字，通常使用词汇表(<strong>vocabulary</strong>)(也就是使用单词级表示)，并且可以使用独热编码(one-hot encoding)将这个数字输入神经网络。</p></blockquote><p>常用的方法包括BOW或者N-Grams</p><h4 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag-of-Words"></a>Bag-of-Words</h4><p>在解决文本分类等任务时，我们需要能够通过一个固定大小的向量来表示文本，我们将将其用作最终分类器的输入。</p><blockquote><p>最简单的方法之一是组合所有单独的单词表示，例如。通过添加它们。如果我们为每个单词添加独热编码，我们最终会得到一个频率向量，显示每个单词在文本中出现的次数。文本的这种表示称为词袋（BoW）</p><p>BoW 本质上表示文本中出现的单词和数量，这确实可以很好地指示文本的内容</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    counter.update(tokenizer(line))</span><br><span class="line">vocab = torchtext.vocab.vocab(counter, min_freq=<span class="number">1</span>)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Vocab size if <span class="subst">&#123;vocab_size&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">stoi = vocab.get_stoi() <span class="comment"># dict to convert tokens to indices</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [stoi[s] <span class="keyword">for</span> s <span class="keyword">in</span> tokenizer(x)]</span><br><span class="line"></span><br><span class="line">encode(<span class="string">&#x27;I love to play with my words&#x27;</span>)</span><br><span class="line"></span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_bow</span>(<span class="params">text,bow_vocab_size=vocab_size</span>):</span></span><br><span class="line">    res = torch.zeros(bow_vocab_size,dtype=torch.float32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> encode(text):</span><br><span class="line">        <span class="keyword">if</span> i&lt;bow_vocab_size:</span><br><span class="line">            res[i] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(to_bow(train_dataset[<span class="number">0</span>][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>简单来说就是根据原本的语义资料,统计词频先建立一个counter,类似于一个字典,key是词,value是频次.根据counter(或者OrderDict)建立一个vocab. vocab建立一个词汇到index的一个字典,然后根据这个字典获得一个词的index,但是并直接使用index作为词的表示,而是使用类似one-hot encoding,出现了一个词,获取其index,再在一个大小为vocab_size的tensor上的index处加1,这样一个句子的BOW就有了.</p><p><img data-src="https://i.imgur.com/MXAQdkP.png" alt="image-20231023115954650"></p><p><img data-src="https://i.imgur.com/G7ll59u.png" alt="image-20231023115905862"></p><p>BoW 的问题在于某些常用词，例如 and、is 等出现在大多数文本中，并且它们的频率最高，掩盖了真正重要的单词。我们可以通过考虑单词在整个文档集合中出现的频率来降低这些单词的重要性。</p><h4 id="N-Grams"><a href="#N-Grams" class="headerlink" title="N-Grams"></a>N-Grams</h4><p>在自然语言中，单词的精确含义只能在上下文中确定。例如，神经网和钓鱼网.</p><p>一种解决办法是使用单词对(pairs of words)(也就是不使用单个单词而是多个单词,因为单个单词在不同语境下含义由差异),然后将单词对(pairs of words)视为单独的词汇标记。</p><p>这样相当于把一个句子的表示变多了,除了所有单个单词,还有单词对.</p><p>这种方法的问题在于字典大小显着增长，并且像go fishing和go shopping这样的组合由不同的标记呈现，尽管动词相同，但它们没有任何语义相似性。</p><blockquote><p>在某些情况下，我们也可以考虑使用三元语法 - 三个单词的组合。因此，这种方法通常被称为n-grams。此外，使用具有<strong>字符级表示的 n 元语法</strong>是有意义的，在这种情况下，n-gram 将大致对应于不同的音节。</p></blockquote><p>可以使用sklearn或者pytorch库,均能实现.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bigram_vectorizer = CountVectorizer(ngram_range=(<span class="number">1</span>, <span class="number">2</span>), token_pattern=<span class="string">r&#x27;\b\w+\b&#x27;</span>, min_df=<span class="number">1</span>)</span><br><span class="line">corpus = [</span><br><span class="line">        <span class="string">&#x27;I like hot dogs.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The dog ran fast.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Its hot outside.&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">bigram_vectorizer.fit_transform(corpus)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Vocabulary:\n&quot;</span>,bigram_vectorizer.vocabulary_)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(bigram_vectorizer.vocabulary_))</span><br><span class="line">bigram_vectorizer.transform([<span class="string">&#x27;My dog likes hot dogs on a hot day.&#x27;</span>]).toarray()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/ES1UYjS.png" alt="image-20231023121801392"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    l = tokenizer(line)</span><br><span class="line">    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">bi_vocab = torchtext.vocab.vocab(counter, min_freq=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bigram vocabulary length = &quot;</span>,<span class="built_in">len</span>(bi_vocab))</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/LvNDH2u.png" alt="image-20231023120608981"></p><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>在 BoW 表示中，无论单词本身如何，单词出现次数都是均匀加权的。但是，很明显，与专业术语相比，常用词（例如a，in等）对于分类的重要性要低得多。事实上，在大多数NLP任务中，有些单词比其他单词更相关。</p><p>TF-IDF 代表术语频率 – 反向文档频率。它是BOW的变体，其中使用浮点值而不是指示单词在文档中出现的二进制 0/1 值，这与语料库中单词出现的频率有关。</p><p>主要引入了document文档概念,如果一个词在多个文档中出现,那么其权重会降低.</p><p><img data-src="https://i.imgur.com/BLTg4Tp.png" alt="image-20231023120917811"></p><p>其中tf~ij~表示在j文档中i词出现的次数,N表示总文档数,df~i~表示出现i这个词的文档数.</p><p>这样就计算出了单个文档中词i的权重,这里的文档也可以是单个句子.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">corpus = [</span><br><span class="line">        <span class="string">&#x27;I like hot dogs.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;The dog ran fast.&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Its hot outside.&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line">vectorizer = TfidfVectorizer(ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">vectorizer.fit_transform(corpus)</span><br><span class="line">vectorizer.transform([<span class="string">&#x27;My dog likes hot dogs on a hot day.&#x27;</span>]).toarray()</span><br></pre></td></tr></table></figure><p>这里结合了N-gram和TF-IDF. 由于其中使用了TfidfVectorizer,默认参数如下</p><p><img data-src="https://i.imgur.com/W998L4F.png" alt="image-20231023122145845"></p><p>将其中的<code>I,I like</code>去掉了,所以词汇表少了两个.此外sklearn库中的算法与上面的公式也不同.默认为log [ n / df(t) ] + 1(设置<code>smooth_idf=False</code>)</p><p>上面的方法对于句子中词的语义理解能力有限,而且通常维度是整个训练资料的vocab大小,维度高且稀疏.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>嵌入的想法是通过<strong>低维密集向量</strong>来表示单词，这在某种程度上<strong>反映了单词的语义</strong>含义。</p><p>也就是从上面简单的text representation中的vocab_size变为embedding_size,输出不是one hot encoding的高维向量了。</p><p>训练方式与BOW类似,但是需要填充.比如一个batch中有多个句子,每个句子长度不同,需要padding成这个batch中最大的句子的encode(就是计算BOW)长度.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.fc = torch.nn.Linear(embed_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;after embedding&quot;</span>,x.shape)</span><br><span class="line">        x = torch.mean(x,dim=<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(x.shape)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padify</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># b is the list of tuples of length batch_size</span></span><br><span class="line">    <span class="comment">#   - first element of a tuple = label,</span></span><br><span class="line">    <span class="comment">#   - second = feature (text sequence)</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># first, compute max length of a sequence in this minibatch</span></span><br><span class="line">    l = <span class="built_in">max</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of two tensors - labels and features</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=padify, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><blockquote><p>需要将所有序列填充到相同的长度，以便将它们放入小批量中。这不是表示可变长度序列的最有效方法.</p><p>另一种选择是使用偏移向量，这将保留存储在一个大向量中的所有序列的偏移量。</p></blockquote><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="constructor">EmbedClassifier(<span class="params">torch</span>.<span class="params">nn</span>.Module)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>, <span class="params">vocab_size</span>, <span class="params">embed_dim</span>, <span class="params">num_class</span>)</span>:</span><br><span class="line">        super<span class="literal">()</span>.<span class="constructor">__init__()</span></span><br><span class="line">        self.embedding = torch.nn.<span class="constructor">EmbeddingBag(<span class="params">vocab_size</span>, <span class="params">embed_dim</span>)</span></span><br><span class="line">        self.fc = torch.nn.<span class="constructor">Linear(<span class="params">embed_dim</span>, <span class="params">num_class</span>)</span></span><br><span class="line"></span><br><span class="line">    def forward(self, text, off):</span><br><span class="line">        x = self.embedding(text, off) <span class="comment">//它以内容向量和偏移向量为输入</span></span><br><span class="line">        return self.fc(x)</span><br><span class="line">        </span><br><span class="line">        def offsetify(b):</span><br><span class="line">    # first, compute data tensor from all sequences</span><br><span class="line">    x = <span class="literal">[<span class="identifier">torch</span>.<span class="identifier">tensor</span>(<span class="identifier">encode</span>(<span class="identifier">t</span>[<span class="number">1</span>]</span>)) <span class="keyword">for</span> t <span class="keyword">in</span> b]</span><br><span class="line">    # now, compute the offsets by accumulating the tensor <span class="keyword">of</span> sequence lengths</span><br><span class="line">    o = <span class="literal">[<span class="number">0</span>]</span> + <span class="literal">[<span class="identifier">len</span>(<span class="identifier">t</span>) <span class="identifier">for</span> <span class="identifier">t</span> <span class="identifier">in</span> <span class="identifier">x</span>]</span></span><br><span class="line">    o = torch.tensor(o<span class="literal">[:-<span class="number">1</span>]</span>).cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    return (</span><br><span class="line">        torch.<span class="constructor">LongTensor([<span class="params">t</span>[0]-1 <span class="params">for</span> <span class="params">t</span> <span class="params">in</span> <span class="params">b</span>])</span>, # labels</span><br><span class="line">        torch.cat(x), # text</span><br><span class="line">        o</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.<span class="constructor">DataLoader(<span class="params">train_dataset</span>, <span class="params">batch_size</span>=16, <span class="params">collate_fn</span>=<span class="params">offsetify</span>, <span class="params">shuffle</span>=True)</span></span><br></pre></td></tr></table></figure><p>可以看到数据集多了一个数据,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">net = EmbedClassifier(vocab_size,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_emb</span>(<span class="params">net,dataloader,lr=<span class="number">0.01</span>,optimizer=<span class="literal">None</span>,loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>),epoch_size=<span class="literal">None</span>, report_freq=<span class="number">200</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(),lr=lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line">    total_loss,acc,count,i = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> labels,text,off <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        labels,text,off = labels.to(device), text.to(device), off.to(device)</span><br><span class="line">        out = net(text, off)</span><br><span class="line">        loss = loss_fn(out,labels) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss+=loss</span><br><span class="line">        _,predicted = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        acc+=(predicted==labels).<span class="built_in">sum</span>()</span><br><span class="line">        count+=<span class="built_in">len</span>(labels)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i%report_freq==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;count&#125;</span>: acc=<span class="subst">&#123;acc.item()/count&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch_size <span class="keyword">and</span> count&gt;epoch_size:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/count, acc.item()/count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_epoch_emb(net,train_loader, lr=<span class="number">4</span>, epoch_size=<span class="number">25000</span>)</span><br></pre></td></tr></table></figure><p>在前面的示例中，模型嵌入层学习将单词映射到向量表示，但是这种表示没有太多的语义意义。应该学到的是:相似的单词或同义词将对应于在某些向量距离（例如欧几里得距离）方面彼此接近的向量</p><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>为此，我们需要以特定方式在大量文本上预训练我们的嵌入模型。</p><p>训练语义嵌入的第一种方法称为Word2Vec。它基于两个主要体系结构,用于生成单词的分布式表示,包括COW和Skip-Ngram.</p><p><img data-src="https://i.imgur.com/pOIIEEj.png" alt="image-20231023164434940"></p><p>在CBOW，我们训练模型从周围上下文中预测单词。给定 ngram （W−2，W−1，W0，W1，W2），模型的目标是从 （W−2，W−1，W1，W2） 预测 W0。</p><h4 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h4><p>通过学习每个单词的向量表示以及每个单词中的字符 n 元语法来构建 Word2Vec。然后在每个训练步骤中将表示值平均为一个向量。虽然这为预训练增加了大量额外的计算，但它使词嵌入能够对子词信息进行编码。</p><h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p>GloVe利用分解共现矩阵( co-occurrence matrix)的思想，使用神经方法将共现矩阵分解为更具表现力和非线性的词向量。</p><p><img data-src="https://i.imgur.com/zKMz0Hk.png" alt="image-20231023203859837"></p><p>传统的预训练嵌入表示（如 Word2Vec）的一个关键限制是词义消歧问题。虽然预训练嵌入可以在上下文中捕获单词的某些含义，但单词的每个可能含义都编码到相同的嵌入中。这可能会导致下游模型中出现问题，因为许多单词（例如“play”）具有不同的含义，具体取决于它们使用的上下文。</p><p>为了克服这个限制，我们需要基于语言模型构建嵌入，该语言模型在大量文本语料库上进行训练，并且知道如何在不同上下文中将单词组合在一起(我的理解是相当于自己训练一个专注于自己下游任务的embedding)</p><h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>语言建模背后的主要思想是以<strong>无监督的方式在未标记的数据集上训练它们</strong>。这很重要，因为我们有大量的未标记文本可用，而标记文本的数量始终受到我们可以在标记上花费的工作量的限制。</p><blockquote><p>大多数情况下，我们可以构建可以<strong>预测文本中缺失单词的语言模型</strong>，因为很容易屏蔽文本中的随机单词并将其用作训练样本.</p></blockquote><p>为了建立一个网络来预测下一个单词，我们需要提供相邻单词作为输入，并获取单词编号作为输出。</p><p>CBoW网络的架构如下：</p><p>输入单词通过嵌入层传递。这个嵌入层将是我们的 Word2Vec 嵌入，因此我们将它单独定义为嵌入变量。在这个例子中，我们将使用嵌入大小 = 30，即使你可能想尝试更高的维度（真正的 word2vec 有 300）</p><p>然后，嵌入向量将被传递到预测输出字的线性层。因此它具有vocab_size神经</p><p><img data-src="https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png" alt="word2vec_skipgrams" style="zoom:67%;" /></p><p><img data-src="https://i.imgur.com/GgvayYN.png" alt="image-20231023195917251"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">ngrams = <span class="number">1</span>, min_freq = <span class="number">1</span>, vocab_size = <span class="number">5000</span> , lines_cnt = <span class="number">500</span></span>):</span></span><br><span class="line">    tokenizer = torchtext.data.utils.get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading dataset...&quot;</span>)</span><br><span class="line">    test_dataset, train_dataset  = torchtext.datasets.AG_NEWS(root=<span class="string">&#x27;./data&#x27;</span>)</span><br><span class="line">    train_dataset = <span class="built_in">list</span>(train_dataset)</span><br><span class="line">    test_dataset = <span class="built_in">list</span>(test_dataset)</span><br><span class="line">    classes = [<span class="string">&#x27;World&#x27;</span>, <span class="string">&#x27;Sports&#x27;</span>, <span class="string">&#x27;Business&#x27;</span>, <span class="string">&#x27;Sci/Tech&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Building vocab...&#x27;</span>)</span><br><span class="line">    counter = collections.Counter()</span><br><span class="line">    <span class="keyword">for</span> i, (_, line) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=ngrams))</span><br><span class="line">        <span class="keyword">if</span> i == lines_cnt:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    vocab = torchtext.vocab.Vocab(collections.Counter(<span class="built_in">dict</span>(counter.most_common(vocab_size))))</span><br><span class="line">    <span class="keyword">return</span> train_dataset, test_dataset, classes, vocab, tokenizer</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">x, vocabulary, tokenizer = tokenizer</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [vocabulary[s] <span class="keyword">for</span> s <span class="keyword">in</span> tokenizer(x)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_cbow</span>(<span class="params">sent,window_size=<span class="number">2</span></span>):</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(sent):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(<span class="number">0</span>,i-window_size),<span class="built_in">min</span>(i+window_size+<span class="number">1</span>,<span class="built_in">len</span>(sent))):</span><br><span class="line">            <span class="keyword">if</span> i!=j:</span><br><span class="line">                res.append([sent[j],x])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(to_cbow([<span class="string">&#x27;I&#x27;</span>,<span class="string">&#x27;like&#x27;</span>,<span class="string">&#x27;to&#x27;</span>,<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;networks&#x27;</span>]))</span><br><span class="line"><span class="built_in">print</span>(to_cbow(encode(<span class="string">&#x27;I like to train networks&#x27;</span>, vocab)))</span><br></pre></td></tr></table></figure><p>在设计数据集的时候,得到的就是例如[2,3],[4,3],其中3是预测的词,2,4是其周围的词,这样也不需要padding了.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleIterableDataset</span>(<span class="params">torch.utils.data.IterableDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, X, Y</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleIterableDataset).__init__()</span><br><span class="line">        self.data = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            self.data.append( (Y[i], X[i]) )</span><br><span class="line">        random.shuffle(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(self.data)</span><br><span class="line"></span><br><span class="line">ds = SimpleIterableDataset(X, Y)</span><br><span class="line">dl = torch.utils.data.DataLoader(ds, batch_size = <span class="number">256</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span>(<span class="params">net, dataloader, lr = <span class="number">0.01</span>, optimizer = <span class="literal">None</span>, loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>), epochs = <span class="literal">None</span>, report_freq = <span class="number">1</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(), lr = lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        total_loss, j = <span class="number">0</span>, <span class="number">0</span>, </span><br><span class="line">        <span class="keyword">for</span> labels, features <span class="keyword">in</span> dataloader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            features, labels = features.to(device), labels.to(device)</span><br><span class="line">            out = net(features)</span><br><span class="line">            loss = loss_fn(out, labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i % report_freq == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: loss=<span class="subst">&#123;total_loss.item()/j&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/j</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/0xvQ6IJ.png" alt="image-20231023201214794"></p><p>关键是生成了一堆数据,句子中的某个单词由周围N个单词生成(CBOW).模型是简单的embedding层加一个全连接,输出特征大小是vocab_size,用softmax损失,最后就能无监督训练得到一个embedding层.</p><h2 id="RNN-Recurrent-Neural-Networks"><a href="#RNN-Recurrent-Neural-Networks" class="headerlink" title="RNN(Recurrent Neural Networks)"></a>RNN(Recurrent Neural Networks)</h2><blockquote><p>之前直接使用的是全连接层,这种架构的作用是捕获句子中单词的聚合含义，但它没有考虑单词的顺序，因为嵌入之上的聚合操作从原始文本中删除了此信息。由于这些模型无法对单词排序进行建模，因此它们无法解决更复杂或模糊的任务，例如文本生成或问答。</p></blockquote><p>给定标记 X~0~,…,X~n~ 的输入序列，RNN 创建一个神经网络块序列，并使用反向传播端到端地训练该序列。每个网络块将一对（X~i~，S~i~）作为输入，并产生S~i+1~。最终状态 S~n~ 或（输出 Y~n~）进入线性分类器以产生结果。所有网络块共享相同的权重，并使用一个反向传播通道进行端到端训练。</p><p>为了捕捉文本序列的含义，我们需要使用另一种神经网络架构，称为递归神经网络或RNN。在 RNN 中，我们通过<strong>网络一次传递一个符号，网络产生一些状态，然后我们用下一个符号再次传递给网络</strong>。</p><p><img data-src="https://i.imgur.com/FwxIpWX.png" alt="image-20231023222833797"></p><p>pytorch中普通RNN隐状态通过了tanh激活,每一层的隐状态与输出是一样.</p><p>RNN循环网络是每次拿每个batch中的一个sequence中的一个,大小是embed_size(或者直接是one-hot编码的vacab_size,同时可以输入一个初始状态,shape是hidden_size,然后两个矩阵分别是(embed_size,hidden_size),(hidden_size,hidden_size),其实就是连个全连接然后直接concat通过激活函数,这就是简单的RNN,),</p><p><img data-src="https://i.imgur.com/5uUeVVj.png" alt="image-20231023224359316"></p><p>对于一个句子的数据,X是(seq_length,embedding_size),权重W是(embedding_size,hidden_dim),H是(hidden_dim,hidden_dim),S是(seq_length,hidden_dim),S是上一层的输出,也就是W×X~i~+H×S~i-1~+b.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/images/rnn.png" alt="RNN"></p><p>由于状态向量 S0,…,Sn 通过网络传递，因此它能够学习单词之间的顺序依赖关系。例如，当单词没有出现在序列中的某个地方时，它可以学习否定状态向量中的某些元素，从而导致否定.</p><p><strong>RNN内部结构</strong></p><p><img data-src="https://i.imgur.com/dk3vOfq.png" alt="image-20231023212717744"></p><p>简单的RNN接受先前的状态 S~i-1~和当前符号 X~i~作为输入，并且必须产生输出状态 S~i~（有时，我们也对其他一些输出 Y~i~ 感兴趣，例如生成网络的情况）</p><p><img data-src="https://i.imgur.com/YwIbQc4.jpg" alt="img"></p><blockquote><p>注意,上面的seq_length是输入的长度,但并不是每一句的长度,因为每一句长度很可能不一样,这样RNN无法计算,是一个batch中vocab最大的长度,也就是在一个batch中padding到最大长度</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padify</span>(<span class="params">b,voc=<span class="literal">None</span>,tokenizer=tokenizer</span>):</span></span><br><span class="line">    <span class="comment"># b is the list of tuples of length batch_size</span></span><br><span class="line">    <span class="comment">#   - first element of a tuple = label, </span></span><br><span class="line">    <span class="comment">#   - second = feature (text sequence)</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>],voc=voc,tokenizer=tokenizer) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch</span></span><br><span class="line">    l = <span class="built_in">max</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of two tensors - labels and features</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v])</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=padify, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>在许多情况下，输入token在进入 RNN 之前通过嵌入层以降低维度。每一层输出是σ(W×X~i~+H×S~i-1~+b)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">100</span>   <span class="comment"># 输入数据编码的维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>   <span class="comment"># 隐含层维度</span></span><br><span class="line">num_layers = <span class="number">4</span>     <span class="comment"># 隐含层层数</span></span><br><span class="line"></span><br><span class="line">rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rnn:&quot;</span>,rnn)</span><br><span class="line"></span><br><span class="line">seq_len = <span class="number">10</span>        <span class="comment"># 句子长度</span></span><br><span class="line">batch_size = <span class="number">1</span>      </span><br><span class="line">x = torch.randn(seq_len,batch_size,input_size)        <span class="comment"># 输入数据</span></span><br><span class="line">h0 = torch.zeros(num_layers,batch_size,hidden_size)   <span class="comment"># 输入数据</span></span><br><span class="line"></span><br><span class="line">out, h = rnn(x, h0)  <span class="comment"># 输出数据</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;out.shape:&quot;</span>,out.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;h.shape:&quot;</span>,h.shape)</span><br></pre></td></tr></table></figure><p>注意,pytorch RNN默认输入数据是(seq_length,batch_size,embedding_size),除非设置<code>batch_first=True</code></p><h4 id="LSTM-amp-amp-GRU"><a href="#LSTM-amp-amp-GRU" class="headerlink" title="LSTM&amp;&amp;GRU"></a>LSTM&amp;&amp;GRU</h4><p><img data-src="https://i.imgur.com/VYCgrZW.png" alt="image-20231023214516563"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-<span class="number">0.5</span></span><br><span class="line">        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x,(h,c) = self.rnn(x)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">net = LSTMClassifier(vocab_size,<span class="number">64</span>,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">train_epoch(net,train_loader, lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><p>LSTM增加了三个门用来控制隐状态,输入.</p><ul><li>忘记门采用隐藏的向量并确定我们需要忘记向量 c 的哪些分量，以及要通过哪些分量。</li><li>输入门从输入和隐藏向量中获取一些信息，并将其插入状态。</li><li>输出门通过具有tanh激活的某个线性层转换状态，然后使用隐藏向量H~i~选择其部分组件以产生新的状态c~i+1~。</li></ul><p>而GRU结构要简单一些,支持隐状态的门控. 重置门允许我们控制“可能还想记住”的过去状态的数量, 更新门将允许我们控制新状态中有多少个是旧状态的副本。</p><p><img data-src="https://i.imgur.com/v83gWCu.png" alt="image-20231023214736240"></p><p><img data-src="https://i.imgur.com/eMlaE2s.png" alt="image-20231023214808081"></p><p><img data-src="https://i.imgur.com/ARG5B52.png" alt="image-20231026095926569"></p><p><img data-src="https://i.imgur.com/r4xeKjo.png" alt="image-20231023214906219"></p><p><img data-src="C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231023214919424.png" alt="image-20231023214919424"></p><p><img data-src="https://i.imgur.com/8r65rwW.png" alt="image-20231023214931931"></p><h4 id="PACKED-SEQUENCE"><a href="#PACKED-SEQUENCE" class="headerlink" title="PACKED SEQUENCE"></a>PACKED SEQUENCE</h4><p>填充一批可变长度序列</p><p>我们必须用零向量填充小批量中的所有序列。虽然这会导致一些内存浪费，但对于 RNN,为填充的输入项创建额外的 RNN 单元更为重要，这些输入项参与训练，但不携带任何重要的输入信息。<strong>仅将 RNN 训练到实际序列大小会好得多</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line">seq = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">lens = [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">packed = pack_padded_sequence(seq, lens, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">packed</span><br><span class="line">seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=<span class="literal">True</span>)</span><br><span class="line">seq_unpacked</span><br><span class="line">lens_unpacked</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_length</span>(<span class="params">b</span>):</span></span><br><span class="line">    <span class="comment"># build vectorized sequence</span></span><br><span class="line">    v = [encode(x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> b]</span><br><span class="line">    <span class="comment"># compute max length of a sequence in this minibatch and length sequence itself</span></span><br><span class="line">    len_seq = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">len</span>,v))</span><br><span class="line">    l = <span class="built_in">max</span>(len_seq)</span><br><span class="line">    <span class="keyword">return</span> ( <span class="comment"># tuple of three tensors - labels, padded features, length sequence</span></span><br><span class="line">        torch.LongTensor([t[<span class="number">0</span>]-<span class="number">1</span> <span class="keyword">for</span> t <span class="keyword">in</span> b]),</span><br><span class="line">        torch.stack([torch.nn.functional.pad(torch.tensor(t),(<span class="number">0</span>,l-<span class="built_in">len</span>(t)),mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> v]),</span><br><span class="line">        torch.tensor(len_seq)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">16</span>, collate_fn=pad_length, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMPackClassifier</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim, hidden_dim, num_class</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-<span class="number">0.5</span></span><br><span class="line">        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, lengths</span>):</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=<span class="literal">True</span>,enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        pad_x,(h,c) = self.rnn(pad_x)</span><br><span class="line">        x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x,batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.fc(h[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">net = LSTMPackClassifier(vocab_size,<span class="number">64</span>,<span class="number">32</span>,<span class="built_in">len</span>(classes)).to(device)</span><br><span class="line">train_epoch_emb(net,train_loader_len, lr=<span class="number">0.001</span>,use_pack_sequence=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>要生成打包序列，我们可以使用<code>torch.nn.utils.rnn.pack_padded_sequence</code>函数。所有循环层，包括RNN，LSTM和GRU，都支持打包序列作为输入，并产生可以使用<code>torch.nn.utils.rnn.pad_packed_sequence</code>解码打包输出。</p><p>训练时,传入<code>len_seq = list(map(len,v))</code>,使用<code>torch.nn.utils.rnn.pack_padded_sequence</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=<span class="literal">True</span>,enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">pad_x,(h,c) = self.rnn(pad_x)</span><br></pre></td></tr></table></figure><p>再使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x,batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>可以解码打包的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch_emb</span>(<span class="params">net,dataloader,lr=<span class="number">0.01</span>,optimizer=<span class="literal">None</span>,loss_fn = torch.nn.CrossEntropyLoss(<span class="params"></span>),epoch_size=<span class="literal">None</span>, report_freq=<span class="number">200</span>,use_pack_sequence=<span class="literal">False</span></span>):</span></span><br><span class="line">    optimizer = optimizer <span class="keyword">or</span> torch.optim.Adam(net.parameters(),lr=lr)</span><br><span class="line">    loss_fn = loss_fn.to(device)</span><br><span class="line">    net.train()</span><br><span class="line">    total_loss,acc,count,i = <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> labels,text,off <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        labels,text = labels.to(device), text.to(device)</span><br><span class="line">        <span class="keyword">if</span> use_pack_sequence:</span><br><span class="line">            off = off.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            off = off.to(device)</span><br><span class="line">        out = net(text, off)</span><br><span class="line">        loss = loss_fn(out,labels) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss+=loss</span><br><span class="line">        _,predicted = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        acc+=(predicted==labels).<span class="built_in">sum</span>()</span><br><span class="line">        count+=<span class="built_in">len</span>(labels)</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i%report_freq==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;count&#125;</span>: acc=<span class="subst">&#123;acc.item()/count&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> epoch_size <span class="keyword">and</span> count&gt;epoch_size:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> total_loss.item()/count, acc.item()/count</span><br></pre></td></tr></table></figure><blockquote><p>目前，pack_padded_sequence函数要求长度序列张量位于CPU设备上，因此训练函数在训练时需要避免将长度序列数据移动到GPU。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> use_pack_sequence:</span><br><span class="line">            off = off.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            off = off.to(device)</span><br></pre></td></tr></table></figure><h4 id="Bidirectional-and-Multilayer-RNNs"><a href="#Bidirectional-and-Multilayer-RNNs" class="headerlink" title="Bidirectional and Multilayer RNNs"></a>Bidirectional and Multilayer RNNs</h4><p>由于在许多实际情况下，我们可以随机访问输入序列，因此在两个方向上运行循环计算可能是有意义的。这样的网络被称为双向RNN。在处理双向网络时，我们需要两个隐藏状态向量，每个方向一个。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg" alt="Image showing a Multilayer long-short-term-memory- RNN"></p><p>与卷积网络一样，可以在第一层之上构建另一个循环层，以捕获更高级别的模式，并从第一层提取的低级模式进行构建。这导致我们得出多层RNN的概念，它由两个或多个循环网络组成，其中前一层的输出作为输入传递到下一层。</p><h2 id="GRN-Generative-Recurrent-Networks"><a href="#GRN-Generative-Recurrent-Networks" class="headerlink" title="GRN(Generative Recurrent Networks)"></a>GRN(Generative Recurrent Networks)</h2><p>递归神经网络（RNN）及其门控细胞变体，如长短期记忆细胞（LSTM）和门控循环单元（GRU）为语言建模提供了一种机制，因为它们可以学习单词顺序并为序列中的下一个单词提供预测。这使我们能够将 RNN 用于生成任务，例如<strong>普通文本生成、机器翻译，甚至图像字幕</strong>。</p><p>每个 RNN 单元产生下一个隐藏状态作为输出。但是，我们也可以为每个循环单元添加另一个输出，这将允许我们输出一个序列（长度等于原始序列）。此外，我们可以使用在每一步都不接受输入的 RNN 单元，只需获取一些初始状态向量，然后生成一系列输出。分别对应多对多与一对多.</p><p><img data-src="https://i.imgur.com/srjJVXN.png" alt="image-20231023223320804"></p><p><img data-src="https://i.imgur.com/SeTNrFy.png" alt="image-20231023223412819"></p><ul><li>一对一是一个输入和一个输出的传统神经网络</li><li>一对多是一种生成式体系结构，它接受一个输入值，并生成一系列输出值。例如，如果我们想训练一个图像字幕网络来生成图片的文本描述，我们可以将图片作为输入，通过CNN传递以获得其隐藏状态，然后让循环链逐字生成标题。</li><li>多对一对应于我们在上一个单元中描述的 RNN 架构，例如文本分类</li><li>多对多或<strong>序列到序列</strong>对应于机器翻译等任务，其中我们首先让 RNN 将所有信息从输入序列收集到隐藏状态，另一个 RNN 链将此状态展开到输出序列中。</li></ul><p>对于生成序列任务而言,在每一步中，我们将获取长度为 nchars 的字符序列，并要求网络为每个输入字符生成下一个输出字符</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate.png" alt="Image showing an example RNN generation of the word &#39;HELLO&#39;."></p><p>在生成文本时（在推理过程中），从一些提示开始，该提示通过 RNN 单元格生成其中间状态，然后从该状态开始生成。我们一次生成一个字符，并将状态和生成的字符传递给另一个 RNN 单元以生成下一个，直到我们生成足够的字符。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/images/rnn-generate-inf.png" alt="img"></p><p>这样需要添加一些特殊字符表明开始与结尾,比如\<eos>(在训练数据中添加).</p><p>如果只需要无穷的生成字符,只需要固定序列大小,比如为nchars,在l长的序列中就有l-nchars这么多个数据.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">char_tokenizer</span>(<span class="params">words</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(words) <span class="comment">#[word for word in words]</span></span><br><span class="line"></span><br><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">for</span> (label, line) <span class="keyword">in</span> train_dataset:</span><br><span class="line">    counter.update(char_tokenizer(line))</span><br><span class="line">vocab = torchtext.vocab.vocab(counter)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enc</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(encode(x,voc=vocab,tokenizer=char_tokenizer))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span>(<span class="params">s,nchars=nchars</span>):</span></span><br><span class="line">    ins = torch.zeros(<span class="built_in">len</span>(s)-nchars,nchars,dtype=torch.long,device=device)</span><br><span class="line">    outs = torch.zeros(<span class="built_in">len</span>(s)-nchars,nchars,dtype=torch.long,device=device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)-nchars):</span><br><span class="line">        ins[i] = enc(s[i:i+nchars])</span><br><span class="line">        outs[i] = enc(s[i+<span class="number">1</span>:i+nchars+<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> ins,outs <span class="comment"># 获得成对的数据 每个数据长度nchars</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">net,size=<span class="number">100</span>,start=<span class="string">&#x27;today &#x27;</span></span>):</span></span><br><span class="line">        chars = <span class="built_in">list</span>(start)</span><br><span class="line">        out, s = net(enc(chars).view(<span class="number">1</span>,-<span class="number">1</span>).to(device))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            nc = torch.argmax(out[<span class="number">0</span>][-<span class="number">1</span>])</span><br><span class="line">            chars.append(vocab.get_itos()[nc])</span><br><span class="line">            out, s = net(nc.view(<span class="number">1</span>,-<span class="number">1</span>),s)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br></pre></td></tr></table></figure><p>因为网络以字符作为输入，词汇量很小，我们不需要嵌入层，独热编码输入可以直接进入LSTM单元。</p><p>但是，由于我们将字符号作为输入传递，因此我们需要在传递给 LSTM 之前对它们进行独热编码。输出编码器将是一个线性层，它将隐藏状态转换为独热编码输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMGenerator</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = torch.nn.Linear(hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, s=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)</span><br><span class="line">        x,s = self.rnn(x,s)</span><br><span class="line">        <span class="keyword">return</span> self.fc(x),s</span><br></pre></td></tr></table></figure><blockquote><p>在训练期间希望能够对生成的文本进行采样。定义 generate 函数，该函数将从初始字符串开始生成长度大小的输出字符串。</p></blockquote><p>首先将通过传递整个起始字符串，并取出输出状态 s 和下一个预测字符。由于 out 是独热编码的，我们采用 argmax 来获取词汇表中字符 nc 的索引，并使用 itos 找出实际字符并将其附加到生成的字符字符列表中。生成一个字符的过程是重复<code>size</code>次数以生成所需数量的字符。</p><p>说人话就是,搭建的模型输出shape是vacab_size(就是RNN或者LSTM的输出),其中最大值的index就是vocab的index.使用交叉熵损失,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">net = LSTMGenerator(vocab_size,<span class="number">64</span>).to(device)</span><br><span class="line"></span><br><span class="line">samples_to_train = <span class="number">10000</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(),<span class="number">0.01</span>)</span><br><span class="line">loss_fn = torch.nn.CrossEntropyLoss()</span><br><span class="line">net.train()</span><br><span class="line"><span class="keyword">for</span> i,x <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">    <span class="comment"># x[0] is class label, x[1] is text</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(x[<span class="number">1</span>])-nchars&lt;<span class="number">10</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    samples_to_train-=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> samples_to_train: <span class="keyword">break</span></span><br><span class="line">    text_in, text_out = get_batch(x[<span class="number">1</span>])</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out,s = net(text_in)</span><br><span class="line">    loss = torch.nn.functional.cross_entropy(out.view(-<span class="number">1</span>,vocab_size),text_out.flatten()) <span class="comment">#cross_entropy(out,labels)</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1000</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Current loss = <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(generate(net))</span><br></pre></td></tr></table></figure><p>可以改进的地方</p><ol><li>我们准备训练数据的方式是从一个样本生成一个小批量。这并不理想，因为<strong>小批量的大小都不同，其中一些甚至无法生成</strong>，因为文本小于 nchars。此外，<strong>小批量不能充分利用GPU</strong>。更明智的做法是从<strong>所有样本中获取一大块文本，然后生成所有输入输出对，打乱它们，并生成大小相等的小批量</strong>。</li><li><strong>多层 LSTM</strong>。尝试 2 或 3 层 LSTM 单元是有意义的。正如我们在上一个单元中提到的，LSTM 的每一层都从文本中提取某些模式，在字符级生成器的情况下，我们可以期望较低的 LSTM 级别负责提取音节，而较高的级别负责提取单词和单词组合。这可以通过将层数参数传递给 LSTM 构造函数来简单地实现。</li></ol><h4 id="soft-text-generation-and-temperature"><a href="#soft-text-generation-and-temperature" class="headerlink" title="soft text generation and temperature"></a>soft text generation and temperature</h4><p>在前面的 generate 定义中，我们始终将概率最高的字符作为生成文本中的下一个字符。这导致文本经常一次又一次地在相同的字符序列之间“循环”(来回就是那那几个字符,类似石头剪刀布,石头经常赢剪刀,剪刀经常赢布)</p><p>但是，如果我们看一下下一个字符的概率分布，可能是几个最高概率之间的差异并不大，例如一个字符的概率为 0.2，另一个字符的概率为 0.19，等等。例如，当在序列“play”中查找下一个字符时，下一个字符同样可以是空格或e。</p><p>所以选择概率较高的字符并不总是“公平的”，因为选择第二高的字符仍可能使我们获得有意义的文本。从网络输出给出的概率分布中对字符进行采样更为明智。可以使用实现所谓多项式分布的多项式函数( <strong>multinomial distribution</strong>)进行此采样。实现此软文本生成的函数定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_soft</span>(<span class="params">net,size=<span class="number">100</span>,start=<span class="string">&#x27;today &#x27;</span>,temperature=<span class="number">1.0</span></span>):</span></span><br><span class="line">        chars = <span class="built_in">list</span>(start)</span><br><span class="line">        out, s = net(enc(chars).view(<span class="number">1</span>,-<span class="number">1</span>).to(device))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            <span class="comment">#nc = torch.argmax(out[0][-1])</span></span><br><span class="line">            out_dist = out[<span class="number">0</span>][-<span class="number">1</span>].div(temperature).exp()</span><br><span class="line">            nc = torch.multinomial(out_dist,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            chars.append(vocab.get_itos()[nc])</span><br><span class="line">            out, s = net(nc.view(<span class="number">1</span>,-<span class="number">1</span>),s)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(chars)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0.3</span>,<span class="number">0.8</span>,<span class="number">1.0</span>,<span class="number">1.3</span>,<span class="number">1.8</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;--- Temperature = <span class="subst">&#123;i&#125;</span>\n<span class="subst">&#123;generate_soft(net,size=<span class="number">300</span>,start=<span class="string">&#x27;Today &#x27;</span>,temperature=i)&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure><p>引入了一个称为温度的参数，用于指示应该坚持最高概率的力度(温度越低越严格).</p><p>如果温度为 1.0，我们进行公平的多项式采样，当温度变为无穷大时.</p><p>所有概率都变得相等，我们随机选择下一个字符。当我们过度升高温度时，文本变得毫无意义，当它接近 0 时，它类似于“循环”的硬生成文本。</p><p>核心是下面代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out_dist = out[<span class="number">0</span>][-<span class="number">1</span>].div(temperature).exp()</span><br><span class="line">nc = torch.multinomial(out_dist,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">chars.append(vocab.get_itos()[nc])</span><br></pre></td></tr></table></figure><h3 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h3><p>NLP领域最重要的问题之一是机器翻译,这是谷歌翻译等工具的基本任务,或者更一般地说，任何序列到序列的任务.</p><p>循环网络的一个主要缺点是序列中的<strong>所有单词对结果都有相同的影响</strong>。这会导致标准 LSTM 编码器-解码器模型在序列到序列任务（如命名实体识别和机器翻译）中性能欠佳。实际上，输入序列中的特定单词通常比其他单词对顺序输出的影响更大。</p><blockquote><p>GRN,LSTM等引入遗忘门,更新门这种机制试图解决长序列遗忘问题,但不能解决不同单词权重的问题</p></blockquote><p>注意力机制提供了一种<strong>加权每个输入向量对RNN的每个输出预测的上下文影响的方法</strong>。它的实现方式是在输入 RNN 的中间状态和输出 RNN 之间创建快捷方式。</p><h4 id="Positional-Encoding-Embedding"><a href="#Positional-Encoding-Embedding" class="headerlink" title="Positional Encoding/Embedding"></a>Positional Encoding/Embedding</h4><p>使用 RNN 时，token的相对位置由步数表示(因为RNN不是并行的,由第一个token开始累积状态)，因此不需要显式表示。然而,如果使用注意力层，就需要知道token在序列中的相对位置(因为将整个sequence作为整体).</p><p>为了获得位置编码,使用序列中的标记位置序列（即数字序列 0,1 等）与token本身相加.</p><p>要将位置（整数）转换为向量，我们可以使用不同的方法：</p><ul><li>可训练嵌入，类似于token嵌入。这就是我们在这里考虑的方法。我们在标记及其位置之上应用嵌入层，从而产生相同维度的嵌入向量，然后将它们相加。</li><li>固定位置编码(比如使用一个余弦函数,使用0~len作为输入).</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TokenAndPositionEmbedding</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, maxlen, vocab_size, embed_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TokenAndPositionEmbedding, self).__init__()</span><br><span class="line">        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)</span><br><span class="line">        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)</span><br><span class="line">        self.maxlen = maxlen</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        maxlen = self.maxlen</span><br><span class="line">        positions = tf.<span class="built_in">range</span>(start=<span class="number">0</span>, limit=maxlen, delta=<span class="number">1</span>)</span><br><span class="line">        positions = self.pos_emb(positions)</span><br><span class="line">        x = self.token_emb(x)</span><br><span class="line">        <span class="keyword">return</span> x+positions</span><br></pre></td></tr></table></figure><p>这里使用两个embedding,分别处理token和position.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/images/pos-embedding.png?raw=1" alt="img" style="zoom: 25%;" /></p><p>transformer层如图,主要使用了multi-head attn,然后使用了resnet中的思想添加了输入x,也就是x+f(x),normalization使用layernorm,对一个sample中的所有维进行规范化.</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/images/transformer-layer.png?raw=1" alt="img" style="zoom:33%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, num_heads, ff_dim, rate=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerBlock, self).__init__()</span><br><span class="line">        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=<span class="string">&#x27;attn&#x27;</span>)</span><br><span class="line">        self.ffn = keras.Sequential(</span><br><span class="line">            [keras.layers.Dense(ff_dim, activation=<span class="string">&quot;relu&quot;</span>), keras.layers.Dense(embed_dim),]</span><br><span class="line">        )</span><br><span class="line">        self.layernorm1 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout1 = keras.layers.Dropout(rate)</span><br><span class="line">        self.dropout2 = keras.layers.Dropout(rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training</span>):</span></span><br><span class="line">        attn_output = self.att(inputs, inputs)</span><br><span class="line">        attn_output = self.dropout1(attn_output, training=training)</span><br><span class="line">        out1 = self.layernorm1(inputs + attn_output)</span><br><span class="line">        ffn_output = self.ffn(out1)</span><br><span class="line">        ffn_output = self.dropout2(ffn_output, training=training)</span><br><span class="line">        <span class="keyword">return</span> self.layernorm2(out1 + ffn_output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">embed_dim = <span class="number">32</span>  <span class="comment"># Embedding size for each token</span></span><br><span class="line">num_heads = <span class="number">2</span>  <span class="comment"># Number of attention heads</span></span><br><span class="line">ff_dim = <span class="number">32</span>  <span class="comment"># Hidden layer size in feed forward network inside transformer</span></span><br><span class="line">maxlen = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">20000</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential([keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_sequence_length=maxlen, input_shape=(<span class="number">1</span>,)),</span><br><span class="line">    TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim),</span><br><span class="line">    TransformerBlock(embed_dim, num_heads, ff_dim),</span><br><span class="line">    keras.layers.GlobalAveragePooling1D(),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.1</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">20</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">    keras.layers.Dropout(<span class="number">0.1</span>),</span><br><span class="line">    keras.layers.Dense(<span class="number">4</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training tokenizer&#x27;</span>)</span><br><span class="line">model.layers[<span class="number">0</span>].adapt(ds_train.<span class="built_in">map</span>(extract_text))</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,metrics=[<span class="string">&#x27;acc&#x27;</span>], optimizer=<span class="string">&#x27;adam&#x27;</span>)</span><br><span class="line">model.fit(ds_train.<span class="built_in">map</span>(tupelize).batch(<span class="number">128</span>),validation_data=ds_test.<span class="built_in">map</span>(tupelize).batch(<span class="number">128</span>))</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/sFOcUVC.png" alt="image-20231106225205449"></p><p>网络结构如上.</p><p>可以看看这篇文章<a href="https://zhuanlan.zhihu.com/p/366592542">注意力,多头注意力,自注意力及Pytorch实现 - 知乎 (zhihu.com)</a></p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT是一个非常大的多层transformer网络，其中 12 层用于 BERT 基础，24 层用于 BERT-large。该模型首先使用无监督训练（预测句子中的掩饰词）在大型文本数据语料库（WikiPedia + 书籍）上进行预训练。</p><p>在预训练期间，模型吸收了大量语言理解，然后可以通过微调将其与其他数据集一起使用。这个过程称为迁移学习。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/jalammarBERT-language-modeling-masked-lm.png" alt="picture from http://jalammar.github.io/illustrated-bert/"></p><h4 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h4><p><img data-src="https://i.imgur.com/bCtBKQC.png" alt="img"></p><h2 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h2><blockquote><p>到目前为止，我们主要关注一项 NLP 任务——分类。然而，还有其他 NLP 任务可以通过神经网络来完成。其中一项任务是命名实体识别 (NER)，它处理识别文本中的特定实体，例如地点、人名、日期时间间隔、化学式等。</p></blockquote><p>假设您想开发一个自然语言聊天机器人，类似于 Amazon Alexa 或 Google Assistant。智能聊天机器人的工作方式是通过对输入句子进行文本分类来了解用户想要什么。这种分类的结果就是所谓的意图(<strong>intent</strong>)，它决定了聊天机器人应该做什么。</p><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/19-NER/images/bot-ner.png" alt="Bot NER" style="zoom:67%;" /></p><p>然而，用户可以提供一些参数作为短语的一部分。例如，当询问天气时，她可能会指定地点或日期。机器人应该能够理解这些实体，并在执行操作之前相应地填充参数槽。这正是 NER 发挥作用的地方。</p><p>也就是说,从原本的一句话分类变成对一个单词的分类和理解。</p><p>NER 模型本质上是 token 分类模型,因为对于每个输入 token,我们需要决定它是否属于一个实体,如果属于,则属于哪个实体类。</p><p>由于 NER 模型本质上是一个 token 分类模型，因此我们可以使用我们已经熟悉的 RNN 来完成此任务。在这种情况下，循环网络的每个块都会返回token ID。</p><p>也就是说每个token会给一个tag,这个tag包含这个entity是否是第一个,以及所属得类别.类似下面的tag.</p><div class="table-container"><table><thead><tr><th>Token</th><th>Tag</th></tr></thead><tbody><tr><td>Tricuspid</td><td>B-DIS</td></tr><tr><td>valve</td><td>I-DIS</td></tr><tr><td>regurgitation</td><td>I-DIS</td></tr><tr><td>and</td><td>O</td></tr><tr><td>lithium</td><td>B-CHEM</td></tr><tr><td>carbonate</td><td>I-CHEM</td></tr><tr><td>toxicity</td><td>B-DIS</td></tr><tr><td>in</td><td>O</td></tr><tr><td>a</td><td>O</td></tr><tr><td>newborn</td><td>O</td></tr><tr><td>infant</td><td>O</td></tr><tr><td>.</td><td>O</td></tr></tbody></table></div><h2 id="Pre-Trained-Large-Language-Models"><a href="#Pre-Trained-Large-Language-Models" class="headerlink" title="Pre-Trained Large Language Models"></a>Pre-Trained Large Language Models</h2><p>在我们之前的所有任务中，我们都在使用标记数据集训练神经网络来执行特定任务。对于大型转换器模型，如BERT，我们以自监督的方式使用语言建模来构建语言模型，然后通过进一步的领域特定训练将其专门用于特定的下游任务。</p><p><strong>然而，已经证明，大型语言模型也可以在没有任何特定领域训练的情况下解决许多任务。一个能够做到这一点的模型家族被称为GPT： Generative Pre-Trained Transformer。</strong></p><p><img data-src="https://i.imgur.com/6rtsvGL.png" alt="image-20231118230348795"></p><p>因为GPT已经根据大量数据进行了训练，以理解语言和代码，所以它们会根据输入（提示）提供输出。提示是GPT输入或查询，用于向模型提供下一次完成任务的指令。为了获得想要的结果，你需要最有效的提示，包括选择正确的单词、格式、短语甚至符号.</p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前学过一段时间NLP,因为其中涉及到一些深度学习常用的知识或者框架,但苦于不系统以及没有任务focus不能长久.这里借助微软的教程写点东西.&lt;br&gt;</summary>
    
    
    
    
    <category term="NLP" scheme="https://www.sekyoro.top/tags/NLP/"/>
    
    <category term="Deep Learning" scheme="https://www.sekyoro.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>目标检测_初识</title>
    <link href="https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/"/>
    <id>https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/</id>
    <published>2023-10-21T13:22:20.000Z</published>
    <updated>2023-11-30T15:05:40.572Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>需要一些基本的cv知识<br><span id="more"></span></p><h3 id="图像分辨率"><a href="#图像分辨率" class="headerlink" title="图像分辨率"></a>图像分辨率</h3><blockquote><p>图像分辨率可以定义为图像中存在的像素数。当像素数量增加时，图像的质量会增加。我们已经在前面看到了图像的形状，它给出了行和列的数量。这可以说是该图像的分辨率。几乎所有人都知道的一些标准分辨率是320 x 240像素（主要适用于小屏幕设备）、1024 x 768像素（适用于在标准计算机显示器上观看）、720 x 576像素（适合在宽高比为4:3的标准清晰度电视机上观看），1280 x 1024像素（适用于在宽高比为5:4的液晶显示器上全屏观看）、1920 x 1080像素（用于在高清电视上观看），现在我们甚至有4K、5K和8K分辨率，超高清显示器和电视分别支持3840 x 2160像素、5120 x 2880像素和7680 x 4320像素</p></blockquote><p>图像像素的高位包含的信息比低位更多,我们可以将图像划分为不同级别的位平面。例如，将图像划分为8位（0-7）平面，其中最后几个平面包含图像的大部分信息。</p><p><img data-src="https://editor.analyticsvidhya.com/uploads/61607page%2015.gif" alt="bit plans "></p><p><img data-src="https://s2.loli.net/2023/11/23/li2FYa5bBWDy6uv.png" alt="image-20231123110414030"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img  = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line">c1 = np.mod(img,<span class="number">2</span>)</span><br><span class="line">c2 = np.mod(np.floor(img/<span class="number">2</span>),<span class="number">2</span>)</span><br><span class="line">c3 = np.mod(np.floor(img/<span class="number">4</span>),<span class="number">2</span>)</span><br><span class="line">c4 = np.mod(np.floor(img/<span class="number">8</span>),<span class="number">2</span>)</span><br><span class="line">c5 = np.mod(np.floor(img/<span class="number">16</span>),<span class="number">2</span>)</span><br><span class="line">c6 = np.mod(np.floor(img/<span class="number">32</span>),<span class="number">2</span>)</span><br><span class="line">c7 = np.mod(np.floor(img/<span class="number">64</span>),<span class="number">2</span>)</span><br><span class="line">c8 = np.mod(np.floor(img/<span class="number">128</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">cc = <span class="number">2</span>*(<span class="number">2</span>*(<span class="number">2</span>*c8+c7)+c6)</span><br><span class="line">to_plot = [img,c1,c2,c3,c4,c5,c6,c7,c8,cc]</span><br><span class="line">fig,axes = plt.subplots(<span class="number">2</span>,<span class="number">5</span>, subplot_kw=&#123;<span class="string">&#x27;xticks&#x27;</span>: [], <span class="string">&#x27;yticks&#x27;</span>: []&#125;)</span><br><span class="line">fig.subplots_adjust(hspace=<span class="number">0.05</span>, wspace=<span class="number">0.05</span>)</span><br><span class="line"><span class="keyword">for</span> ax,i <span class="keyword">in</span> <span class="built_in">zip</span>(axes.flat, to_plot):</span><br><span class="line">    ax.imshow(i, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>可以使用像素的高位重建图像</p><h4 id="图像金字塔"><a href="#图像金字塔" class="headerlink" title="图像金字塔"></a>图像金字塔</h4><p><img data-src="https://docs.opencv.org/3.4/Pyramids_Tutorial_Pyramid_Theory.png" alt="Pyramids_Tutorial_Pyramid_Theory.png"></p><p><strong>图像金字塔是一组图像，所有图像都来自一张原始图像，这些图像被连续下采样，直到达到某个期望的停止点</strong>。</p><p>有两种常见的图像金字塔：</p><ul><li>高斯金字塔：用于对图像进行下采样</li><li>拉普拉斯金字塔：用于从金字塔中较低的图像重建上采样图像（分辨率较低）</li></ul><h3 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a>图像直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">his</span>(<span class="params">img_gray</span>):</span></span><br><span class="line">    hist = cv2.calcHist([img_gray], [<span class="number">0</span>], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&quot;Grayscale Histogram&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;bins&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;pixels&quot;</span>)</span><br><span class="line">    plt.plot(hist)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/OzzSilQ.png" alt="image-20231123134730139"></p><h3 id="图像深度"><a href="#图像深度" class="headerlink" title="图像深度"></a>图像深度</h3><p>数字化图像的每个像素是用一组二进制数进行描述，像素的色彩由RGB通道决定，其中包含表示图像颜色的位数称为图像深度。如灰度图像，每个像素颜色占用1个字节8位，则称图像深度为8位，而RGB的彩色图像占用3字节，图像深度为24位。</p><p>图像深度又称为色深（Color Depth），它确定了一幅图像中最多能使用的颜色数，即彩色图像的每个像素最大的颜色数，或者确定灰度图像的每个像素最大的灰度级数。<br>使用opencv的imread读取模式有</p><blockquote><p>IMREAD_UNCHANGED = -1, //返回原始图像。alpha通道不会被忽略，如果有的话。加载给定格式的图像，包括alpha通道。Alpha通道存储透明度信息——Alpha通道的值越高，像素就越不透明<br>IMREAD_GRAYSCALE = 0, //返回灰度图像<br>IMREAD_COLOR = 1, //返回通道顺序为BGR的彩色图像<br>IMREAD_ANYDEPTH = 2, //当输入具有相应的深度时返回16位/ 32位图像，否则将其转换为8位。.<br>IMREAD_ANYCOLOR = 4, //则以任何可能的颜色格式读取图像。</p></blockquote><h3 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h3><p>颜色空间是一种协议(protocol)，用于以易于再现的方式表示颜色。我们知道，灰度图像具有单个像素值，彩色图像每个像素包含3个值——红色、绿色和蓝色通道的强度。</p><p>大多数计算机视觉用例处理RGB格式的图像。然而,<strong>视频压缩和设备独立存储等应用程序在很大程度上依赖于其他颜色空间，如色相(Hue即色相，就是我们平时所说的红、绿，如果你分的更细的话可能还会有洋红、草绿等等)、饱和度(色彩的深浅度(0-100%，对于一种颜色比如红色，我们可以用浅红——大红——深红——红得发紫等等语言来描述它)、色调(纯度，色彩的亮度(0-100%) ，这个在调整屏幕亮度的时候比较常见)即HSV颜色空间</strong>。</p><p><strong>RGB图像由不同颜色通道的颜色强度组成，即强度和颜色信息在RGB颜色空间中混合</strong>，但<strong>在HSV颜色空间中,颜色和强度信息彼此分离。这将使HSV颜色空间对光源更改更加稳健</strong>。</p><p><img data-src="https://i.imgur.com/gkNZfIa.png" alt="image-20231123115359582"></p><h3 id="图像resize"><a href="#图像resize" class="headerlink" title="图像resize"></a>图像resize</h3><p><strong>机器学习模型使用固定大小的输入</strong>。同样的想法也适用于计算机视觉模型。<strong>我们用于训练模型的图像必须具有相同的大小</strong>。现在，<strong>如果我们通过从各种来源抓取图像来创建自己的数据集，这可能会成为问题。这就是调整图像大小的功能凸显出来的地方</strong>。</p><p><img data-src="https://i.imgur.com/Agk6m0B.jpg" alt=""></p><blockquote><p>INTER_NEAREST:最近邻插值</p><p>INTER_LINEAR:双线性插值</p><p>INTER_AREA：使用像素面积关系重新采样</p><p>INTER_CUBIC:4×4像素邻域上的双三次插值</p><p>INTER_LANCZOS4:8邻域上的Lanczos插值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> npimg = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;img&quot;</span>,img)</span><br><span class="line">smaller_img = cv2.resize(img,(<span class="number">200</span>,<span class="number">200</span>),interpolation=cv2.INTER_LINEAR)</span><br><span class="line">cv2.imshow(<span class="string">&quot;smaller_img&quot;</span>,smaller_img)</span><br></pre></td></tr></table></figure><h3 id="图像旋转以及平移"><a href="#图像旋转以及平移" class="headerlink" title="图像旋转以及平移"></a>图像旋转以及平移</h3><p>可以用作图像增强的技术</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line">rows,cols = img.shape[:<span class="number">2</span>]</span><br><span class="line">M = cv2.getRotationMatrix2D((cols/<span class="number">2</span>,rows/<span class="number">2</span>),<span class="number">45</span>,<span class="number">1</span>)</span><br><span class="line">dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class="line">cv2.imshow(<span class="string">&quot;dst&quot;</span>,dst)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p>会使用到cv2.getRotationMatrix2D与cv2.warpAffine.</p><p>cv2.getRotationMatrix2D参数分别是中心,旋转角度以及缩放系数.</p><p>cv2.warpAffine是做仿射变换,</p><p><img data-src="https://img-blog.csdnimg.cn/ab91284739724a70a016342a38f84baa.png#pic_center" alt="在这里插入图片描述"></p><p>图像平移可以用于为模型添加平移不变性<strong>，因为通过平移，我们可以改变对象在图像中的位置，为模型提供更多的多样性，从而获得更好的可推广性</strong>，这在困难的条件下有效，即当对象没有完全对准图像中心时。这种增强技术还可以帮助模型正确地对具有部分可见对象的图像进行分类。以下图为例。即使图像中没有完整的鞋子，模型也应该能够将其分类为鞋子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">M = np.float32([[<span class="number">1</span>,<span class="number">0</span>,-<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,-<span class="number">100</span>]])</span><br><span class="line">dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class="line">plt.imshow(dst)</span><br><span class="line">cv2.imshow(<span class="string">&quot;dst&quot;</span>,dst)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Nsds6wk.png" alt="image-20231123123137312"></p><h3 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a>图像阈值</h3><p>阈值分割是一种图像分割方法。它将像素值与阈值进行比较，并相应地进行更新。图像阈值的一个简单应用可以将图像划分为前景和背景,阈值只能应用于灰度图像。</p><p><img data-src="https://i.imgur.com/GY8rrIF.png" alt="image-20231123123310090"></p><p>上面是简单阈值,此外还有自适应阈值</p><p>简单阈值是全局的，对于在不同区域具有不同照明条件的图像可能不太适用，此时可以使用自适应阈值处理。<strong>算法计算图像的局部阈值，在同一图像的不同区域获得不同的阈值，并为具有不同照明的图像提供了更好的结果</strong>。在自适应阈值的情况下，对图像的不同部分使用不同的阈值。该函数可为具有不同照明条件的图像提供更好的结果，因此被称为“自适应”。<strong>Otsu的二值化方法为整个图像找到一个最佳阈值。它适用于双峰图像</strong>（直方图中有2个峰值的图像）。</p><p><code>cv2.ADAPTIVE_THRESH_MEAN_C</code>：阈值是邻域的<strong>平均值</strong>。<br><code>cv2.ADAPTIVE_THRESH_GAUSSIAN_C</code>：阈值是邻域值的<strong>加权和</strong>，其中权重是高斯窗口。<br><code>Block Size</code> 决定邻域的大小。<br><code>C</code> 从计算的平均值或加权平均值中减去常数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#import the libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#ADAPTIVE THRESHOLDING</span></span><br><span class="line">gray_image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ret,thresh_global = cv2.threshold(gray_image,<span class="number">127</span>,<span class="number">255</span>,cv2.THRESH_BINARY)</span><br><span class="line"><span class="comment">#here 11 is the pixel neighbourhood that is used to calculate the threshold value</span></span><br><span class="line">thresh_mean = cv2.adaptiveThreshold(gray_image,<span class="number">255</span>,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">thresh_gaussian = cv2.adaptiveThreshold(gray_image,<span class="number">255</span>,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,<span class="number">11</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">names = [<span class="string">&#x27;Original Image&#x27;</span>,<span class="string">&#x27;Global Thresholding&#x27;</span>,<span class="string">&#x27;Adaptive Mean Threshold&#x27;</span>,<span class="string">&#x27;Adaptive Gaussian Thresholding&#x27;</span>]</span><br><span class="line">images = [gray_image,thresh_global,thresh_mean,thresh_gaussian]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">2</span>,i+<span class="number">1</span>),plt.imshow(images[i],<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(names[i])</span><br><span class="line">    plt.xticks([]),plt.yticks([])</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Gb1mmoJ.png" alt="image-20231123135259991"></p><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><p><strong>图像分割是将图像中的每个像素分类到某个类别的任务</strong>。例如，将每个像素分类为前景或背景。图像分割对于从图像中提取相关部分是重要的。</p><p>分水岭算法是一种经典的图像分割算法。它将图像中的像素值视为地形。为了找到对象边界，它将初始标记作为输入。然后，该算法开始从标记淹没盆地(flooding the basin from the markers)，直到标记在对象边界相遇。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/watershed.gif" alt="watershed algorithm"></p><h3 id="位操作"><a href="#位操作" class="headerlink" title="位操作"></a>位操作</h3><p>按位运算包括AND、OR、NOT和XOR。在计算机视觉中，当我们<strong>有一个遮罩图像并想将该遮罩应用于另一个图像以提取感兴趣的区域时</strong>，这些操作非常有用。</p><p><img data-src="https://i.imgur.com/O85ljV8.png" alt="image-20231123161116735"></p><p><img data-src="https://i.imgur.com/Tzrz5xe.png" alt="image-20231123161520256"></p><h3 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h3><p><strong>边缘是图像中图像亮度急剧变化或具有不连续性的点</strong>。这种不连续性通常对应于：</p><ul><li>深度不连续</li><li>表面方向的不连续性</li><li>材料特性的变化</li><li>场景照明的变化</li></ul><p>边缘是图像的非常有用的特征，可以用于不同的应用，如图像中对象的分类和定位。甚至深度学习模型也会计算边缘特征，以提取图像中存在的对象的信息。</p><p><img data-src="https://i.imgur.com/NTrwEOY.png" alt="image-20231123161625316"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&quot;../imgs/00000.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">edges = cv2.Canny(img, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">&quot;edges&quot;</span>, edges)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><h3 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h3><p>在图像滤波中，使用像素值的相邻值来更新像素值。但是，这些值最初是如何更新的呢？，有多种更新像素值的方法，例如从邻居中选择最大值，使用邻居的平均值等。每种方法都有自己的用途。例如，对邻域中的像素值取平均值用于图像模糊。</p><p>高斯滤波也用于图像模糊，其根据相邻像素与所考虑像素的距离为相邻像素赋予不同的权重。对于图像过滤，我们使用内核。核是不同形状的数字矩阵，如3 x 3、5 x 5等。核用于计算图像一部分的点积。当计算像素的新值时，内核中心与像素重叠。相邻像素值与内核中的相应值相乘。计算出的值被分配给与内核中心重合的像素。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/Image-Filtering.png" alt="img"></p><p><img data-src="https://i.imgur.com/0EahLmk.png" alt="image-20231123161852790"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;../imgs/test.png&#x27;</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.float32)/<span class="number">25</span></span><br><span class="line"><span class="comment">#using the averaging kernel for image smoothening</span></span><br><span class="line">averaging_kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.float32)/<span class="number">9</span></span><br><span class="line">filtered_image = cv2.filter2D(image,-<span class="number">1</span>,kernel)</span><br><span class="line">cv2.imshow(<span class="string">&quot;avg_filtered_image&quot;</span>,filtered_image)</span><br><span class="line"><span class="comment">#get a one dimensional Gaussian Kernel</span></span><br><span class="line">gaussian_kernel_x = cv2.getGaussianKernel(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">gaussian_kernel_y = cv2.getGaussianKernel(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment">#converting to two dimensional kernel using matrix multiplication</span></span><br><span class="line">gaussian_kernel = gaussian_kernel_x * gaussian_kernel_y.T</span><br><span class="line"><span class="comment">#you can also use cv2.GaussianBLurring(image,(shape of kernel),standard deviation) instead of cv2.filter2D</span></span><br><span class="line">filtered_image = cv2.filter2D(image,-<span class="number">1</span>,gaussian_kernel)</span><br><span class="line">cv2.imshow(<span class="string">&quot;filtered_image&quot;</span>,filtered_image)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/Wtr5Pab.png" alt="image-20231123170248409"></p><p>上图的两个卷积核分别是</p><script type="math/tex; mode=display">\left.\left[\begin{matrix}0&-1&0\\-1&5&-1\\0&-1&0\\\end{matrix}\right.\right]</script><script type="math/tex; mode=display">\left.\left[\begin{matrix}0&-1&0\\-1&4&-1\\0&-1&0\\\end{matrix}\right.\right]</script><h3 id="图像轮廓-contours"><a href="#图像轮廓-contours" class="headerlink" title="图像轮廓(contours)"></a>图像轮廓(contours)</h3><p>轮廓是表示图像中对象边界的点或线段的闭合曲线。<strong>轮廓本质上是图像中对象的形状。与边缘不同，轮廓不是图像的一部分。相反，它们是与图像中对象的形状相对应的点和线段的抽象集合</strong>。我们可以使用轮廓来计算图像中对象的数量，根据对象的形状对其进行分类，或者从图像中选择特定形状的对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = cv2.imread(<span class="string">&#x27;../imgs/test.png&#x27;</span>)</span><br><span class="line"><span class="comment">#converting RGB image to Binary</span></span><br><span class="line">gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line">ret,thresh = cv2.threshold(gray_image,<span class="number">127</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment">#calculate the contours from binary image</span></span><br><span class="line">contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">with_contours = cv2.drawContours(image,contours,-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">plt.imshow(with_contours[...,::-<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/ttcJEOh.png" alt="image-20231123163932600"></p><h3 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h3><p>使用SIFT或SURF从不同图像中提取的特征可以进行匹配，以找到存在于不同图像中的相似对象/模式。OpenCV库支持多种特征匹配算法，如brute force 匹配、knn特征匹配等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#reading images in grayscale format</span></span><br><span class="line">image1 = cv2.imread(<span class="string">&#x27;../imgs/00000.png&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">image2 = cv2.warpAffine(image1, np.float32([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">100</span>], [<span class="number">0</span>, <span class="number">1</span>, -<span class="number">100</span>]]), (image1.shape[<span class="number">1</span>], image1.shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"><span class="comment">#finding out the keypoints and their descriptors</span></span><br><span class="line">keypoints1,descriptors1 = sift.detectAndCompute(image1,<span class="literal">None</span>)</span><br><span class="line">keypoints2,descriptors2 = sift.detectAndCompute(image2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#matching the descriptors from both the images</span></span><br><span class="line">bf = cv2.BFMatcher()</span><br><span class="line">matches = bf.knnMatch(descriptors1,descriptors2,k = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#selecting only the good features</span></span><br><span class="line">good_matches = []</span><br><span class="line"><span class="keyword">for</span> m,n <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="keyword">if</span> m.distance &lt; <span class="number">0.75</span>*n.distance:</span><br><span class="line">        good_matches.append([m])</span><br><span class="line">image3 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good_matches,<span class="literal">None</span>,flags=<span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">&quot;image3&quot;</span>,image3)</span><br><span class="line">cv2.waitKey()</span><br></pre></td></tr></table></figure><p><img data-src="https://i.imgur.com/BBs3dlN.jpg" alt="image-20231123165033904"></p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/02/keypoint_matching.png" alt="img"></p><p>在上面的图像中，我们可以看到从原始图像（左侧）中提取的关键点与其旋转版本的关键点相匹配。这是因为特征是使用SIFT提取的，SIFT对这种变换是不变的。</p><h3 id="人脸检测"><a href="#人脸检测" class="headerlink" title="人脸检测"></a>人脸检测</h3><p>OpenCV支持基于haar级联的对象检测。<strong>Haar级联是基于机器学习的分类器，用于计算图像中的不同特征，如边缘、线条等</strong>。然后，这些分类器使用多个正样本和负样本进行训练。OpenCV Github仓库中提供了针对人脸、眼睛等不同对象的经过训练的分类器，也可以针对任何对象训练自己的haar级联。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#load the classifiers downloaded </span></span><br><span class="line">face_cascade = cv2.CascadeClassifier(<span class="string">&#x27;haarcascade_frontalface_default.xml&#x27;</span>)</span><br><span class="line">eye_cascade = cv2.CascadeClassifier(<span class="string">&#x27;haarcascade_eye.xml&#x27;</span>)</span><br><span class="line"><span class="comment">#read the image and convert to grayscale format</span></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;rotated_face.jpg&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#calculate coordinates </span></span><br><span class="line">faces = face_cascade.detectMultiScale(gray, <span class="number">1.1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    roi_gray = gray[y:y+h, x:x+w]</span><br><span class="line">    roi_color = img[y:y+h, x:x+w]</span><br><span class="line">    eyes = eye_cascade.detectMultiScale(roi_gray)</span><br><span class="line">    <span class="comment">#draw bounding boxes around detected features</span></span><br><span class="line">    <span class="keyword">for</span> (ex,ey,ew,eh) <span class="keyword">in</span> eyes:</span><br><span class="line">        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line"><span class="comment">#plot the image</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="comment">#write image </span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;face_detection.jpg&#x27;</span>,img)</span><br></pre></td></tr></table></figure><p>很多级联器的xml文件都是直接拿别人的,网上也有训练的教程.</p><h3 id="图像梯度向量"><a href="#图像梯度向量" class="headerlink" title="图像梯度向量"></a>图像梯度向量</h3><p>image gradient vector</p><p>图像梯度矢量被定义为每个单独像素的度量,包含x轴和y轴上的像素颜色变化。该定义与连续多变量函数的梯度一致，该函数是所有变量的偏导数的向量。</p><h3 id="边缘检测算子"><a href="#边缘检测算子" class="headerlink" title="边缘检测算子"></a>边缘检测算子</h3><p><img data-src="https://editor.analyticsvidhya.com/uploads/81269Capture.PNG" alt="Sharpening An Image  2"></p><h4 id="prewitt"><a href="#prewitt" class="headerlink" title="prewitt"></a>prewitt</h4><p>Prewitt算子不是只依赖于四个直接相邻的邻居，而是利用八个周围的像素来获得更平滑的结果。</p><p><img data-src="https://s2.loli.net/2023/11/23/8LM4JgTNA2ZDVpy.png" alt="image-20231123095504365"></p><h4 id="sobel"><a href="#sobel" class="headerlink" title="sobel"></a>sobel</h4><p>为了更加强调直接相邻像素的影响，它们被分配了更高的权重。</p><p><img data-src="https://i.imgur.com/XXcPkBM.png" alt="image-20231123172141630"></p><h3 id="角点检测"><a href="#角点检测" class="headerlink" title="角点检测"></a>角点检测</h3><blockquote><p>角点检测(Corner Detection)是计算机视觉系统中用来获得图像特征的一种方法，广泛应用于运动检测、图像匹配、视频跟踪、三维建模和目标识别等领域中。也称为特征点检测。 <strong>角点通常被定义为两条边的交点，更严格的说，角点的局部邻域应该具有两个不同区域的不同方向的边界。</strong>而实际应用中，大多数所谓的角点检测方法检测的是拥有特定特征的图像点，而不仅仅是“角点”。这些特征点在图像中有具体的坐标，并具有某些数学特征，如局部最大或最小灰度、某些梯度特征等</p></blockquote><h4 id="Harris"><a href="#Harris" class="headerlink" title="Harris"></a>Harris</h4><ul><li>计算窗口中各像素点在x和y方向的梯度；</li><li>计算两个方向梯度的乘积,即Ix ^ 2 , Iy ^ 2 , IxIy(可以用一些一阶梯度算子求得图像梯度)</li><li>使用滤波核对窗口中的每一像素进行加权，生成矩阵M和元素A，B，C</li><li>计算每个像素的Harris响应值R，并对小于某阈值T的R置0；</li><li>由于角点所在区域的一定邻域内都有可能被检测为角点，所以为了防止角点聚集，最后在3×3或5×5的邻域内进行非极大值抑制，局部最大值点即为图像中的角点。</li></ul><p><a href="https://blog.csdn.net/SESESssss/article/details/106774854">【理解】经典角点检测算法—Harris角点-CSDN博客</a></p><h3 id="常用特征"><a href="#常用特征" class="headerlink" title="常用特征"></a>常用特征</h3><h4 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h4><p>关键点是处理图像时应该注意的一个概念。这些基本上是图像中的兴趣点。关键点类似于给定图像的特征。它们是定义图像中有趣内容的位置。关键点很重要，因为无论图像如何修改（旋转、收缩、扩展、失真），我们都会为图像找到相同的关键点。</p><p>尺度不变特征变换（SIFT）是一种非常流行的关键点检测算法。它包括以下步骤：</p><ul><li>Scale-space extrema detection</li><li>Keypoint localization</li><li>Orientation assignment</li><li>Keypoint descriptor</li><li>Keypoint matching</li></ul><p>从SIFT提取的特征可用于图像拼接、对象检测等应用。下面的代码和输出显示了使用SIFT计算的关键点及其方向。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment">#show OpenCV version</span></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="comment">#read the iamge and convert to grayscale</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#create sift object</span></span><br><span class="line">sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"><span class="comment">#calculate keypoints and their orientation</span></span><br><span class="line">keypoints,descriptors = sift.detectAndCompute(gray,<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#plot keypoints on the image</span></span><br><span class="line">with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class="line"><span class="comment">#plot the image</span></span><br><span class="line">plt.imshow(with_keypoints)</span><br></pre></td></tr></table></figure><blockquote><p>SIFT算法有助于定位图像中的局部特征，通常称为图像的“关键点”。这些关键点是尺度和旋转不变量，可用于各种计算机视觉应用，如图像匹配、对象检测、场景检测等。我们还可以在模型训练期间使用使用SIFT生成的关键点作为图像的特征。SIFT特征、边缘特征或弓形特征的主要优点是它们不受图像的大小或方向的影响。</p></blockquote><p>整个过程可以分为4个部分：</p><p><strong>Constructing a Scale Space</strong>：确保要素与scale无关 </p><p><strong>Keypoint Localisation</strong>：识别合适的特征或关键点 </p><p><strong>Orientation Assignment</strong>：确保关键点旋转不变 </p><p><strong>Keypoint Descriptor:</strong>：为每个关键点分配一个唯一的id</p><h5 id="Constructing-the-Scale-Space"><a href="#Constructing-the-Scale-Space" class="headerlink" title="Constructing the Scale Space"></a>Constructing the Scale Space</h5><p>我们需要识别给定输入图像中最明显的特征，同时忽略任何噪声。此外，我们需要确保这些功能不依赖于scale。</p><p>对于图像中的每个像素，高斯模糊会基于其具有特定σ值的相邻像素来计算一个值。</p><p><strong>纹理和次要细节将从图像中删除，只保留相关信息，如形状和边缘</strong></p><blockquote><p>比例空间是从单个图像生成的具有不同比例的图像的集合。</p></blockquote><p>因此，这些模糊图像是为多个比例创建的。为了创建一组不同比例的新图像，将拍摄原始图像并将比例缩小一半。对于每个新图像，我们将创建模糊版本。</p><p>理想的缩放次数是四次，对于每次缩放，模糊图像的数量应该是五个。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-24-18-27-46.png" alt="sift octave | SIFT algorithm"></p><blockquote><p>高斯差分是一种特征增强算法，它涉及<strong>将原始图像的一个模糊版本与另一个模糊程度较低的原始图像版本相减</strong>。</p></blockquote><p>DoG为每个octave创建另一组图像，方法是从相同比例的前一个图像中减去每个图像。</p><p>到目前为止，我们已经创建了<strong>多个尺度的图像</strong>（通常用σ表示），并对<strong>每个尺度使用高斯模糊来减少图像中的噪声</strong>。接下来将尝试使用一种名为<strong>高斯差分</strong>（DoG）的技术来增强这些特征。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-12-48-03-300x205.png" alt="difference of gaussian"></p><p>如下图,在左边，我们有5个图像，都来自第一个octave(我的理解就是不同scale的图像)。通过在前一图像上应用高斯模糊来创建每个后续图像。在右边，我们有四个通过减去连续的高斯而生成的图像。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-14-18-26.png" alt="difference of gaussian | SIFT algorithm"></p><p>我们为这些图像中的每一个都增强了功能。现在我们有了一组新的图像，我们将使用它来找到重要的关键点</p><h5 id="Keypoint-Localization"><a href="#Keypoint-Localization" class="headerlink" title="Keypoint Localization"></a>Keypoint Localization</h5><p>一旦创建了图像，下一步就是从图像中<strong>找到可用于特征匹配的重要关键点</strong>。其思想是<strong>找到图像的局部最大值和最小值</strong>。</p><p>本部分分为两个步骤：1)求局部最大值和最小值 2)删除低对比度关键点（关键点选择）</p><blockquote><p>为了定位局部最大值和最小值，我们遍历图像中的每个像素，并将其与相邻像素进行比较。</p></blockquote><p>当说“相邻”时，这不仅包括该图像的周围像素（像素所在），还包括octave中上一个和下一个图像的九个像素。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-16-50-01-300x207.png" alt="Scale invariant"></p><p>这意味着将每个像素值与其他26个像素值进行比较，以确定它是否是称为极值的局部最大值/最小值。例如，有三个来自第一个octave的图像。标记为x的像素与相邻像素（绿色）进行比较，如果它是相邻像素中最高或最低的，则选择它作为关键点或兴趣点</p><h5 id="Keypoint-Selection"><a href="#Keypoint-Selection" class="headerlink" title="Keypoint Selection"></a>Keypoint Selection</h5><p>已经成功地生成了<strong>尺度不变的关键点</strong>。但是<strong>这些关键点中的一些可能对噪声不具有鲁棒性</strong>。我们需要进行最终检查，以确保我们有最准确的关键点来表示图像特征</p><p>因此，我们<strong>将消除对比度低或非常靠近边缘的关键点</strong>。为了处理低对比度关键点，为每个关键点计算二阶泰勒展开。如果结果值小于0.03（以大小计），我们将拒绝关键点。</p><h5 id="Keypoint-Descriptor"><a href="#Keypoint-Descriptor" class="headerlink" title="Keypoint Descriptor"></a>Keypoint Descriptor</h5><p>到目前为止已经有了尺度不变和旋转不变的稳定关键点。</p><p>最后将使用相邻的像素、它们的方向和大小来为这个关键点生成一个独特的特征，称为“描述符”。</p><p>首先在关键点周围取一个16×16的邻域。这个16×16的块被进一步划分为4×4个子块，对于这些子块中的每一个子块，我们使用幅度和方向来生成直方图。</p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-26-20-10-52.png" alt="sift feature"></p><p>在这个阶段，bin的尺寸增加了，我们只取了8个bins。这些箭头中的每一个表示8bins，箭头的长度定义了大小。因此，我们将为每个关键点总共有128个bin值。</p><h5 id="Feature-Matching"><a href="#Feature-Matching" class="headerlink" title="Feature Matching"></a>Feature Matching</h5><p>现在将使用SIFT特征进行特征匹配。为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># read images</span></span><br><span class="line">img1 = cv2.imread(<span class="string">&#x27;eiffel_2.jpeg&#x27;</span>)  </span><br><span class="line">img2 = cv2.imread(<span class="string">&#x27;eiffel_1.jpg&#x27;</span>) </span><br><span class="line"></span><br><span class="line">img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class="line">img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sift</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"></span><br><span class="line">keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class="literal">None</span>)</span><br><span class="line">keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># read images</span></span><br><span class="line">img1 = cv2.imread(<span class="string">&#x27;eiffel_2.jpeg&#x27;</span>)  </span><br><span class="line">img2 = cv2.imread(<span class="string">&#x27;eiffel_1.jpg&#x27;</span>) </span><br><span class="line"></span><br><span class="line">img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class="line">img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sift</span></span><br><span class="line">sift = cv2.xfeatures2d.SIFT_create()</span><br><span class="line"></span><br><span class="line">keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class="literal">None</span>)</span><br><span class="line">keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#feature matching</span></span><br><span class="line">bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">matches = bf.match(descriptors_1,descriptors_2)</span><br><span class="line">matches = <span class="built_in">sorted</span>(matches, key = <span class="keyword">lambda</span> x:x.distance)</span><br><span class="line"></span><br><span class="line">img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:<span class="number">50</span>], img2, flags=<span class="number">2</span>)</span><br><span class="line">plt.imshow(img3),plt.show()</span><br></pre></td></tr></table></figure><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/index_71.png" alt="SIFT algorithm"></p><p><img data-src="https://av-eks-blogoptimized.s3.amazonaws.com/index_61.png" alt="feature matching | SIFT algorithm"></p><ul><li>SIFT（Scale Invariant Feature Transform，尺度不变特征变换）是一种强大的图像匹配技术，它可以识别和匹配图像中对<strong>缩放、旋转和仿射失真不变的特征</strong>。</li><li>它被广泛应用于计算机视觉应用，包括图像匹配、物体识别和三维重建。SIFT技术包括生成具有不同尺度的图像的尺度空间，然后使用高斯差分（DoG）方法来识别图像中的关键点。</li><li>它还涉及为每个关键点计算描述符，这些描述符可用于特征匹配和对象识别。</li><li>它可以使用Python和OpenCV库来实现，OpenCV库提供了一组用于检测关键点、计算描述符和匹配特征的函数。</li></ul><h4 id="SURF"><a href="#SURF" class="headerlink" title="SURF"></a>SURF</h4><p>Speeded Up Robust Features（SURF）是SIFT的增强版。它的工作速度要快得多，并且对图像转换更健壮。</p><p>在SIFT中，使用高斯拉普拉斯算子来近似尺度空间。拉普拉斯算子是用于计算图像边缘的核。拉普拉斯核通过近似图像的二阶导数来工作。因此，它对噪声非常敏感。<strong>我们通常将高斯核应用于拉普拉斯核之前的图像，因此将其命名为高斯拉普拉斯</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import required libraries</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#show OpenCV version</span></span><br><span class="line"><span class="built_in">print</span>(cv2.__version__)</span><br><span class="line"><span class="comment">#read image and convert to grayscale</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;index.png&#x27;</span>)</span><br><span class="line">gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment">#instantiate surf object</span></span><br><span class="line">surf  = cv2.xfeatures2d.SURF_create(<span class="number">400</span>)</span><br><span class="line"><span class="comment">#calculate keypoints and their orientation</span></span><br><span class="line">keypoints,descriptors = surf.detectAndCompute(gray,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class="line"></span><br><span class="line">plt.imshow(with_keypoints)</span><br></pre></td></tr></table></figure><h4 id="HOG"><a href="#HOG" class="headerlink" title="HOG"></a>HOG</h4><p>面向梯度直方图（HOG）是一种从像素颜色中提取特征的有效方法，用于构建对象识别分类器。</p><ol><li>预处理图像，包括调整大小和颜色标准化。</li><li>计算每个像素的梯度矢量，以及其大小和方向。</li></ol><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/image-gradient-vector-pixel-location.png" alt="img" style="zoom: 33%;" /></p><ol><li>将图像划分为许多8x8像素的单元格。在每个单元中，这64个单元的幅度值被装箱，并累积添加到9个无符号方向的bucket中（没有符号，因此0-180度而不是0-360度；这是基于经验实验的实际选择）。</li></ol><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/HOG-histogram-creation.png" alt="img"></p><p>4.然后，我们在图像上滑动一个2x2个单元格（因此是16x16像素）的块。在每个块区域中，<strong>4个单元的4个直方图被连接成36个值的一维向量，然后被归一化为具有单位权重。最终的HOG特征向量是所有块向量的级联</strong>。</p><p><img data-src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-19-18-24-37-300x87.png" alt="img"></p><p>它可以被输入到像SVM这样的分类器中，用于学习对象识别任务。</p><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/block_histogram.png" alt="img"></p><h3 id="Camera-Calibration"><a href="#Camera-Calibration" class="headerlink" title="Camera Calibration"></a>Camera Calibration</h3><p>相机是一种将3D世界转换为2D图像的设备。相机在捕捉三维图像并将其存储在二维图像中起着非常重要的作用。</p><p>相机校准是图像处理或计算机视觉领域中常用的词。<strong>相机校准方法旨在识别图像创建过程的几何特征</strong>。这是在许多计算机视觉应用中执行的重要步骤，尤其是当需要场景上的度量信息时。在这些应用中，<strong>相机通常根据一组固有参数进行分类，如轴的偏斜、焦距和主点，其方向由旋转和平移等外部参数表示。线性或非线性算法用于实时利用已知点及其在图像平面中的投影来估计内在和外在参数</strong>。</p><p><img data-src="https://editor.analyticsvidhya.com/uploads/887441.png" alt="Overview of Camera Calibration"></p><p>1.内在或内部参数它允许在图像帧中的像素坐标和相机坐标之间进行映射。例如，透镜的光学中心、焦距和径向失真系数。</p><p>2.外部或外部参数它描述了相机的方向和位置。这是指相机相对于某个世界坐标系的旋转和平移。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>综述</p><ol><li><a href="https://viso.ai/deep-learning/object-detection/">Object Detection in 2023: The Definitive Guide - viso.ai</a></li><li><a href="https://arxiv.org/abs/1905.05055">[1905.05055] Object Detection in 20 Years: A Survey (arxiv.org)</a></li><li><a href="https://arxiv.org/abs/2104.11892">[2104.11892] A Survey of Modern Deep Learning based Object Detection Models (arxiv.org)</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S1051200422004298">A comprehensive review of object detection with deep learning - ScienceDirect</a></li></ol><p>博客</p><ol><li><a href="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/">Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/?utm_source=reading_list&amp;utm_medium=https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/">A Beginner’s Guide to Image Processing With OpenCV and Python (analyticsvidhya.com)</a></li><li>也许还不错的学习网站<a href="https://pyimagesearch.com/">PyImageSearch - You can master Computer Vision, Deep Learning, and OpenCV.</a></li><li>维基百科 卷积核<a href="https://en.wikipedia.org/wiki/Kernel_(image_processing">Kernel (image processing) - Wikipedia</a>)</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;需要一些基本的cv知识&lt;br&gt;</summary>
    
    
    
    
    <category term="object detection" scheme="https://www.sekyoro.top/tags/object-detection/"/>
    
    <category term="cv" scheme="https://www.sekyoro.top/tags/cv/"/>
    
  </entry>
  
  <entry>
    <title>Python并行计算</title>
    <link href="https://www.sekyoro.top/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"/>
    <id>https://www.sekyoro.top/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/</id>
    <published>2023-10-20T12:20:56.000Z</published>
    <updated>2023-10-21T08:52:53.594Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>主要是因为Python库的设计很不错,通过这门语言进一步学习并行,涉及到进程线程以及异步编程等.建议是对性能有要求的利用其他语言实现,但是基本的思想、方法是一样的.<br><span id="more"></span></p><h2 id="创建进程-amp-amp-线程"><a href="#创建进程-amp-amp-线程" class="headerlink" title="创建进程&amp;&amp;线程"></a>创建进程&amp;&amp;线程</h2><ul><li>进程可以包含多个并行运行的线程。</li><li>通常，操作系统创建和管理线程比进程更能节省CPU的资源。线程用于一些小任务，进程用于繁重的任务——运行应用程序。</li><li>同一个进程下的线程共享地址空间和其他资源，进程之间相互独立</li></ul><p>进程有自己的地址空间，数据栈和其他的辅助数据来追踪执行过程；系统会管理所有进程的执行，通过调度程序来分配计算资源等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## The following modules must be imported</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">## this is the code to execute</span></span><br><span class="line">program = <span class="string">&quot;python&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process calling&quot;</span>)</span><br><span class="line">arguments = [<span class="string">&quot;called_Process.py&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">## we call the called_Process.py script</span></span><br><span class="line">os.execvp(program, (program,) + <span class="built_in">tuple</span>(arguments))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Good Bye!!&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello Python Parallel Cookbook!!&quot;</span>)</span><br><span class="line">closeInput = <span class="built_in">input</span>(<span class="string">&quot;Press ENTER to exit&quot;</span>)</span><br><span class="line"><span class="built_in">print</span><span class="string">&quot;Closing calledProcess&quot;</span></span><br></pre></td></tr></table></figure><p>线程创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To use threads you need import Thread using the following code:</span></span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread</span><br><span class="line"><span class="comment"># Also we use the sleep function to make the thread &quot;sleep&quot;</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># To create a thread in Python you&#x27;ll want to make your class work as a thread.</span></span><br><span class="line"><span class="comment"># For this, you should subclass your class from the Thread class</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookBook</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.message = <span class="string">&quot;Hello Parallel Python CookBook!!\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># this method prints only the message</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">print_message</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(self.message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The run method prints ten times the message</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Thread Starting\n&quot;</span>)</span><br><span class="line">        x = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> (x &lt; <span class="number">10</span>):</span><br><span class="line">            self.print_message()</span><br><span class="line">            sleep(<span class="number">2</span>)</span><br><span class="line">            x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Thread Ended\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># start the main process</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process Started&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an instance of the HelloWorld class</span></span><br><span class="line">hello_Python = CookBook()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the message...starting the thread</span></span><br><span class="line">hello_Python.start()</span><br><span class="line"></span><br><span class="line"><span class="comment"># end the main process</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process Ended&quot;</span>)</span><br></pre></td></tr></table></figure><p>Python解释器并不完全是线程安全的。为了支持多线程的Python程序，CPython使用了一个叫做全局解释器锁（Global Interpreter Lock， GIL）的技术。这意味着同一时间只有一个线程可以执行Python代码；执行某一个线程一小段时间之后，Python会自动切换到下一个线程。GIL并没有完全解决线程安全的问题，如果多个线程试图使用共享数据，还是可能导致未确定的行为。</p><h2 id="线程的并行"><a href="#线程的并行" class="headerlink" title="线程的并行"></a>线程的并行</h2><blockquote><p>在软件应用中使用最广泛的并发编程范例是多线程。通常，一个应用有一个进程，分成多个独立的线程，并行运行、互相配合，执行不同类型的任务。</p><p>线程是独立的处理流程，可以和系统的其他线程并行或并发地执行。多线程可以共享数据和资源，利用所谓的共享内存空间。线程和进程的具体实现取决于你要运行的操作系统，但是总体来讲，我们可以说线程是包含在进程中的，同一进程的多个不同的线程可以共享相同的资源。相比而言，进程之间不会共享资源。</p><p>每一个线程基本上包含3个元素：程序计数器，寄存器和栈。与同一进程的其他线程共享的资源基本上包括数据和系统资源。每一个线程也有自己的运行状态，可以和其他线程同步，这点和进程一样。线程的状态大体上可以分为ready,running,blocked。线程的典型应用是应用软件的并行化——为了充分利用现代的多核处理器，使每个核心可以运行单个线程。相比于进程，使用线程的优势主要是性能。相比之下，在进程之间切换上下文要比在统一进程的多线程之间切换上下文要重的多。</p></blockquote><p>多线程编程一般使用共享内容空间进行线程间的通讯。这就使管理内容空间成为多线程编程的重点和难点。</p><p>使用Python的<code>threading</code>模块管理多线程.</p><p>线程被创建之后并不会马上运行，需要手动调用 <code>start()</code> ， <code>join()</code> 让调用它的线程一直等待直到执行结束</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function called by thread %i&quot;</span> % i)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">slowProcess</span>():</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;slow process done&quot;</span>)</span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=slowProcess, args=())</span><br><span class="line">t.start()</span><br><span class="line">t.join()</span><br><span class="line">t = threading.Thread(target=func, args=(<span class="number">1</span>,))</span><br><span class="line">t.start()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;process quit&quot;</span>)</span><br></pre></td></tr></table></figure><p>设置<code>t = threading.Thread(target=slowProcess, args=(),name=&quot;slowp&quot;)</code>线程名称</p><p>线程被创建之后并不会马上运行，需要手动调用 <code>start()</code>.此外<code>join()</code> 让<strong>调用它的线程一直等待直到执行结束</strong>（即阻塞调用它的主线程， <code>t</code> 线程执行结束，主线程才会继续执行）</p><p><code>threading.current_thread().name</code>访问执行当前代码的线程的名称.</p><p>主线程是<code>MainThread</code>. 实现多线程可以选择继承threading.Thread类或者直接使用<code>threading.Thread</code>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">exitFlag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myThread</span>(<span class="params">threading.Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, threadID, name, counter</span>):</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.threadID = threadID</span><br><span class="line">        self.name = name</span><br><span class="line">        self.counter = counter</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;starting &quot;</span> + self.name)</span><br><span class="line">        print_time(self.name, self.counter, <span class="number">5</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;exiting &quot;</span> + self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_time</span>(<span class="params">threadName, delay, counter</span>):</span></span><br><span class="line">    <span class="keyword">while</span> counter:</span><br><span class="line">        <span class="keyword">if</span> exitFlag:</span><br><span class="line">            _thread.exit()</span><br><span class="line">        time.sleep(delay)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s:%s&quot;</span> % (threadName, time.ctime(time.time())))</span><br><span class="line">        counter -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">thread1 = myThread(<span class="number">1</span>, <span class="string">&quot;thread-1&quot;</span>, <span class="number">1</span>)</span><br><span class="line">thread2 = myThread(<span class="number">2</span>, <span class="string">&quot;thread-2&quot;</span>, <span class="number">2</span>)</span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line">thread1.join()</span><br><span class="line">thread2.join()</span><br></pre></td></tr></table></figure><p><code>threading</code> 模块是创建和管理线程的首选形式。每一个线程都通过一个继承 <code>Thread</code> 类，重写 <code>run()</code> 方法来实现逻辑，这个方法是线程的入口。在主程序中，我们创建了多个 <code>myThread</code> 的类型实例，然后执行 <code>start()</code> 方法启动它们。调用 <code>Thread.__init__</code> 构造器方法是必须的，通过它我们可以给线程定义一些名字或分组之类的属性。调用 <code>start()</code> 之后线程变为活跃状态，并且持续直到 <code>run()</code> 结束，或者中间出现异常。所有的线程都执行完成之后，程序结束。</p><h3 id="线程的同步"><a href="#线程的同步" class="headerlink" title="线程的同步"></a>线程的同步</h3><p>当两个或以上对共享内存的操作发生在并发线程中，并且至少有一个可以改变数据，又没有同步机制的条件下，就会产生竞争条件，可能会导致执行无效代码、bug、或异常行为。</p><h4 id="Lock锁同步"><a href="#Lock锁同步" class="headerlink" title="Lock锁同步"></a>Lock锁同步</h4><p>竞争条件最简单的解决方法是使用锁。锁的操作非常简单，当一个线程需要访问部分共享内存时，它必须先获得锁才能访问。此线程对这部分共享资源使用完成之后，该线程必须释放锁，然后其他线程就可以拿到这个锁并访问这部分资源了。</p><p>使用lock同步线程,通过它我们可以将共享资源某一时刻的访问限制在单一线程或单一类型的线程上，线程必须得到锁才能使用资源，并且之后必须允许其他线程使用相同的资源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">shared_resource_with_lock = <span class="number">0</span></span><br><span class="line">shared_resource_with_no_lock = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">COUNT = <span class="number">1000000</span></span><br><span class="line"></span><br><span class="line">shared_resource_lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_with_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_lock.acquire()</span><br><span class="line">        shared_resource_with_lock += <span class="number">1</span></span><br><span class="line">        shared_resource_lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrement_with_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_lock.acquire()</span><br><span class="line">        shared_resource_with_lock  -=<span class="number">1</span></span><br><span class="line">        shared_resource_lock.release()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_without_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_no_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_with_no_lock +=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrement_without_lock</span>():</span></span><br><span class="line">    <span class="keyword">global</span> shared_resource_with_no_lock</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(COUNT):</span><br><span class="line">        shared_resource_with_no_lock -=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    t1 = threading.Thread(target=increment_with_lock)</span><br><span class="line">    t2 = threading.Thread(target=decrement_with_lock)</span><br><span class="line">    t3 = threading.Thread(target=increment_without_lock)</span><br><span class="line">    t4 = threading.Thread(target=decrement_without_lock)</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t3.start()</span><br><span class="line">    t4.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    t3.join()</span><br><span class="line">    t4.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the value of shared variable with lock management is %s&quot;</span> % shared_resource_with_lock)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;the value of shared variable with race condition is %s&quot;</span> % shared_resource_with_no_lock)</span><br></pre></td></tr></table></figure><ul><li>锁有两种状态： locked（被某一线程拿到）和unlocked（可用状态）</li><li><p>我们有两个方法来操作锁： <code>acquire()</code> 和 <code>release()</code></p></li><li><p>如果状态是unlocked， 可以调用 <code>acquire()</code> 将状态改为locked</p></li><li>如果状态是locked， <code>acquire()</code> 会被block直到另一线程调用 <code>release()</code> 释放锁</li><li>如果状态是unlocked， 调用 <code>release()</code> 将导致 <code>RuntimError</code> 异常</li><li>如果状态是locked， 可以调用 <code>release()</code> 将状态改为unlocked</li></ul><blockquote><p>尽管理论上行得通，但是锁的策略不仅会导致有害的僵持局面。还会对应用程序的其他方面产生负面影响。这是一种保守的方法，经常会引起不必要的开销，也会限制程序的可扩展性和可读性。更重要的是，有时候需要对多进程共享的内存分配优先级，使用锁可能和这种优先级冲突。最后，从实践的经验来看，使用锁的应用将对debug带来不小的麻烦。所以，最好使用其他可选的方法确保同步读取共享内存，避免竞争条件。</p></blockquote><p>事实上我执行这段代码时跟线程是否join有关,基本上上面代码是否加锁都没有出问题</p><h4 id="RLock锁同步"><a href="#RLock锁同步" class="headerlink" title="RLock锁同步"></a>RLock锁同步</h4><p>如果你想让只有拿到锁的线程才能释放该锁，那么应该使用 <code>RLock()</code> 对象。和 <code>Lock()</code> 对象一样， <code>RLock()</code> 对象有两个方法： <code>acquire()</code> 和 <code>release()</code> 。当你需要在类外面保证线程安全，又要在类内使用同样方法的时候 <code>RLock()</code> 就很实用了</p><blockquote><p>RLock其实叫做“Reentrant Lock”，就是可以重复进入的锁，也叫做“递归锁”。这种锁对比Lock有是三个特点：1. 谁拿到谁释放。如果线程A拿到锁，线程B无法释放这个锁，只有A可以释放；2. 同一线程可以多次拿到该锁，即可以acquire多次；3. acquire多少次就必须release多少次，只有最后一次release才能改变RLock的状态为unlocked</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Box</span>:</span></span><br><span class="line">    lock = threading.RLock()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.total_items = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">execute</span>(<span class="params">self,n</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.total_items += n</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.execute(<span class="number">1</span>)</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">remove</span>(<span class="params">self</span>):</span></span><br><span class="line">        Box.lock.acquire()</span><br><span class="line">        self.execute(-<span class="number">1</span>)</span><br><span class="line">        Box.lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adder</span>(<span class="params">box, items</span>):</span></span><br><span class="line">    <span class="keyword">while</span> items &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;adding 1 item in the box&quot;</span>)</span><br><span class="line">        box.add()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        items -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remover</span>(<span class="params">box, items</span>):</span></span><br><span class="line">    <span class="keyword">while</span> items &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;removing 1 item in the box&quot;</span>)</span><br><span class="line">        box.remove()</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        items -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    items = <span class="number">5</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;putting %s items in the box &quot;</span> % items)</span><br><span class="line">    box = Box()</span><br><span class="line">    t1 = threading.Thread(target=adder, args=(box, items))</span><br><span class="line">    t2 = threading.Thread(target=remover, args=(box, items))</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s items still remain in the box &quot;</span> % Box().total_items)</span><br></pre></td></tr></table></figure><p>相比于Lock有一些更稳定的设定.</p><h4 id="信号量同步"><a href="#信号量同步" class="headerlink" title="信号量同步"></a>信号量同步</h4><p>信号量是一个内部数据，用于标明当前的共享资源可以有多少并发读取。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">semaphore = threading.Semaphore(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;consumer is waiting.&quot;</span>)</span><br><span class="line">    semaphore.acquire()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed item number %s &quot;</span> % item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>():</span></span><br><span class="line">    <span class="keyword">global</span> item</span><br><span class="line">    time.sleep(<span class="number">10</span>)</span><br><span class="line">    item = random.randint(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;producer notify:produced item number %s&quot;</span> % item)</span><br><span class="line">    semaphore.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">        t1 = threading.Thread(target=producer)</span><br><span class="line">        t2 = threading.Thread(target=consumer)</span><br><span class="line">        t1.start()</span><br><span class="line">        t2.start()</span><br><span class="line">        t1.join()</span><br><span class="line">        t2.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;program terminated&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>信号量的一个特殊用法是互斥量。互斥量是初始值为1的信号量，可以实现数据、资源的互斥访问。</p><p>信号量在支持多线程的编程语言中依然应用很广，然而这可能导致死锁的情况。例如，现在有一个线程t1先等待信号量s1，然后等待信号量s2，而线程t2会先等待信号量s2，然后再等待信号量s1，这样就可能会发生死锁，导致t1等待s2，但是t2在等待s1。</p></blockquote><h4 id="条件进行同步"><a href="#条件进行同步" class="headerlink" title="条件进行同步"></a>条件进行同步</h4><p>条件指的是应用程序状态的改变。这是另一种同步机制，其中某些线程在等待某一条件发生，其他的线程会在该条件发生的时候进行通知。一旦条件发生，线程会拿到共享资源的唯一权限</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread, Condition</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">items = []</span><br><span class="line">condition = Condition()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">consumer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">consume</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> condition</span><br><span class="line">        <span class="keyword">global</span> items</span><br><span class="line">        condition.acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(items) == <span class="number">0</span>:</span><br><span class="line">            condition.wait()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;consumer notify: no item to consume&quot;</span>)</span><br><span class="line">        items.pop()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed 1 item&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;consumer notify: items to consume are &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">20</span>):</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            self.consume()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">producer</span>(<span class="params">Thread</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">produce</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> condition</span><br><span class="line">        <span class="keyword">global</span> items</span><br><span class="line">        condition.acquire()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(items) == <span class="number">10</span>:</span><br><span class="line">            condition.wait()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Producer notify : items producted are &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Producer notify : stop the production!!&quot;</span>)</span><br><span class="line">        items.append(<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Producer notify : total items producted &quot;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(items)))</span><br><span class="line">        condition.notify()</span><br><span class="line">        condition.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">20</span>):</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            self.produce()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    producer = producer()</span><br><span class="line">    consumer = consumer()</span><br><span class="line">    producer.start()</span><br><span class="line">    consumer.start()</span><br><span class="line">    producer.join()</span><br><span class="line">    consumer.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="事件同步"><a href="#事件同步" class="headerlink" title="事件同步"></a>事件同步</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread,Event</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">items = []</span><br><span class="line">event = Event()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">consumer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,items,event</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.items = items</span><br><span class="line">        self.event = event</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            time.sleep(<span class="number">10</span>)</span><br><span class="line">            self.event.wait()</span><br><span class="line">            item = self.items.pop()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;consumer notify: consumed 1 item&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">producer</span>(<span class="params">Thread</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, items, event</span>):</span></span><br><span class="line">        Thread.__init__(self)</span><br><span class="line">        self.items = items</span><br><span class="line">        self.event = event</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">global</span> item</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            item = random.randint(<span class="number">0</span>, <span class="number">256</span>)</span><br><span class="line">            self.items.append(item)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Producer notify : item N° %d appended to list by %s&#x27;</span> % (item, self.name))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Producer notify : event set by %s&#x27;</span> % self.name)</span><br><span class="line">            self.event.<span class="built_in">set</span>()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Produce notify : event cleared by %s &#x27;</span>% self.name)</span><br><span class="line">            self.event.clear()</span><br><span class="line">            </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    t1 = producer(items, event)</span><br><span class="line">    t2 = consumer(items, event)</span><br><span class="line">    t1.start()</span><br><span class="line">    t2.start()</span><br><span class="line">    t1.join()</span><br><span class="line">    t2.join()</span><br></pre></td></tr></table></figure><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/event.png" alt="../_images/event.png" style="zoom:67%;" /></p><h4 id="使用with简化"><a href="#使用with简化" class="headerlink" title="使用with简化"></a>使用with简化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.DEBUG, <span class="built_in">format</span>=<span class="string">&#x27;(%(threadName)-10s) %(message)s&#x27;</span>,)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">threading_with</span>(<span class="params">statement</span>):</span></span><br><span class="line">    <span class="keyword">with</span> statement:</span><br><span class="line">        logging.debug(<span class="string">&#x27;%s acquired via with&#x27;</span> % statement)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">threading_not_with</span>(<span class="params">statement</span>):</span></span><br><span class="line">    statement.acquire()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logging.debug(<span class="string">&#x27;%s acquired directly&#x27;</span> % statement )</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        statement.release()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># let&#x27;s create a test battery</span></span><br><span class="line">    lock = threading.Lock()</span><br><span class="line">    rlock = threading.RLock()</span><br><span class="line">    condition = threading.Condition()</span><br><span class="line">    mutex = threading.Semaphore(<span class="number">1</span>)</span><br><span class="line">    threading_synchronization_list = [lock, rlock, condition, mutex]</span><br><span class="line">    <span class="comment"># in the for cycle we call the threading_with e threading_no_with function</span></span><br><span class="line">    <span class="keyword">for</span> statement <span class="keyword">in</span> threading_synchronization_list :</span><br><span class="line">       t1 = threading.Thread(target=threading_with, args=(statement,))</span><br><span class="line">       t2 = threading.Thread(target=threading_not_with, args=(statement,))</span><br><span class="line">       t1.start()</span><br><span class="line">       t2.start()</span><br><span class="line">       t1.join()</span><br><span class="line">       t2.join()</span><br></pre></td></tr></table></figure><p>此外,当线程之间共享资源时,可以利用上面的原语,也可以使用queue.队列操作起来更容易，也使多线程编程更安全，因为队列可以将资源的使用通过单线程进行完全控制，并且允许使用更加整洁和可读性更高的设计模式。</p><p>Queue常用的方法有以下四个：</p><ul><li><code>put()</code>: 往queue中放一个item</li><li><code>get()</code>: 从queue删除一个item，并返回删除的这个item</li><li><code>task_done()</code>: 每次item被处理的时候需要调用这个方法</li><li><code>join()</code>: 所有item都被处理之前一直阻塞</li></ul><h3 id="进程的并行"><a href="#进程的并行" class="headerlink" title="进程的并行"></a>进程的并行</h3><p>由父进程创建子进程。父进程既可以在产生子进程之后继续异步执行，也可以暂停等待子进程创建完成之后再继续执行.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">i</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;called function in process:%s&#x27;</span> % i)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Process_jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=foo, args=(i,))</span><br><span class="line">        Process_jobs.append(p)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br></pre></td></tr></table></figure><p>使用进程对象调用 <code>join()</code> 方法。如果没有 <code>join()</code> ，主进程退出之后子进程会留在idle中，必须手动杀死它们。</p><p>进程名字与获取与线程类似.</p><h4 id="后台运行进程"><a href="#后台运行进程" class="headerlink" title="后台运行进程"></a>后台运行进程</h4><blockquote><p>如果需要处理比较巨大的任务，又不需要人为干预，将其作为后台进程执行是个非常常用的编程模型。此进程又可以和其他进程并发执行。通过Python的multiprocessing模块的后台进程选项，我们可以让进程在后台运行</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    name = multiprocessing.current_process().name</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting %s \n&quot;</span> % name)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exiting %s \n&quot;</span> % name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    background_process = multiprocessing.Process(name=<span class="string">&quot;background_process&quot;</span>, target=foo)</span><br><span class="line">    background_process.daemon = <span class="literal">True</span></span><br><span class="line">    no_background_process = multiprocessing.Process(name=<span class="string">&quot;no_background_process&quot;</span>, target=foo)</span><br><span class="line">    no_background_process.daemon = <span class="literal">False</span></span><br><span class="line">    background_process.start()</span><br><span class="line">    no_background_process.start()</span><br></pre></td></tr></table></figure><p>为了在后台运行进程，我们设置 <code>daemon</code> 参数为 <code>True</code></p><p>在非后台运行的进程会看到一个输出，后台运行的没有输出，<strong>后台运行进程在主进程结束之后会自动结束</strong></p><blockquote><p>注意，后台进程不允许创建子进程。否则，当后台进程跟随父进程退出的时候，子进程会变成孤儿进程。另外，它们并不是Unix的守护进程或服务（daemons or services），所以当非后台进程退出，它们会被终结。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;i=&#123;&#125;,foo thread daemon is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(i, threading.current_thread().daemon))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=foo, name=<span class="string">&#x27;foo_thread&#x27;</span>, daemon=<span class="literal">True</span>) <span class="comment"># set daemon to True to make it a daemon thread which will exit when the main thread exits</span></span><br><span class="line">t.start()</span><br><span class="line"><span class="comment"># t.join() important otherwise the main thread will exit before the foo thread</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Main thread daemon is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(threading.current_thread().daemon))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Main Thread Exit.&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="杀掉进程"><a href="#杀掉进程" class="headerlink" title="杀掉进程"></a>杀掉进程</h4><p>可以使用 <code>terminate()</code> 方法立即杀死一个进程。另外，我们可以使用 <code>is_alive()</code> 方法来判断一个进程是否还存活。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Starting function&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished function&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    p = multiprocessing.Process(target=foo)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process before execution:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.start()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process running:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.terminate()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process terminated:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    p.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process joined:&#x27;</span>, p, p.is_alive())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Process exit code:&#x27;</span>, p.exitcode)</span><br></pre></td></tr></table></figure><p>进程的 <code>ExitCode</code> 状态码（status code）验证进程已经结束， <code>ExitCode</code> 可能的值如下：</p><ul><li>== 0: 没有错误正常退出</li><li>> 0: 进程有错误，并以此状态码退出</li><li>&lt; 0: 进程被 <code>-1 *</code> 的信号杀死并以此作为 ExitCode 退出</li></ul><h4 id="子类中使用进程"><a href="#子类中使用进程" class="headerlink" title="子类中使用进程"></a>子类中使用进程</h4><p>实现一个自定义的进程子类，需要以下三步：</p><ul><li>定义 <code>Process</code> 的子类</li><li>覆盖 <code>__init__(self [,args])</code> 方法来添加额外的参数</li><li>覆盖 <code>run(self, [.args])</code> 方法来实现 <code>Process</code> 启动的时候执行的任务</li></ul><p>创建 <code>Porcess</code> 子类之后，你可以创建它的实例并通过 <code>start()</code> 方法启动它，启动之后会运行 <code>run()</code> 方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 自定义子类进程</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProcess</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&#x27;called run method in process: %s&#x27;</span> % self.name)</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    timestart = timeit.default_timer()</span><br><span class="line">    jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            p = MyProcess()</span><br><span class="line">            jobs.append(p)</span><br><span class="line">            p.start()</span><br><span class="line">            p.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Time elapsed:&#x27;</span>, (timeit.default_timer() - timestart))</span><br></pre></td></tr></table></figure><p><code>join()</code> 命令可以让主进程等待其他进程结束最后退出。</p><h4 id="进程中交换对象"><a href="#进程中交换对象" class="headerlink" title="进程中交换对象"></a>进程中交换对象</h4><blockquote><p>并行应用常常需要在进程之间交换数据。Multiprocessing库有两个Communication Channel可以交换对象：队列(queue)和管道（pipe）</p></blockquote><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/communication-channel.png" alt="../_images/communication-channel.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Producer</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, queue</span>):</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            item = random.randint(<span class="number">0</span>, <span class="number">256</span>)</span><br><span class="line">            self.queue.put(item)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Process Producer : item %d appended to queue %s&quot;</span> % (item, self.name))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;The size of queue is %s&quot;</span> % self.queue.qsize())</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Consumer</span>(<span class="params">multiprocessing.Process</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, queue</span>):</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.queue = queue</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> self.queue.empty():</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;the queue is empty&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                time.sleep(<span class="number">2</span>)</span><br><span class="line">                item = self.queue.get()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Process Consumer : item %d popped from by %s \n&#x27;</span> % (item, self.name))</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    queue = multiprocessing.Queue()</span><br><span class="line">    process_producer = Producer(queue)</span><br><span class="line">    process_consumer = Consumer(queue)</span><br><span class="line">    process_producer.start()</span><br><span class="line">    process_consumer.start()</span><br><span class="line">    process_producer.join()</span><br><span class="line">    process_consumer.join()</span><br></pre></td></tr></table></figure><p>队列还有一个 <code>JoinableQueue</code> 子类，它有以下两个额外的方法：</p><ul><li><code>task_done()</code>: 此方法意味着之前入队的一个任务已经完成，比如， <code>get()</code> 方法从队列取回item之后调用。所以此方法只能被队列的消费者调用。</li><li><code>join()</code>: 此方法将进程阻塞，直到队列中的item全部被取出并执行。</li></ul><p>此外还可以通过Pipe交换对象.</p><h4 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h4><p>多个进程可以协同工作来完成一项任务。通常需要共享数据。所以在多进程之间保持数据的一致性就很重要了。需要共享数据协同的进程必须以适当的策略来读写数据。相关的同步原语和线程的库很类似。</p><p>进程的同步原语如下：</p><ul><li><strong>Lock</strong>: 这个对象可以有两种装填：锁住的（locked）和没锁住的（unlocked）。一个Lock对象有两个方法， <code>acquire()</code> 和 <code>release()</code> ，来控制共享数据的读写权限。</li><li><strong>Event</strong>: 实现了进程间的简单通讯，一个进程发事件的信号，另一个进程等待事件的信号。 <code>Event</code> 对象有两个方法， <code>set()</code> 和 <code>clear()</code> ，来管理自己内部的变量。</li><li><strong>Condition</strong>: 此对象用来同步部分工作流程，在并行的进程中，有两个基本的方法： <code>wait()</code> 用来等待进程， <code>notify_all()</code> 用来通知所有等待此条件的进程。</li><li><strong>Semaphore</strong>: 用来共享资源，例如，支持固定数量的共享连接。</li><li><strong>Rlock</strong>: 递归锁对象。其用途和方法同 <code>Threading</code> 模块一样。</li><li><strong>Barrier</strong>: 将程序分成几个阶段，适用于有些进程必须在某些特定进程之后执行。处于障碍（Barrier）之后的代码不能同处于障碍之前的代码并行。</li></ul><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import multiprocessing</span><br><span class="line">from multiprocessing import Barrier, Lock, Process</span><br><span class="line">from time import time</span><br><span class="line">from datetime import datetime</span><br><span class="line"></span><br><span class="line">def test<span class="constructor">_with_barrier(<span class="params">synchronizer</span>, <span class="params">serializer</span>)</span>:</span><br><span class="line">    name = multiprocessing.current<span class="constructor">_process()</span>.name</span><br><span class="line">    synchronizer.wait<span class="literal">()</span></span><br><span class="line">    now = time<span class="literal">()</span></span><br><span class="line">    <span class="keyword">with</span> serializer:</span><br><span class="line">        print(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line">def test<span class="constructor">_without_barrier()</span>:</span><br><span class="line">    name = multiprocessing.current<span class="constructor">_process()</span>.name</span><br><span class="line">    now = time<span class="literal">()</span></span><br><span class="line">    print(<span class="string">&quot;process %s ----&gt; %s&quot;</span> % (name, datetime.fromtimestamp(now)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__<span class="operator"> == </span>&#x27;__main__&#x27;:</span><br><span class="line">    synchronizer = <span class="constructor">Barrier(2)</span></span><br><span class="line">    serializer = <span class="constructor">Lock()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p1</span> - <span class="params">test_with_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_with_barrier</span>, <span class="params">args</span>=(<span class="params">synchronizer</span>,<span class="params">serializer</span>)</span>).start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p2</span> - <span class="params">test_with_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_with_barrier</span>, <span class="params">args</span>=(<span class="params">synchronizer</span>,<span class="params">serializer</span>)</span>).start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p3</span> - <span class="params">test_without_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_without_barrier</span>)</span>.start<span class="literal">()</span></span><br><span class="line">    <span class="constructor">Process(<span class="params">name</span>=&#x27;<span class="params">p4</span> - <span class="params">test_without_barrier</span>&#x27;, <span class="params">target</span>=<span class="params">test_without_barrier</span>)</span>.start<span class="literal">()</span></span><br></pre></td></tr></table></figure><h4 id="进程之间管理状态"><a href="#进程之间管理状态" class="headerlink" title="进程之间管理状态"></a>进程之间管理状态</h4><p>Python的多进程模块提供了在所有的用户间管理共享信息的管理者(Manager)。一个管理者对象控制着持有Python对象的服务进程，并允许其它进程操作共享对象。</p><p>管理者有以下特性：</p><ul><li>它控制着管理共享对象的服务进程</li><li>它确保当某一进程修改了共享对象之后，所有的进程拿到额共享对象都得到了更新</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker</span>(<span class="params">dictionary,key,item</span>):</span></span><br><span class="line">    dictionary[key] = item</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;key = %d value = %d&quot;</span> %(key,item))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    mgr = multiprocessing.Manager()</span><br><span class="line">    dictionary = mgr.<span class="built_in">dict</span>()</span><br><span class="line">    jobs = [ multiprocessing.Process(target=worker,args=(dictionary,i,i*<span class="number">2</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">        j.start()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span>  jobs:</span><br><span class="line">        j.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Results:&#x27;</span>,dictionary)</span><br></pre></td></tr></table></figure><h4 id="使用进程池"><a href="#使用进程池" class="headerlink" title="使用进程池"></a>使用进程池</h4><p>多进程库提供了 <code>Pool</code> 类来实现简单的多进程任务。 <code>Pool</code> 类有以下方法：</p><ul><li><code>apply()</code>: 直到得到结果之前一直阻塞。</li><li><code>apply_async()</code>: 这是 <code>apply()</code> 方法的一个变体，返回的是一个result对象。这是一个异步的操作，在所有的子类执行之前不会锁住主进程。</li><li><code>map()</code>: 这是内置的 <code>map()</code> 函数的并行版本。在得到结果之前一直阻塞，此方法将可迭代的数据的每一个元素作为进程池的一个任务来执行。</li><li><code>map_async()</code>: 这是 <code>map()</code> 方法的一个变体，返回一个result对象。如果指定了回调函数，回调函数应该是callable的，并且只接受一个参数。当result准备好时会自动调用回调函数（除非调用失败）。回调函数应该立即完成，否则，持有result的进程将被阻塞。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_square</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Hi&quot;</span>)</span><br><span class="line">    time.sleep(<span class="number">12</span>)</span><br><span class="line">    result = data*data</span><br><span class="line">    <span class="built_in">print</span>(multiprocessing.current_process().pid)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    inputs = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">    pool = multiprocessing.Pool(<span class="number">2</span>)</span><br><span class="line">    pool_outputs = pool.map_async(function_square, inputs)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Pool:&#x27;</span>, pool_outputs)</span><br><span class="line">    <span class="comment"># p.daemon = True</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="异步编程"><a href="#异步编程" class="headerlink" title="异步编程"></a>异步编程</h3><h4 id="concurrent-futures"><a href="#concurrent-futures" class="headerlink" title="concurrent.futures"></a>concurrent.futures</h4><p><code>concurrent.futures</code> 模块，这个模块具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能。</p><p>此模块由以下部分组成：</p><ul><li><code>concurrent.futures.Executor</code>: 这是一个虚拟基类，提供了异步执行的方法。</li><li><code>submit(function, argument)</code>: 调度函数（可调用的对象）的执行，将 <code>argument</code> 作为参数传入。</li><li><code>map(function, argument)</code>: 将 <code>argument</code> 作为参数执行函数，以 <strong>异步</strong> 的方式。</li><li><code>shutdown(Wait=True)</code>: 发出让执行者释放所有资源的信号。</li><li><code>concurrent.futures.Future</code>: 其中包括函数的异步执行。Future对象是submit任务（即带有参数的functions）到executor的实例。</li></ul><p>Executor是抽象类，可以通过子类访问，即线程或进程的 <code>ExecutorPools</code> 。因为，线程或进程的实例是依赖于资源的任务，所以最好以“池”的形式将他们组织在一起，作为可以重用的launcher或executor。</p><blockquote><p>线程池或进程池是用于在程序中优化和简化线程/进程的使用。通过池，你可以提交任务给executor。池由两部分组成，一部分是内部的队列，存放着待执行的任务；另一部分是一系列的进程或线程，用于执行这些任务。池的概念主要目的是为了重用：让线程或进程在生命周期内可以多次使用。它减少了创建创建线程和进程的开销，提高了程序性能。重用不是必须的规则，但它是程序员在应用中使用池的主要原因。</p></blockquote><p><img data-src="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/pooling-management.png" alt="../_images/pooling-management.png" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">number_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_item</span>(<span class="params">x</span>):</span></span><br><span class="line">    result_item = count(x)</span><br><span class="line">    <span class="keyword">return</span> result_item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span>(<span class="params">number</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10000000</span>):</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> i * number</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> number_list:</span><br><span class="line">        <span class="built_in">print</span>(evaluate_item(item))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sequential execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line">    start_time_1 = time.time()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">5</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        futures = [executor.submit(evaluate_item, item) <span class="keyword">for</span> item <span class="keyword">in</span> number_list]</span><br><span class="line">        <span class="keyword">for</span> future <span class="keyword">in</span> concurrent.futures.as_completed(futures):</span><br><span class="line">            <span class="built_in">print</span>(future.result())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Thread pool execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time_1), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line">    start_time_2 = time.time()</span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ProcessPoolExecutor(max_workers=<span class="number">5</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        futures = [executor.submit(evaluate_item, item) <span class="keyword">for</span> item <span class="keyword">in</span> number_list]</span><br><span class="line">        <span class="keyword">for</span> future <span class="keyword">in</span> concurrent.futures.as_completed(futures):</span><br><span class="line">            <span class="built_in">print</span>(future.result())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Process pool execution in &quot;</span> + <span class="built_in">str</span>(time.time() - start_time_2), <span class="string">&quot;seconds&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个计算平方的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个ThreadPoolExecutor对象，设置线程池中的线程数量为2</span></span><br><span class="line"><span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">2</span>) <span class="keyword">as</span> executor:</span><br><span class="line">    <span class="comment"># 提交任务给线程池，并获取Future对象</span></span><br><span class="line">    future1 = executor.submit(square, <span class="number">5</span>)</span><br><span class="line">    future2 = executor.submit(square, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取任务的执行结果</span></span><br><span class="line">    result1 = future1.result()</span><br><span class="line">    result2 = future2.result()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Result 1: <span class="subst">&#123;result1&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Result 2: <span class="subst">&#123;result2&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Asyncio管理事件循环"><a href="#Asyncio管理事件循环" class="headerlink" title="Asyncio管理事件循环"></a>Asyncio管理事件循环</h4><p>Python的Asyncio模块提供了管理事件、协程、任务和线程的方法，以及编写并发代码的原语。此模块的主要组件和概念包括：</p><ul><li><strong>事件循环</strong>: 在Asyncio模块中，每一个进程都有一个事件循环。</li><li><strong>协程</strong>: 这是子程序的泛化概念。协程可以在执行期间暂停，这样就可以等待外部的处理（例如IO）完成之后，从之前暂停的地方恢复执行。</li><li><strong>Futures</strong>: 定义了 <code>Future</code> 对象，和 <code>concurrent.futures</code> 模块一样，表示尚未完成的计算。</li><li><strong>Tasks</strong>: 这是Asyncio的子类，用于封装和管理并行模式下的协程。</li></ul><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable"><span class="keyword">while</span></span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="variable">events</span> = <span class="function"><span class="title">getEvents</span>();</span></span><br><span class="line"><span class="function">    <span class="variable">for</span> (<span class="variable">e</span> <span class="variable"><span class="keyword">in</span></span> <span class="variable">events</span>)</span></span><br><span class="line">        <span class="function"><span class="title">processEvent</span>(<span class="variable">e</span>);</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>可以产生事件的实体叫做事件源</strong>，<strong>能处理事件的实体叫做事件处理者</strong>。此外，<strong>还有一些第三方实体叫做事件循环</strong>。它的作用是管理所有的事件，在整个程序运行过程中不断循环执行，追踪事件发生的顺序将它们放到队列中，当主线程空闲的时候，调用相应的事件处理者处理事件。</p></blockquote><p>Asyncio提供了一下方法来管理事件循环：</p><ul><li><code>loop = get_event_loop()</code>: 得到当前上下文的事件循环。</li><li><code>loop.call_later(time_delay, callback, argument)</code>: 延后 <code>time_delay</code> 秒再执行 <code>callback</code> 方法。</li><li><code>loop.call_soon(callback, argument)</code>: 尽可能快调用 <code>callback</code>, <code>call_soon()</code> 函数结束，主线程回到事件循环之后就会马上调用 <code>callback</code> 。</li><li><code>loop.time()</code>: 以float类型返回当前时间循环的内部时间。</li><li><code>asyncio.set_event_loop()</code>: 为当前上下文设置事件循环。</li><li><code>asyncio.new_event_loop()</code>: 根据此策略创建一个新的时间循环并返回。</li><li><code>loop.run_forever()</code>: 在调用 <code>stop()</code> 之前将一直运行。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_1</span>(<span class="params">end_time,loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function_1 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_2, end_time,loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_2</span>(<span class="params">end_time,loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;function_2 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> loop.time() + <span class="number">1.0</span> &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_3, end_time,loop)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_3</span>(<span class="params">end_time, loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;function_3 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_1, end_time, loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_4</span>(<span class="params">end_time, loop</span>):</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;function_5 called&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (loop.time() + <span class="number">1.0</span>) &lt; end_time:</span><br><span class="line">        loop.call_later(<span class="number">1</span>, function_4, end_time, loop)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loop.stop()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    end_loop = loop.time() + <span class="number">9.0</span></span><br><span class="line">    loop.call_soon(function_1, end_loop,loop)</span><br><span class="line">    loop.run_forever()</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Asyncio管理协程"><a href="#Asyncio管理协程" class="headerlink" title="Asyncio管理协程"></a>Asyncio管理协程</h4><p>子程序不能单独执行，只能在主程序的请求下执行，主程序负责协调使用各个子程序。协程就是子程序的泛化。和子程序一样的事，协程只负责计算任务的一步</p><p>和子程序不一样的是，协程没有主程序来进行调度。这是因为协程通过管道连接在一起，没有监视函数负责顺序调用它们。在协程中，执行点可以被挂起，可以被从之前挂起的点恢复执行。通过协程池就可以插入到计算中：运行第一个任务，直到它返回(yield)执行权，然后运行下一个，这样顺着执行下去。</p><p>协程的另外一些重要特性如下：</p><ul><li>协程可以有多个入口点，并可以yield多次</li><li>协程可以将执行权交给其他协程</li></ul><p>yield表示协程在此暂停，并且将执行权交给其他协程。因为协程可以将值与控制权一起传递给另一个协程，所以“yield一个值”就表示将值传给下一个执行的协程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">StartState</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start State called \n&quot;</span>)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State2(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Resume of the Transition : \nStart State calling &quot;</span> + result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State2</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 2 with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Evaluating...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (input_value == <span class="number">0</span>):</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State3(input_value)</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">EndState</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;End State with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Stop Computation...&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> outputValue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State3</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 3 with transition value = %s \n&quot;</span> % transition_value)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;...Evaluating...&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State1(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> EndState(input_value)</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">State1</span>(<span class="params">transition_value</span>):</span></span><br><span class="line">    outputValue = <span class="built_in">str</span>(<span class="string">&quot;State 1 with transition value = &quot;</span> + <span class="built_in">str</span>(transition_value) + <span class="string">&quot; \n&quot;</span>)</span><br><span class="line">    input_value = randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;...Evaluating...&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> input_value == <span class="number">0</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State3(input_value)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        result = <span class="keyword">await</span> State2(input_value)</span><br><span class="line">    result = <span class="string">&quot;State 1 calling &quot;</span> + result</span><br><span class="line">    <span class="keyword">return</span> outputValue + <span class="built_in">str</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(StartState())</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>object async_generator can&#39;t be used in &#39;await&#39; expression</code> async函数中如果使用了yield相当于<code>async_generator</code>,后者中不能使用await.</p><h4 id="Asyncio控制任务"><a href="#Asyncio控制任务" class="headerlink" title="Asyncio控制任务"></a>Asyncio控制任务</h4><blockquote><p>Asyncio是用来处理事件循环中的异步进程和并发任务执行的。它还提供了 <code>asyncio.Task()</code> 类，可以在任务中使用协程。它的作用是，在同一事件循环中,运行某一个任务的同时可以并发地运行多个任务。当协程被包在任务中，它会自动将任务和事件循环连接起来，当事件循环启动的时候，任务自动运行。这样就提供了一个可以自动驱动协程的机制。</p></blockquote><p><code>asyncio.Task(coroutine)</code> 方法来处理计算任务，它可以调度协程的执行。任务对协程对象在事件循环的执行负责。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">factorial</span>(<span class="params">number</span>):</span></span><br><span class="line">    f = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, number + <span class="number">1</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: Compute factorial(%s)&quot;</span> % (i))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        f *= i</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: factorial(%s) = %s&quot;</span> % (number, f))</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span>(<span class="params">number</span>):</span></span><br><span class="line">    a,b = <span class="number">0</span>,<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(number):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: Compute fibonacci(%s)&quot;</span> % (i))</span><br><span class="line">        <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line">        a,b = b, a + b</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Asyncio.Task: fibonacci(%s) = %s&quot;</span> % (number, a))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tasks = [asyncio.Task(factorial(<span class="number">10</span>)), asyncio.Task(fibonacci(<span class="number">10</span>))]</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><h4 id="使用asyncio和futures"><a href="#使用asyncio和futures" class="headerlink" title="使用asyncio和futures"></a>使用asyncio和futures</h4><blockquote><p>Asyncio 模块的另一个重要的组件是 <code>Future</code> 类。它和 <code>concurrent.futures.Futures</code> 很像，但是针对Asyncio的事件循环做了很多定制。 <code>asyncio.Futures</code> 类代表还未完成的结果（有可能是一个Exception）。所以综合来说，它是一种抽象，代表还没有做完的事情。</p></blockquote><ul><li><code>cancel()</code>: 取消future的执行，调度回调函数</li><li><code>result()</code>: 返回future代表的结果</li><li><code>exception()</code>: 返回future中的Exception</li><li><code>add_done_callback(fn)</code>: 添加一个回调函数，当future执行的时候会调用这个回调函数</li><li><code>remove_done_callback(fn)</code>: 从“call whten done”列表中移除所有callback的实例</li><li><code>set_result(result)</code>: 将future标为执行完成，并且设置result的值</li><li><code>set_exception(exception)</code>: 将future标为执行完成，并设置Exception</li></ul><p>类似于js的Promise?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line">future = asyncio.Future</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">first_coroutine</span>(<span class="params">future, N</span>):</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N + <span class="number">1</span>):</span><br><span class="line">        count += i</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">3</span>)</span><br><span class="line">    future.set_result(<span class="string">&quot;First coroutine total count: &quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">second_coroutine</span>(<span class="params">future, N</span>):</span></span><br><span class="line">    count = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, N + <span class="number">1</span>):</span><br><span class="line">        count *= i</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">    future.set_result(<span class="string">&quot;Second coroutine total count: &quot;</span> + <span class="built_in">str</span>(count))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">got_result</span>(<span class="params">future</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(future.result())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    N = <span class="built_in">int</span>(sys.argv[<span class="number">1</span>])</span><br><span class="line">    future1 = asyncio.Future()</span><br><span class="line">    future2 = asyncio.Future()</span><br><span class="line">    tasks = [</span><br><span class="line">        first_coroutine(future1, N),</span><br><span class="line">        second_coroutine(future2, N)</span><br><span class="line">    ]</span><br><span class="line">    future1.add_done_callback(got_result)</span><br><span class="line">    future2.add_done_callback(got_result)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/chapter2/01_Introduction.html">1. 介绍 — python-parallel-programming-cookbook-cn 1.0 文档 (python-parallel-programmning-cookbook.readthedocs.io)</a></li><li><a href="https://blog.csdn.net/weixin_45665318/article/details/106686332">[Python 多线程] 详解daemon属性值None,False,True的区别_daemon=true-CSDN博客</a></li><li><a href="https://ruanyifeng.com/blog/2019/11/python-asyncio.html">Python 异步编程入门 - 阮一峰的网络日志 (ruanyifeng.com)</a></li><li><a href="https://www.freecodecamp.org/chinese/news/introduction-to-python-threading/">介绍 Python 线程及其实现 (freecodecamp.org)</a></li><li><a href="https://medium.com/dev-bits/a-minimalistic-guide-for-understanding-asyncio-in-python-52c436c244ea">A minimalistic guide for understanding asyncio in Python | by Naren Yellavula | Dev bits | Medium</a></li><li><a href="https://tutorialedge.net/python/concurrency/getting-started-with-asyncio-python/">Getting Started with Asyncio in Python | TutorialEdge.net</a></li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;主要是因为Python库的设计很不错,通过这门语言进一步学习并行,涉及到进程线程以及异步编程等.建议是对性能有要求的利用其他语言实现,但是基本的思想、方法是一样的.&lt;br&gt;</summary>
    
    
    
    
    <category term="parallel" scheme="https://www.sekyoro.top/tags/parallel/"/>
    
  </entry>
  
  <entry>
    <title>目标检测学习_P2</title>
    <link href="https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/"/>
    <id>https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/</id>
    <published>2023-10-17T02:21:51.000Z</published>
    <updated>2023-11-02T13:59:11.299Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。</p><span id="more"></span><p>R-CNN家族中的模型都是基于regions的。检测分为两个阶段：</p><p>（1）首先，该模型通过<strong>选择搜索</strong>或<strong>区域建议网络</strong>来提出一组感兴趣的区域。所提出的区域是稀疏的，因为潜在的边界框候选者可以是无限的。</p><p>（2） 然后分类器只处理候选区域。</p><p>这里深入细节实现R-CNN系列的检测网络.</p><h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><blockquote><p>R-CNN (<a href="https://arxiv.org/abs/1311.2524">Girshick et al., 2014</a>) is short for “Region-based Convolutional Neural Networks”. The main idea is composed of two steps. First, using <a href="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#selective-search">selective search</a>, it identifies a manageable number of bounding-box object region candidates (“region of interest” or “RoI”). And then it extracts CNN features from each region independently for classification.</p></blockquote><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png" alt="img"></p><h4 id="选择算法-selective-search"><a href="#选择算法-selective-search" class="headerlink" title="选择算法(selective search)"></a>选择算法(selective search)</h4><p>主要涉及到选择算法,用于提供可能包含对象的区域建议。它建立在图像分割输出的基础上，并使用基于区域的特征（注意：不仅仅是单个像素的属性）来进行自下而上的分层分组。</p><ol><li>在初始化阶段，首先应用Felzenszwalb和Huttenlocher的基于图的图像分割算法来创建区域。</li><li>使用贪婪算法迭代地将区域分组在一起：<ul><li>首先计算所有相邻区域之间的相似性。</li><li>将两个最相似的区域分组在一起，并计算得到的区域与其相邻区域之间的新相似性。</li></ul></li><li>重复对最相似区域进行分组的过程（步骤2），直到整个图像变成单个区域。</li></ol><p>可以使用颜色,材质,大小和形状作为相似度量.</p><p><img data-src="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/selective-search-algorithm.png" alt="img" style="zoom:67%;" /></p><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p>R-CNN流程:</p><ol><li><p>使用一个预训练CNN网络,假设网络输出是K类.</p></li><li><p>通过选择性搜索提出与类别无关的感兴趣区域（每个图像约2k个候选）。这些区域可能包含目标对象，并且它们具有不同的大小。</p></li><li><p>区域被扭曲成一个固定大小.</p></li><li><p>对于第K+1类,在一个扭曲的候选区域上微调CNN(附加的一个类指的是背景（没有感兴趣的对象)）。在微调阶段，我们应该使用更小的学习率，并且小批量对阳性病例进行过采样，因为大多数提出的区域只是背景。</p></li><li><p>给定每个图像区域，通过CNN的一次正向传播生成一个特征向量。然后，该特征向量输入针对每个类独立训练的二进制SVM。</p><p>正样本是IoU&gt;=0.3的区域，而负样本是不相关的其他区域。</p></li><li><p>为了减少定位误差，训练回归模型来使用CNN特征校正边界框校正偏移上的预测检测窗口。</p></li></ol><p><img data-src="https://i.imgur.com/8NYb1o7.png" alt="image-20231022172302856"></p><h4 id="常用技巧"><a href="#常用技巧" class="headerlink" title="常用技巧"></a>常用技巧</h4><ol><li>Non-Maximum Suppression</li></ol><p>模型可能能够为同一对象找到多个边界框。非极大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。然后跳过与这个边界框具有高IoU（即大于0.5）的剩余框,重复这个过程直到挑选出需要数量的bbox</strong></p><blockquote><p>非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于人脸的概率 分别为A、B、C、D、E、F。</p><ol><li>从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</li><li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</li><li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</li><li>就这样一直重复，找到所有被保留下来的矩形框。</li></ol></blockquote><ol><li>Hard Negative Mining</li></ol><p>我们将没有对象的边界框视为Negative示例。</p><p>并非所有的Negative例子都同样难以识别。例如，如果它包含纯空背景，那么它很可能是一个“容易否定的”；但是，<strong>如果盒子中包含奇怪的嘈杂纹理或部分对象，可能很难被识别为背景，这些都是“硬阴性”</strong>。严厉的反面例子很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。</p><blockquote><p>也就说增加容易被FP的数据</p></blockquote><p>通过查看R-CNN的学习步骤，您可以很容易地发现训练R-CNN模型既昂贵又缓慢，因为以下步骤需要大量工作：</p><ol><li>运行选择性搜索，为每个图像提出2000个区域候选</li><li>为每个图像区域生成CNN特征向量（N个图像*2000）</li><li>整个过程分别涉及三个模型，没有太多的共享计算：用于图像分类和特征提取的卷积神经网络；用于识别目标对象的顶部SVM分类器；以及用于收紧区域边界框的回归模型。</li></ol><h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>opencv实现了选择性算法.</p><p>可以参考<a href="https://github.com/Hulkido/RCNN/tree/master">Hulkido/RCNN: FULL Implementation of RCNN from scratch (github.com)</a></p><h4 id="生成Region-proposals"><a href="#生成Region-proposals" class="headerlink" title="生成Region proposals"></a>生成Region proposals</h4><blockquote><p>区域建议(Region proposals)只是图像的较小区域，可能包含我们在输入图像中搜索的对象。为了减少R-CNN中的区域建议，使用了一种称为选择性搜索的贪婪算法。</p></blockquote><p>首先需要定义IoU计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculating dimension of common area between these two boxes.</span></span><br><span class="line">x_left = <span class="built_in">max</span>(bb1[<span class="string">&#x27;x1&#x27;</span>], bb2[<span class="string">&#x27;x1&#x27;</span>])</span><br><span class="line">y_bottom = <span class="built_in">max</span>(bb1[<span class="string">&#x27;y1&#x27;</span>], bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">x_right = <span class="built_in">min</span>(bb1[<span class="string">&#x27;x2&#x27;</span>], bb2[<span class="string">&#x27;x2&#x27;</span>])</span><br><span class="line">y_top = <span class="built_in">min</span>(bb1[<span class="string">&#x27;y2&#x27;</span>], bb2[<span class="string">&#x27;y2&#x27;</span>])</span><br><span class="line"><span class="comment"># if there is no overlap output 0 as intersection area is zero.</span></span><br><span class="line"><span class="keyword">if</span> x_right &lt; x_left <span class="keyword">or</span> y_bottom &lt; y_top:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"><span class="comment"># calculating intersection area.</span></span><br><span class="line"><span class="comment"># 计算交集</span></span><br><span class="line">intersection_area = (x_right - x_left) * (y_top - y_bottom)</span><br><span class="line"><span class="comment"># individual areas of both these bounding boxes.</span></span><br><span class="line"><span class="comment"># 计算各自区域面积</span></span><br><span class="line">bb1_area = (bb1[<span class="string">&#x27;x2&#x27;</span>] - bb1[<span class="string">&#x27;x1&#x27;</span>]) * (bb1[<span class="string">&#x27;y2&#x27;</span>] - bb1[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">bb2_area = (bb2[<span class="string">&#x27;x2&#x27;</span>] - bb2[<span class="string">&#x27;x1&#x27;</span>]) * (bb2[<span class="string">&#x27;y2&#x27;</span>] - bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line"><span class="comment"># union area = area of bb1_+ area of bb2 - intersection of bb1 and bb2.</span></span><br><span class="line"><span class="comment"># 并集就是各自之和减去交集</span></span><br><span class="line">iou = intersection_area / <span class="built_in">float</span>(bb1_area + bb2_area - intersection_area)</span><br></pre></td></tr></table></figure><p>遍历选择性搜索得到的区域,计算每个区域与对应bbox(bounding box)的IoU.</p><p>然后将iou大于阈值(这里设置为0.7)的定为region proposal,同时resize这个区域建议.</p><blockquote><p>本身应该是warp,但我觉得差别不大.另外region proposal和ROI差别其实也不大,有说法是<strong>region proposal是对图像而言的，roi是针对feature map上的</strong>.</p></blockquote><p><img data-src="https://i.imgur.com/j8O2rCN.png" alt="image-20231022171507221"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">cv2.setUseOptimized(<span class="literal">True</span>);</span><br><span class="line">ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span><br><span class="line">ss.setBaseImage(image)   <span class="comment"># setting given image as base image</span></span><br><span class="line">            ss.switchToSelectiveSearchFast()     <span class="comment"># running selective search on bae image</span></span><br><span class="line">            ssresults = ss.process()     <span class="comment"># processing to get the outputs</span></span><br><span class="line">            imout = image.copy()</span><br><span class="line">            counter = <span class="number">0</span></span><br><span class="line">            falsecounter = <span class="number">0</span></span><br><span class="line">            flag = <span class="number">0</span></span><br><span class="line">            fflag = <span class="number">0</span></span><br><span class="line">            bflag = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> e,result <span class="keyword">in</span> <span class="built_in">enumerate</span>(ssresults):</span><br><span class="line">                <span class="keyword">if</span> e &lt; <span class="number">2000</span> <span class="keyword">and</span> flag == <span class="number">0</span>:     <span class="comment"># till 2000 to get top 2000 regions only</span></span><br><span class="line">                    <span class="keyword">for</span> gtval <span class="keyword">in</span> gtvalues:</span><br><span class="line">                        x,y,w,h = result</span><br><span class="line">                        iou = get_iou(gtval,&#123;<span class="string">&quot;x1&quot;</span>:x,<span class="string">&quot;x2&quot;</span>:x+w,<span class="string">&quot;y1&quot;</span>:y,<span class="string">&quot;y2&quot;</span>:y+h&#125;)  <span class="comment"># calculating IoU for each of the proposed regions</span></span><br><span class="line">                        <span class="keyword">if</span> counter &lt; <span class="number">30</span>:       <span class="comment"># getting only 30 psoitive examples</span></span><br><span class="line">                            <span class="keyword">if</span> iou &gt; <span class="number">0.70</span>:     <span class="comment"># IoU or being positive is 0.7</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">1</span>)</span><br><span class="line">                                counter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            fflag =<span class="number">1</span>              <span class="comment"># to insure we have collected all psotive examples</span></span><br><span class="line">                        <span class="keyword">if</span> falsecounter &lt;<span class="number">30</span>:      <span class="comment"># 30 negatve examples are allowed only</span></span><br><span class="line">                            <span class="keyword">if</span> iou &lt; <span class="number">0.3</span>:         <span class="comment"># IoU or being negative is 0.3</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">0</span>)</span><br><span class="line">                                falsecounter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            bflag = <span class="number">1</span>             <span class="comment">#to ensure we have collected all negative examples</span></span><br><span class="line">                    <span class="keyword">if</span> fflag == <span class="number">1</span> <span class="keyword">and</span> bflag == <span class="number">1</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&quot;inside&quot;</span>)</span><br><span class="line">                        flag = <span class="number">1</span>        <span class="comment"># to signal the complition of data extaction from a particular image</span></span><br></pre></td></tr></table></figure><p>上面一共最多遍历生成的2000个ROI,选取其中的正例(超过阈值)的30,负例30.</p><p>下面是一张图像与其得到的正例和负例</p><p><img data-src="https://i.imgur.com/elheTvF.png" alt="image-20231022174714613"></p><p><img data-src="https://i.imgur.com/YTCcKjA.png" alt="image-20231022174748063"></p><h4 id="使用CNN模型二分类"><a href="#使用CNN模型二分类" class="headerlink" title="使用CNN模型二分类"></a>使用CNN模型二分类</h4><p>一般直接使用预训练模型,这里使用keras,加载VGG16模型.这里只做目标是否在而不做具体分类,所以输出单个值作为二分类.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vgg = tf.keras.applications.vgg16.VGG16(include_top=<span class="literal">True</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>, input_tensor=<span class="literal">None</span>, input_shape=<span class="literal">None</span>, pooling=<span class="literal">None</span>, classes=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.layers[:-<span class="number">2</span>]:</span><br><span class="line">  layer.trainable = <span class="literal">False</span></span><br><span class="line">x = vgg.get_layer(<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">last_output =  x.output</span><br><span class="line">x = tf.keras.layers.Dense(<span class="number">1</span>,activation = <span class="string">&#x27;sigmoid&#x27;</span>)(last_output)</span><br><span class="line">model = tf.keras.Model(vgg.<span class="built_in">input</span>,x)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = <span class="string">&quot;adam&quot;</span>,</span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              metrics = [<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line">model.fit(X_new,Y_new,batch_size = <span class="number">64</span>,epochs = <span class="number">3</span>, verbose = <span class="number">1</span>,validation_split=<span class="number">.05</span>,shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="预训练模型提取特征再使用SVM二分类"><a href="#预训练模型提取特征再使用SVM二分类" class="headerlink" title="预训练模型提取特征再使用SVM二分类"></a>预训练模型提取特征再使用SVM二分类</h4><p>原文中为每一类使用了一个二分类的SVM(毕竟一般的一个SVM只能二分类)</p><p>可以再上面的预训练模型只使用特征提取层,然后加个SVM</p><h4 id="bbox-regression"><a href="#bbox-regression" class="headerlink" title="bbox regression"></a>bbox regression</h4><p>最后需要求的bbox的回归用于修正误差.</p><blockquote><p>回归后得到四个参数，即x，y中心点偏移量和高、宽缩放因子，利用这四个参数对剩余的高质量目标建议框进行调整，取得分最高的称为Bounding Box，完成定位任务。</p></blockquote><p><img data-src="https://i.imgur.com/2xOfOpy.png" alt="image-20231031112707018"></p><p>如图,x,y坐标的修正由p~w~d~x~(p)+p~x~,而w,h由p~w~exp(d~w~(p))修正,已知t~i~函数,需要求d~i~(p)的回归值.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">curr_iou = iou([gta[bbox_num, <span class="number">0</span>], gta[bbox_num, <span class="number">2</span>], gta[bbox_num, <span class="number">1</span>], gta[bbox_num, <span class="number">3</span>]], [x1_anc, y1_anc, x2_anc, y2_anc])</span><br><span class="line"><span class="comment"># calculate the regression targets if they will be needed</span></span><br><span class="line"><span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num] <span class="keyword">or</span> curr_iou &gt; C.rpn_max_overlap:</span><br><span class="line">cx = (gta[bbox_num, <span class="number">0</span>] + gta[bbox_num, <span class="number">1</span>]) / <span class="number">2.0</span> <span class="comment"># center point of bbox</span></span><br><span class="line">cy = (gta[bbox_num, <span class="number">2</span>] + gta[bbox_num, <span class="number">3</span>]) / <span class="number">2.0</span></span><br><span class="line">cxa = (x1_anc + x2_anc)/<span class="number">2.0</span> <span class="comment"># center point of anchor box which scales to resized image</span></span><br><span class="line">cya = (y1_anc + y2_anc)/<span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">tx = (cx - cxa) / (x2_anc - x1_anc) <span class="comment"># regression targets</span></span><br><span class="line">ty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class="line">tw = np.log((gta[bbox_num, <span class="number">1</span>] - gta[bbox_num, <span class="number">0</span>]) / (x2_anc - x1_anc))</span><br><span class="line">th = np.log((gta[bbox_num, <span class="number">3</span>] - gta[bbox_num, <span class="number">2</span>]) / (y2_anc - y1_anc))</span><br><span class="line">                            <span class="keyword">if</span> img_data[<span class="string">&#x27;bboxes&#x27;</span>][bbox_num][<span class="string">&#x27;class&#x27;</span>] != <span class="string">&#x27;bg&#x27;</span>:</span><br><span class="line"></span><br><span class="line"><span class="comment"># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class="line"><span class="keyword">if</span> curr_iou &gt; best_iou_for_bbox[bbox_num]:</span><br><span class="line">best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class="line">best_iou_for_bbox[bbox_num] = curr_iou</span><br><span class="line">best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class="line">best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]</span><br></pre></td></tr></table></figure><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>为了使R-CNN更快，Girshick（2015）通过将三个独立的模型统一到一个联合训练的框架中并增加共享计算结果（称为Fast R-CNN）来改进训练过程。</p><p>不同于R-CNN对于每个region proposals提取特征,，而是将它们聚合到整个图像上的一个 CNN 前向传递中，并且region proposals共享此特征矩阵。然后，将相同的特征矩阵分支出来，用于学习对象分类器和边界框回归器。总之，计算共享加速了R-CNN。</p><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/fast-RCNN.png" alt="img"></p><h4 id="ROI-Pooling"><a href="#ROI-Pooling" class="headerlink" title="ROI Pooling"></a>ROI Pooling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">16</span>).reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">rois = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])</span><br><span class="line">torchvision.ops.roi_pool(X,rois, output_size=(<span class="number">2</span>, <span class="number">2</span>), spatial_scale=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-pooling.png" alt="img" style="zoom:67%;" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roi_pooling</span>(<span class="params">features, rois, output_size</span>):</span></span><br><span class="line">    <span class="comment"># features: 输入特征图 (N, C, H, W)</span></span><br><span class="line">    <span class="comment"># rois: 区域候选框 (N, 4) 其中每行表示一个候选框的坐标 (x1, y1, x2, y2)</span></span><br><span class="line">    <span class="comment"># output_size: ROI Pooling的输出尺寸 (H&#x27;, W&#x27;)</span></span><br><span class="line"></span><br><span class="line">    num_rois = rois.size(<span class="number">0</span>)</span><br><span class="line">    output = torch.zeros(num_rois, features.size(<span class="number">1</span>), output_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_rois):</span><br><span class="line">        roi = rois[i]</span><br><span class="line">        x1, y1, x2, y2 = roi</span><br><span class="line">        roi_features = features[:, :, y1:y2 + <span class="number">1</span>, x1:x2 + <span class="number">1</span>]</span><br><span class="line">        roi_features = F.adaptive_max_pool2d(roi_features, output_size)</span><br><span class="line">        output[i] = roi_features</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><blockquote><p>上面的算法其实使用的是pytorch的adaptive_max_pool2d,这两个东西并不一样,但很多简易实现就是利用了这个函数</p></blockquote><p>由于要进行分类和回归(都使用一个FC),需要一个固定大小的输入.由于此时输入已经成了可训练的feature map而不是直接的图像,需要一个可微分的操作.</p><p>RoI 池化类似于 max-pooling。说白了就是将原本的region proposals分成hxw的grid,这里的h,w就是后面fc需要的输入,在每个grid中做max pooling</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastRCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        vgg = torchvision.models.vgg19_bn(pretrained=<span class="literal">True</span>)</span><br><span class="line">        self.features = nn.Sequential(*<span class="built_in">list</span>(vgg.features.children())[:-<span class="number">1</span>])</span><br><span class="line">        self.roipool = ROIPooling(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">        <span class="comment">#roipooling之后得到B,C,7,7直接</span></span><br><span class="line">        self.output = nn.Sequential(*<span class="built_in">list</span>(vgg.classifier.children())[:-<span class="number">1</span>])</span><br><span class="line">        self.prob = nn.Linear(<span class="number">4096</span>, num_classes+<span class="number">1</span>)</span><br><span class="line">        self.loc = nn.Linear(<span class="number">4096</span>, <span class="number">4</span> * (num_classes + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.cat_loss = nn.CrossEntropyLoss()</span><br><span class="line">        self.loc_loss = nn.SmoothL1Loss()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img, rois, roi_idx</span>):</span></span><br><span class="line">        res = self.features(img)</span><br><span class="line">        res = self.roipool(res, rois, roi_idx)</span><br><span class="line">        res = res.view(res.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        features = self.output(res)</span><br><span class="line">        prob = self.prob(features)</span><br><span class="line">        loc = self.loc(features).view(-<span class="number">1</span>, self.num_classes+<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> prob, loc</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, prob, bbox, label, gt_bbox, lmb=<span class="number">1.0</span></span>):</span></span><br><span class="line">        loss_cat = self.cat_loss(prob, label)</span><br><span class="line">        lbl = label.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        mask = (label != <span class="number">0</span>).<span class="built_in">float</span>().view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        loss_loc = self.loc_loss(gt_bbox * mask, bbox.gather(<span class="number">1</span>, lbl).squeeze(<span class="number">1</span>) * mask)</span><br><span class="line">        loss = loss_cat + lmb * loss_loc</span><br><span class="line">        <span class="keyword">return</span> loss, loss_cat, loss_loc</span><br></pre></td></tr></table></figure><ol><li>首先，在图像分类任务上预训练卷积神经网络。</li><li>通过选择性搜索提出区域建议（每张图像~2k个候选者）。</li><li>更改预训练的 CNN：</li><li><ul><li>将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。</li><li>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。</li></ul></li><li>最后，模型分为两个输出:K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。</li></ol><p>损失函数有分类损失和回归损失,回归损失用于计算bouding box的回归损失,可使用L1损失。</p><script type="math/tex; mode=display">L_1^\text{smooth}(x) = \begin{cases}    0.5 x^2             & \text{if } \vert x \vert < 1\\    \vert x \vert - 0.5 & \text{otherwise}\end{cases}</script><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/l1-smooth.png" alt="img"></p><blockquote><p>在Fast RCNN中,改进并不显著，因为区域提案是由另一个模型单独生成的，而且非常耗时。</p></blockquote><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h4><p>首先，在图像分类任务上预训练卷积神经网络。</p><p>通过选择性搜索提出区域建议（每张图像~2k个候选者）。</p><p>更改预训练的 CNN：将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。</p><p>RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。</p><p>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。最后，模型分支为两个输出层：K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。</p><p>其中关键是如何利用选择搜索得到的region proposals映射到通过CNN得到的feature map上,这样就只用在整个图像上进行一次CNN而不是单独在每个region proposal上滤波.如果只进行一次滤波,如何<code>准确地将输入图像的一个区域投影到卷积特征图的一个区域上</code>,此外还有ROI Pooling得到固定的ROI区域也是关键.</p><p>SPPNet介绍了 ROI Projection,看起来貌似就是算回去,不过在设计CNN时,padding = kernel_size/2这样避免计算复杂</p><p><img data-src="https://i.imgur.com/xsrzh3i.png" alt="image-20231029125650323"></p><p>​    </p><p><img data-src="https://miro.medium.com/v2/resize:fit:700/1*U95lm-Jkwkpy3p8X6IOKlQ.png" alt="img"></p><p>Loss包括分类损失和bouding box回归损失</p><p><img data-src="https://i.imgur.com/2leLfbr.png" alt="image-20231029204052670"></p><h3 id="Faster-RNN"><a href="#Faster-RNN" class="headerlink" title="Faster RNN"></a>Faster RNN</h3><p>重点说一下faster rcnn</p><p><img data-src="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/faster-RCNN.png" alt="img"></p><p>将选择性搜索算法(也就是用于生成region proposals的算法)融合到深度学习模型中</p><p><img data-src="http://zh.d2l.ai/_images/faster-rcnn.svg" alt="../_images/faster-rcnn.svg"></p><p>比如下面将一张图像通过一个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">ef nn_base(input_tensor=<span class="literal">None</span>, trainable=<span class="literal">False</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Determine proper input shape</span></span><br><span class="line">    <span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_first&#x27;</span>:</span><br><span class="line">        input_shape = (<span class="number">3</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_shape = (<span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_tensor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        img_input = Input(shape=input_shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> K.is_keras_tensor(input_tensor):</span><br><span class="line">            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img_input = input_tensor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_last&#x27;</span>:</span><br><span class="line">        bn_axis = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bn_axis = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    x = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(img_input)</span><br><span class="line"></span><br><span class="line">    x = Convolution2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides=(<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">&#x27;conv1&#x27;</span>, trainable = trainable)(x)</span><br><span class="line">    x = FixedBatchNormalization(axis=bn_axis, name=<span class="string">&#x27;bn_conv1&#x27;</span>)(x)</span><br><span class="line">    x = Activation(<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    x = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;a&#x27;</span>, strides=(<span class="number">1</span>, <span class="number">1</span>), trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;a&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;d&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    x = conv_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;a&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;b&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;c&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;d&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;e&#x27;</span>, trainable = trainable)</span><br><span class="line">    x = identity_block(x, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;f&#x27;</span>, trainable = trainable)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>提出了<strong>Region Proposal Networks</strong></p><h4 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h4><p>主要更改就是对于roi projection,之前还是在原图上使用ss得到region proposals然后映射到feature map上,现在用区域提案网络 (RPN)替代,其他部分不变.</p><p>RPN,简单地说，它是一个小型的全卷积网络，它接受feature map，并输出一组区域和每个区域的“objectness”分数（该区域包含对象的可能性）。</p><p><img data-src="https://i.imgur.com/BL8dkf1.png" alt="image-20231029172807621"></p><p>首先使用一个cnn模型提取特征得到feature map,而RPN就会利用这个feature map,并且提出了anchor boxes概念,在feature map上使用一个大小为3x3的sliding window,在sliding window移动到一个位置的时候生成多个不同大小的anchor boxes.</p><p>在生成相关数据的时候,对于一个feature map,由于它已经经过多次卷积包含许多信息,在这个feature map上生成anchor box.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initialise empty output objectives</span></span><br><span class="line">y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))</span><br><span class="line">y_is_box_valid = np.zeros((output_height, output_width, num_anchors))</span><br><span class="line">y_rpn_regr = np.zeros((output_height, output_width, num_anchors * <span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>其中num_anchors设定为9,包含三种不同大小以及比例的的anchor box.</p><p><code>y_rpn_overlap</code>表示这个anchor box是否包含物体,在生成数据时会将与bbox的iou大于一个阈值(比如0.7)的作为pos,小于一个阈值(0.3)作为neg表示不包含物体,也就是背景.在代码中,如果一个大于0.7的anchor box都没有那就直接取与bounding box的iou最大的作为包含物体的anchor box正例.</p><p><code>y_is_box_valid</code>表示所有正负例anchor box,去掉iou在0.3~0.7的,认为这些anchor box不明显.一般这个值会设置一个固定值,注意,生成的anchor box如果是pos,只表示其与其中一个或多个bbox的iou大于阈值,并不能知道其与哪个物体位置更相近.</p><p><code>y_rpn_regr</code>表示<code>anchor box</code>的具体位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_cls, y_rpn_regr = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class="number">1</span>)</span><br><span class="line">y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class="number">4</span>, axis=<span class="number">1</span>), y_rpn_regr], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>计算rpn输出时为什么要caoncat这些数据?这是在代码实现层面的事,calc_rpn计算得到固定个数的正负例作为训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative</span></span><br><span class="line"><span class="comment"># regions. We also limit it to 256 regions.</span></span><br><span class="line">num_regions = <span class="number">256</span></span><br><span class="line"><span class="comment"># 对于一张图 只选取256个anchor box</span></span><br><span class="line"><span class="comment"># change from len(pos_locs[0]) to len(pos_locs)</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pos_locs) &gt; num_regions/<span class="number">2</span>:</span><br><span class="line">val_locs = random.sample(<span class="built_in">range</span>(<span class="built_in">len</span>(pos_locs[<span class="number">0</span>])), <span class="built_in">len</span>(pos_locs[<span class="number">0</span>]) - num_regions/<span class="number">2</span>)</span><br><span class="line">y_is_box_valid[<span class="number">0</span>, pos_locs[<span class="number">0</span>][val_locs], pos_locs[<span class="number">1</span>][val_locs], pos_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br><span class="line">num_pos = num_regions/<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(neg_locs) + num_pos &gt; num_regions:</span><br><span class="line">val_locs = random.sample(<span class="built_in">range</span>(<span class="built_in">len</span>(neg_locs[<span class="number">0</span>])), <span class="built_in">len</span>(neg_locs[<span class="number">0</span>]) - num_pos)</span><br><span class="line">y_is_box_valid[<span class="number">0</span>, neg_locs[<span class="number">0</span>][val_locs], neg_locs[<span class="number">1</span>][val_locs], neg_locs[<span class="number">2</span>][val_locs]] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  这里 y_rpn_regr 为什么要concatenate四个正例?</span></span><br><span class="line">y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class="number">1</span>)</span><br><span class="line">y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class="number">4</span>, axis=<span class="number">1</span>), y_rpn_regr], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#  y_rpn_cls  (1, 18, 26, 35)</span></span><br><span class="line"><span class="comment">#  y_rpn_regr (1, 72, 26, 35)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpn_accuracy_rpn_monitor = []</span><br><span class="line">rpn_accuracy_for_epoch = []</span><br></pre></td></tr></table></figure><p>训练代码中设计了<code>rpn_accuracy_rpn_monitor</code>和<code>rpn_accuracy_rpn_monitor</code>,前者会在处理过的图像达到一定数量后计算rpn的准确率,也就是生成的roi是否总是正例.</p><p>后者会在每次epoch完后计算mean loss和acc,因为在这个目标检测任务中,每次epoch的batch_size并不是像其他任务取多张图像,而是每次epoch中又取epoch_length,每次取一张图片.</p><h5 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h5><blockquote><p>RPN如何设计的,3x3的sliding window有什么用,如果是为了生成anchor box,有必要设计一个sliding window吗?</p></blockquote><p>在通过一个预训练模型的feature层后(比如vgg,resnet,inception等等),再使用一个3x3的sliding window,通道数256(可以理解为再聚合一下信息),然后在原本的feature map上针对每个pixel生成多个anchor box.</p><p>遍历一个图像上的所有bbox,看它与生成的anchor box的iou,取一个大于0.7的最大的anchor box的pos,其余的小于0.3的是neg.</p><p>首先将feature map做一次3x3卷积,在进行全卷积的输出分别得到针对一个anchor box的二分类以及修正坐标(坐标回归)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn</span>(<span class="params">base_layers,num_anchors</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># important! same width,hight</span></span><br><span class="line">    x = Convolution2D(<span class="number">512</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_initializer=<span class="string">&#x27;normal&#x27;</span>, name=<span class="string">&#x27;rpn_conv1&#x27;</span>)(base_layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get class and regr output</span></span><br><span class="line">    <span class="comment"># fully convolution</span></span><br><span class="line">    x_class = Convolution2D(num_anchors, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;sigmoid&#x27;</span>, kernel_initializer=<span class="string">&#x27;uniform&#x27;</span>, name=<span class="string">&#x27;rpn_out_class&#x27;</span>)(x)</span><br><span class="line">    x_regr = Convolution2D(num_anchors * <span class="number">4</span>, (<span class="number">1</span>, <span class="number">1</span>), activation=<span class="string">&#x27;linear&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>, name=<span class="string">&#x27;rpn_out_regress&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [x_class, x_regr, base_layers]</span><br></pre></td></tr></table></figure><p>然后利用roi pooling以及classifier输出针对于21类的概率和20类回归坐标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span>(<span class="params">base_layers, input_rois, num_rois, nb_classes = <span class="number">21</span>, trainable=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> K.backend() == <span class="string">&#x27;tensorflow&#x27;</span>:</span><br><span class="line">        pooling_regions = <span class="number">14</span></span><br><span class="line">        input_shape = (num_rois,<span class="number">14</span>,<span class="number">14</span>,<span class="number">1024</span>)</span><br><span class="line">    <span class="keyword">elif</span> K.backend() == <span class="string">&#x27;theano&#x27;</span>:</span><br><span class="line">        pooling_regions = <span class="number">7</span></span><br><span class="line">        input_shape = (num_rois,<span class="number">1024</span>,<span class="number">7</span>,<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class="line">    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    out = TimeDistributed(Flatten())(out)</span><br><span class="line"></span><br><span class="line">    out_class = TimeDistributed(Dense(nb_classes, activation=<span class="string">&#x27;softmax&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>), name=<span class="string">&#x27;dense_class_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(nb_classes))(out)</span><br><span class="line">    <span class="comment"># note: no regression target for bg class</span></span><br><span class="line">    out_regr = TimeDistributed(Dense(<span class="number">4</span> * (nb_classes-<span class="number">1</span>), activation=<span class="string">&#x27;linear&#x27;</span>, kernel_initializer=<span class="string">&#x27;zero&#x27;</span>), name=<span class="string">&#x27;dense_regress_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(nb_classes))(out)</span><br><span class="line">    <span class="keyword">return</span> [out_class, out_regr]</span><br></pre></td></tr></table></figure><p>这样就训练三个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_rpn = Model(img_input, rpn[:<span class="number">2</span>])</span><br><span class="line">model_classifier = Model([img_input, roi_input], classifier)</span><br><span class="line"></span><br><span class="line"><span class="comment"># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class="line">model_all = Model([img_input, roi_input], rpn[:<span class="number">2</span>] + classifier)</span><br></pre></td></tr></table></figure><p>rpn损失计算,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_regr</span>(<span class="params">num_anchors</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_regr_fixed_num</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line"><span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_first&#x27;</span>:</span><br><span class="line">x = y_true[:, <span class="number">4</span> * num_anchors:, :, :] - y_pred</span><br><span class="line">x_abs = K.<span class="built_in">abs</span>(x)</span><br><span class="line">x_bool = K.less_equal(x_abs, <span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_regr * K.<span class="built_in">sum</span>(</span><br><span class="line">y_true[:, :<span class="number">4</span> * num_anchors, :, :] * (x_bool * (<span class="number">0.5</span> * x * x) + (<span class="number">1</span> - x_bool) * (x_abs - <span class="number">0.5</span>))) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :<span class="number">4</span> * num_anchors, :, :])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">x = y_true[:, :, :, <span class="number">4</span> * num_anchors:] - y_pred</span><br><span class="line">x_abs = K.<span class="built_in">abs</span>(x)</span><br><span class="line">x_bool = K.cast(K.less_equal(x_abs, <span class="number">1.0</span>), tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> lambda_rpn_regr * K.<span class="built_in">sum</span>(</span><br><span class="line">y_true[:, :, :, :<span class="number">4</span> * num_anchors] * (x_bool * (<span class="number">0.5</span> * x * x) + (<span class="number">1</span> - x_bool) * (x_abs - <span class="number">0.5</span>))) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :, :, :<span class="number">4</span> * num_anchors])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rpn_loss_regr_fixed_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_cls</span>(<span class="params">num_anchors</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_loss_cls_fixed_num</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line"><span class="keyword">if</span> K.image_data_format() == <span class="string">&#x27;channels_last&#x27;</span>:</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_class * K.<span class="built_in">sum</span>(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> lambda_rpn_class * K.<span class="built_in">sum</span>(y_true[:, :num_anchors, :, :] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, num_anchors:, :, :])) / K.<span class="built_in">sum</span>(epsilon + y_true[:, :num_anchors, :, :])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rpn_loss_cls_fixed_num</span><br></pre></td></tr></table></figure><p>计算得到P_rpn也就是在feature maps上通过RPN网络得到的300个anchor boxes坐标以及sigmoid分数(表示是否包含物体)</p><p>然后调整anchor box使其区域在feature map内,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">anchor_x = (anchor_size * anchor_ratio[<span class="number">0</span>])/C.rpn_stride</span><br><span class="line">anchor_y = (anchor_size * anchor_ratio[<span class="number">1</span>])/C.rpn_stride</span><br><span class="line">X, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>, :, :, curr_layer] = X - anchor_x/<span class="number">2</span></span><br><span class="line">A[<span class="number">1</span>, :, :, curr_layer] = Y - anchor_y/<span class="number">2</span></span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = anchor_x</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = anchor_y</span><br><span class="line"></span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = np.maximum(<span class="number">1</span>, A[<span class="number">2</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = np.maximum(<span class="number">1</span>, A[<span class="number">3</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] += A[<span class="number">0</span>, :, :, curr_layer]</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] += A[<span class="number">1</span>, :, :, curr_layer]</span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>, :, :, curr_layer] = np.maximum(<span class="number">0</span>, A[<span class="number">0</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">1</span>, :, :, curr_layer] = np.maximum(<span class="number">0</span>, A[<span class="number">1</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">2</span>, :, :, curr_layer] = np.minimum(cols-<span class="number">1</span>, A[<span class="number">2</span>, :, :, curr_layer])</span><br><span class="line">A[<span class="number">3</span>, :, :, curr_layer] = np.minimum(rows-<span class="number">1</span>, A[<span class="number">3</span>, :, :, curr_layer])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此外就是NMS了,输入是anchor box shape是(h*w*9,4)以及对应的一个概率对应着(h*w*9)值在0-1之间表示对应anchor box包含物体的概率.将从一个图像中得到的所有anchor box经过NMS,这样一张图像中的anchor box最多也就一定数量(比如300)了.</p><p>然后对于这些anchor box,计算与其iou最大的bbox,要求最大的iou大于一定值(比如0.7),那么这个就是roi,其类别就是对应bbox类别,并且设置其位置(修正位置).</p><p>然后设置一定数量需要的rois,尽量均匀分正负例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">non_max_suppression_fast</span>(<span class="params">boxes, probs, overlap_thresh=<span class="number">0.9</span>, max_boxes=<span class="number">300</span></span>):</span></span><br><span class="line">    <span class="comment"># code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</span></span><br><span class="line">    <span class="comment"># if there are no boxes, return an empty list</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(boxes) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># grab the coordinates of the bounding boxes</span></span><br><span class="line">    x1 = boxes[:, <span class="number">0</span>]</span><br><span class="line">    y1 = boxes[:, <span class="number">1</span>]</span><br><span class="line">    x2 = boxes[:, <span class="number">2</span>]</span><br><span class="line">    y2 = boxes[:, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    np.testing.assert_array_less(x1, x2)</span><br><span class="line">    np.testing.assert_array_less(y1, y2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if the bounding boxes integers, convert them to floats --</span></span><br><span class="line">    <span class="comment"># this is important since we&#x27;ll be doing a bunch of divisions</span></span><br><span class="line">    <span class="keyword">if</span> boxes.dtype.kind == <span class="string">&quot;i&quot;</span>:</span><br><span class="line">        boxes = boxes.astype(<span class="string">&quot;float&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the list of picked indexes</span></span><br><span class="line">    pick = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate the areas</span></span><br><span class="line">    area = (x2 - x1) * (y2 - y1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort the bounding boxes</span></span><br><span class="line">    idxs = np.argsort(probs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># keep looping while some indexes still remain in the indexes</span></span><br><span class="line">    <span class="comment"># list</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(idxs) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># grab the last index in the indexes list and add the</span></span><br><span class="line">        <span class="comment"># index value to the list of picked indexes</span></span><br><span class="line">        last = <span class="built_in">len</span>(idxs) - <span class="number">1</span></span><br><span class="line">        i = idxs[last]</span><br><span class="line">        pick.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the intersection</span></span><br><span class="line"></span><br><span class="line">        xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class="line">        yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class="line">        xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class="line">        yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class="line"></span><br><span class="line">        ww_int = np.maximum(<span class="number">0</span>, xx2_int - xx1_int)</span><br><span class="line">        hh_int = np.maximum(<span class="number">0</span>, yy2_int - yy1_int)</span><br><span class="line"></span><br><span class="line">        area_int = ww_int * hh_int</span><br><span class="line"></span><br><span class="line">        <span class="comment"># find the union</span></span><br><span class="line">        area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the ratio of overlap</span></span><br><span class="line">        overlap = area_int / (area_union + <span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># delete all indexes from the index list that have</span></span><br><span class="line">        idxs = np.delete(idxs, np.concatenate(([last],</span><br><span class="line">                                               np.where(overlap &gt; overlap_thresh)[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pick) &gt;= max_boxes:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># return only the bounding boxes that were picked using the integer data type</span></span><br><span class="line">    boxes = boxes[pick].astype(<span class="string">&quot;int&quot;</span>)</span><br><span class="line">    probs = probs[pick]</span><br><span class="line">    <span class="keyword">return</span> boxes, probs</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>大致逻辑是选取probs最大的(也就是通过RPN算出来包含物体概率最大的anchor box),计算其与剩余的anchor box的iou,去掉大于某个阈值(比如0.9)的,然后再重复,选择剩下来的probs最大的anchor box.一共选择固定数量的anchor box.返回相应的boxes以及probs.</p><p>可以看看这个<a href="https://zhuanlan.zhihu.com/p/43812909">Faster R-CNN 论文阅读记录（一）：概览 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/31426458">一文读懂Faster RCNN - 知乎 (zhihu.com)</a></p><h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/mask-rcnn.png" alt="img" style="zoom:67%;" /></p><p>主要是提出了ROIAlign将模型用于实例分割中,可以与原先的任务并行,</p><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-align.png" alt="img" style="zoom:50%;" /></p><p>RoIAlign 层旨在修复 RoI 池化中由量化(quantization)引起的位置错位。RoIAlign 删除哈希量化，例如，使用 x/16 而不是 [x/16]，以便提取的特征可以与输入像素正确对齐。<strong>双线性插值</strong>用于计算输入中的浮点位置值。</p><blockquote><p>作者认为Faster RCNN中ROI pooling的取整(quantization )操作会使得定位不准确</p><p>这种quantization还体现在proposals(在原图上)映射到特征层上的操作,因为这会导致不对准,对于detection任务有较大影响,</p></blockquote><p><img data-src="https://i.imgur.com/Md5zxS2.png" alt="image-20231102202646398"></p><p>上图蓝框就是proposals到feature map上的框,没有取整.然后在每个roi中选择采样点计算这些采样的均值即为每个roi的值</p><p>可以看看这篇文章<a href="https://blog.csdn.net/qq_37541097/article/details/123754766">【精选】Mask R-CNN网络详解_mask rcnn详解-CSDN博客</a></p><p><a href="https://blog.csdn.net/qq_37541097/article/details/112564822">【精选】双线性插值-CSDN博客</a></p><h3 id="Region-based-Fully-Convolutional-Network-R-FCN"><a href="#Region-based-Fully-Convolutional-Network-R-FCN" class="headerlink" title="Region-based Fully Convolutional Network (R-FCN)"></a>Region-based Fully Convolutional Network (R-FCN)</h3><p><img data-src="https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png" alt="r-fcn image"></p><p>可以了解一下全卷积网络(<a href="http://zh.d2l.ai/chapter_computer-vision/fcn.html">13.11. 全卷积网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>).1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性</p><blockquote><p>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数,最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测</p></blockquote><p>R-FCN核心操作是将ROI pooling改为了position-sensitive score maps.而且原本在ROI pooling之后的卷积层和全连接层(被认为是位置不敏感的操作,这种操作会影响目标检测的精度而且浪费神经网络的分类能力),所以文章将全部操作改为卷积</p><p><img data-src="https://i.imgur.com/BaAJNpE.png" alt="image-20231102205517036"></p><p>大致流程如下:</p><ul><li>首先选择一张需要处理的图片，并对这张图片进行相应的预处理操作；</li><li>接着，我们将预处理后的图片送入一个预训练好的分类网络中（这里使用了ResNet-101网络的Conv4之前的网络），固定其对应的网络参数；</li><li>接着，在预训练网络的最后一个卷积层获得的feature map上存在3个分支，第1个分支就是在该feature map上面进行RPN操作，获得相应的ROI；第2个分支就是在该feature map上获得一个K*K*（C+1）维的位置敏感得分映射（position-sensitive score map），用来进行分类；第3个分支就是在该feature map上获得一个4*K*K维的位置敏感得分映射，用来进行回归；</li><li>最后，在K*K*（C+1）维的位置敏感得分映射和4<em>K</em>K维的位置敏感得分映射上面分别执行位置敏感的ROI池化操作（Position-Sensitive Rol Pooling，这里使用的是平均池化操作），获得对应的类别和位置信息。</li></ul><p>具体来说,通过一个CNN网络后得到feature map,一方面使用全卷积网络将通道数调整为k*k*(C+1)(对于分类任务),此外也需要一个RPN网络,用于提取roi区域,得到roi之后,将每个roi分为k*k个bins,这个时候每个bin就对应一个类别中k*k个通道之一,池化操作也就在这个通道上进行操作.i,j表示每个roi中的某个bin,c表示通道数,其中n表示bin中的像素数,相当于对于某个类别c中的某个bin,计算得到的scores(其实就是一个通道的和)除以bin中的像素数量.论文中的所谓vote就是简单地使用均值得到k*k个position-sensitive scores.</p><p><img data-src="https://pic1.zhimg.com/80/v2-0c3fd2c903db6887128ec390b8981ef0_720w.webp" alt="img"></p><p>然后按此计算某个类别的池化值,也就是k*k个bin的值的和,每个bin是映射在某个score map上的avg pooling.</p><p><img data-src="https://i.imgur.com/fwsdL0f.png" alt="image-20231102214505751"></p><p>得到C+1个值然后做softmax作为评价交叉熵巡视以及对roi的排名.</p><p><img data-src="https://i.imgur.com/9R3Qfia.png" alt="image-20231102214651913"></p><p><a href="https://zhuanlan.zhihu.com/p/30867916">详解R-FCN - 知乎 (zhihu.com)</a></p><h3 id="Summary-of-R-CNN-family"><a href="#Summary-of-R-CNN-family" class="headerlink" title="Summary of R-CNN family"></a>Summary of R-CNN family</h3><p><img data-src="http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png" alt="img"></p><p>以上都是two-stage  detector,另一种不同的方法跳过区域建议阶段，直接在可能位置的密集采样上运行检测。这就是单阶段目标检测算法的工作原理。这更快、更简单，但可能会降低performance。</p><p>在One-stage中对象检测是一个简单的回归问题，需要输入并学习概率类和边界框坐标。YOLO、YOLO v2、SSD、RetinaNet等属于一个相位检测器。对象检测是图像分类的一种高级形式，其中神经网络预测图像中的对象，并以边界框的形式引起人们的注意。</p><h2 id="论文相关"><a href="#论文相关" class="headerlink" title="论文相关"></a>论文相关</h2><h3 id="Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation"><a href="#Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation" class="headerlink" title="Rich feature hierarchies for accurate object detection and semantic segmentation"></a>Rich feature hierarchies for accurate object detection and semantic segmentation</h3><p>2014年的论文,为后面RCNN系列目标检测方法奠定基础</p><h4 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h4><blockquote><p>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.The best-performing methods are <strong>complex ensemble systems that typically combine multiple low-level image features with high-level context.</strong>In this paper, we propose a simple and <strong>scalable detection algorithm that improves mean average precision</strong> (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%.Our approach combines two key insights: (1) <strong>one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects</strong> and (2) when labeled training data is scarce, <strong>supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning,</strong> yields a significant performance boost.</p></blockquote><h3 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask-RCNN"></a>Mask-RCNN</h3><h4 id="abs-1"><a href="#abs-1" class="headerlink" title="abs"></a>abs</h4><blockquote><p>We present a conceptually simple, flexible, and general framework for <strong>object instance segmentation</strong>. Our approach efficiently <strong>detects objects</strong> in an image while <strong>simultaneously generating a high-quality segmentation mask for each instance</strong>.</p><p>The method, called Mask R-CNN, <strong>extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition</strong>. Mask R-CNN is <strong>simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps</strong>.</p><p>Moreover, Mask R-CNN is <strong>easy to generalize to other tasks</strong>, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection.</p></blockquote><h3 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h3><h4 id="abs-2"><a href="#abs-2" class="headerlink" title="abs"></a>abs</h4><blockquote><p>We present <strong>region-based</strong>, <strong>fully convolutional networks</strong> for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply <strong>a costly per-region subnetwork hundreds of times</strong>, our region-based detector is fully convolutional with almost all computation shared on the entire image.</p><p>To achieve this goal, we propose <strong>position-sensitive score maps</strong> to <strong>address a dilemma between translation-invariance in image classification and translation-variance in object detection</strong>.</p><p>Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/">Object Detection Part 4: Fast Detection Models | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://www.analyticsvidhya.com/blog/2022/09/object-detection-using-yolo-and-mobilenet-ssd/?utm_source=reading_list&amp;utm_medium=https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/">Object Detection Using YOLO And Mobilenet SSD Computer Vision - (analyticsvidhya.com)</a></li><li><a href="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/">Object Detection for Dummies Part 3: R-CNN Family | Lil’Log (lilianweng.github.io)</a></li><li><a href="https://tjmachinelearning.com/lectures/1718/obj/">Object Detection | TJHSST Machine Learning Club (tjmachinelearning.com)</a></li><li><a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN | by Dhruv Parthasarathy | Athelas</a></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li>PyTorch: <a href="https://github.com/longcw/faster_rcnn_pytorch">https://github.com/longcw/faster_rcnn_pytorch</a></li><li>Keras:<a href="https://github.com/drowning-in-codes/Keras-frcnn">drowning-in-codes/Keras-frcnn: Keras Implementation of Faster R-CNN (github.com)</a> 目前许多keras项目版本有点老,keras版本目前到了2.14,tensorflow也是.   我之前学过keras,也是新版本的了,大概5年前的老代码差异还是有的.</li><li>PyTorch: <a href="https://github.com/felixgwu/mask_rcnn_pytorch">https://github.com/felixgwu/mask_rcnn_pytorch</a></li><li><a href="https://blog.csdn.net/yx123919804/article/details/114800885">【精选】保姆级 Keras 实现 Faster R-CNN 一_keras实现rcnn-CSDN博客</a> l</li></ol><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。&lt;/p&gt;</summary>
    
    
    
    
    <category term="SSD" scheme="https://www.sekyoro.top/tags/SSD/"/>
    
    <category term="YOLO" scheme="https://www.sekyoro.top/tags/YOLO/"/>
    
  </entry>
  
</feed>
