<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=这里介绍一些细节信息.有关位置编码信息和用于图像的transformer. name=description><meta content=article property=og:type><meta content="transformer and attention(三)" property=og:title><meta content=https://www.sekyoro.top/2024/02/16/transformer-and-attention-%E4%B8%89/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=这里介绍一些细节信息.有关位置编码信息和用于图像的transformer. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/02/17/wE3HgYJ7KnxZ5pe.png property=og:image><meta content=https://s2.loli.net/2024/02/17/fhBMIgcsqzZQt3k.png property=og:image><meta content=https://s2.loli.net/2024/02/17/mdybruOEWxgSDVt.png property=og:image><meta content=https://pic1.zhimg.com/80/v2-1c6aa554d8fc53daa7bf79c755b1f86c_720w.webp property=og:image><meta content=https://github.com/rishikksh20/convolution-vision-transformers/raw/master/assets/model.PNG property=og:image><meta content=https://s2.loli.net/2024/02/18/w8EUDyAJ4sIv51n.png property=og:image><meta content=https://s2.loli.net/2024/02/21/buMGx8v6TfNOQht.png property=og:image><meta content=https://s2.loli.net/2024/02/21/OhtbAGq8NgvKnEX.png property=og:image><meta content=https://s2.loli.net/2024/02/19/LHY9bVrE8w2DlZs.png property=og:image><meta content=https://s2.loli.net/2024/02/18/cfmP5yO41sN6h8o.png property=og:image><meta content=https://s2.loli.net/2024/02/18/ncu63LwS7bKhl8j.png property=og:image><meta content=https://s2.loli.net/2024/02/16/4esqYAdLkgNuybn.png property=og:image><meta content=https://pic4.zhimg.com/80/v2-f6d057978590bd14fd876856500b69df_720w.webp property=og:image><meta content=https://pic4.zhimg.com/80/v2-5ee0eed4bc859e400591d7c83047bffb_720w.webp property=og:image><meta content=https://pic1.zhimg.com/80/v2-733f110568f1c83519ada84af1e32014_720w.webp property=og:image><meta content=https://s2.loli.net/2024/02/16/PX9Ev1HjwKDB5xt.png property=og:image><meta content=https://pic1.zhimg.com/80/v2-16c2aa40bbf7a888a62d9dc1373d6c94_720w.webp property=og:image><meta content=https://pic3.zhimg.com/80/v2-b5436edfcde32b292cdf24c7f39d9c0e_720w.webp property=og:image><meta content=https://user-images.githubusercontent.com/19909320/137499552-3bdf3189-7f57-4f95-a85e-8d5dd2ef6fd0.png property=og:image><meta content=https://s2.loli.net/2024/02/17/2S9UbyF7DYfujVw.png property=og:image><meta content=https://s2.loli.net/2024/02/17/CHqOLZKXNxhIdzj.png property=og:image><meta content=https://s2.loli.net/2024/02/17/2rIStXgva96PhTZ.png property=og:image><meta content=https://s2.loli.net/2024/02/18/NMFCXv8EKabh6cp.png property=og:image><meta content=https://s2.loli.net/2024/02/18/WBD3do8KnrHqlLU.png property=og:image><meta content=https://s2.loli.net/2024/02/21/If19Km8TFPneM6x.png property=og:image><meta content=https://s2.loli.net/2024/02/21/6izranSXgP7CobN.png property=og:image><meta content=https://github.com/lucidrains/vit-pytorch/raw/main/images/twins_svt.png property=og:image><meta content=https://s2.loli.net/2024/02/18/1Zf5pWymzPTaHoB.png property=og:image><meta content=2024-02-16T14:44:00.000Z property=article:published_time><meta content=2024-02-21T15:14:28.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content=transformers property=article:tag><meta content=attention property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/02/17/wE3HgYJ7KnxZ5pe.png name=twitter:image><link href=https://www.sekyoro.top/2024/02/16/transformer-and-attention-%E4%B8%89/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>transformer and attention(三) | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/02/16/transformer-and-attention-%E4%B8%89/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>transformer and attention(三)</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-02-16 22:44:00" datetime=2024-02-16T22:44:00+08:00>2024-02-16</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-02-21 23:14:28" datetime=2024-02-21T23:14:28+08:00 itemprop=dateModified>2024-02-21</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>38k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>34 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>这里介绍一些细节信息.有关位置编码信息和用于图像的transformer.<br><span id=more></span><h2 id=线性注意力><a class=headerlink href=#线性注意力 title=线性注意力></a>线性注意力</h2><script type="math/tex; mode=display">
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})=softmax\left(\boldsymbol{Q}\boldsymbol{K}^\top\right)\boldsymbol{V}</script><p>其中$Q\in\mathbb{R}^{n\times d_k},\boldsymbol{K}\in\mathbb{R}^{m\times d_k},\boldsymbol{V}\in\mathbb{R}^{m\times d_v}$​,一般情况下n>d甚至n>>d.所以如果对QK^T^进行softmax操作,复杂度为O(mn),所以去掉Softmax的Attention的复杂度可以降到最理想的线性级别Linear Attention.</p><script type="math/tex; mode=display">
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})_i=\frac{\sum_{j=1}^nsim(\boldsymbol{q}_i,\boldsymbol{k}_j)\boldsymbol{v}_j}{\sum_{j=1}^nsim(\boldsymbol{q}_i,\boldsymbol{k}_j)}</script><p>只要保证Attention相似的分布特性,要求sim(q~i~,k~j~)≥0恒成立.比如可以把核函数改为激活函数使得输出大于0.<p>还可以改成softmax.<p><img alt=image-20240217224419083 data-src=https://s2.loli.net/2024/02/17/wE3HgYJ7KnxZ5pe.png><p>其中softmax1、softmax2分别指在第一个（n）、第二个维度（d）进行Softmax运算.<p><a href=https://spaces.ac.cn/archives/7546 rel=noopener target=_blank>线性Attention的探索：Attention必须有个Softmax吗？ - 科学空间|Scientific Spaces</a>提出将指数<p>e^qK^泰勒展开,$e^{\boldsymbol{q}_i^\top\boldsymbol{k}_j}\approx1+\boldsymbol{q}_i^\top\boldsymbol{k}_j$<p><img alt=image-20240217224836831 data-src=https://s2.loli.net/2024/02/17/fhBMIgcsqzZQt3k.png><p>此外还有稀疏注意力,这里就不多介绍了.<h2 id=图像中的transformer与attention><a class=headerlink href=#图像中的transformer与attention title=图像中的transformer与attention></a>图像中的transformer与attention</h2><p>注意力机制以及transformer都是先在NLP领域发展,所以一般attention可能会处理一些1维数据,有CNN与transformer结合的Conformer<a href=https://arxiv.org/abs/2005.08100 rel=noopener target=_blank>[2005.08100] Conformer: Convolution-augmented Transformer for Speech Recognition (arxiv.org)</a>,conformer中的编码采用相对位置编码.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br><span class=line>132</span><br><span class=line>133</span><br><span class=line>134</span><br><span class=line>135</span><br><span class=line>136</span><br><span class=line>137</span><br><span class=line>138</span><br><span class=line>139</span><br><span class=line>140</span><br><span class=line>141</span><br><span class=line>142</span><br><span class=line>143</span><br><span class=line>144</span><br><span class=line>145</span><br><span class=line>146</span><br><span class=line>147</span><br><span class=line>148</span><br><span class=line>149</span><br><span class=line>150</span><br><span class=line>151</span><br><span class=line>152</span><br><span class=line>153</span><br><span class=line>154</span><br><span class=line>155</span><br><span class=line>156</span><br><span class=line>157</span><br><span class=line>158</span><br><span class=line>159</span><br><span class=line>160</span><br><span class=line>161</span><br><span class=line>162</span><br><span class=line>163</span><br><span class=line>164</span><br><span class=line>165</span><br><span class=line>166</span><br><span class=line>167</span><br><span class=line>168</span><br><span class=line>169</span><br><span class=line>170</span><br><span class=line>171</span><br><span class=line>172</span><br><span class=line>173</span><br><span class=line>174</span><br><span class=line>175</span><br><span class=line>176</span><br><span class=line>177</span><br><span class=line>178</span><br><span class=line>179</span><br><span class=line>180</span><br><span class=line>181</span><br><span class=line>182</span><br><span class=line>183</span><br><span class=line>184</span><br><span class=line>185</span><br><span class=line>186</span><br><span class=line>187</span><br><span class=line>188</span><br><span class=line>189</span><br><span class=line>190</span><br><span class=line>191</span><br><span class=line>192</span><br><span class=line>193</span><br><span class=line>194</span><br><span class=line>195</span><br><span class=line>196</span><br><span class=line>197</span><br><span class=line>198</span><br><span class=line>199</span><br><span class=line>200</span><br><span class=line>201</span><br><span class=line>202</span><br><span class=line>203</span><br><span class=line>204</span><br><span class=line>205</span><br><span class=line>206</span><br><span class=line>207</span><br><span class=line>208</span><br><span class=line>209</span><br><span class=line>210</span><br><span class=line>211</span><br><span class=line>212</span><br><span class=line>213</span><br><span class=line>214</span><br><span class=line>215</span><br><span class=line>216</span><br><span class=line>217</span><br><span class=line>218</span><br><span class=line>219</span><br><span class=line>220</span><br><span class=line>221</span><br><span class=line>222</span><br><span class=line>223</span><br><span class=line>224</span><br><span class=line>225</span><br><span class=line>226</span><br><span class=line>227</span><br><span class=line>228</span><br><span class=line>229</span><br><span class=line>230</span><br><span class=line>231</span><br><span class=line>232</span><br><span class=line>233</span><br><span class=line>234</span><br><span class=line>235</span><br><span class=line>236</span><br><span class=line>237</span><br><span class=line>238</span><br><span class=line>239</span><br><span class=line>240</span><br><span class=line>241</span><br><span class=line>242</span><br><span class=line>243</span><br><span class=line>244</span><br><span class=line>245</span><br><span class=line>246</span><br><span class=line>247</span><br><span class=line>248</span><br><span class=line>249</span><br><span class=line>250</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line><span class=keyword>from</span> einops <span class=keyword>import</span> rearrange</span><br><span class=line><span class=keyword>from</span> einops.layers.torch <span class=keyword>import</span> Rearrange</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn, einsum</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>exists</span>(<span class=params>val</span>):</span></span><br><span class=line>    <span class=keyword>return</span> val <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span></span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>default</span>(<span class=params>val, d</span>):</span></span><br><span class=line>    <span class=keyword>return</span> val <span class=keyword>if</span> exists(val) <span class=keyword>else</span> d</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Swish</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> x * x.sigmoid()</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>FeedForward</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, mult=<span class=number>4</span>, dropout=<span class=number>0.0</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.net = nn.Sequential(</span><br><span class=line>            nn.Linear(dim, dim * mult),</span><br><span class=line>            Swish(),  <span class=comment># or can be replace by nn.silu()</span></span><br><span class=line>            nn.Dropout(dropout),</span><br><span class=line>            nn.Linear(dim * mult, dim),</span><br><span class=line>            nn.Dropout(dropout),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.net(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Attention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, heads=<span class=number>8</span>, dim_head=<span class=number>64</span>, dropout=<span class=number>0.0</span>, max_pos_emb=<span class=number>512</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        inner_dim = dim_head * heads</span><br><span class=line>        self.heads = heads</span><br><span class=line>        self.scale = dim_head**-<span class=number>0.5</span></span><br><span class=line>        self.to_q = nn.Linear(dim, inner_dim, bias=<span class=literal>False</span>)</span><br><span class=line>        self.to_kv = nn.Linear(dim, inner_dim * <span class=number>2</span>, bias=<span class=literal>False</span>)</span><br><span class=line>        self.to_out = nn.Linear(inner_dim, dim)</span><br><span class=line></span><br><span class=line>        self.max_pos_emb = max_pos_emb</span><br><span class=line>        self.rel_pos_emb = nn.Embedding(<span class=number>2</span> * max_pos_emb + <span class=number>1</span>, dim_head)</span><br><span class=line></span><br><span class=line>        self.dropout = nn.Dropout(dropout)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, context=<span class=literal>None</span>, mask=<span class=literal>None</span>, context_mask=<span class=literal>None</span></span>):</span></span><br><span class=line>        n, device, h, max_pos_emb, has_context = (</span><br><span class=line>            x.shape[-<span class=number>2</span>],</span><br><span class=line>            x.device,</span><br><span class=line>            self.heads,</span><br><span class=line>            self.max_pos_emb,</span><br><span class=line>            exists(context),</span><br><span class=line>        )</span><br><span class=line>        context = default(context, x)</span><br><span class=line></span><br><span class=line>        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(<span class=number>2</span>, dim=-<span class=number>1</span>))</span><br><span class=line>        q, k, v = <span class=built_in>map</span>(<span class=keyword>lambda</span> t: rearrange(t, <span class=string>"b n (h d) -> b h n d"</span>, h=h), (q, k, v))</span><br><span class=line></span><br><span class=line>        dots = einsum(<span class=string>"b h i d, b h j d -> b h i j"</span>, q, k) * self.scale</span><br><span class=line></span><br><span class=line>        <span class=comment># shaw's relative positional embedding</span></span><br><span class=line>        seq = torch.arange(n, device=device)</span><br><span class=line>        dist = rearrange(seq, <span class=string>"i -> i ()"</span>) - rearrange(seq, <span class=string>"j -> () j"</span>)</span><br><span class=line>        dist = dist.clamp(-max_pos_emb, max_pos_emb) + max_pos_emb</span><br><span class=line>        rel_pos_emb = self.rel_pos_emb(dist).to(q)</span><br><span class=line>        pos_attn = einsum(<span class=string>"b h n d, n r d -> b h n r"</span>, q, rel_pos_emb) * self.scale</span><br><span class=line>        dots = dots + pos_attn</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> exists(mask) <span class=keyword>or</span> exists(context_mask):</span><br><span class=line>            mask = default(mask, <span class=keyword>lambda</span>: torch.ones(*x.shape[:<span class=number>2</span>], device=device))</span><br><span class=line>            context_mask = (</span><br><span class=line>                default(context_mask, mask)</span><br><span class=line>                <span class=keyword>if</span> <span class=keyword>not</span> has_context</span><br><span class=line>                <span class=keyword>else</span> default(</span><br><span class=line>                    context_mask, <span class=keyword>lambda</span>: torch.ones(*context.shape[:<span class=number>2</span>], device=device)</span><br><span class=line>                )</span><br><span class=line>            )</span><br><span class=line>            mask_value = -torch.finfo(dots.dtype).<span class=built_in>max</span></span><br><span class=line>            mask = rearrange(mask, <span class=string>"b i -> b () i ()"</span>) * rearrange(</span><br><span class=line>                context_mask, <span class=string>"b j -> b () () j"</span></span><br><span class=line>            )</span><br><span class=line>            dots.masked_fill_(~mask, mask_value)</span><br><span class=line></span><br><span class=line>        attn = dots.softmax(dim=-<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        out = einsum(<span class=string>"b h i j, b h j d -> b h i d"</span>, attn, v)</span><br><span class=line>        out = rearrange(out, <span class=string>"b h n d -> b n (h d)"</span>)</span><br><span class=line>        out = self.to_out(out)</span><br><span class=line>        <span class=keyword>return</span> self.dropout(out)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>calc_same_padding</span>(<span class=params>kernel_size</span>):</span></span><br><span class=line>    pad = kernel_size // <span class=number>2</span></span><br><span class=line>    <span class=keyword>return</span> pad, pad - (kernel_size + <span class=number>1</span>) % <span class=number>2</span></span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>DepthWiseConv1d</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, chan_in, chan_out, kernel_size, padding</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.padding = padding</span><br><span class=line>        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups=chan_in)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = F.pad(x, self.padding)</span><br><span class=line>        <span class=keyword>return</span> self.conv(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>GLU</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.dim = dim</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        out, gate = x.chunk(<span class=number>2</span>, dim=self.dim)</span><br><span class=line>        <span class=keyword>return</span> out * gate.sigmoid()</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ConformerConvModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self, dim, causal=<span class=literal>False</span>, expansion_factor=<span class=number>2</span>, kernel_size=<span class=number>31</span>, dropout=<span class=number>0.0</span></span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        inner_dim = dim * expansion_factor</span><br><span class=line>        padding = calc_same_padding(kernel_size) <span class=keyword>if</span> <span class=keyword>not</span> causal <span class=keyword>else</span> (kernel_size - <span class=number>1</span>, <span class=number>0</span>)</span><br><span class=line>        self.net = nn.Sequential(</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>            Rearrange(<span class=string>"b n d -> b d n"</span>),</span><br><span class=line>            nn.Conv1d(dim, inner_dim * <span class=number>2</span>, <span class=number>1</span>),</span><br><span class=line>            GLU(dim=<span class=number>1</span>),</span><br><span class=line>            DepthWiseConv1d(</span><br><span class=line>                inner_dim, inner_dim, kernel_size=kernel_size, padding=padding</span><br><span class=line>            ),</span><br><span class=line>            nn.BatchNorm1d(inner_dim) <span class=keyword>if</span> <span class=keyword>not</span> causal <span class=keyword>else</span> nn.Identity(),</span><br><span class=line>            Swish(),</span><br><span class=line>            nn.Conv1d(inner_dim, dim, <span class=number>1</span>),</span><br><span class=line>            Rearrange(<span class=string>"b d n -> b n d"</span>),</span><br><span class=line>            nn.Dropout(dropout),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.net(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Scale</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, scale, fn</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.scale = scale</span><br><span class=line>        self.fn = fn</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, **kwargs</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.fn(x, **kwargs) * self.scale</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>PreNorm</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, fn</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.fn = fn</span><br><span class=line>        self.norm = nn.LayerNorm(dim)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, **kwargs</span>):</span></span><br><span class=line>        x = self.norm(x)</span><br><span class=line>        <span class=keyword>return</span> self.fn(x, **kwargs)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ConformerBlock</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        *,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        dim_head=<span class=number>64</span>,</span></span></span><br><span class=line><span class=params><span class=function>        heads=<span class=number>8</span>,</span></span></span><br><span class=line><span class=params><span class=function>        ff_mult=<span class=number>4</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_expansion_factor=<span class=number>2</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_kernel_size=<span class=number>31</span>,</span></span></span><br><span class=line><span class=params><span class=function>        attn_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        ff_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_causal=<span class=literal>False</span></span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.ff1 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)</span><br><span class=line>        self.attn = Attention(</span><br><span class=line>            dim=dim, dim_head=dim_head, heads=heads, dropout=attn_dropout</span><br><span class=line>        )</span><br><span class=line>        self.conv = ConformerConvModule(</span><br><span class=line>            dim=dim,</span><br><span class=line>            causal=conv_causal,</span><br><span class=line>            expansion_factor=conv_expansion_factor,</span><br><span class=line>            kernel_size=conv_kernel_size,</span><br><span class=line>            dropout=conv_dropout,</span><br><span class=line>        )</span><br><span class=line>        self.ff2 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)</span><br><span class=line></span><br><span class=line>        self.attn = PreNorm(dim, self.attn)</span><br><span class=line>        self.ff1 = Scale(<span class=number>0.5</span>, PreNorm(dim, self.ff1))</span><br><span class=line>        self.ff2 = Scale(<span class=number>0.5</span>, PreNorm(dim, self.ff2))</span><br><span class=line></span><br><span class=line>        self.post_norm = nn.LayerNorm(dim)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, mask=<span class=literal>None</span></span>):</span></span><br><span class=line>        x = self.ff1(x) + x</span><br><span class=line>        x = self.attn(x, mask=mask) + x</span><br><span class=line>        x = self.conv(x) + x</span><br><span class=line>        x = self.ff2(x) + x</span><br><span class=line>        x = self.post_norm(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Conformer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        *,</span></span></span><br><span class=line><span class=params><span class=function>        depth,</span></span></span><br><span class=line><span class=params><span class=function>        dim_head=<span class=number>64</span>,</span></span></span><br><span class=line><span class=params><span class=function>        heads=<span class=number>8</span>,</span></span></span><br><span class=line><span class=params><span class=function>        ff_mult=<span class=number>4</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_expansion_factor=<span class=number>2</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_kernel_size=<span class=number>31</span>,</span></span></span><br><span class=line><span class=params><span class=function>        attn_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        ff_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        conv_causal=<span class=literal>False</span></span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.dim = dim</span><br><span class=line>        self.layers = nn.ModuleList([])</span><br><span class=line>        <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(depth):</span><br><span class=line>            self.layers.append(</span><br><span class=line>                ConformerBlock(</span><br><span class=line>                    dim=dim,</span><br><span class=line>                    dim_head=dim_head,</span><br><span class=line>                    heads=heads,</span><br><span class=line>                    ff_mult=ff_mult,</span><br><span class=line>                    conv_expansion_factor=conv_expansion_factor,</span><br><span class=line>                    conv_kernel_size=conv_kernel_size,</span><br><span class=line>                    conv_causal=conv_causal,</span><br><span class=line>                )</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>for</span> block <span class=keyword>in</span> self.layers:</span><br><span class=line>            x = block(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br></pre></table></figure><p>上一节中其实已经充分使用了feature map也就是二维数据上的注意力机制,现在介绍一下在视觉领域表现出色的transformer及其变体.<h2 id=Vision-Transformer><a title="Vision Transformer" class=headerlink href=#Vision-Transformer></a>Vision Transformer</h2><p><img alt=image-20240217121859843 data-src=https://s2.loli.net/2024/02/17/mdybruOEWxgSDVt.png><p>将transformer拿到CV领域的出名作品,通过patch embedding得到序列,再加上位置编码就能像在nlp一样处理问题.<p><img alt=img data-src=https://pic1.zhimg.com/80/v2-1c6aa554d8fc53daa7bf79c755b1f86c_720w.webp><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br><span class=line>132</span><br><span class=line>133</span><br><span class=line>134</span><br><span class=line>135</span><br><span class=line>136</span><br><span class=line>137</span><br><span class=line>138</span><br><span class=line>139</span><br><span class=line>140</span><br><span class=line>141</span><br><span class=line>142</span><br><span class=line>143</span><br><span class=line>144</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> einops <span class=keyword>import</span> rearrange</span><br><span class=line><span class=keyword>from</span> einops.layers.torch <span class=keyword>import</span> Rearrange</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line><span class=comment># helpers</span></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>pair</span>(<span class=params>t</span>):</span></span><br><span class=line>    <span class=keyword>return</span> t <span class=keyword>if</span> <span class=built_in>isinstance</span>(t, <span class=built_in>tuple</span>) <span class=keyword>else</span> (t, t)</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>posemb_sincos_2d</span>(<span class=params>h, w, dim, temperature: <span class=built_in>int</span> = <span class=number>10000</span>, dtype=torch.float32</span>):</span></span><br><span class=line>    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=<span class=string>"ij"</span>)</span><br><span class=line>    <span class=keyword>assert</span> (dim % <span class=number>4</span>) == <span class=number>0</span>, <span class=string>"feature dimension must be multiple of 4 for sincos emb"</span></span><br><span class=line>    omega = torch.arange(dim // <span class=number>4</span>) / (dim // <span class=number>4</span> - <span class=number>1</span>)</span><br><span class=line>    omega = <span class=number>1.0</span> / (temperature**omega)</span><br><span class=line></span><br><span class=line>    y = y.flatten()[:, <span class=literal>None</span>] * omega[<span class=literal>None</span>, :]</span><br><span class=line>    x = x.flatten()[:, <span class=literal>None</span>] * omega[<span class=literal>None</span>, :]</span><br><span class=line>    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=<span class=number>1</span>)</span><br><span class=line>    <span class=keyword>return</span> pe.<span class=built_in>type</span>(dtype)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=comment># classes</span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>FeedForward</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, hidden_dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.net = nn.Sequential(</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>            nn.Linear(dim, hidden_dim),</span><br><span class=line>            nn.GELU(),</span><br><span class=line>            nn.Linear(hidden_dim, dim),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.net(x)</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Attention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, heads=<span class=number>8</span>, dim_head=<span class=number>64</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        inner_dim = dim_head * heads</span><br><span class=line>        self.heads = heads</span><br><span class=line>        self.scale = dim_head**-<span class=number>0.5</span></span><br><span class=line>        self.norm = nn.LayerNorm(dim)</span><br><span class=line></span><br><span class=line>        self.attend = nn.Softmax(dim=-<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        self.to_qkv = nn.Linear(dim, inner_dim * <span class=number>3</span>, bias=<span class=literal>False</span>)</span><br><span class=line>        self.to_out = nn.Linear(inner_dim, dim, bias=<span class=literal>False</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.norm(x)</span><br><span class=line></span><br><span class=line>        qkv = self.to_qkv(x).chunk(<span class=number>3</span>, dim=-<span class=number>1</span>)</span><br><span class=line>        q, k, v = <span class=built_in>map</span>(<span class=keyword>lambda</span> t: rearrange(t, <span class=string>"b n (h d) -> b h n d"</span>, h=self.heads), qkv)</span><br><span class=line></span><br><span class=line>        dots = torch.matmul(q, k.transpose(-<span class=number>1</span>, -<span class=number>2</span>)) * self.scale</span><br><span class=line></span><br><span class=line>        attn = self.attend(dots)</span><br><span class=line></span><br><span class=line>        out = torch.matmul(attn, v)</span><br><span class=line>        out = rearrange(out, <span class=string>"b h n d -> b n (h d)"</span>)</span><br><span class=line>        <span class=keyword>return</span> self.to_out(out)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Transformer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, depth, heads, dim_head, mlp_dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.norm = nn.LayerNorm(dim)</span><br><span class=line>        self.layers = nn.ModuleList([])</span><br><span class=line>        <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(depth):</span><br><span class=line>            self.layers.append(</span><br><span class=line>                nn.ModuleList(</span><br><span class=line>                    [</span><br><span class=line>                        Attention(dim, heads=heads, dim_head=dim_head),</span><br><span class=line>                        FeedForward(dim, mlp_dim),</span><br><span class=line>                    ]</span><br><span class=line>                )</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>for</span> attn, ff <span class=keyword>in</span> self.layers:</span><br><span class=line>            x = attn(x) + x</span><br><span class=line>            x = ff(x) + x</span><br><span class=line>        <span class=keyword>return</span> self.norm(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>SimpleViT</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        *,</span></span></span><br><span class=line><span class=params><span class=function>        image_size,</span></span></span><br><span class=line><span class=params><span class=function>        patch_size,</span></span></span><br><span class=line><span class=params><span class=function>        num_classes,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        depth,</span></span></span><br><span class=line><span class=params><span class=function>        heads,</span></span></span><br><span class=line><span class=params><span class=function>        mlp_dim,</span></span></span><br><span class=line><span class=params><span class=function>        channels=<span class=number>3</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dim_head=<span class=number>64</span></span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        image_height, image_width = pair(image_size)</span><br><span class=line>        patch_height, patch_width = pair(patch_size)</span><br><span class=line></span><br><span class=line>        <span class=keyword>assert</span> (</span><br><span class=line>            image_height % patch_height == <span class=number>0</span> <span class=keyword>and</span> image_width % patch_width == <span class=number>0</span></span><br><span class=line>        ), <span class=string>"Image dimensions must be divisible by the patch size."</span></span><br><span class=line></span><br><span class=line>        patch_dim = channels * patch_height * patch_width</span><br><span class=line></span><br><span class=line>        self.to_patch_embedding = nn.Sequential(</span><br><span class=line>            Rearrange(</span><br><span class=line>                <span class=string>"b c (h p1) (w p2) -> b (h w) (p1 p2 c)"</span>,</span><br><span class=line>                p1=patch_height,</span><br><span class=line>                p2=patch_width,</span><br><span class=line>            ),</span><br><span class=line>            nn.LayerNorm(patch_dim),</span><br><span class=line>            nn.Linear(patch_dim, dim),</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>        self.pos_embedding = posemb_sincos_2d(</span><br><span class=line>            h=image_height // patch_height,</span><br><span class=line>            w=image_width // patch_width,</span><br><span class=line>            dim=dim,</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)</span><br><span class=line></span><br><span class=line>        self.pool = <span class=string>"mean"</span></span><br><span class=line>        self.to_latent = nn.Identity()</span><br><span class=line></span><br><span class=line>        self.linear_head = nn.Linear(dim, num_classes)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, img</span>):</span></span><br><span class=line>        device = img.device</span><br><span class=line></span><br><span class=line>        x = self.to_patch_embedding(img)</span><br><span class=line>        x += self.pos_embedding.to(device, dtype=x.dtype)</span><br><span class=line></span><br><span class=line>        x = self.transformer(x)</span><br><span class=line>        x = x.mean(dim=<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        x = self.to_latent(x)</span><br><span class=line>        <span class=keyword>return</span> self.linear_head(x)</span><br></pre></table></figure><p>上面做了patch之后的位置编码使用三角函数绝对编码,attention和feednetwork与transformer没有什么差别.<h2 id=卷积注意力><a class=headerlink href=#卷积注意力 title=卷积注意力></a>卷积注意力</h2><p>使用vision transformer中使用的绝对位置注意力,但是也可以使用相对位置注意力或者卷积注意力.<blockquote><p>卷积位置嵌入( CPE )方法考虑了输入序列的2D性质。采用补零的方式进行2D卷积采集位置信息。卷积位置嵌入( Convolutional Position嵌入，CPE )可用于合并ViT不同阶段的位置数据。CPE可以具体引入到自注意力模块，前馈网络，或者在两个编码器层之间的。</blockquote><p>卷积注意力通常方法是利用2D卷积或者depth-wise的卷积将已经做了patch的图像数据进行处理.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>ConvolutionalPositionEmbedding</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, d_model, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.conv = nn.Conv2d(d_model, d_model, kernel_size, padding=padding)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = x.transpose(<span class=number>1</span>, <span class=number>2</span>)  <span class=comment># 将通道维度和序列长度维度交换</span></span><br><span class=line>        x = x.unsqueeze(<span class=number>2</span>)  <span class=comment># 在通道维度和序列长度维度之间添加一个维度</span></span><br><span class=line>        x = self.conv(x)  <span class=comment># 对输入进行卷积操作</span></span><br><span class=line>        x = x.squeeze(<span class=number>2</span>)  <span class=comment># 移除添加的维度</span></span><br><span class=line>        x = x.transpose(<span class=number>1</span>, <span class=number>2</span>)  <span class=comment># 将通道维度和序列长度维度交换回来</span></span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><h3 id=CVT><a class=headerlink href=#CVT title=CVT></a>CVT</h3><p><img alt=img data-src=https://github.com/rishikksh20/convolution-vision-transformers/raw/master/assets/model.PNG><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br><span class=line>132</span><br><span class=line>133</span><br><span class=line>134</span><br><span class=line>135</span><br><span class=line>136</span><br><span class=line>137</span><br><span class=line>138</span><br><span class=line>139</span><br><span class=line>140</span><br><span class=line>141</span><br><span class=line>142</span><br><span class=line>143</span><br><span class=line>144</span><br><span class=line>145</span><br><span class=line>146</span><br><span class=line>147</span><br><span class=line>148</span><br><span class=line>149</span><br><span class=line>150</span><br><span class=line>151</span><br><span class=line>152</span><br><span class=line>153</span><br><span class=line>154</span><br><span class=line>155</span><br><span class=line>156</span><br><span class=line>157</span><br><span class=line>158</span><br><span class=line>159</span><br><span class=line>160</span><br><span class=line>161</span><br><span class=line>162</span><br><span class=line>163</span><br><span class=line>164</span><br><span class=line>165</span><br><span class=line>166</span><br><span class=line>167</span><br><span class=line>168</span><br><span class=line>169</span><br><span class=line>170</span><br><span class=line>171</span><br><span class=line>172</span><br><span class=line>173</span><br><span class=line>174</span><br><span class=line>175</span><br><span class=line>176</span><br><span class=line>177</span><br><span class=line>178</span><br><span class=line>179</span><br><span class=line>180</span><br><span class=line>181</span><br><span class=line>182</span><br><span class=line>183</span><br><span class=line>184</span><br><span class=line>185</span><br><span class=line>186</span><br><span class=line>187</span><br><span class=line>188</span><br><span class=line>189</span><br><span class=line>190</span><br><span class=line>191</span><br><span class=line>192</span><br><span class=line>193</span><br><span class=line>194</span><br><span class=line>195</span><br><span class=line>196</span><br><span class=line>197</span><br><span class=line>198</span><br><span class=line>199</span><br><span class=line>200</span><br><span class=line>201</span><br><span class=line>202</span><br><span class=line>203</span><br><span class=line>204</span><br><span class=line>205</span><br><span class=line>206</span><br><span class=line>207</span><br><span class=line>208</span><br><span class=line>209</span><br><span class=line>210</span><br><span class=line>211</span><br><span class=line>212</span><br><span class=line>213</span><br><span class=line>214</span><br><span class=line>215</span><br><span class=line>216</span><br><span class=line>217</span><br><span class=line>218</span><br><span class=line>219</span><br><span class=line>220</span><br><span class=line>221</span><br><span class=line>222</span><br><span class=line>223</span><br><span class=line>224</span><br><span class=line>225</span><br><span class=line>226</span><br><span class=line>227</span><br><span class=line>228</span><br><span class=line>229</span><br><span class=line>230</span><br><span class=line>231</span><br><span class=line>232</span><br><span class=line>233</span><br><span class=line>234</span><br><span class=line>235</span><br><span class=line>236</span><br><span class=line>237</span><br><span class=line>238</span><br><span class=line>239</span><br><span class=line>240</span><br><span class=line>241</span><br><span class=line>242</span><br><span class=line>243</span><br><span class=line>244</span><br><span class=line>245</span><br><span class=line>246</span><br><span class=line>247</span><br><span class=line>248</span><br><span class=line>249</span><br><span class=line>250</span><br><span class=line>251</span><br><span class=line>252</span><br><span class=line>253</span><br><span class=line>254</span><br><span class=line>255</span><br><span class=line>256</span><br><span class=line>257</span><br><span class=line>258</span><br><span class=line>259</span><br><span class=line>260</span><br><span class=line>261</span><br><span class=line>262</span><br><span class=line>263</span><br><span class=line>264</span><br><span class=line>265</span><br><span class=line>266</span><br><span class=line>267</span><br><span class=line>268</span><br><span class=line>269</span><br><span class=line>270</span><br><span class=line>271</span><br><span class=line>272</span><br><span class=line>273</span><br><span class=line>274</span><br><span class=line>275</span><br><span class=line>276</span><br><span class=line>277</span><br><span class=line>278</span><br><span class=line>279</span><br><span class=line>280</span><br><span class=line>281</span><br><span class=line>282</span><br><span class=line>283</span><br><span class=line>284</span><br><span class=line>285</span><br><span class=line>286</span><br><span class=line>287</span><br><span class=line>288</span><br><span class=line>289</span><br></pre><td class=code><pre><span class=line><span class=comment>#   #!/usr/bin/env python</span></span><br><span class=line><span class=comment>#   #-*- coding:utf-8 -*-</span></span><br><span class=line><span class=comment>#  Copyleft (C) 2024 proanimer, Inc. All Rights Reserved</span></span><br><span class=line><span class=comment>#   author:proanimer</span></span><br><span class=line><span class=comment>#   createTime:2024/2/18 上午10:38</span></span><br><span class=line><span class=comment>#   lastModifiedTime:2024/2/18 上午10:38</span></span><br><span class=line><span class=comment>#   file:cvt.py</span></span><br><span class=line><span class=comment>#   software: classicNets</span></span><br><span class=line><span class=comment>#</span></span><br><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>from</span> einops <span class=keyword>import</span> rearrange, repeat</span><br><span class=line><span class=keyword>from</span> einops.layers.torch <span class=keyword>import</span> Rearrange</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> einsum</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>SepConv2d</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        in_channels,</span></span></span><br><span class=line><span class=params><span class=function>        out_channels,</span></span></span><br><span class=line><span class=params><span class=function>        kernel_size,</span></span></span><br><span class=line><span class=params><span class=function>        stride=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        padding=<span class=number>0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dilation=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>(SepConv2d, self).__init__()</span><br><span class=line>        self.depthwise = torch.nn.Conv2d(</span><br><span class=line>            in_channels,</span><br><span class=line>            in_channels,</span><br><span class=line>            kernel_size=kernel_size,</span><br><span class=line>            stride=stride,</span><br><span class=line>            padding=padding,</span><br><span class=line>            dilation=dilation,</span><br><span class=line>            groups=in_channels,</span><br><span class=line>        )</span><br><span class=line>        self.bn = torch.nn.BatchNorm2d(in_channels)</span><br><span class=line>        self.pointwise = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.depthwise(x)</span><br><span class=line>        x = self.bn(x)</span><br><span class=line>        x = self.pointwise(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Residual</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, fn</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.fn = fn</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, **kwargs</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.fn(x, **kwargs) + x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>PreNorm</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, fn</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.norm = nn.LayerNorm(dim)</span><br><span class=line>        self.fn = fn</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, **kwargs</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.fn(self.norm(x), **kwargs)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>FeedForward</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, hidden_dim, dropout=<span class=number>0.0</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.net = nn.Sequential(</span><br><span class=line>            nn.Linear(dim, hidden_dim),</span><br><span class=line>            nn.GELU(),</span><br><span class=line>            nn.Dropout(dropout),</span><br><span class=line>            nn.Linear(hidden_dim, dim),</span><br><span class=line>            nn.Dropout(dropout),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.net(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ConvAttention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        img_size,</span></span></span><br><span class=line><span class=params><span class=function>        heads=<span class=number>8</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dim_head=<span class=number>64</span>,</span></span></span><br><span class=line><span class=params><span class=function>        kernel_size=<span class=number>3</span>,</span></span></span><br><span class=line><span class=params><span class=function>        q_stride=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        k_stride=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        v_stride=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        last_stage=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.last_stage = last_stage</span><br><span class=line>        self.img_size = img_size</span><br><span class=line>        inner_dim = dim_head * heads</span><br><span class=line>        project_out = <span class=keyword>not</span> (heads == <span class=number>1</span> <span class=keyword>and</span> dim_head == dim)</span><br><span class=line></span><br><span class=line>        self.heads = heads</span><br><span class=line>        self.scale = dim_head**-<span class=number>0.5</span></span><br><span class=line>        pad = (kernel_size - q_stride) // <span class=number>2</span></span><br><span class=line>        self.to_q = SepConv2d(dim, inner_dim, kernel_size, q_stride, pad)</span><br><span class=line>        self.to_k = SepConv2d(dim, inner_dim, kernel_size, k_stride, pad)</span><br><span class=line>        self.to_v = SepConv2d(dim, inner_dim, kernel_size, v_stride, pad)</span><br><span class=line></span><br><span class=line>        self.to_out = (</span><br><span class=line>            nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))</span><br><span class=line>            <span class=keyword>if</span> project_out</span><br><span class=line>            <span class=keyword>else</span> nn.Identity()</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        b, n, _, h = *x.shape, self.heads</span><br><span class=line>        <span class=keyword>if</span> self.last_stage:</span><br><span class=line>            cls_token = x[:, <span class=number>0</span>]</span><br><span class=line>            x = x[:, <span class=number>1</span>:]</span><br><span class=line>            cls_token = rearrange(cls_token.unsqueeze(<span class=number>1</span>), <span class=string>"b n (h d) -> b h n d"</span>, h=h)</span><br><span class=line>        x = rearrange(x, <span class=string>"b (l w) n -> b n l w"</span>, l=self.img_size, w=self.img_size)</span><br><span class=line>        q = self.to_q(x)</span><br><span class=line>        q = rearrange(q, <span class=string>"b (h d) l w -> b h (l w) d"</span>, h=h)</span><br><span class=line></span><br><span class=line>        v = self.to_v(x)</span><br><span class=line>        v = rearrange(v, <span class=string>"b (h d) l w -> b h (l w) d"</span>, h=h)</span><br><span class=line></span><br><span class=line>        k = self.to_k(x)</span><br><span class=line>        k = rearrange(k, <span class=string>"b (h d) l w -> b h (l w) d"</span>, h=h)</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> self.last_stage:</span><br><span class=line>            q = torch.cat((cls_token, q), dim=<span class=number>2</span>)</span><br><span class=line>            v = torch.cat((cls_token, v), dim=<span class=number>2</span>)</span><br><span class=line>            k = torch.cat((cls_token, k), dim=<span class=number>2</span>)</span><br><span class=line></span><br><span class=line>        dots = einsum(<span class=string>"b h i d, b h j d -> b h i j"</span>, q, k) * self.scale</span><br><span class=line></span><br><span class=line>        attn = dots.softmax(dim=-<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        out = einsum(<span class=string>"b h i j, b h j d -> b h i d"</span>, attn, v)</span><br><span class=line>        out = rearrange(out, <span class=string>"b h n d -> b n (h d)"</span>)</span><br><span class=line>        out = self.to_out(out)</span><br><span class=line>        <span class=keyword>return</span> out</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Transformer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        img_size,</span></span></span><br><span class=line><span class=params><span class=function>        depth,</span></span></span><br><span class=line><span class=params><span class=function>        heads,</span></span></span><br><span class=line><span class=params><span class=function>        dim_head,</span></span></span><br><span class=line><span class=params><span class=function>        mlp_dim,</span></span></span><br><span class=line><span class=params><span class=function>        dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        last_stage=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.layers = nn.ModuleList([])</span><br><span class=line>        <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(depth):</span><br><span class=line>            self.layers.append(</span><br><span class=line>                nn.ModuleList(</span><br><span class=line>                    [</span><br><span class=line>                        PreNorm(</span><br><span class=line>                            dim,</span><br><span class=line>                            ConvAttention(</span><br><span class=line>                                dim,</span><br><span class=line>                                img_size,</span><br><span class=line>                                heads=heads,</span><br><span class=line>                                dim_head=dim_head,</span><br><span class=line>                                dropout=dropout,</span><br><span class=line>                                last_stage=last_stage,</span><br><span class=line>                            ),</span><br><span class=line>                        ),</span><br><span class=line>                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)),</span><br><span class=line>                    ]</span><br><span class=line>                )</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>for</span> attn, ff <span class=keyword>in</span> self.layers:</span><br><span class=line>            x = attn(x) + x</span><br><span class=line>            x = ff(x) + x</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>cvt</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        image_size,</span></span></span><br><span class=line><span class=params><span class=function>        in_channels,</span></span></span><br><span class=line><span class=params><span class=function>        num_classes,</span></span></span><br><span class=line><span class=params><span class=function>        dim=<span class=number>64</span>,</span></span></span><br><span class=line><span class=params><span class=function>        kernels=[<span class=number>7</span>, <span class=number>3</span>, <span class=number>3</span>],</span></span></span><br><span class=line><span class=params><span class=function>        strides=[<span class=number>4</span>, <span class=number>2</span>, <span class=number>2</span>],</span></span></span><br><span class=line><span class=params><span class=function>        heads=[<span class=number>1</span>, <span class=number>3</span>, <span class=number>6</span>],</span></span></span><br><span class=line><span class=params><span class=function>        depth=[<span class=number>1</span>, <span class=number>2</span>, <span class=number>10</span>],</span></span></span><br><span class=line><span class=params><span class=function>        pool=<span class=string>"cls"</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        emb_dropout=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        scale_dim=<span class=number>4</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>(cvt, self).__init__()</span><br><span class=line>        <span class=keyword>assert</span> pool <span class=keyword>in</span> {</span><br><span class=line>            <span class=string>"cls"</span>,</span><br><span class=line>            <span class=string>"mean"</span>,</span><br><span class=line>        }, <span class=string>"pool type must be either cls (cls token) or mean (mean pooling)"</span></span><br><span class=line>        self.pool = pool</span><br><span class=line>        self.dim = dim</span><br><span class=line>        self.stage1_conv_embed = nn.Sequential(</span><br><span class=line>            nn.Conv2d(in_channels, dim, kernels[<span class=number>0</span>], strides[<span class=number>0</span>], <span class=number>2</span>),</span><br><span class=line>            Rearrange(<span class=string>"b c h w -> b (h w) c"</span>, h=image_size // <span class=number>4</span>, w=image_size // <span class=number>4</span>),</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>        )</span><br><span class=line>        self.stage_1_transformer = nn.Sequential(</span><br><span class=line>            Transformer(</span><br><span class=line>                dim,</span><br><span class=line>                img_size=image_size // <span class=number>4</span>,</span><br><span class=line>                depth=depth[<span class=number>0</span>],</span><br><span class=line>                heads=heads[<span class=number>0</span>],</span><br><span class=line>                dim_head=dim // heads[<span class=number>0</span>],</span><br><span class=line>                mlp_dim=dim * scale_dim,</span><br><span class=line>                dropout=dropout,</span><br><span class=line>                last_stage=<span class=literal>True</span>,</span><br><span class=line>            ),</span><br><span class=line>            Rearrange(<span class=string>"b (h w) c -> b c h w"</span>, h=image_size // <span class=number>4</span>, w=image_size // <span class=number>4</span>),</span><br><span class=line>        )</span><br><span class=line>        <span class=comment>#     stage 2</span></span><br><span class=line>        in_channels = dim</span><br><span class=line>        scale = heads[<span class=number>1</span>] // heads[<span class=number>0</span>]</span><br><span class=line>        dim = scale * dim</span><br><span class=line>        self.stage2_conv_embed = nn.Sequential(</span><br><span class=line>            nn.Conv2d(in_channels, dim, kernels[<span class=number>1</span>], strides[<span class=number>1</span>], <span class=number>1</span>),</span><br><span class=line>            Rearrange(<span class=string>"b c h w -> b (h w) c"</span>, h=image_size // <span class=number>8</span>, w=image_size // <span class=number>8</span>),</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>        )</span><br><span class=line>        self.stage_2_transformer = nn.Sequential(</span><br><span class=line>            Transformer(</span><br><span class=line>                dim,</span><br><span class=line>                img_size=image_size // <span class=number>8</span>,</span><br><span class=line>                depth=depth[<span class=number>1</span>],</span><br><span class=line>                heads=heads[<span class=number>1</span>],</span><br><span class=line>                dim_head=dim // heads[<span class=number>1</span>],</span><br><span class=line>                mlp_dim=dim * scale_dim,</span><br><span class=line>                dropout=dropout,</span><br><span class=line>                last_stage=<span class=literal>True</span>,</span><br><span class=line>            ),</span><br><span class=line>            Rearrange(<span class=string>"b (h w) c -> b c h w"</span>, h=image_size // <span class=number>8</span>, w=image_size // <span class=number>8</span>),</span><br><span class=line>        )</span><br><span class=line>        <span class=comment>#     stage 3</span></span><br><span class=line>        in_channels = dim</span><br><span class=line>        scale = heads[<span class=number>2</span>] // heads[<span class=number>1</span>]</span><br><span class=line>        dim = scale * dim</span><br><span class=line>        self.stage3_conv_embed = nn.Sequential(</span><br><span class=line>            nn.Conv2d(in_channels, dim, kernels[<span class=number>2</span>], strides[<span class=number>2</span>], <span class=number>1</span>),</span><br><span class=line>            Rearrange(<span class=string>"b c h w -> b (h w) c"</span>, h=image_size // <span class=number>16</span>, w=image_size // <span class=number>16</span>),</span><br><span class=line>            nn.LayerNorm(dim),</span><br><span class=line>        )</span><br><span class=line>        self.stage_3_transformer = nn.Sequential(</span><br><span class=line>            Transformer(</span><br><span class=line>                dim=dim,</span><br><span class=line>                img_size=image_size // <span class=number>16</span>,</span><br><span class=line>                depth=depth[<span class=number>2</span>],</span><br><span class=line>                heads=heads[<span class=number>2</span>],</span><br><span class=line>                dim_head=self.dim,</span><br><span class=line>                mlp_dim=dim * scale_dim,</span><br><span class=line>                dropout=dropout,</span><br><span class=line>                last_stage=<span class=literal>True</span>,</span><br><span class=line>            ),</span><br><span class=line>        )</span><br><span class=line>        self.cls_token = nn.Parameter(torch.randn(<span class=number>1</span>, <span class=number>1</span>, dim))</span><br><span class=line>        self.drop_large = nn.Dropout(emb_dropout)</span><br><span class=line></span><br><span class=line>        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self,img</span>):</span></span><br><span class=line>        xs = self.stage1_conv_embed(img)</span><br><span class=line>        xs = self.stage1_transformer(xs)</span><br><span class=line></span><br><span class=line>        xs = self.stage2_conv_embed(xs)</span><br><span class=line>        xs = self.stage2_transformer(xs)</span><br><span class=line></span><br><span class=line>        xs = self.stage3_conv_embed(xs)</span><br><span class=line>        b, n, _ = xs.shape</span><br><span class=line>        cls_tokens = repeat(self.cls_token, <span class=string>'() n d -> b n d'</span>, b=b)</span><br><span class=line>        xs = torch.cat((cls_tokens, xs), dim=<span class=number>1</span>)</span><br><span class=line>        xs = self.stage3_transformer(xs)</span><br><span class=line>        xs = xs.mean(dim=<span class=number>1</span>) <span class=keyword>if</span> self.pool == <span class=string>'mean'</span> <span class=keyword>else</span> xs[:, <span class=number>0</span>]</span><br><span class=line>        xs = self.mlp_head(xs)</span><br><span class=line>        <span class=keyword>return</span> xs</span><br></pre></table></figure><h3 id=PVT><a class=headerlink href=#PVT title=PVT></a>PVT</h3><p><img alt=image-20240218105527163 data-src=https://s2.loli.net/2024/02/18/w8EUDyAJ4sIv51n.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br><span class=line>132</span><br><span class=line>133</span><br><span class=line>134</span><br><span class=line>135</span><br><span class=line>136</span><br><span class=line>137</span><br><span class=line>138</span><br><span class=line>139</span><br><span class=line>140</span><br><span class=line>141</span><br><span class=line>142</span><br><span class=line>143</span><br><span class=line>144</span><br><span class=line>145</span><br><span class=line>146</span><br><span class=line>147</span><br><span class=line>148</span><br><span class=line>149</span><br><span class=line>150</span><br><span class=line>151</span><br><span class=line>152</span><br><span class=line>153</span><br><span class=line>154</span><br><span class=line>155</span><br><span class=line>156</span><br><span class=line>157</span><br><span class=line>158</span><br><span class=line>159</span><br><span class=line>160</span><br><span class=line>161</span><br><span class=line>162</span><br><span class=line>163</span><br><span class=line>164</span><br><span class=line>165</span><br><span class=line>166</span><br><span class=line>167</span><br><span class=line>168</span><br><span class=line>169</span><br><span class=line>170</span><br><span class=line>171</span><br><span class=line>172</span><br><span class=line>173</span><br><span class=line>174</span><br><span class=line>175</span><br><span class=line>176</span><br><span class=line>177</span><br><span class=line>178</span><br><span class=line>179</span><br><span class=line>180</span><br><span class=line>181</span><br><span class=line>182</span><br><span class=line>183</span><br><span class=line>184</span><br><span class=line>185</span><br><span class=line>186</span><br><span class=line>187</span><br><span class=line>188</span><br><span class=line>189</span><br><span class=line>190</span><br><span class=line>191</span><br><span class=line>192</span><br><span class=line>193</span><br><span class=line>194</span><br><span class=line>195</span><br><span class=line>196</span><br><span class=line>197</span><br><span class=line>198</span><br><span class=line>199</span><br><span class=line>200</span><br><span class=line>201</span><br><span class=line>202</span><br><span class=line>203</span><br><span class=line>204</span><br><span class=line>205</span><br><span class=line>206</span><br><span class=line>207</span><br><span class=line>208</span><br><span class=line>209</span><br><span class=line>210</span><br><span class=line>211</span><br><span class=line>212</span><br><span class=line>213</span><br><span class=line>214</span><br><span class=line>215</span><br><span class=line>216</span><br><span class=line>217</span><br><span class=line>218</span><br><span class=line>219</span><br><span class=line>220</span><br><span class=line>221</span><br><span class=line>222</span><br><span class=line>223</span><br><span class=line>224</span><br><span class=line>225</span><br><span class=line>226</span><br><span class=line>227</span><br><span class=line>228</span><br><span class=line>229</span><br><span class=line>230</span><br><span class=line>231</span><br><span class=line>232</span><br><span class=line>233</span><br><span class=line>234</span><br><span class=line>235</span><br><span class=line>236</span><br><span class=line>237</span><br><span class=line>238</span><br><span class=line>239</span><br><span class=line>240</span><br><span class=line>241</span><br><span class=line>242</span><br><span class=line>243</span><br><span class=line>244</span><br><span class=line>245</span><br><span class=line>246</span><br><span class=line>247</span><br><span class=line>248</span><br><span class=line>249</span><br><span class=line>250</span><br><span class=line>251</span><br><span class=line>252</span><br><span class=line>253</span><br><span class=line>254</span><br><span class=line>255</span><br><span class=line>256</span><br><span class=line>257</span><br><span class=line>258</span><br><span class=line>259</span><br><span class=line>260</span><br><span class=line>261</span><br><span class=line>262</span><br><span class=line>263</span><br><span class=line>264</span><br><span class=line>265</span><br><span class=line>266</span><br><span class=line>267</span><br><span class=line>268</span><br><span class=line>269</span><br><span class=line>270</span><br><span class=line>271</span><br><span class=line>272</span><br><span class=line>273</span><br><span class=line>274</span><br><span class=line>275</span><br><span class=line>276</span><br><span class=line>277</span><br><span class=line>278</span><br><span class=line>279</span><br><span class=line>280</span><br><span class=line>281</span><br><span class=line>282</span><br><span class=line>283</span><br><span class=line>284</span><br><span class=line>285</span><br><span class=line>286</span><br><span class=line>287</span><br><span class=line>288</span><br><span class=line>289</span><br><span class=line>290</span><br><span class=line>291</span><br><span class=line>292</span><br><span class=line>293</span><br><span class=line>294</span><br><span class=line>295</span><br><span class=line>296</span><br><span class=line>297</span><br><span class=line>298</span><br><span class=line>299</span><br><span class=line>300</span><br><span class=line>301</span><br><span class=line>302</span><br><span class=line>303</span><br><span class=line>304</span><br><span class=line>305</span><br><span class=line>306</span><br><span class=line>307</span><br><span class=line>308</span><br><span class=line>309</span><br><span class=line>310</span><br><span class=line>311</span><br><span class=line>312</span><br><span class=line>313</span><br><span class=line>314</span><br><span class=line>315</span><br><span class=line>316</span><br><span class=line>317</span><br><span class=line>318</span><br></pre><td class=code><pre><span class=line><span class=comment>#   #!/usr/bin/env python</span></span><br><span class=line><span class=comment>#   #-*- coding:utf-8 -*-</span></span><br><span class=line><span class=comment>#  Copyleft (C) 2024 proanimer, Inc. All Rights Reserved</span></span><br><span class=line><span class=comment>#   author:proanimer</span></span><br><span class=line><span class=comment>#   createTime:2024/2/18 下午2:22</span></span><br><span class=line><span class=comment>#   lastModifiedTime:2024/2/18 下午2:22</span></span><br><span class=line><span class=comment>#   file:pvt.py</span></span><br><span class=line><span class=comment>#   software: classicNets</span></span><br><span class=line><span class=comment>#</span></span><br><span class=line><span class=keyword>from</span> functools <span class=keyword>import</span> partial</span><br><span class=line></span><br><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line><span class=keyword>from</span> timm.models.layers <span class=keyword>import</span> DropPath, to_2tuple, trunc_normal_</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Mlp</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        in_features,</span></span></span><br><span class=line><span class=params><span class=function>        hidden_features=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>        out_features=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>        act_layer=nn.GELU,</span></span></span><br><span class=line><span class=params><span class=function>        drop=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        out_features = out_features <span class=keyword>or</span> in_features</span><br><span class=line>        hidden_features = hidden_features <span class=keyword>or</span> in_features</span><br><span class=line>        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class=line>        self.act = act_layer()</span><br><span class=line>        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class=line>        self.drop = nn.Dropout(drop)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.fc1(x)</span><br><span class=line>        x = self.act(x)</span><br><span class=line>        x = self.drop(x)</span><br><span class=line>        x = self.fc2(x)</span><br><span class=line>        x = self.drop(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Attention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        num_heads=<span class=number>8</span>,</span></span></span><br><span class=line><span class=params><span class=function>        qkv_bias=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>        qk_scale=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>        attn_drop=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        proj_drop=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        sr_ratio=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        <span class=keyword>assert</span> (</span><br><span class=line>            dim % num_heads == <span class=number>0</span></span><br><span class=line>        ), <span class=string>f"dim <span class=subst>{dim}</span> should be divided by num_heads <span class=subst>{num_heads}</span>."</span></span><br><span class=line></span><br><span class=line>        self.dim = dim</span><br><span class=line>        self.num_heads = num_heads</span><br><span class=line>        head_dim = dim // num_heads</span><br><span class=line>        self.scale = qk_scale <span class=keyword>or</span> head_dim**-<span class=number>0.5</span></span><br><span class=line></span><br><span class=line>        self.q = nn.Linear(dim, dim, bias=qkv_bias)</span><br><span class=line>        self.kv = nn.Linear(dim, dim * <span class=number>2</span>, bias=qkv_bias)</span><br><span class=line>        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class=line>        self.proj = nn.Linear(dim, dim)</span><br><span class=line>        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class=line></span><br><span class=line>        self.sr_ratio = sr_ratio</span><br><span class=line>        <span class=keyword>if</span> sr_ratio > <span class=number>1</span>:</span><br><span class=line>            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)</span><br><span class=line>            self.norm = nn.LayerNorm(dim)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, H, W</span>):</span></span><br><span class=line>        B, N, C = x.shape</span><br><span class=line>        q = (</span><br><span class=line>            self.q(x)</span><br><span class=line>            .reshape(B, N, self.num_heads, C // self.num_heads)</span><br><span class=line>            .permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>, <span class=number>3</span>)</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> self.sr_ratio > <span class=number>1</span>:</span><br><span class=line>            x_ = x.permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>).reshape(B, C, H, W)</span><br><span class=line>            x_ = self.sr(x_).reshape(B, C, -<span class=number>1</span>).permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)</span><br><span class=line>            x_ = self.norm(x_)</span><br><span class=line>            kv = (</span><br><span class=line>                self.kv(x_)</span><br><span class=line>                .reshape(B, -<span class=number>1</span>, <span class=number>2</span>, self.num_heads, C // self.num_heads)</span><br><span class=line>                .permute(<span class=number>2</span>, <span class=number>0</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>4</span>)</span><br><span class=line>            )</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            kv = (</span><br><span class=line>                self.kv(x)</span><br><span class=line>                .reshape(B, -<span class=number>1</span>, <span class=number>2</span>, self.num_heads, C // self.num_heads)</span><br><span class=line>                .permute(<span class=number>2</span>, <span class=number>0</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>4</span>)</span><br><span class=line>            )</span><br><span class=line>        k, v = kv[<span class=number>0</span>], kv[<span class=number>1</span>]</span><br><span class=line></span><br><span class=line>        attn = (q @ k.transpose(-<span class=number>2</span>, -<span class=number>1</span>)) * self.scale  <span class=comment># q (B,H,N,C)  K(B,H,C,N)</span></span><br><span class=line>        attn = attn.softmax(dim=-<span class=number>1</span>)</span><br><span class=line>        attn = self.attn_drop(attn)</span><br><span class=line></span><br><span class=line>        x = (</span><br><span class=line>            (attn @ v).transpose(<span class=number>1</span>, <span class=number>2</span>).reshape(B, N, C)</span><br><span class=line>        )  <span class=comment># (B,H,N,N) @ (B,H,N,C) -> (B,H,N,C)</span></span><br><span class=line>        x = self.proj(x)</span><br><span class=line>        x = self.proj_drop(x)</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Block</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        dim,</span></span></span><br><span class=line><span class=params><span class=function>        num_heads,</span></span></span><br><span class=line><span class=params><span class=function>        mlp_ratio=<span class=number>4.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        qkv_bias=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>        qk_scale=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>        drop=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        attn_drop=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        drop_path=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        act_layer=nn.GELU,</span></span></span><br><span class=line><span class=params><span class=function>        norm_layer=nn.LayerNorm,</span></span></span><br><span class=line><span class=params><span class=function>        sr_ratio=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.norm1 = norm_layer(dim)</span><br><span class=line>        self.attn = Attention(</span><br><span class=line>            dim,</span><br><span class=line>            num_heads=num_heads,</span><br><span class=line>            qkv_bias=qkv_bias,</span><br><span class=line>            qk_scale=qk_scale,</span><br><span class=line>            attn_drop=attn_drop,</span><br><span class=line>            proj_drop=drop,</span><br><span class=line>            sr_ratio=sr_ratio,</span><br><span class=line>        )</span><br><span class=line>        <span class=comment># <span class=doctag>NOTE:</span> drop path for stochastic depth, we shall see if this is better than dropout here</span></span><br><span class=line>        self.drop_path = DropPath(drop_path) <span class=keyword>if</span> drop_path > <span class=number>0.0</span> <span class=keyword>else</span> nn.Identity()</span><br><span class=line>        self.norm2 = norm_layer(dim)</span><br><span class=line>        mlp_hidden_dim = <span class=built_in>int</span>(dim * mlp_ratio)</span><br><span class=line>        self.mlp = Mlp(</span><br><span class=line>            in_features=dim,</span><br><span class=line>            hidden_features=mlp_hidden_dim,</span><br><span class=line>            act_layer=act_layer,</span><br><span class=line>            drop=drop,</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, H, W</span>):</span></span><br><span class=line>        x = x + self.drop_path(self.attn(self.norm1(x), H, W))</span><br><span class=line>        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>PatchEmbed</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>"""Image to Patch Embedding"""</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, img_size=<span class=number>224</span>, patch_size=<span class=number>16</span>, in_chans=<span class=number>3</span>, embed_dim=<span class=number>768</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        img_size = to_2tuple(img_size)</span><br><span class=line>        patch_size = to_2tuple(patch_size)</span><br><span class=line></span><br><span class=line>        self.img_size = img_size</span><br><span class=line>        self.patch_size = patch_size</span><br><span class=line>        <span class=keyword>assert</span> (</span><br><span class=line>            img_size[<span class=number>0</span>] % patch_size[<span class=number>0</span>] == <span class=number>0</span> <span class=keyword>and</span> img_size[<span class=number>1</span>] % patch_size[<span class=number>1</span>] == <span class=number>0</span></span><br><span class=line>        ), <span class=string>f"img_size <span class=subst>{img_size}</span> should be divided by patch_size <span class=subst>{patch_size}</span>."</span></span><br><span class=line>        self.H, self.W = img_size[<span class=number>0</span>] // patch_size[<span class=number>0</span>], img_size[<span class=number>1</span>] // patch_size[<span class=number>1</span>]</span><br><span class=line>        self.num_patches = self.H * self.W</span><br><span class=line>        self.proj = nn.Conv2d(</span><br><span class=line>            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size</span><br><span class=line>        )</span><br><span class=line>        self.norm = nn.LayerNorm(embed_dim)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        B, C, H, W = x.shape</span><br><span class=line></span><br><span class=line>        x = (</span><br><span class=line>            self.proj(x).flatten(<span class=number>2</span>).transpose(<span class=number>1</span>, <span class=number>2</span>)</span><br><span class=line>        )  <span class=comment># B,C,H,W->B,embed_dim,seq*seq->B,seq*seq,embed_dim</span></span><br><span class=line>        x = self.norm(x)</span><br><span class=line>        H, W = H // self.patch_size[<span class=number>0</span>], W // self.patch_size[<span class=number>1</span>]</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> x, (H, W)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>PyramidVisionTransformer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        img_size=<span class=number>224</span>,</span></span></span><br><span class=line><span class=params><span class=function>        patch_size=<span class=number>16</span>,</span></span></span><br><span class=line><span class=params><span class=function>        in_chans=<span class=number>3</span>,</span></span></span><br><span class=line><span class=params><span class=function>        num_classes=<span class=number>1000</span>,</span></span></span><br><span class=line><span class=params><span class=function>        embed_dims=[<span class=number>64</span>, <span class=number>128</span>, <span class=number>256</span>, <span class=number>512</span>],</span></span></span><br><span class=line><span class=params><span class=function>        num_heads=[<span class=number>1</span>, <span class=number>2</span>, <span class=number>4</span>, <span class=number>8</span>],</span></span></span><br><span class=line><span class=params><span class=function>        mlp_ratios=[<span class=number>4</span>, <span class=number>4</span>, <span class=number>4</span>, <span class=number>4</span>],</span></span></span><br><span class=line><span class=params><span class=function>        qkv_bias=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>        qk_scale=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>        drop_rate=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        attn_drop_rate=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        drop_path_rate=<span class=number>0.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        norm_layer=nn.LayerNorm,</span></span></span><br><span class=line><span class=params><span class=function>        depths=[<span class=number>3</span>, <span class=number>4</span>, <span class=number>6</span>, <span class=number>3</span>],</span></span></span><br><span class=line><span class=params><span class=function>        sr_ratios=[<span class=number>8</span>, <span class=number>4</span>, <span class=number>2</span>, <span class=number>1</span>],</span></span></span><br><span class=line><span class=params><span class=function>        F4=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>        num_stages=<span class=number>4</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.depths = depths</span><br><span class=line>        self.F4 = F4</span><br><span class=line>        self.num_stages = num_stages</span><br><span class=line></span><br><span class=line>        dpr = [</span><br><span class=line>            x.item() <span class=keyword>for</span> x <span class=keyword>in</span> torch.linspace(<span class=number>0</span>, drop_path_rate, <span class=built_in>sum</span>(depths))</span><br><span class=line>        ]  <span class=comment># stochastic depth decay rule</span></span><br><span class=line>        cur = <span class=number>0</span></span><br><span class=line></span><br><span class=line>        <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_stages):</span><br><span class=line>            patch_embed = PatchEmbed(</span><br><span class=line>                img_size=img_size <span class=keyword>if</span> i == <span class=number>0</span> <span class=keyword>else</span> img_size // (<span class=number>2</span> ** (i + <span class=number>1</span>)),</span><br><span class=line>                patch_size=patch_size <span class=keyword>if</span> i == <span class=number>0</span> <span class=keyword>else</span> <span class=number>2</span>,</span><br><span class=line>                in_chans=in_chans <span class=keyword>if</span> i == <span class=number>0</span> <span class=keyword>else</span> embed_dims[i - <span class=number>1</span>],</span><br><span class=line>                embed_dim=embed_dims[i],</span><br><span class=line>            )  <span class=comment># [B,seq=num_patches,dim=patch_size**2*embed_dim]</span></span><br><span class=line>            num_patches = (</span><br><span class=line>                patch_embed.num_patches</span><br><span class=line>                <span class=keyword>if</span> i != num_stages - <span class=number>1</span></span><br><span class=line>                <span class=keyword>else</span> patch_embed.num_patches + <span class=number>1</span></span><br><span class=line>            )</span><br><span class=line>            pos_embed = nn.Parameter(torch.zeros(<span class=number>1</span>, num_patches, embed_dims[i]))</span><br><span class=line>            pos_drop = nn.Dropout(p=drop_rate)</span><br><span class=line></span><br><span class=line>            block = nn.ModuleList(</span><br><span class=line>                [</span><br><span class=line>                    Block(</span><br><span class=line>                        dim=embed_dims[i],</span><br><span class=line>                        num_heads=num_heads[i],</span><br><span class=line>                        mlp_ratio=mlp_ratios[i],</span><br><span class=line>                        qkv_bias=qkv_bias,</span><br><span class=line>                        qk_scale=qk_scale,</span><br><span class=line>                        drop=drop_rate,</span><br><span class=line>                        attn_drop=attn_drop_rate,</span><br><span class=line>                        drop_path=dpr[cur + j],</span><br><span class=line>                        norm_layer=norm_layer,</span><br><span class=line>                        sr_ratio=sr_ratios[i],</span><br><span class=line>                    )</span><br><span class=line>                    <span class=keyword>for</span> j <span class=keyword>in</span> <span class=built_in>range</span>(depths[i])</span><br><span class=line>                ]</span><br><span class=line>            )</span><br><span class=line>            cur += depths[i]</span><br><span class=line></span><br><span class=line>            <span class=built_in>setattr</span>(self, <span class=string>f"patch_embed<span class=subst>{i + <span class=number>1</span>}</span>"</span>, patch_embed)</span><br><span class=line>            <span class=built_in>setattr</span>(self, <span class=string>f"pos_embed<span class=subst>{i + <span class=number>1</span>}</span>"</span>, pos_embed)</span><br><span class=line>            <span class=built_in>setattr</span>(self, <span class=string>f"pos_drop<span class=subst>{i + <span class=number>1</span>}</span>"</span>, pos_drop)</span><br><span class=line>            <span class=built_in>setattr</span>(self, <span class=string>f"block<span class=subst>{i + <span class=number>1</span>}</span>"</span>, block)</span><br><span class=line></span><br><span class=line>            trunc_normal_(pos_embed, std=<span class=number>0.02</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># init weights</span></span><br><span class=line>        self.apply(self._init_weights)</span><br><span class=line>        <span class=comment># self.init_weights(pretrained)</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_init_weights</span>(<span class=params>self, m</span>):</span></span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>isinstance</span>(m, nn.Linear):</span><br><span class=line>            trunc_normal_(m.weight, std=<span class=number>0.02</span>)</span><br><span class=line>            <span class=keyword>if</span> <span class=built_in>isinstance</span>(m, nn.Linear) <span class=keyword>and</span> m.bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>                nn.init.constant_(m.bias, <span class=number>0</span>)</span><br><span class=line>        <span class=keyword>elif</span> <span class=built_in>isinstance</span>(m, nn.LayerNorm):</span><br><span class=line>            nn.init.constant_(m.bias, <span class=number>0</span>)</span><br><span class=line>            nn.init.constant_(m.weight, <span class=number>1.0</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_get_pos_embed</span>(<span class=params>self, pos_embed, patch_embed, H, W</span>):</span></span><br><span class=line>        <span class=keyword>if</span> H * W == self.patch_embed1.num_patches:</span><br><span class=line>            <span class=keyword>return</span> pos_embed</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            <span class=keyword>return</span> (</span><br><span class=line>                F.interpolate(</span><br><span class=line>                    pos_embed.reshape(<span class=number>1</span>, patch_embed.H, patch_embed.W, -<span class=number>1</span>).permute(</span><br><span class=line>                        <span class=number>0</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>2</span></span><br><span class=line>                    ),</span><br><span class=line>                    size=(H, W),</span><br><span class=line>                    mode=<span class=string>"bilinear"</span>,</span><br><span class=line>                )</span><br><span class=line>                .reshape(<span class=number>1</span>, -<span class=number>1</span>, H * W)</span><br><span class=line>                .permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward_features</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        outs = []</span><br><span class=line>        B = x.shape[<span class=number>0</span>]</span><br><span class=line>        <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(self.num_stages):</span><br><span class=line>            patch_embed = <span class=built_in>getattr</span>(self, <span class=string>f"patch_embed<span class=subst>{i + <span class=number>1</span>}</span>"</span>)</span><br><span class=line>            pos_embed = <span class=built_in>getattr</span>(self, <span class=string>f"pos_embed<span class=subst>{i + <span class=number>1</span>}</span>"</span>)</span><br><span class=line>            pos_drop = <span class=built_in>getattr</span>(self, <span class=string>f"pos_drop<span class=subst>{i + <span class=number>1</span>}</span>"</span>)</span><br><span class=line>            block = <span class=built_in>getattr</span>(self, <span class=string>f"block<span class=subst>{i + <span class=number>1</span>}</span>"</span>)</span><br><span class=line>            x, (H, W) = patch_embed(x)</span><br><span class=line>            <span class=keyword>if</span> i == self.num_stages - <span class=number>1</span>:</span><br><span class=line>                pos_embed = self._get_pos_embed(pos_embed[:, <span class=number>1</span>:], patch_embed, H, W)</span><br><span class=line>            <span class=keyword>else</span>:</span><br><span class=line>                pos_embed = self._get_pos_embed(pos_embed, patch_embed, H, W)</span><br><span class=line></span><br><span class=line>            x = pos_drop(x + pos_embed)</span><br><span class=line>            <span class=keyword>for</span> blk <span class=keyword>in</span> block:</span><br><span class=line>                x = blk(x, H, W)</span><br><span class=line>            x = x.reshape(B, H, W, -<span class=number>1</span>).permute(<span class=number>0</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>2</span>).contiguous()</span><br><span class=line>            outs.append(x)</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> outs</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.forward_features(x)</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> self.F4:</span><br><span class=line>            x = x[<span class=number>3</span>:<span class=number>4</span>]</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><h3 id=PVT-v2><a title="PVT v2" class=headerlink href=#PVT-v2></a>PVT v2</h3><p><a href=https://arxiv.org/pdf/2106.13797.pdf rel=noopener target=_blank>2106.13797.pdf (arxiv.org)</a>对之前的pvt进行了改进,包括空间大小降低放的方法,patch embdedding改为了有重叠区域的patch embedding.FeedNetwork中加了depth-wise卷积.<p><img alt=image-20240221212026465 data-src=https://s2.loli.net/2024/02/21/buMGx8v6TfNOQht.png><p><img alt=image-20240221212055900 data-src=https://s2.loli.net/2024/02/21/OhtbAGq8NgvKnEX.png><h3 id=CPVT中的PEG><a class=headerlink href=#CPVT中的PEG title=CPVT中的PEG></a>CPVT中的PEG</h3><p><img alt=image-20240219150034479 data-src=https://s2.loli.net/2024/02/19/LHY9bVrE8w2DlZs.png><p>conditional position encoding<p><img alt=image-20240218103528794 data-src=https://s2.loli.net/2024/02/18/cfmP5yO41sN6h8o.png><p>出自论文<a href=https://arxiv.org/pdf/2102.10882.pdf rel=noopener target=_blank>2102.10882.pdf (arxiv.org)</a><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>PEG</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim=<span class=number>256</span>, k=<span class=number>3</span></span>):</span></span><br><span class=line>        self.proj = nn.Conv2d(dim, dim, k, <span class=number>1</span>, k//<span class=number>2</span>, groups=dim)</span><br><span class=line>        <span class=comment># Only for demo use, more complicated functions are effective too.</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, H, W</span>):</span></span><br><span class=line>        B, N, C = x.shape</span><br><span class=line>        cls_token, feat_token = x[:, <span class=number>0</span>], x[:, <span class=number>1</span>:] <span class=comment># cls token不参与PEG</span></span><br><span class=line>        cnn_feat = feat_token.transpose(<span class=number>1</span>, <span class=number>2</span>).view(B, C, H, W)</span><br><span class=line>        x = self.proj(cnn_feat) + cnn_feat <span class=comment># 产生PE加上自身</span></span><br><span class=line>        x = x.flatten(<span class=number>2</span>).transpose(<span class=number>1</span>, <span class=number>2</span>)</span><br><span class=line>        x = torch.cat((cls_token.unsqueeze(<span class=number>1</span>), x), dim=<span class=number>1</span>)</span><br><span class=line>    <span class=keyword>return</span> x</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>VisionTransformer</span>:</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>layers=<span class=number>12</span>, dim=<span class=number>192</span>, nhead=<span class=number>3</span>, img_size=<span class=number>224</span>, patch_size=<span class=number>16</span></span>):</span></span><br><span class=line>        self.pos_block = PEG(dim)</span><br><span class=line>        self.blocks = nn.ModuleList([TransformerEncoderLayer(dim</span><br><span class=line>, nhead, dim*<span class=number>4</span>) <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(layers)])</span><br><span class=line>        self.patch_embed = PatchEmbed(img_size, patch_size, dim</span><br><span class=line>*<span class=number>4</span>)</span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward_features</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        B, C, H, W = x.shape</span><br><span class=line>        x, patch_size = self.patch_embed(x)</span><br><span class=line>        _H, _W = H // patch_size, W // patch_size</span><br><span class=line>        x = torch.cat((self.cls_tokens, x), dim=<span class=number>1</span>)</span><br><span class=line>        <span class=keyword>for</span> i, blk <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.blocks):</span><br><span class=line>            x = blk(x)</span><br><span class=line>            <span class=keyword>if</span> i == <span class=number>0</span>: <span class=comment># 第一个encoder之后施加PEG</span></span><br><span class=line>                x = self.pos_block(x, _H, _W)</span><br><span class=line>        <span class=keyword>return</span> x[:, <span class=number>0</span>]</span><br></pre></table></figure><h3 id=LocalVit><a class=headerlink href=#LocalVit title=LocalVit></a>LocalVit</h3><p><img alt=image-20240218105718876 data-src=https://s2.loli.net/2024/02/18/ncu63LwS7bKhl8j.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Transformer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim, depth, heads, dim_head, patch_height, patch_width, scale = <span class=number>4</span>, depth_kernel = <span class=number>3</span>, dropout = <span class=number>0.</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.layers = nn.ModuleList([])</span><br><span class=line>        <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(depth):</span><br><span class=line>            self.layers.append(nn.ModuleList([</span><br><span class=line>                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),</span><br><span class=line>                Residual(PreNorm(dim, ConvFF(dim, scale, depth_kernel, patch_height, patch_width)))</span><br><span class=line>            ]))</span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line></span><br><span class=line>        <span class=keyword>for</span> attn, convff <span class=keyword>in</span> self.layers:</span><br><span class=line>            x = attn(x)</span><br><span class=line>            cls_tokens = x[:, <span class=number>0</span>]</span><br><span class=line>            x = convff(x[:, <span class=number>1</span>:])</span><br><span class=line>            x = torch.cat((cls_tokens.unsqueeze(<span class=number>1</span>), x), dim=<span class=number>1</span>) </span><br><span class=line>        <span class=keyword>return</span> xclass ConvFF(nn.Module):</span><br><span class=line>    </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim = <span class=number>192</span>, scale = <span class=number>4</span>, depth_kernel = <span class=number>3</span>, patch_height = <span class=number>14</span>, patch_width = <span class=number>14</span>, dropout=<span class=number>0.</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        </span><br><span class=line>        scale_dim = dim*scale</span><br><span class=line>        self.up_proj = nn.Sequential(</span><br><span class=line>                                    Rearrange(<span class=string>'b (h w) c -> b c h w'</span>, h=patch_height, w=patch_width),</span><br><span class=line>                                    nn.Conv2d(dim, scale_dim, kernel_size=<span class=number>1</span>),</span><br><span class=line>                                    nn.Hardswish()</span><br><span class=line>                                    )</span><br><span class=line>        </span><br><span class=line>        self.depth_conv = nn.Sequential(</span><br><span class=line>                        nn.Conv2d(scale_dim, scale_dim, kernel_size=depth_kernel, padding=<span class=number>1</span>, groups=scale_dim, bias=<span class=literal>True</span>),</span><br><span class=line>                        nn.Conv2d(scale_dim, scale_dim, kernel_size=<span class=number>1</span>, bias=<span class=literal>True</span>),</span><br><span class=line>                        nn.Hardswish()</span><br><span class=line>                        )</span><br><span class=line>        </span><br><span class=line>        self.down_proj = nn.Sequential(</span><br><span class=line>                                    nn.Conv2d(scale_dim, dim, kernel_size=<span class=number>1</span>),</span><br><span class=line>                                    nn.Dropout(dropout),</span><br><span class=line>                                    Rearrange(<span class=string>'b c h w ->b (h w) c'</span>)</span><br><span class=line>                                    )</span><br></pre></table></figure><p>在feed-forward中使用2d的卷积.<h2 id=transformer中的绝对和相对位置编码><a class=headerlink href=#transformer中的绝对和相对位置编码 title=transformer中的绝对和相对位置编码></a>transformer中的绝对和相对位置编码</h2><p>位置编码可以分为使用<code>nn.Embedding</code>或者<code>nn.Parameter</code>的可学习参数,也可以直接使用固定的值,比如三角函数编码.此外可以分为相对位置和绝对位置编码<h3 id=绝对位置编码><a class=headerlink href=#绝对位置编码 title=绝对位置编码></a>绝对位置编码</h3><p>transformer中使用了位置编码信息,被认为是绝对位置编码<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>PositionalEncoding</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>"Implement the PE function."</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, d_model, dropout, max_len=<span class=number>5000</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>(PositionalEncoding, self).__init__()</span><br><span class=line>        self.dropout = nn.Dropout(p=dropout)</span><br><span class=line></span><br><span class=line>        <span class=comment># Compute the positional encodings once in log space.</span></span><br><span class=line>        pe = torch.zeros(max_len, d_model)</span><br><span class=line>        position = torch.arange(<span class=number>0</span>, max_len).unsqueeze(<span class=number>1</span>)</span><br><span class=line>        div_term = torch.exp(torch.arange(<span class=number>0</span>, d_model, <span class=number>2</span>) *</span><br><span class=line>                             -(math.log(<span class=number>10000.0</span>) / d_model))</span><br><span class=line>        pe[:, <span class=number>0</span>::<span class=number>2</span>] = torch.sin(position * div_term)</span><br><span class=line>        pe[:, <span class=number>1</span>::<span class=number>2</span>] = torch.cos(position * div_term)</span><br><span class=line>        pe = pe.unsqueeze(<span class=number>0</span>)</span><br><span class=line>        self.register_buffer(<span class=string>'pe'</span>, pe)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = x + Variable(self.pe[:, :x.size(<span class=number>1</span>)],</span><br><span class=line>                         requires_grad=<span class=literal>False</span>)</span><br><span class=line>        <span class=keyword>return</span> self.dropout(x)</span><br></pre></table></figure><blockquote><p>我们可能希望使用相对位置编码而不是绝对位置编码，原因有很多。首先，使用绝对位置信息必然意味着模型可以处理的token数量有限制。假设一个语言模型最多只能编码1024个位置。这必然意味着任何长于1024个token的序列都不能被模型处理;相对位置编码可以推广到看不见长度的序列，因为理论上它编码的唯一信息是两个标记之间的相对成对距离。</blockquote><h3 id=相对位置编码的历史><a class=headerlink href=#相对位置编码的历史 title=相对位置编码的历史></a>相对位置编码的历史</h3><blockquote><p>相对位置嵌入( Relative Position Embedding，RPE )技术主要用于将与相对位置相关的信息纳入到注意力模块中。该技术基于这样的思想：块之间的空间关系比它们的绝对位置承载更多的权重。为了计算RPE值，使用了基于可学习参数的查找表。查找过程由图像patch间的相对距离决定。虽然RPE技术可以扩展到不同长度的序列，但它可能会增加训练和测试时间。</blockquote><p>在<code>attention is all you need</code>中的attention中,自我注意力可以表述为如下,并使用三角函数索引进行位置编码.</p><script type="math/tex; mode=display">
z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V) \\
\alpha_{ij}=\frac{\exp e_{ij}}{\sum_{k=1}^n\exp e_{ik}} \\
e_{ij}=\frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}</script><h3 id=1D数据><a class=headerlink href=#1D数据 title=1D数据></a>1D数据</h3><h4 id=Shaw><a class=headerlink href=#Shaw title=Shaw></a>Shaw</h4><p>相对位置编码在swin-transformer以及Self-Attention with Relative Position Representations中都有体现.较早的论文<a href=https://arxiv.org/pdf/1803.02155.pdf rel=noopener target=_blank>1803.02155.pdf (arxiv.org)</a></p><script type="math/tex; mode=display">
z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V+a_{ij}^V) \\
e_{ij}=\frac{x_iW^Q(x_jW^K+a_{ij}^K)^T}{\sqrt{d_z}} \\
\begin{aligned}
a_{ij}^{K}& =w_{\mathrm{clip}(j-i,k)}^{K}  \\
a_{ij}^{V}& =w_{\mathrm{clip}(j-i,k)}^{V}  \\
\operatorname{clip}(x,k)& =\max(-k,\min(k,x)) 
\end{aligned}</script><p>其中的w^k^和w^v^是需要训练的参数.</p><script type="math/tex; mode=display">
w^{K}=(w_{-k}^{K},\ldots,w_{k}^{K}) \\
w^{V}=(\dot{w_{-k}^{V}},\ldots,w_{k}^{V})</script><p>以下是<a href=https://arxiv.org/pdf/1803.02155.pdf rel=noopener target=_blank>1803.02155.pdf (arxiv.org)</a>中的相对位置注意力<p><img style="zoom: 67%;" alt=image-20240216225108501 data-src=https://s2.loli.net/2024/02/16/4esqYAdLkgNuybn.png><p><img alt=img data-src=https://pic4.zhimg.com/80/v2-f6d057978590bd14fd876856500b69df_720w.webp><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=comment># shaw's relative positional embedding</span></span><br><span class=line>seq = torch.arange(n, device=device)</span><br><span class=line>dist = rearrange(seq, <span class=string>"i -> i ()"</span>) - rearrange(seq, <span class=string>"j -> () j"</span>)</span><br><span class=line>dist = dist.clamp(-max_pos_emb, max_pos_emb) + max_pos_emb</span><br><span class=line>rel_pos_emb = self.rel_pos_emb(dist).to(q)</span><br><span class=line>pos_attn = einsum(<span class=string>"b h n d, n r d -> b h n r"</span>, q, rel_pos_emb) * self.scale</span><br><span class=line>dots = dots + pos_attn</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> exists(mask) <span class=keyword>or</span> exists(context_mask):</span><br><span class=line>    mask = default(mask, <span class=keyword>lambda</span>: torch.ones(*x.shape[:<span class=number>2</span>], device=device))</span><br><span class=line>    context_mask = (</span><br><span class=line>        default(context_mask, mask)</span><br><span class=line>        <span class=keyword>if</span> <span class=keyword>not</span> has_context</span><br><span class=line>        <span class=keyword>else</span> default(</span><br><span class=line>            context_mask, <span class=keyword>lambda</span>: torch.ones(*context.shape[:<span class=number>2</span>], device=device)</span><br><span class=line>        )</span><br><span class=line>    )</span><br><span class=line>    mask_value = -torch.finfo(dots.dtype).<span class=built_in>max</span></span><br><span class=line>    mask = rearrange(mask, <span class=string>"b i -> b () i ()"</span>) * rearrange(</span><br><span class=line>        context_mask, <span class=string>"b j -> b () () j"</span></span><br><span class=line>    )</span><br><span class=line>    dots.masked_fill_(~mask, mask_value)</span><br><span class=line></span><br><span class=line>attn = dots.softmax(dim=-<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>out = einsum(<span class=string>"b h i j, b h j d -> b h i d"</span>, attn, v)</span><br><span class=line>out = rearrange(out, <span class=string>"b h n d -> b n (h d)"</span>)</span><br><span class=line>out = self.to_out(out)</span><br></pre></table></figure><h4 id=transformer-xl><a class=headerlink href=#transformer-xl title=transformer-xl></a>transformer-xl</h4><p>众所周知,q=xW~Q~,k=xW~K~,加入相对位置编码后,展开一般注意力公式有<p><img alt=img data-src=https://pic4.zhimg.com/80/v2-5ee0eed4bc859e400591d7c83047bffb_720w.webp><p><img alt=img data-src=https://pic1.zhimg.com/80/v2-733f110568f1c83519ada84af1e32014_720w.webp><p>Transformer-XL的做法很简单，直接将 $p<em>j$ 替换为相对位置向量 $R</em>{i-j}$, 至于两个 $p_i$ , 则干脆替换为两个可训练的问量 $u,v$<p>之后的改进也是基于此,并且不再改动计算V了.<p>在transformer-xl(或者也是XLNET中使用的编码)中</p><script type="math/tex; mode=display">
e_{ij}=\frac{(\mathbf{x}_i\mathbf{W}^Q+\mathbf{u})(\mathbf{x}_j\mathbf{W}^K)^T+(\mathbf{x}_i\mathbf{W}^Q+\mathbf{v})(\mathbf{s}_{i-j}\mathbf{W}^R)^T}{\sqrt{d_z}},</script><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>PositionalEmbedding</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, demb</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(PositionalEmbedding, self).__init__()</span><br><span class=line>        self.demb = demb</span><br><span class=line>        inv_freq = <span class=number>1</span> / (<span class=number>10000</span> ** (torch.arange(<span class=number>0.0</span>, demb, <span class=number>2.0</span>) / demb))</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, pos_seq</span>):</span></span><br><span class=line>        sinusoid_inp = torch.outer(pos_seq, self.inv_freq) <span class=comment># 向量之间相乘</span></span><br><span class=line>        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-<span class=number>1</span>)</span><br><span class=line>        <span class=keyword>return</span> pos_emb[:,<span class=literal>None</span>,:]</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br></pre><td class=code><pre><span class=line>w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           <span class=comment># qlen x bsz x n_head x d_head</span></span><br><span class=line>w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           <span class=comment># qlen x bsz x n_head x d_head</span></span><br><span class=line>w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           <span class=comment># qlen x bsz x n_head x d_head</span></span><br><span class=line></span><br><span class=line>r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                <span class=comment># qlen x n_head x d_head</span></span><br><span class=line></span><br><span class=line><span class=comment>#### compute attention score</span></span><br><span class=line>rw_head_q = w_head_q + r_w_bias   <span class=comment>#加上biase                                       # qlen x bsz x n_head x d_head</span></span><br><span class=line>AC = torch.einsum(<span class=string>'ibnd,jbnd->ijbn'</span>, (rw_head_q, w_head_k))             <span class=comment># qlen x klen x bsz x n_head</span></span><br><span class=line></span><br><span class=line>rr_head_q = w_head_q + r_r_bias  <span class=comment>#加上biase  </span></span><br><span class=line>BD = torch.einsum(<span class=string>'ibnd,jnd->ijbn'</span>, (rr_head_q, r_head_k))              <span class=comment># qlen x klen x bsz x n_head</span></span><br><span class=line>BD = self._rel_shift(BD)</span><br><span class=line></span><br><span class=line><span class=comment># [qlen x klen x bsz x n_head]</span></span><br><span class=line>attn_score = AC + BD</span><br><span class=line>attn_score.mul_(self.scale)</span><br></pre></table></figure><p>其中u,v是两个可学习参数,W^R^是一个矩阵将s~i-j~投影到一个与位置相关的key向量.<h4 id=Music-transformer><a title="Music transformer" class=headerlink href=#Music-transformer></a>Music transformer</h4><p>后来Huang对shaw的相对位置编码进行改进<p><img style="zoom: 67%;" alt=image-20240216225143335 data-src=https://s2.loli.net/2024/02/16/PX9Ev1HjwKDB5xt.png><h4 id=Huang><a class=headerlink href=#Huang title=Huang></a>Huang</h4><p>此外还有<a href=https://arxiv.org/pdf/2009.13658.pdf rel=noopener target=_blank>2009.13658.pdf (arxiv.org)</a>提出的</p><script type="math/tex; mode=display">
e_{ij}=\frac{(\mathbf{x}_i\mathbf{W}^Q+\mathbf{p}_{ij})(\mathbf{x}_j\mathbf{W}^K+\mathbf{p}_{ij})^T-\mathbf{p}_{ij}\mathbf{p}_{ij}^T}{\sqrt{d_z}},</script><h4 id=T5><a class=headerlink href=#T5 title=T5></a>T5</h4><p><img alt=img data-src=https://pic1.zhimg.com/80/v2-16c2aa40bbf7a888a62d9dc1373d6c94_720w.webp style=zoom:50%;><h4 id=DeBERTa><a class=headerlink href=#DeBERTa title=DeBERTa></a>DeBERTa</h4><p><img alt=img data-src=https://pic3.zhimg.com/80/v2-b5436edfcde32b292cdf24c7f39d9c0e_720w.webp><p>总结下来就是在计算attention权重时或者在计算最后的注意力时加上一个与相对位置信息相关的值.这个值的计算通常类似如下<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=comment># shaw's relative positional embedding</span></span><br><span class=line>seq = torch.arange(n, device=device)</span><br><span class=line>dist = rearrange(seq, <span class=string>"i -> i ()"</span>) - rearrange(seq, <span class=string>"j -> () j"</span>)</span><br><span class=line>dist = dist.clamp(-max_pos_emb, max_pos_emb) + max_pos_emb</span><br><span class=line>rel_pos_emb = self.rel_pos_emb(dist).to(q)</span><br></pre></table></figure><p>以上大多用于1D数据比如音频和文字.<h3 id=2D数据><a class=headerlink href=#2D数据 title=2D数据></a>2D数据</h3><h4 id=Stand-Alone-Self-Attention-in-Vision-Models><a title="Stand-Alone Self-Attention in Vision Models" class=headerlink href=#Stand-Alone-Self-Attention-in-Vision-Models></a>Stand-Alone Self-Attention in Vision Models</h4><p><img alt=SASA data-src=https://user-images.githubusercontent.com/19909320/137499552-3bdf3189-7f57-4f95-a85e-8d5dd2ef6fd0.png style=zoom:50%;><p>公式如下</p><script type="math/tex; mode=display">
y_{ij}=\sum_{a,b\in\mathcal{N}_{k}(i,j)}\text{softmax}_{ab}\left(q_{ij}^{\top}k_{ab}+q_{ij}^{\top}r_{a-i,b-j}\right)v_{ab}</script><p>对相对距离进行维度分解，每个元素ab∈N~k(i,j)~得到两个距离：行偏移量a-i和列偏移量b-j .<p>行偏移和列偏移分别与一个嵌入r~a-i~和r~b-j~相关联，每个嵌入维度为1/2d~out~,行偏移嵌入和列偏移嵌入被串联起来形成r~a-i,b-j~。<p>或者表示如下</p><script type="math/tex; mode=display">
e_{ij}=\frac{(\mathbf{x}_i\mathbf{W}^Q)(\mathbf{x}_j\mathbf{W}^K+concat(\mathbf{p}_{\delta\bar{x}}^K,\mathbf{p}_{\delta\bar{y}}^K))^T}{\sqrt{d_z}},</script><p>其中p是可训练参数,长度是1/2d~z~<p><img alt=image-20240217180330619 data-src=https://s2.loli.net/2024/02/17/2S9UbyF7DYfujVw.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line></span><br><span class=line>use_cuda = torch.cuda.is_available()</span><br><span class=line>device = torch.device(<span class=string>"cuda"</span> <span class=keyword>if</span> use_cuda <span class=keyword>else</span> <span class=string>"cpu"</span>)</span><br><span class=line>    </span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>SASA_Layer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, kernel_size=<span class=number>7</span>, num_heads=<span class=number>8</span>, image_size=<span class=number>224</span>, inference=<span class=literal>False</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>(SASA_Layer, self).__init__()</span><br><span class=line>        self.kernel_size = <span class=built_in>min</span>(kernel_size, image_size) <span class=comment># receptive field shouldn't be larger than input H/W         </span></span><br><span class=line>        self.num_heads = num_heads</span><br><span class=line>        self.dk = self.dv = in_channels</span><br><span class=line>        self.dkh = self.dk // self.num_heads</span><br><span class=line>        self.dvh = self.dv // self.num_heads</span><br><span class=line></span><br><span class=line>        <span class=keyword>assert</span> self.dk % self.num_heads == <span class=number>0</span>, <span class=string>"dk should be divided by num_heads. (example: dk: 32, num_heads: 8)"</span></span><br><span class=line>        <span class=keyword>assert</span> self.dk % self.num_heads == <span class=number>0</span>, <span class=string>"dv should be divided by num_heads. (example: dv: 32, num_heads: 8)"</span>  </span><br><span class=line>        </span><br><span class=line>        self.k_conv = nn.Conv2d(self.dk, self.dk, kernel_size=<span class=number>1</span>).to(device)</span><br><span class=line>        self.q_conv = nn.Conv2d(self.dk, self.dk, kernel_size=<span class=number>1</span>).to(device)</span><br><span class=line>        self.v_conv = nn.Conv2d(self.dv, self.dv, kernel_size=<span class=number>1</span>).to(device)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Positional encodings</span></span><br><span class=line>        self.rel_encoding_h = nn.Parameter(torch.randn(self.dk // <span class=number>2</span>, self.kernel_size, <span class=number>1</span>), requires_grad=<span class=literal>True</span>)</span><br><span class=line>        self.rel_encoding_w = nn.Parameter(torch.randn(self.dk // <span class=number>2</span>, <span class=number>1</span>, self.kernel_size), requires_grad=<span class=literal>True</span>)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># later access attention weights</span></span><br><span class=line>        self.inference = inference</span><br><span class=line>        <span class=keyword>if</span> self.inference:</span><br><span class=line>            self.register_parameter(<span class=string>'weights'</span>, <span class=literal>None</span>)</span><br><span class=line>            </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        batch_size, _, height, width = x.size()</span><br><span class=line></span><br><span class=line>        <span class=comment># Compute k, q, v</span></span><br><span class=line>        padded_x = F.pad(x, [(self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>, (self.kernel_size-<span class=number>1</span>)-((self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>), (self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>, (self.kernel_size-<span class=number>1</span>)-((self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>)])</span><br><span class=line>        k = self.k_conv(padded_x)</span><br><span class=line>        q = self.q_conv(x)</span><br><span class=line>        v = self.v_conv(padded_x)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Unfold patches into [BS, num_heads*depth, horizontal_patches, vertical_patches, kernel_size, kernel_size]</span></span><br><span class=line>        k = k.unfold(<span class=number>2</span>, self.kernel_size, <span class=number>1</span>).unfold(<span class=number>3</span>, self.kernel_size, <span class=number>1</span>)</span><br><span class=line>        v = v.unfold(<span class=number>2</span>, self.kernel_size, <span class=number>1</span>).unfold(<span class=number>3</span>, self.kernel_size, <span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># Reshape into [BS, num_heads, horizontal_patches, vertical_patches, depth_per_head, kernel_size*kernel_size]</span></span><br><span class=line>        k = k.reshape(batch_size, self.num_heads, height, width, self.dkh, -<span class=number>1</span>)</span><br><span class=line>        v = v.reshape(batch_size, self.num_heads, height, width, self.dvh, -<span class=number>1</span>)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Reshape into [BS, num_heads, height, width, depth_per_head, 1]</span></span><br><span class=line>        q = q.reshape(batch_size, self.num_heads, height, width, self.dkh, <span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        qk = torch.matmul(q.transpose(<span class=number>4</span>, <span class=number>5</span>), k)    </span><br><span class=line>        qk = qk.reshape(batch_size, self.num_heads, height, width, self.kernel_size, self.kernel_size)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Add positional encoding</span></span><br><span class=line>        qr_h = torch.einsum(<span class=string>'bhxydz,cij->bhxyij'</span>, q, self.rel_encoding_h)</span><br><span class=line>        qr_w = torch.einsum(<span class=string>'bhxydz,cij->bhxyij'</span>, q, self.rel_encoding_w)</span><br><span class=line>        qk += qr_h</span><br><span class=line>        qk += qr_w</span><br><span class=line>        </span><br><span class=line>        qk = qk.reshape(batch_size, self.num_heads, height, width, <span class=number>1</span>, self.kernel_size*self.kernel_size)</span><br><span class=line>        weights = F.softmax(qk, dim=-<span class=number>1</span>)    </span><br><span class=line>        </span><br><span class=line>        <span class=keyword>if</span> self.inference:</span><br><span class=line>            self.weights = nn.Parameter(weights)</span><br><span class=line>        </span><br><span class=line>        attn_out = torch.matmul(weights, v.transpose(<span class=number>4</span>, <span class=number>5</span>)) </span><br><span class=line>        attn_out = attn_out.reshape(batch_size, -<span class=number>1</span>, height, width)</span><br><span class=line>        <span class=keyword>return</span> attn_out</span><br><span class=line>    </span><br></pre></table></figure><p>上面的代码可能有些问题,应该是将i,j的距离差嵌入到一个<code>embedding</code>中更合适<h4 id=Rethinking-and-Improving-Relative-Position-Encoding-for-Vision-Transformer><a title="Rethinking and Improving Relative Position Encoding for Vision Transformer" class=headerlink href=#Rethinking-and-Improving-Relative-Position-Encoding-for-Vision-Transformer></a>Rethinking and Improving Relative Position Encoding for Vision Transformer</h4><p>这是篇好文章,关于注意力中相对位置用于2d图像数据的方法.也是在上面SASA的一种改进.<p><img alt=image-20240217181329312 data-src=https://s2.loli.net/2024/02/17/CHqOLZKXNxhIdzj.png><p>以往的相对位置编码方法都依赖于输入嵌入。这就带来了一个问题，即编码能否独立于输入?<p>论文引入相对位置编码的偏向模式和语境模式来研究该问题。前者独立于输入嵌入，而后者考虑了与查询、键或值的交互。也就上图的两种模式.</p><script type="math/tex; mode=display">
e_{ij}=\frac{(\mathbf{x}_i\mathbf{W}^Q)(\mathbf{x}_j\mathbf{W}^K)^T\color{blue}{+}b_{ij}}{\sqrt{d_z}} \\
b_{ij}=\bold{r}_{ij} \space for \space  bias \space mode\\
b_{ij}=(x_{i}W^Q)r_{ij}\space for\space  context  \space mode\\</script><p>计算attention weight加上一个偏置,在bias模式下,这个偏置是一个可学习的参数,表示相对位置的权重.<p>在context模式下,有多种可行的方式.其中r是一个可训练的向量,也表示相对位置,但它会与Q或K交互.</p><script type="math/tex; mode=display">
b_{ij}=(\mathbf{x}_i\mathbf{W}^Q)(\mathbf{r}_{ij}^K)^T+(\mathbf{x}_j\mathbf{W}^K)(\mathbf{r}_{ij}^Q)^T</script><p>此外context模式也可以应用于value嵌入</p><script type="math/tex; mode=display">
\mathbf{z}_i=\sum_{j=1}^n\alpha_{ij}(\mathbf{x}_j\mathbf{W}^V\color{red}{+}\mathbf{r}_{ij}^V),</script><p>为了计算二维图像平面上的相对位置并定义相对权重r~ij~,提出了两种无向映射方法Euclidean和Quantization，以及两种有向映射方法Cross和Product。</p><script type="math/tex; mode=display">
\mathbf{r}_{ij}=\mathbf{p}_{I(i,j)},</script><script type="math/tex; mode=display">
I(i,j)=g(\sqrt{(\tilde{x}_i-\tilde{x}_j)^2+(\tilde{y}_i-\tilde{y}_j)^2}),</script><p>在上述欧几里得方法中，距离较近的两个具有不同相对距离的邻居可能被映射到同一个索引中，例如二维相对位置( 1、0 )和( 1 , 1)都被映射到索引1中。假设近邻应该是分离的。因此对欧氏距离进行量化，即将不同的实数映射成不同的整数。</p><script type="math/tex; mode=display">
I(i,j)=g(quant(\sqrt{(\tilde{x}_i-\tilde{x}_j)^2+(\tilde{y}_i-\tilde{y}_j)^2}).</script><p>运算quant ( · )将一组实数{ 0，1，1.41，2，2.24，.. }映射为一组整数{ 0，1，2，3，4，.. } .这种方法也是无向的.<p>像素的位置方向对图像也很重要，因此提出了有向映射方法。这种方法被称为Cross方法，它分别在水平和垂直方向上计算编码，然后进行汇总。方法如下</p><script type="math/tex; mode=display">
\begin{gathered}
\mathbf{r}_{ij}=\mathbf{p}_{I^{\tilde{x}}(i,j)}^{\tilde{x}}+\mathbf{p}_{I^{\tilde{y}}(i,j)}^{\tilde{y}}, \\
I^{\tilde{x}}(i,j)=g(\tilde{x_{i}}-\tilde{x_{j}}), \\
I^{\tilde{y}}(i,j)=g(\tilde{y}_i-\tilde{y}_j), 
\end{gathered}</script><p>如果某个方向上的距离是相同的，那么Cross方法将不同的相对位置编码到同一个嵌入中，此外带来了额外的计算开销。为了提高效率并包含更多的方向性信息，设计了Product方法，公式如下：<p><img alt=image-20240217223648427 data-src=https://s2.loli.net/2024/02/17/2rIStXgva96PhTZ.png><h3 id=其他><a class=headerlink href=#其他 title=其他></a>其他</h3><h3 id=Swin-transformer><a title="Swin transformer" class=headerlink href=#Swin-transformer></a>Swin transformer</h3><p><a href=https://arxiv.org/abs/2103.14030 rel=noopener target=_blank>[2103.14030] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (arxiv.org)</a><p><a href=https://arxiv.org/abs/2111.09883 rel=noopener target=_blank>[2111.09883] Swin Transformer V2: Scaling Up Capacity and Resolution (arxiv.org)</a><p><img alt=image-20240218140849412 data-src=https://s2.loli.net/2024/02/18/NMFCXv8EKabh6cp.png></p><script type="math/tex; mode=display">
\begin{aligned}\Omega(\mathbf{MSA})&=4hwC^2+2(hw)^2C,\\\Omega(\mathbf{W-MSA})&=4hwC^2+2M^2hwC,\end{aligned}</script><p><img alt=image-20240218141119075 data-src=https://s2.loli.net/2024/02/18/WBD3do8KnrHqlLU.png><blockquote><p>将Transformer从语言转换到视觉的挑战来自于两个领域之间的差异，例如视觉实体的尺度变化较大，图像中的像素相对于文本中的文字分辨率较高。<p>为了解决这些差异，提出了一个分层Transformer，其表示由Shifted窗口计算。移位窗口方案通过将自注意力计算限制在不重叠的局部窗口，同时允许跨窗口连接，从而带来更高的效率。这种分层架构具有在各种尺度下建模的灵活性，并且具有与图像大小相关的线性计算复杂度。</blockquote><p><img alt=image-20240221231106703 data-src=https://s2.loli.net/2024/02/21/If19Km8TFPneM6x.png><h3 id=Swin-transformerV2><a class=headerlink href=#Swin-transformerV2 title=Swin-transformerV2></a>Swin-transformerV2</h3><p><a href=https://arxiv.org/abs/2111.09883 rel=noopener target=_blank>[2111.09883] Swin Transformer V2: Scaling Up Capacity and Resolution (arxiv.org)</a><p><img alt=image-20240221231423057 data-src=https://s2.loli.net/2024/02/21/6izranSXgP7CobN.png><h3 id=Twins><a class=headerlink href=#Twins title=Twins></a>Twins</h3><p><a href=https://arxiv.org/abs/2104.13840 rel=noopener target=_blank>[2104.13840] Twins: Revisiting the Design of Spatial Attention in Vision Transformers (arxiv.org)</a><p><img alt=img data-src=https://github.com/lucidrains/vit-pytorch/raw/main/images/twins_svt.png style=zoom:67%;><p><img alt=image-20240218141741213 data-src=https://s2.loli.net/2024/02/18/1Zf5pWymzPTaHoB.png><blockquote><p>在这项工作中，重新审视了空间注意力的设计，并证明了一个精心设计但简单的空间注意力机制与最先进的方案相比具有良好的性能。因此，我们提出了两种视觉转换器结构，即Twins - PCPVT和TwinsSVT。我们提出的架构高效且易于实现，只涉及在现代深度学习框架中高度优化的矩阵乘法。更重要的是，所提出的架构在包括图像级cla在内的广泛的视觉任务上取得了优异的性能</blockquote><p>此外随着时间发展,目前已经有了空间注意力,通道注意力等等可以用于2D数据的注意力模型.但是基本思想是类似的.<h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://zhuanlan.zhihu.com/p/364828960 rel=noopener target=_blank>Relative position embedding - 知乎 (zhihu.com)</a><li><a href=https://arxiv.org/abs/1803.02155 rel=noopener target=_blank>[1803.02155] Self-Attention with Relative Position Representations (arxiv.org)</a><li><a href=https://placebokkk.github.io/asr/2021/01/14/asr-rpe.html rel=noopener target=_blank>Relative Positional Embedding | Chao Yang (placebokkk.github.io)</a><li><a href=https://aclanthology.org/2020.findings-emnlp.298.pdf rel=noopener target=_blank>Improve Transformer Models with Better Relative Position Embeddings (aclanthology.org)</a><li><a href=https://zhuanlan.zhihu.com/p/352898810 rel=noopener target=_blank>让研究人员绞尽脑汁的Transformer位置编码 - 知乎 (zhihu.com)</a><li><a href=https://zhuanlan.zhihu.com/p/669523714 rel=noopener target=_blank>《A survey of the Vision Transformers and its CNN-Transformer based Variants》第一期 - 知乎 (zhihu.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="transformer and attention(三)" href=https://www.sekyoro.top/2024/02/16/transformer-and-attention-%E4%B8%89/>https://www.sekyoro.top/2024/02/16/transformer-and-attention-三/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/transformers/ rel=tag><i class="fa fa-tag"></i> transformers</a><a href=/tags/attention/ rel=tag><i class="fa fa-tag"></i> attention</a></div><div class=post-nav><div class=post-nav-item><a title="TypeScript on the way:学习TypeScript" href=/2024/02/11/TypeScript-on-the-way-%E5%AD%A6%E4%B9%A0TypeScript/ rel=prev> <i class="fa fa-chevron-left"></i> TypeScript on the way:学习TypeScript </a></div><div class=post-nav-item><a href=/2024/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9C%89%E7%94%A8%E7%9A%84%E5%BA%93%E4%BB%A5%E5%8F%8A%E4%BB%8B%E7%BB%8D/ rel=next title=深度学习有用的库以及介绍> 深度学习有用的库以及介绍 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#%E7%BA%BF%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B><span class=nav-number>1.</span> <span class=nav-text>线性注意力</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84transformer%E4%B8%8Eattention><span class=nav-number>2.</span> <span class=nav-text>图像中的transformer与attention</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Vision-Transformer><span class=nav-number>3.</span> <span class=nav-text>Vision Transformer</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%B7%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B><span class=nav-number>4.</span> <span class=nav-text>卷积注意力</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#CVT><span class=nav-number>4.1.</span> <span class=nav-text>CVT</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#PVT><span class=nav-number>4.2.</span> <span class=nav-text>PVT</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#PVT-v2><span class=nav-number>4.3.</span> <span class=nav-text>PVT v2</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#CPVT%E4%B8%AD%E7%9A%84PEG><span class=nav-number>4.4.</span> <span class=nav-text>CPVT中的PEG</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#LocalVit><span class=nav-number>4.5.</span> <span class=nav-text>LocalVit</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#transformer%E4%B8%AD%E7%9A%84%E7%BB%9D%E5%AF%B9%E5%92%8C%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81><span class=nav-number>5.</span> <span class=nav-text>transformer中的绝对和相对位置编码</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81><span class=nav-number>5.1.</span> <span class=nav-text>绝对位置编码</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E5%8E%86%E5%8F%B2><span class=nav-number>5.2.</span> <span class=nav-text>相对位置编码的历史</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#1D%E6%95%B0%E6%8D%AE><span class=nav-number>5.3.</span> <span class=nav-text>1D数据</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Shaw><span class=nav-number>5.3.1.</span> <span class=nav-text>Shaw</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#transformer-xl><span class=nav-number>5.3.2.</span> <span class=nav-text>transformer-xl</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Music-transformer><span class=nav-number>5.3.3.</span> <span class=nav-text>Music transformer</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Huang><span class=nav-number>5.3.4.</span> <span class=nav-text>Huang</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#T5><span class=nav-number>5.3.5.</span> <span class=nav-text>T5</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#DeBERTa><span class=nav-number>5.3.6.</span> <span class=nav-text>DeBERTa</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#2D%E6%95%B0%E6%8D%AE><span class=nav-number>5.4.</span> <span class=nav-text>2D数据</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Stand-Alone-Self-Attention-in-Vision-Models><span class=nav-number>5.4.1.</span> <span class=nav-text>Stand-Alone Self-Attention in Vision Models</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Rethinking-and-Improving-Relative-Position-Encoding-for-Vision-Transformer><span class=nav-number>5.4.2.</span> <span class=nav-text>Rethinking and Improving Relative Position Encoding for Vision Transformer</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%85%B6%E4%BB%96><span class=nav-number>5.5.</span> <span class=nav-text>其他</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Swin-transformer><span class=nav-number>5.6.</span> <span class=nav-text>Swin transformer</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Swin-transformerV2><span class=nav-number>5.7.</span> <span class=nav-text>Swin-transformerV2</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Twins><span class=nav-number>5.8.</span> <span class=nav-text>Twins</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>6.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>236</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/attention/ rel=tag>attention</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/transformers/ rel=tag>transformers</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.6m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>39:01</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>