<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=最近大语言模型及其相关应用实在是太火了,可以在一些公司或者个人博客查看最前沿进展,也可以通过代码项目、课程学习,还可以看看经典论文.这里就看看一些LLM的论文学习学习. name=description><meta content=article property=og:type><meta content=LLM论文阅读 property=og:title><meta content=https://www.sekyoro.top/2024/05/01/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=最近大语言模型及其相关应用实在是太火了,可以在一些公司或者个人博客查看最前沿进展,也可以通过代码项目、课程学习,还可以看看经典论文.这里就看看一些LLM的论文学习学习. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/05/06/F5zWu6VldQqDBjs.png property=og:image><meta content=https://s2.loli.net/2024/05/06/hFNoCHDY23OmxiR.png property=og:image><meta content=https://s2.loli.net/2024/05/06/FcjiyX4oesVva2B.png property=og:image><meta content=https://s2.loli.net/2024/05/14/JQUyVlr7xCc4I2m.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505224532005.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505223449790.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505231432035.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505231734946.png property=og:image><meta content=https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT property=og:image><meta content=https://s2.loli.net/2024/05/11/XQ7vgiNjn9oLkAc.png property=og:image><meta content=https://s2.loli.net/2024/05/11/FRJflki8SWwLBN4.png property=og:image><meta content=https://s2.loli.net/2024/05/11/Ui7bAhFEXkIqLQs.png property=og:image><meta content=https://s2.loli.net/2024/05/14/tz9cBO5UowfKvqg.png property=og:image><meta content=https://s2.loli.net/2024/05/15/Q3Ax1eLJoGz42ZS.png property=og:image><meta content=https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg property=og:image><meta content=https://github.com/microsoft/generative-ai-for-beginners/raw/main/15-rag-and-vector-databases/images/encoder-decode.png?WT.mc_id=academic-105485-koreyst property=og:image><meta content=2024-05-01T07:39:46.000Z property=article:published_time><meta content=2024-05-15T08:29:31.240Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="个人博客 技术学习 计算机 互联网 人工智能" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/05/06/F5zWu6VldQqDBjs.png name=twitter:image><link href=https://www.sekyoro.top/2024/05/01/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>LLM论文阅读 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/05/01/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>LLM论文阅读</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-05-01 15:39:46" datetime=2024-05-01T15:39:46+08:00>2024-05-01</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-05-15 16:29:31" datetime=2024-05-15T16:29:31+08:00 itemprop=dateModified>2024-05-15</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>2.7k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>2 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>最近大语言模型及其相关应用实在是太火了,可以在一些公司或者个人博客查看最前沿进展,也可以通过代码项目、课程学习,还可以看看经典论文.这里就看看一些LLM的论文学习学习.<br><span id=more></span><p>下面论文都可以在Arxiv上找到.<h2 id=A-Survey-of-Large-Language-Models><a title="A Survey of Large Language Models" class=headerlink href=#A-Survey-of-Large-Language-Models></a>A Survey of Large Language Models</h2><p>这篇文章有80多页,引用有40页.<h4 id=Scaling-laws><a title="Scaling laws" class=headerlink href=#Scaling-laws></a>Scaling laws</h4><script type="math/tex; mode=display">
\begin{aligned}&L(N)&&=\quad\left(\frac{N_c}N\right)^{\alpha_N},\quad\alpha_N\sim0.076,N_c\sim8.8\times10^{13}\\&L(D)&&=\quad\left(\frac{D_c}D\right)^{\alpha_D},\quad\alpha_D\sim0.095,D_c\sim5.4\times10^{13}\\&L(C)&&=\quad\left(\frac{C_c}C\right)^{\alpha_C},\quad\alpha_C\sim0.050,C_c\sim3.1\times10^8\end{aligned}</script><p><strong>KM scaling law</strong><p>Scaling laws for neural language models首次提出了神经语言模型的模型性能与<strong>模型大小</strong>（N）、<strong>数据集大小</strong>（D）和<strong>训练计算量</strong>（C）三大因素的幂律关系模型。<p>$N<em>{c}$、$D</em>{c}$ 和 Cc 分别以非嵌入参数数、训练标记数和 FP 日数来衡量。<p>L(-) 表示 nats 中的交叉熵损失，OpenAI 的后续研究 表明，语言建模损失可分解为两部分，即不可还原损失（真实数据分布的熵）和可还原损失（真实分布和模型分布之间 KL 分歧的估计值）。<p>在一些假设条件下(如一个因素的分析不应受到其他两个因素的瓶颈限制),通过拟合不同数据量(2200 万到 2300 亿个 token)、模型大小(768 万到 1500 亿个非嵌入参数)和训练计算的模型性能,得出了这三个定律.他们的研究表明,模型性能与三个因素有很强的依赖关系.<p><strong>Chinchilla scaling law</strong><p>霍夫曼等人提出了另一种缩放定律形式，用于指导 LLM 的计算优化训练.其中E = 1.69, A = 406.4, B = 410.7, α = 0.34 and β = 0.28</p><script type="math/tex; mode=display">
L(N,D)=E+\frac{A}{N^\alpha}+\frac{B}{D^\beta}</script><p>通过优化 C ≈ 6N D 约束条件下的损失 L(N，D),他们证明了计算预算对模型大小和数据大小的最优分配如下</p><script type="math/tex; mode=display">
N_{opt}(C)=G{\left(\frac C6\right)}^a,\quad D_{opt}(C)=G^{-1}{\left(\frac C6\right)}^b</script><p>a = α/(α+β) , b = β/(α+β) ,G 是一个缩放系数,可由 A、B、α 和 β 计算得出。<h4 id=涌现能力><a class=headerlink href=#涌现能力 title=涌现能力></a>涌现能力</h4><p>在Emergent abilities of large language models中LLMs 的涌现能力被正式定义为 “在小型模型中不存在而在大型模型中出现的能力”，这是 LLMs 区别于以往 PLMs 的最显著特征之一。它进一步引入了出现突发性能力时的一个显著特征：<strong>当规模达到一定程度时，性能会明显高于随机水平。通过类比，这种突现模式与物理学中的相变现象有着密切联系</strong>。原则上，新兴能力可以根据某些复杂任务来定义，而我们更关注的是可用于解决各种任务的一般能力。<p>简要介绍 LLMs 的三种典型涌现能力以及具备这种能力的代表性模型: 1)In-context learning 2) Instruction following 3)step-by-step reasoning<p>GPT-3正式引入了语境中学习（ICL）能力：假设语言模型已经获得了自然语言指令和/或多个任务演示，那么它就可以<strong>通过完成输入文本的词序来生成测试实例的预期输出，而无需额外的训练或梯度更新</strong>。<p>通过对自然语言描述格式的多任务数据集进行微调（称为指令微调）,LLM 在同样以指令形式描述的未见任务上表现出色。<strong>通过指令调整，LLMs 可以在不使用明确示例的情况下，根据任务指令完成新任务，从而提高泛化能力</strong>.<p>对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务，例如数学文字问题.相比之下,使用思维链（CoT）提示策略,LLM 可以利用提示机制来解决此类任务,这种机制涉及到推导最终答案的中间推理步骤.<p><img alt=image-20240506105955822 data-src=https://s2.loli.net/2024/05/06/F5zWu6VldQqDBjs.png><p>LLM 经过漫长的发展才达到了目前的状态：通用的、有能力的学习者。在发展过程中，人们提出了许多重要技术，这些技术在很大程度上提高了 LLM 的能力,包括scaling,training,ability eliciting,Alignment tuning等等.<p><img alt=image-20240506110530834 data-src=https://s2.loli.net/2024/05/06/hFNoCHDY23OmxiR.png><p><img alt=image-20240506110409299 data-src=https://s2.loli.net/2024/05/06/FcjiyX4oesVva2B.png><h3 id=PEFT方法><a class=headerlink href=#PEFT方法 title=PEFT方法></a>PEFT方法</h3><p><img alt=image-20240514232554910 data-src=https://s2.loli.net/2024/05/14/JQUyVlr7xCc4I2m.png><h4 id=Prefix-Tuning><a title="Prefix Tuning" class=headerlink href=#Prefix-Tuning></a>Prefix Tuning</h4><p>Prefix Tuning在语言模型的每个transformer层中预置一系列前缀,这些前缀是一组可训练的连续向量.这些前缀向量是针对特定任务的,可视为虚拟标记嵌入。为了优化前缀向量，有人提出了一种重参数化技巧 ,即学习一个 MLP 函数，将一个较小的矩阵映射到前缀的参数矩阵，而不是直接优化前缀。<h5 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h5><p>在本文中,我们提出了前缀调整技术,它是自然语言生成任务中微调技术的轻量级替代方案,可<strong>保持语言模型参数不变,但会优化一个小的连续任务特定向量</strong>(称为前缀).<p><strong>Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”</strong>.<p>相当于让模型参数不变,优化连续的任务相关数据,让模型关注增加的前缀.<p>我们发现,只需学习 0.1% 的参数,前缀调整就能在全数据环境下获得与之相当的性能,在低数据环境下优于微调,并能更好地推断出训练期间未见过主题的示例.<h5 id=方法><a class=headerlink href=#方法 title=方法></a>方法</h5><p>我们认为适当的语境<strong>可以在不改变 LM 参数的情况下引导 LM</strong>。例如，如果我们希望 LM 生成一个单词（如 Obama），我们可以将其常见搭配作为上下文（如 Barack）的前置词，这样 LM 就会为所需单词分配更高的概率。<p>我们可以将指令优化为连续的单词嵌入，而不是对离散的标记进行优化，其效果将向上传播到所有转换器激活层，并向右传播到后续标记。<p>前缀调整为自回归 LM 预置前缀,得到 z = [PREFIX;x;y],或为编码器和编码器预置前缀,得到 z = [PREFIX;x;PREFIX′;y]。<p><img alt=image-20240505224532005 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505224532005.png></p><script type="math/tex; mode=display">
h_i=\begin{cases}P_\theta[i,:],&\text{if }i\in\mathsf{P_{idx}},\\\mathsf{LM}_\phi(z_i,h_{<i}),&\text{otherwise.}\end{cases}</script><p>前缀调整初始化一个维度为 |$P<em>{idx}$| × dim($h</em>{i}$) 的可训练矩阵 $P<em>{θ}$（参数为 θ）,用于存储前缀参数.$P</em>{idx}$ 表示前缀索引序列,我们用 |$P_{idx}$| 表示前缀长度.语言模型参数 φ 是固定的,前缀参数 θ 是唯一可训练的参数.<p>$h<em>{i}$（对于所有 i）是可训练 $P</em>{θ}$ 的函数。<p>Prefix Tuning<code>是</code>PEFT<code>方法之一，</code>Prefix Tuning<code>之前的工作主要是人工设计模板或者自动化搜索模板，也是</code>prompt<code>范式的第一阶段，就是在输入上加上</code>prompt<code>文本，再对输出进行映射。这种离散模板对模型的鲁棒性很差。所以后续的研究都将离散的方式转成连续。</code>Prefix Tuning在<strong>模型输入前添加一个连续的且任务特定的向量序列称之为prefix</strong><code>，固定</code>PLM(预训练模型)<code>的所有参数，只更新优化特定任务的</code>prefix<h4 id=P-tuning-V1-amp-amp-V2><a title="P-tuning V1&&V2" class=headerlink href=#P-tuning-V1-amp-amp-V2></a>P-tuning V1&&V2</h4><p>提示调整只对使用冻结语言模型的连续提示进行调整,从而大大减少了训练时每个任务的存储和内存使用量.<p><img alt=image-20240505223449790 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505223449790.png><p>在情感分析中,我们可以将样本（如 “了不起的电影！”）与提示语 “这部电影是[MASK]”连接起来，然后要求预先训练好的语言模型预测屏蔽标记为 “好 “和 “坏 “的概率,从而决定样本的标签.<p>提示法完全不需要训练,只需存储一份模型参数。<p>Prompt tuning2 是一种只对连续提示进行调整的想法.在原始输入词嵌入序列中添加可训练的连续嵌入(也称为连续提示)<p><code>P-Tuning v2</code>（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>）,该方法在<strong>每一层</strong>都加入了<code>Prompts tokens</code>作为输入，而不是仅仅加在输入层，这带来两个方面的好处：<ul><li>更多可学习的参数（从<code>P-Tuning</code>和<code>Prompt Tuning</code>的0.01%增加到0.1%-3%），同时也足够参数高效。<li>加入到更深层结构中的<code>Promp</code>能给模型预测带来更直接的影响</ul><p>大模型的<code>Prompt</code>构造方式严重影响下游任务的效果。比如：<code>GPT-3</code>采用人工构造的模版来做上下文学习<code>(in-context learning)</code>，但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化<h4 id=LoRA><a class=headerlink href=#LoRA title=LoRA></a>LoRA</h4><p>自然语言处理的一个重要范式是在一般领域数据上进行大规模预训练，然后适应特定任务或领域。随着我们预训练的模型越来越大，重新训练所有模型参数的全面微调就变得不那么可行了。以 GPT-3 175B 为例，部署微调模型的独立实例（每个实例有 175B 个参数）的成本过高。我们提出了（Low-Rank Adaptation，简称 LoRA）技术，它<strong>可以冻结预训练模型权重，并将可训练的等级分解矩阵注入 Transformer 架构的每一层(injects trainable rank decomposition matrices into each layer of the Transformer architecture)，从而大大减少下游任务的可训练参数数量</strong>.<p>LoRA 允许我们通过优化密集层在适应过程中的变化的秩分解矩阵来间接训练神经网络中的某些密集层,同时保持预先训练的权重不变.<p><img alt=image-20240505231432035 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505231432035.png style=zoom:67%;><p>神经网络包含许多执行矩阵乘法的密集层.这些层中的<strong>权重矩阵通常具有全秩</strong>。Aghajanyan 等人（2020）的研究表明,在适应特定任务时,<strong>预训练的语言模型具有较低的 “本征维度”，即使随机投影到较小的子空间，仍能高效学习</strong>.受此启发，我们假设权重更新在适应过程中也具有较低的 “本征等级”。<p>对于一个预先训练好的权重矩阵 $W<em>{0}∈R^{d×k}$, d是满秩,r是低秩.我们用一个低秩分解 $W</em>{0} + ∆W = W<em>{0} + BA$ 来表示后者,其中 B∈$R</em>{d×r}$, A∈$R_{r×k}$, 秩为 r<< min(d,k).<p>在训练过程中,$W_{0}$被冻结,不会接收梯度更新,而 A 和 B 则包含可训练参数.<p>注意,$W_{0}$ 和 ∆W = BA 都与相同的输入相乘.它们各自的输出向量按坐标相加.<p>对 A 使用随机高斯初始化,对 B 使用零初始化,因此训练开始时 ∆W = BA 为零.然后，我们用 α/r 对 $∆Wx$​ 进行缩放,其中 α 是 r 中的一个常数.在使用Adam进行优化时,如果我们适当缩放初始化,调整 α 与调整学习率大致相同.因此,我们只需将 α 设为我们尝试的第一个 r,而无需调整.</p><script type="math/tex; mode=display">
h=W_0x+\Delta Wx=W_0x+BAx</script><p>1、选择目标层<p>2、初始化映射矩阵和逆映射矩阵<p>为目标层创建两个较小的矩阵<code>A</code>和<code>B</code>，然后进行变换<p><code>A</code>是映射矩阵(一般用随机高斯分布初始化，维度上是降维）<p><code>B</code>是逆映射矩阵(用0矩阵初始化)，维度上是升维<p>之后做参数变换：将目标层的原始参数矩阵W通过映射矩阵<code>A</code>和逆映射矩阵<code>B</code>进行变换，计算公式为：<code>W' = W + A * B</code>，这里<code>W'</code>是变换后的参数矩阵<p>3、微调模型<p>使用新的参数矩阵<code>W'</code>替换目标层的原始参数矩阵<code>W</code>，然后在特定任务的训练数据上对模型进行微调<p>4、梯度更新<h3 id=Adapter><a class=headerlink href=#Adapter title=Adapter></a>Adapter</h3><p>适配器调整将小型神经网络模块（称为适配器）纳入transformer模型 .为了实现适配器模块,中提出了一种瓶颈架构,它首先将原始特征向量压缩到较小的维度（然后进行非线性变换）,然后将其恢复到原始维度.<p><img alt=image-20240505231734946 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505231734946.png style=zoom:67%;><p>在 NLP 中，微调大型预训练模型是一种有效的传输机制。然而，在有许多下游任务的情况下，微调的参数效率很低：每个任务都需要一个全新的模型。作为替代方案，我们建议<strong>使用适配器模块进行转移。适配器模块产生了一个紧凑且可扩展的模型；它们只为每个任务添加少量可训练参数，并且可以添加新任务，而无需重新检查之前的任务</strong>。原始网络的参数保持不变，从而实现了高度的参数共享。<h3 id=Prompt-Tuning><a title="Prompt Tuning" class=headerlink href=#Prompt-Tuning></a>Prompt Tuning</h3><p>Prompt Tuning主要侧重于在输入层加入可训练的提示向量.<p>在离散提示的情况下（Schick 和 Schütze, 2020）,提示标记 {“它”、”是”、”[MASK]”} ⊂ V 可用来对电影评论进行分类。<p>Lester 等人引入了可训练连续提示,作为自然语言提示的替代,用于冻结预训练语言模型的参数的 NLU。给定可训练连续嵌入[h0, …, hi],输入嵌入序列被写成[e(x), h0, …, hi, e(“[MASK]”)], 事实证明,在简单的分类任务中,即时调整的效果与对百亿参数模型进行微调的效果相当.<p><img alt=image-20240107102116261 data-src=https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT>%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107102116261.png)<p>在这项工作中，我们探索了 “提示调整”，这是一种简单而有效的机制，用于学习 “软提示”，使冻结语言模型能够执行特定的下游任务。与 GPT-3 使用的离散文本提示不同，软提示是通过反向传播学习的，可以进行调整，以纳入来自任意数量标注示例的信号<h3 id=Quantization方法><a class=headerlink href=#Quantization方法 title=Quantization方法></a>Quantization方法</h3><p>在模型压缩领域,量化已成为一种广受欢迎的技术，可减轻深度学习模型的存储和计算开销.<strong>传统表示法采用浮点数，而量化则将其转换为整数或其他离散形式</strong>.这种转换大大降低了存储要求和计算复杂度。虽然会有一些固有的精度损失,但仔细的量化技术可以在实现大量模型压缩的同时，将精度降低到最低程度.目前量化方法主要分为PTQ和QAT,前者不需要训练,后者需要数据重新训练,LLMs包含大量的参数,PTQ方法的计算成本远低于QAT方法,因而更受青睐。<p>这方面可以看看<a href=https://www.deeplearning.ai/short-courses/ rel=noopener target=_blank>Short Courses | Learn Generative AI from DeepLearning.AI</a>上的视频,质量很高.<p>在神经网络压缩中,量化通常指从浮点数到整数的映射过程,尤其是 8 位整数量化(即 INT8 量化).对于神经网络模型，通常有两类数据需要量化，即权重（模型参数）和激活（隐藏激活），这两类数据最初用浮点数表示。<p>为了说明模型量化的基本思想，介绍一个简单但常用的量化函数：$x<em>{q} = R(x/S)-Z$，它将浮点数 x 转换为量化值 $x</em>{q}$。在这个函数中,S 和 Z 分别表示缩放因子（涉及两个参数 α 和 β,决定clipping range和zero-point factor(决定对称或不对称量化),R(-) 表示四舍五入运算,将缩放浮动值转换为近似整数。作为逆过程，去量化相应地从量化值中恢复出原始值: x= S - (xq + Z)。量化误差计算为原始值 x 与恢复值 ̃ x 之间的数值差。范围参数 α 和 β 对量化性能有很大影响，通常需要根据实际数据分布进行静态（离线）或动态（运行时）校准。<p>经常使用的线性quantizaiton,<p><img alt=image-20240511194833581 data-src=https://s2.loli.net/2024/05/11/XQ7vgiNjn9oLkAc.png><p>q=int(round(r/s+z))<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_q_with_scale_and_zero_point</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>    tensor, scale, zero_point, dtype = torch.int8</span>):</span></span><br><span class=line></span><br><span class=line>    scaled_and_shifted_tensor = tensor / scale + zero_point</span><br><span class=line></span><br><span class=line>    rounded_tensor = torch.<span class=built_in>round</span>(scaled_and_shifted_tensor)</span><br><span class=line></span><br><span class=line>    q_min = torch.iinfo(dtype).<span class=built_in>min</span></span><br><span class=line>    q_max = torch.iinfo(dtype).<span class=built_in>max</span></span><br><span class=line></span><br><span class=line>    q_tensor = rounded_tensor.clamp(q_min,q_max).to(dtype)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> q_tensor</span><br></pre></table></figure><p>这样得到量化后的结果再转回去与原本的差距,就是quantizaiton error.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br></pre><td class=code><pre><span class=line><span class=comment>### a dummy tensor to test the implementation</span></span><br><span class=line>test_tensor=torch.tensor(</span><br><span class=line>    [[<span class=number>191.6</span>, -<span class=number>13.5</span>, <span class=number>728.6</span>],</span><br><span class=line>     [<span class=number>92.14</span>, <span class=number>295.5</span>,  -<span class=number>184</span>],</span><br><span class=line>     [<span class=number>0</span>,     <span class=number>684.6</span>, <span class=number>245.5</span>]]</span><br><span class=line>)</span><br><span class=line><span class=comment>### these are random values for "scale" and "zero_point"</span></span><br><span class=line><span class=comment>### to test the implementation</span></span><br><span class=line>scale = <span class=number>3.5</span></span><br><span class=line>zero_point = -<span class=number>70</span></span><br><span class=line>quantized_tensor = linear_q_with_scale_and_zero_point(</span><br><span class=line>    test_tensor, scale, zero_point)</span><br><span class=line>dequantized_tensor = scale * (quantized_tensor.<span class=built_in>float</span>() - zero_point)</span><br><span class=line>(dequantized_tensor - test_tensor).square().mean()</span><br></pre></table></figure><p><img alt=image-20240511211342622 data-src=https://s2.loli.net/2024/05/11/FRJflki8SWwLBN4.png><p>要计算s和z,首先我们知道r_min,r_max以及q_min,q_max,由此得到s,再通过s,q_min和r_min计算得到z.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_q_scale_and_zero_point</span>(<span class=params>tensor, dtype=torch.int8</span>):</span></span><br><span class=line>    </span><br><span class=line>    q_min, q_max = torch.iinfo(dtype).<span class=built_in>min</span>, torch.iinfo(dtype).<span class=built_in>max</span></span><br><span class=line>    r_min, r_max = tensor.<span class=built_in>min</span>().item(), tensor.<span class=built_in>max</span>().item()</span><br><span class=line></span><br><span class=line>    scale = (r_max - r_min) / (q_max - q_min)</span><br><span class=line></span><br><span class=line>    zero_point = q_min - (r_min / scale)</span><br><span class=line></span><br><span class=line>    <span class=comment># clip the zero_point to fall in [quantized_min, quantized_max]</span></span><br><span class=line>    <span class=keyword>if</span> zero_point < q_min:</span><br><span class=line>        zero_point = q_min</span><br><span class=line>    <span class=keyword>elif</span> zero_point > q_max:</span><br><span class=line>        zero_point = q_max</span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        <span class=comment># round and cast to int</span></span><br><span class=line>        zero_point = <span class=built_in>int</span>(<span class=built_in>round</span>(zero_point))</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> scale, zero_point</span><br></pre></table></figure><p><img alt=image-20240511211741798 data-src=https://s2.loli.net/2024/05/11/Ui7bAhFEXkIqLQs.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_quantization</span>(<span class=params>tensor, dtype=torch.int8</span>):</span></span><br><span class=line>    scale, zero_point = get_q_scale_and_zero_point(tensor, </span><br><span class=line>                                                   dtype=dtype)</span><br><span class=line>    </span><br><span class=line>    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,</span><br><span class=line>                                                          scale, </span><br><span class=line>                                                          zero_point, </span><br><span class=line>                                                          dtype=dtype)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> quantized_tensor, scale , zero_point</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_dequantization</span>(<span class=params>q_tensor,scale,zero_point</span>):</span></span><br><span class=line>    tensor = (q_tensor.<span class=built_in>float</span>()-zero_point)*scale</span><br><span class=line>    <span class=keyword>return</span> tensor</span><br></pre></table></figure><p>在线性量化中有对称和非对称模式,非对称就是上面的方式.对称模式将[-$r<em>{max}$,$r</em>{max}$]投影到[-$q<em>{max}$,$q</em>{max}$],$r<em>{max}$是max(|$r</em>{tensor}$|)<p>计算公式是</p><script type="math/tex; mode=display">
\begin{cases}q=int(round(r/s))\\s=r_{\max}/q_{\max}\end{cases}</script><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_q_scale_symmetric</span>(<span class=params>tensor, dtype=torch.int8</span>):</span></span><br><span class=line>    r_max = tensor.<span class=built_in>abs</span>().<span class=built_in>max</span>().item()</span><br><span class=line>    q_max = torch.iinfo(dtype).<span class=built_in>max</span></span><br><span class=line></span><br><span class=line>    <span class=comment># return the scale</span></span><br><span class=line>    <span class=keyword>return</span> r_max/q_max</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_q_symmetric</span>(<span class=params>tensor, dtype=torch.int8</span>):</span></span><br><span class=line>    scale = get_q_scale_symmetric(tensor)</span><br><span class=line>    </span><br><span class=line>    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,</span><br><span class=line>                                                     scale=scale,</span><br><span class=line>                   <span class=comment># in symmetric quantization zero point is = 0    </span></span><br><span class=line>                                                    zero_point=<span class=number>0</span>,</span><br><span class=line>                                                      dtype=dtype)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> quantized_tensor, scale</span><br><span class=line></span><br></pre></table></figure><p>对比,对称模式会导致有些量化范围的值用不上,但是它比较简单而且不用存0值. 在量化到更低位时考虑使用非对称模式。<p>量化的不同粒度,包括per tensor,per channel,per group.<p>对于通道来说<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> index <span class=keyword>in</span> <span class=built_in>range</span>(output_dim):</span><br><span class=line>    sub_tensor = test_tensor.select(dim,index)</span><br><span class=line>    <span class=comment># print(sub_tensor)</span></span><br><span class=line>    scale[index] = get_q_scale_symmetric(sub_tensor)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_q_symmetric_per_channel</span>(<span class=params>r_tensor, dim, dtype=torch.int8</span>):</span></span><br><span class=line>    </span><br><span class=line>    output_dim = r_tensor.shape[dim]</span><br><span class=line>    <span class=comment># store the scales</span></span><br><span class=line>    scale = torch.zeros(output_dim)</span><br><span class=line></span><br><span class=line>    <span class=keyword>for</span> index <span class=keyword>in</span> <span class=built_in>range</span>(output_dim):</span><br><span class=line>        sub_tensor = r_tensor.select(dim, index)</span><br><span class=line>        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)</span><br><span class=line></span><br><span class=line>    <span class=comment># reshape the scale</span></span><br><span class=line>    scale_shape = [<span class=number>1</span>] * r_tensor.dim()</span><br><span class=line>    scale_shape[dim] = -<span class=number>1</span></span><br><span class=line>    scale = scale.view(scale_shape)</span><br><span class=line>    quantized_tensor = linear_q_with_scale_and_zero_point(</span><br><span class=line>        r_tensor, scale=scale, zero_point=<span class=number>0</span>, dtype=dtype)</span><br><span class=line>   </span><br><span class=line>    <span class=keyword>return</span> quantized_tensor, scale</span><br><span class=line>dequantized_tensor_0 = linear_dequantization(</span><br><span class=line>    quantized_tensor_0, scale_0, <span class=number>0</span>)</span><br><span class=line></span><br><span class=line>plot_quantization_errors(</span><br><span class=line>    test_tensor, quantized_tensor_0, dequantized_tensor_0)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_q_symmetric_per_group</span>(<span class=params>tensor, group_size,</span></span></span><br><span class=line><span class=params><span class=function>                                 dtype=torch.int8</span>):</span></span><br><span class=line>    </span><br><span class=line>    t_shape = tensor.shape</span><br><span class=line>    <span class=keyword>assert</span> t_shape[<span class=number>1</span>] % group_size == <span class=number>0</span></span><br><span class=line>    <span class=keyword>assert</span> tensor.dim() == <span class=number>2</span></span><br><span class=line>    </span><br><span class=line>    tensor = tensor.view(-<span class=number>1</span>, group_size)</span><br><span class=line>    </span><br><span class=line>    quantized_tensor, scale = linear_q_symmetric_per_channel(</span><br><span class=line>                                tensor, dim=<span class=number>0</span>, dtype=dtype)</span><br><span class=line>    </span><br><span class=line>    quantized_tensor = quantized_tensor.view(t_shape)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> quantized_tensor, scale</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear_dequantization_per_group</span>(<span class=params>quantized_tensor, scale, </span></span></span><br><span class=line><span class=params><span class=function>                                    group_size</span>):</span></span><br><span class=line>    </span><br><span class=line>    q_shape = quantized_tensor.shape</span><br><span class=line>    quantized_tensor = quantized_tensor.view(-<span class=number>1</span>, group_size)</span><br><span class=line>    </span><br><span class=line>    dequantized_tensor = linear_dequantization(quantized_tensor, </span><br><span class=line>                                               scale, <span class=number>0</span>)</span><br><span class=line>    </span><br><span class=line>    dequantized_tensor = dequantized_tensor.view(q_shape)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> dequantized_tensor</span><br></pre></table></figure><p>量化权重和激活.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>quantized_linear_W8A32_without_bias</span>(<span class=params><span class=built_in>input</span>, q_w, s_w, z_w</span>):</span></span><br><span class=line>    <span class=keyword>assert</span> <span class=built_in>input</span>.dtype == torch.float32</span><br><span class=line>    <span class=keyword>assert</span> q_w.dtype == torch.int8</span><br><span class=line></span><br><span class=line>    dequantized_weight = q_w.to(torch.float32) * s_w + z_w</span><br><span class=line>    output = torch.nn.functional.linear(<span class=built_in>input</span>, dequantized_weight)</span><br><span class=line>    </span><br><span class=line>    <span class=keyword>return</span> output</span><br></pre></table></figure><p><img alt=image-20240514215518321 data-src=https://s2.loli.net/2024/05/14/tz9cBO5UowfKvqg.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>output = quantized_linear_W8A32_without_bias(<span class=built_in>input</span>,                                            q_w,                                            s_w,                                             <span class=number>0</span>)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>W8A16LinearLayer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self,in_features,out_features,bias=<span class=literal>True</span>,dtype=torch.float32</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line></span><br><span class=line>        self.register_buffer(</span><br><span class=line>            <span class=string>"int8_weights"</span>,</span><br><span class=line>            torch.randint(</span><br><span class=line>                -<span class=number>128</span>, <span class=number>127</span>, (out_features, in_features), dtype=torch.int8</span><br><span class=line>            )</span><br><span class=line>        )</span><br><span class=line>        self.register_buffer(<span class=string>"scales"</span>,</span><br><span class=line>                             torch.randn((out_features), dtype=dtype))</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> bias:</span><br><span class=line>            self.register_buffer(<span class=string>"bias"</span>,</span><br><span class=line>                                 torch.randn((<span class=number>1</span>, out_features),</span><br><span class=line>                                             dtype=dtype))</span><br><span class=line></span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            self.bias = <span class=literal>None</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>quantize</span>(<span class=params>self, weights</span>):</span></span><br><span class=line>        w_fp32 = weights.clone().to(torch.float32)</span><br><span class=line></span><br><span class=line>        scales = w_fp32.<span class=built_in>abs</span>().<span class=built_in>max</span>(dim=-<span class=number>1</span>).values / <span class=number>127</span></span><br><span class=line>        scales = scales.to(weights.dtype)</span><br><span class=line></span><br><span class=line>        int8_weights = torch.<span class=built_in>round</span>(weights</span><br><span class=line>                                   / scales.unsqueeze(<span class=number>1</span>)).to(torch.int8)</span><br><span class=line>        self.int8_weights = int8_weights</span><br><span class=line>        self.scales = scales</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, <span class=built_in>input</span></span>):</span></span><br><span class=line>        <span class=keyword>return</span> w8_a16_forward(self.int8_weights,</span><br><span class=line>                              <span class=built_in>input</span>, self.scales, self.bias)</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>w8_a16_forward</span>(<span class=params>weight, <span class=built_in>input</span>, scales, bias=<span class=literal>None</span></span>):</span></span><br><span class=line>casted_weights = weight.to(<span class=built_in>input</span>.dtype)</span><br><span class=line>output = F.linear(<span class=built_in>input</span>, casted_weights) * scales</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>output = output + bias</span><br><span class=line></span><br><span class=line><span class=keyword>return</span> output</span><br></pre></table></figure><p>量化线性层,可以替代一些模型的线性层,因为它自带quantize方法可以量化线性层的权重,这里的量化方法都是指的linear quantization,也就是通过原本值的范围和量化后的计算得到的scale和zero_point来将原本的矩阵进行量化.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>replace_linear_with_target</span>(<span class=params>module,</span></span></span><br><span class=line><span class=params><span class=function>                               target_class, module_name_to_exclude</span>):</span></span><br><span class=line>    <span class=keyword>for</span> name, child <span class=keyword>in</span> module.named_children():</span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>isinstance</span>(child, nn.Linear) <span class=keyword>and</span> <span class=keyword>not</span> \</span><br><span class=line>                <span class=built_in>any</span>([x == name <span class=keyword>for</span> x <span class=keyword>in</span> module_name_to_exclude]):</span><br><span class=line>            old_bias = child.bias</span><br><span class=line></span><br><span class=line>            new_module = target_class(child.in_features,</span><br><span class=line>                                      child.out_features,</span><br><span class=line>                                      old_bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>,</span><br><span class=line>                                      child.weight.dtype)</span><br><span class=line>            <span class=built_in>setattr</span>(module, name, new_module)</span><br><span class=line>            <span class=keyword>if</span> old_bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>                <span class=built_in>getattr</span>(module, name).bias = old_bias</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            <span class=comment># Recursively call the function for nested modules</span></span><br><span class=line>            replace_linear_with_target(</span><br><span class=line>                child, target_class, module_name_to_exclude)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>replace_linear_with_target_and_quantize</span>(<span class=params>module, </span></span></span><br><span class=line><span class=params><span class=function>                               target_class, module_name_to_exclude</span>):</span></span><br><span class=line>    <span class=keyword>for</span> name, child <span class=keyword>in</span> module.named_children():</span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>isinstance</span>(child, nn.Linear) <span class=keyword>and</span> <span class=keyword>not</span> \</span><br><span class=line>        <span class=built_in>any</span>([x == name <span class=keyword>for</span> x <span class=keyword>in</span> module_name_to_exclude]):</span><br><span class=line>            old_bias = child.bias</span><br><span class=line>            old_weight = child.weight</span><br><span class=line></span><br><span class=line>            new_module = target_class(child.in_features, </span><br><span class=line>                                      child.out_features, </span><br><span class=line>                                      old_bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>, </span><br><span class=line>                                      child.weight.dtype)</span><br><span class=line>            <span class=built_in>setattr</span>(module, name, new_module)</span><br><span class=line></span><br><span class=line>            <span class=built_in>getattr</span>(module, name).quantize(old_weight)</span><br><span class=line>            </span><br><span class=line>            <span class=keyword>if</span> old_bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>              <span class=built_in>getattr</span>(module, name).bias = old_bias</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            <span class=comment># Recursively call the function for nested modules</span></span><br><span class=line>            replace_linear_with_target_and_quantize(child, </span><br><span class=line>                     target_class, module_name_to_exclude)</span><br></pre></table></figure><p>搭配huggingface量化大模型.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> AutoModelForCausalLM, AutoTokenizer, pipeline</span><br><span class=line></span><br><span class=line>model_id = <span class=string>"./models/Salesforce/codegen-350M-mono"</span></span><br><span class=line></span><br><span class=line>model = AutoModelForCausalLM.from_pretrained(model_id, </span><br><span class=line>                                    torch_dtype=torch.bfloat16, </span><br><span class=line>                                             low_cpu_mem_usage=<span class=literal>True</span>)</span><br><span class=line>tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class=line></span><br><span class=line></span><br><span class=line>pipe = pipeline(<span class=string>"text-generation"</span>, model=model, tokenizer=tokenizer)</span><br><span class=line>replace_linear_with_target_and_quantize(model, </span><br><span class=line>                                        W8A16LinearLayer, [<span class=string>"lm_head"</span>])</span><br><span class=line>                                        <span class=built_in>print</span>(pipe(<span class=string>"def hello_world():"</span>, max_new_tokens=<span class=number>20</span>, </span><br><span class=line>           do_sample=<span class=literal>False</span>)[<span class=number>0</span>][<span class=string>"generated_text"</span>])</span><br></pre></table></figure><h4 id=Weight-packing><a title="Weight packing" class=headerlink href=#Weight-packing></a>Weight packing</h4><p>将可以使用int2,int4保存的多个数据使用一个或多个int8保存起来,也就是pack.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>pack_weights</span>(<span class=params>uint8tensor, bits</span>):</span></span><br><span class=line>    <span class=keyword>if</span> uint8tensor.shape[<span class=number>0</span>] * bits % <span class=number>8</span> != <span class=number>0</span>:</span><br><span class=line>        <span class=keyword>raise</span> ValueError(<span class=string>f"The input shape needs to be a mutiple \</span></span><br><span class=line><span class=string>        of <span class=subst>{<span class=number>8</span> / bits}</span> - got <span class=subst>{uint8tensor.shape[<span class=number>0</span>]}</span>"</span>)</span><br><span class=line></span><br><span class=line>    num_values = uint8tensor.shape[<span class=number>0</span>] * bits // <span class=number>8</span></span><br><span class=line></span><br><span class=line>    num_steps = <span class=number>8</span> // bits</span><br><span class=line></span><br><span class=line>    unpacked_idx = <span class=number>0</span></span><br><span class=line></span><br><span class=line>    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)</span><br><span class=line></span><br><span class=line>    <span class=comment># 1 0 3 2 - 01 00 11 10</span></span><br><span class=line></span><br><span class=line>    <span class=comment># [0000 0000] -> 0000 0001</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 0000 0001</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 0000 0000 - 0000 0000</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 0000 0011 - 0011 0000 - 0011 0001</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 1011 0001</span></span><br><span class=line>    </span><br><span class=line>    <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_values):</span><br><span class=line>        <span class=keyword>for</span> j <span class=keyword>in</span> <span class=built_in>range</span>(num_steps):</span><br><span class=line>            packed_tensor[i] |= uint8tensor[unpacked_idx] << (bits * j)</span><br><span class=line>            unpacked_idx += <span class=number>1</span></span><br><span class=line>    <span class=keyword>return</span> packed_tensor</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>unpack_weights</span>(<span class=params>uint8tensor, bits</span>):</span></span><br><span class=line>    num_values = uint8tensor.shape[<span class=number>0</span>] * <span class=number>8</span> // bits</span><br><span class=line></span><br><span class=line>    num_steps = <span class=number>8</span> // bits</span><br><span class=line></span><br><span class=line>    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)</span><br><span class=line></span><br><span class=line>    unpacked_idx = <span class=number>0</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 1 0 3 2 - 01 00 11 10</span></span><br><span class=line></span><br><span class=line>    <span class=comment># [00000000 00000000 00000000 00000000]</span></span><br><span class=line>    <span class=comment># [10110001 00101100 00001011 00000010]</span></span><br><span class=line>    <span class=comment># [00000001 00000000 00000011 00000010]</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 10110001</span></span><br><span class=line>    <span class=comment># 00000011</span></span><br><span class=line>    </span><br><span class=line>    <span class=comment># 00000001</span></span><br><span class=line></span><br><span class=line>    <span class=comment># 1: [10110001]</span></span><br><span class=line>    <span class=comment># 2: [00101100]</span></span><br><span class=line>    <span class=comment># 3: [00001011]</span></span><br><span class=line></span><br><span class=line>    mask = <span class=number>2</span> ** bits - <span class=number>1</span></span><br><span class=line></span><br><span class=line>    <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(uint8tensor.shape[<span class=number>0</span>]):</span><br><span class=line>        <span class=keyword>for</span> j <span class=keyword>in</span> <span class=built_in>range</span>(num_steps):</span><br><span class=line>            unpacked_tensor[unpacked_idx] |= uint8tensor[i] >> (bits * j)</span><br><span class=line>            unpacked_idx += <span class=number>1</span></span><br><span class=line></span><br><span class=line>    unpacked_tensor &= mask</span><br><span class=line>    <span class=keyword>return</span> unpacked_tensor</span><br></pre></table></figure><p><img alt=image-20240515101338582 data-src=https://s2.loli.net/2024/05/15/Q3Ax1eLJoGz42ZS.png><h3 id=数据处理><a class=headerlink href=#数据处理 title=数据处理></a>数据处理</h3><p>在收集大量文本数据后，必须对数据进行预处理，以构建预训练语料库，特别是去除噪声、冗余、不相关和潜在有毒的数据]，这些数据可能会在很大程度上影响 LLM 的容量和性能。为了方便数据处理,最近的一项研究为 LLMs 提出了一个有用的数据处理系统,名为 Data-Juicer<a href=https://github.com/modelscope/data-juicer rel=noopener target=_blank>modelscope/data-juicer: A one-stop data processing system to make data higher-quality, juicier, and more digestible for LLMs! 🍎 🍋 🌽 ➡️ ➡️🍸 🍹 🍷为大语言模型提供更高质量、更丰富、更易”消化“的数据！ (github.com)</a>,它提供了 50 多个处理算子和工具.在这一部分将回顾详细的数据预处理策略,以提高所收集数据的质量。<h4 id=Quality-Filtering><a title="Quality Filtering" class=headerlink href=#Quality-Filtering></a>Quality Filtering</h4><p>为了从收集到的语料库中剔除低质量数据,现有工作一般采用两种方法：(1) 基于分类器的方法；(2) 基于启发式的方法。前一种方法基于高质量文本训练选择分类器,并利用它来识别和过滤低质量数据。然而一些研究发现基于分类器的方法可能会导致方言、口语和社会方言语言中高质量文本的无意删除，这可能会导致预训练语料中的偏见，并减弱语料的多样性。<p>作为第二种方法，一些研究，如BLOOM 和Gopher，采用启发式方法，通过一组精心设计的规则来消除低质量文本，这些规则可以概括为：<p>·<strong>基于语言的过滤。</strong>如果LLM主要用于某些语言的任务，则可以过滤其他语言的文本。<p>·<strong>基于度量的过滤</strong>。生成文本的评价指标，例如困惑度，可以用来检测和去除不自然的句子。<p>·<strong>基于统计的过滤</strong>。语料库的统计特征，如标点符号分布，符词比和句子长度，可以用来衡量文本质量和过滤低质量数据。<p>·<strong>基于关键词的过滤</strong>。基于特定的关键字集合，噪声或无用的<h4 id=De-duplication><a class=headerlink href=#De-duplication title=De-duplication></a>De-duplication</h4><p><strong>现有工作发现语料库中的重复数据会降低语言模型的多样性，这可能会导致训练过程变得不稳定</strong>，从而影响模型性能。因此，有必要对重复数据删除语料进行预训练。特别地，重复数据删除可以在不同粒度下进行，包括句子级、文档级和数据集级重复数据删除。首先<strong>，应该去除包含重复单词和短语的低质量句子，因为它们可能会在语言建模中引入重复模式</strong>。在文档层面，现有研究<strong>大多依靠文档之间的表面特征重叠率</strong>(例如,单词和n元词串重叠)来检测和去除包含相似内容的重复文档。此外，为了避免数据污染问题，防止训练集和评估集之间的重叠也是至关重要的，通过从训练集中移除可能的重复文本。研究表明，这3个层次的去重对提高LLMs的训练是有益的，应该在实际中联合使用。<h4 id=Privacy-Reduction><a title="Privacy Reduction" class=headerlink href=#Privacy-Reduction></a>Privacy Reduction</h4><p>大多数预训练文本数据是从Web来源获得的，包括用户生成的涉及敏感或个人信息的内容，这可能会增加隐私泄露的风险。因此，有必要从预训练语料中移除个人身份信息( PII )。一种直接而有效的方法是使用基于规则的方法，如关键字检测，来检测和去除PII，如名称，地址和电话号码。此外，研究人员还发现LLMs在隐私攻击下的脆弱性可以归因于预训练语料中存在重复的PII数据<h4 id=Tokenization><a class=headerlink href=#Tokenization title=Tokenization></a>Tokenization</h4><p>normalization是为了去除不需要的空白,小写以及口音.<p>pre-tokenization会生成每个word的偏移量.Model就是BPE等方法.<p><img alt="The tokenization pipeline." data-src=https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg><p>标记化也是数据预处理的关键步骤。它的目的是将原始文本分割成一系列单独的令牌序列,然后将这些令牌序列作为LLMs的输入.在传统的NLP研究(例如,利用条件随机场进行序列标注)中,基于词的标记化是最主要的方法,它更符合人类的语言认知.主要有word-,character-以及<strong>subword</strong>-的分词方式.<blockquote><p>训练tokenizer是一个统计过程，它试图确定哪些子词是特定语料的最佳选择，而选择子词的具体规则取决于标记化算法。它是确定性的，这意味着在同一语料库上使用同一算法进行训练时，总会得到相同的结果。</blockquote><h5 id=Byte-Pair-encoding><a title="Byte-Pair encoding" class=headerlink href=#Byte-Pair-encoding></a>Byte-Pair encoding</h5><p>它从一组基本符号(例如,字母和边界字符)开始,迭代地将语料库中频繁出现的连续两个token对组合为新的token (称为merge ).对于每一个合并,选择标准是基于两个连续标记的共现频率:选择最高的频繁对.合并过程一直持续到达到预定义的大小.<p>开始通过word分词得到类似下面的数据<figure class="highlight avrasm"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line><span class=symbol>Vocabulary:</span> [<span class=string>"b"</span>, <span class=string>"g"</span>, <span class=string>"h"</span>, <span class=string>"n"</span>, <span class=string>"p"</span>, <span class=string>"s"</span>, <span class=string>"u"</span>, <span class=string>"ug"</span>]</span><br><span class=line><span class=symbol>Corpus:</span>(<span class=string>"h"</span> <span class=string>"u"</span> <span class=string>"g"</span>, <span class=number>10</span>), (<span class=string>"p"</span> <span class=string>"u"</span> <span class=string>"g"</span>, <span class=number>5</span>), (<span class=string>"p"</span> <span class=string>"u"</span> <span class=string>"n"</span>, <span class=number>12</span>), (<span class=string>"b"</span> <span class=string>"u"</span> <span class=string>"n"</span>, <span class=number>4</span>), (<span class=string>"h"</span> <span class=string>"u"</span> <span class=string>"g"</span> <span class=string>"s"</span>, <span class=number>5</span>)</span><br></pre></table></figure><p>在获得基本词汇后,会通过学习合并规则来添加新的标记词,直到达到所需的词汇量为止,合并规则就是将现有词汇中的两个元素合并成一个新词汇.因此,一开始这些合并会产生两个字符的词库,然后随着训练的进行,会产生更长的子词.<p>在标记符训练过程中的任何一步，BPE 算法都会搜索现有标记符中出现频率最高的一对（这里的 “一对 “指的是一个词中连续出现的两个标记符）。这对频率最高的词对将被合并，然后我们重复下一步<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> collections <span class=keyword>import</span> defaultdict</span><br><span class=line></span><br><span class=line>word_freqs = defaultdict(<span class=built_in>int</span>)</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> text <span class=keyword>in</span> corpus:</span><br><span class=line>    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class=line>    new_words = [word <span class=keyword>for</span> word, offset <span class=keyword>in</span> words_with_offsets]</span><br><span class=line>    <span class=keyword>for</span> word <span class=keyword>in</span> new_words:</span><br><span class=line>        word_freqs[word] += <span class=number>1</span></span><br><span class=line></span><br><span class=line>alphabet = []</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> word <span class=keyword>in</span> word_freqs.keys():</span><br><span class=line>    <span class=keyword>for</span> letter <span class=keyword>in</span> word:</span><br><span class=line>        <span class=keyword>if</span> letter <span class=keyword>not</span> <span class=keyword>in</span> alphabet:</span><br><span class=line>            alphabet.append(letter)</span><br><span class=line>alphabet.sort()</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(alphabet)</span><br><span class=line>vocab = [<span class=string>"<|endoftext|>"</span>] + alphabet.copy() <span class=comment># 针对不同llm可能会有special tokens.</span></span><br><span class=line>splits = {word: [c <span class=keyword>for</span> c <span class=keyword>in</span> word] <span class=keyword>for</span> word <span class=keyword>in</span> word_freqs.keys()}</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>compute_pair_freqs</span>(<span class=params>splits</span>):</span></span><br><span class=line>    pair_freqs = defaultdict(<span class=built_in>int</span>)</span><br><span class=line>    <span class=keyword>for</span> word, freq <span class=keyword>in</span> word_freqs.items():</span><br><span class=line>        split = splits[word]</span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>len</span>(split) == <span class=number>1</span>:</span><br><span class=line>            <span class=keyword>continue</span></span><br><span class=line>        <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(split) - <span class=number>1</span>):</span><br><span class=line>            pair = (split[i], split[i + <span class=number>1</span>])</span><br><span class=line>            pair_freqs[pair] += freq</span><br><span class=line>    <span class=keyword>return</span> pair_freqs</span><br><span class=line>pair_freqs = compute_pair_freqs(splits)</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> i, key <span class=keyword>in</span> <span class=built_in>enumerate</span>(pair_freqs.keys()):</span><br><span class=line>    <span class=built_in>print</span>(<span class=string>f"<span class=subst>{key}</span>: <span class=subst>{pair_freqs[key]}</span>"</span>)</span><br><span class=line>    <span class=keyword>if</span> i >= <span class=number>5</span>:</span><br><span class=line>        <span class=keyword>break</span>      </span><br><span class=line>best_pair = <span class=string>""</span></span><br><span class=line>max_freq = <span class=literal>None</span></span><br><span class=line></span><br><span class=line><span class=keyword>for</span> pair, freq <span class=keyword>in</span> pair_freqs.items():</span><br><span class=line>    <span class=keyword>if</span> max_freq <span class=keyword>is</span> <span class=literal>None</span> <span class=keyword>or</span> max_freq < freq:</span><br><span class=line>        best_pair = pair</span><br><span class=line>        max_freq = freq</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>merge_pair</span>(<span class=params>a, b, splits</span>):</span></span><br><span class=line>    <span class=keyword>for</span> word <span class=keyword>in</span> word_freqs:</span><br><span class=line>        split = splits[word]</span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>len</span>(split) == <span class=number>1</span>:</span><br><span class=line>            <span class=keyword>continue</span></span><br><span class=line></span><br><span class=line>        i = <span class=number>0</span></span><br><span class=line>        <span class=keyword>while</span> i < <span class=built_in>len</span>(split) - <span class=number>1</span>:</span><br><span class=line>            <span class=keyword>if</span> split[i] == a <span class=keyword>and</span> split[i + <span class=number>1</span>] == b:</span><br><span class=line>                split = split[:i] + [a + b] + split[i + <span class=number>2</span> :]</span><br><span class=line>            <span class=keyword>else</span>:</span><br><span class=line>                i += <span class=number>1</span></span><br><span class=line>        splits[word] = split</span><br><span class=line>    <span class=keyword>return</span> splits</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>tokenize</span>(<span class=params>text</span>):</span></span><br><span class=line>    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class=line>    pre_tokenized_text = [word <span class=keyword>for</span> word, offset <span class=keyword>in</span> pre_tokenize_result]</span><br><span class=line>    splits = [[l <span class=keyword>for</span> l <span class=keyword>in</span> word] <span class=keyword>for</span> word <span class=keyword>in</span> pre_tokenized_text]</span><br><span class=line>    <span class=keyword>for</span> pair, merge <span class=keyword>in</span> merges.items():</span><br><span class=line>        <span class=keyword>for</span> idx, split <span class=keyword>in</span> <span class=built_in>enumerate</span>(splits):</span><br><span class=line>            i = <span class=number>0</span></span><br><span class=line>            <span class=keyword>while</span> i < <span class=built_in>len</span>(split) - <span class=number>1</span>:</span><br><span class=line>                <span class=keyword>if</span> split[i] == pair[<span class=number>0</span>] <span class=keyword>and</span> split[i + <span class=number>1</span>] == pair[<span class=number>1</span>]:</span><br><span class=line>                    split = split[:i] + [merge] + split[i + <span class=number>2</span> :]</span><br><span class=line>                <span class=keyword>else</span>:</span><br><span class=line>                    i += <span class=number>1</span></span><br><span class=line>            splits[idx] = split</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> <span class=built_in>sum</span>(splits, [])</span><br></pre></table></figure><h5 id=SentencePiece><a class=headerlink href=#SentencePiece title=SentencePiece></a>SentencePiece</h5><p>SentencePiece 是一种用于文本预处理的tokenization,它将文本视为 Unicode 字符序列，并用特殊字符 ▁ 替换空格.与 Unigram 算法结合使用，它甚至不需要预标记步骤,这对于不使用空格字符的语言（如中文或日文）非常有用<h5 id=WordPiece><a class=headerlink href=#WordPiece title=WordPiece></a>WordPiece</h5><p>与 BPE 类似,WordPiece 也是从一个小词库开始的,其中包括模型使用的特殊标记和初始字母表.由于它是通过添加前缀（如 BERT 的 ##）来识别子词的,因此每个词最初都是通过将前缀添加到词内的所有字符来分割的.<p>同样的例子.<figure class="highlight 1c"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>(<span class=string>"hug"</span>, <span class=number>10</span>), (<span class=string>"pug"</span>, <span class=number>5</span>), (<span class=string>"pun"</span>, <span class=number>12</span>), (<span class=string>"bun"</span>, <span class=number>4</span>), (<span class=string>"hugs"</span>, <span class=number>5</span>)</span><br></pre></table></figure><p>拆开后有<figure class="highlight livescript"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>(<span class=string>"h"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#g</span>"</span>, <span class=number>10</span>), (<span class=string>"p"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#g</span>"</span>, <span class=number>5</span>), (<span class=string>"p"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#n</span>"</span>, <span class=number>12</span>), (<span class=string>"b"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#n</span>"</span>, <span class=number>4</span>), (<span class=string>"h"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#g</span>"</span> <span class=string>"#<span class=subst>#s</span>"</span>, <span class=number>5</span>)</span><br></pre></table></figure><p>WordPiece 学习合并规则.主要区别在于选择要合并的词对的方式.它不是选择最频繁的词对.<p>通过将词对的频率除以各部分频率的乘积,该算法会优先合并词汇中各部分频率较低的词对.<p>初始词汇将是[“b”、”h”、”p”、”##g”、”##n”、”##s”、”##u”]（如果我们暂时不考虑特殊标记的话）。出现频率最高的词对是（”##u”，”##g”）（出现 20 次），但 “##u “的单个出现频率非常高，因此它的得分并不是最高的（1/36）。所有含有 “##u “的词对实际上都有相同的得分（1/36），因此得分最高的词对是（”##g”, “##s”）—唯一一个没有 “##u “的词对，得分是 1/20，第一次合并的结果是（”##g”, “##s”）->（”##gs”）<figure class="highlight livescript"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>Vocabulary: [<span class=string>"b"</span>, <span class=string>"h"</span>, <span class=string>"p"</span>, <span class=string>"#<span class=subst>#g</span>"</span>, <span class=string>"#<span class=subst>#n</span>"</span>, <span class=string>"#<span class=subst>#s</span>"</span>, <span class=string>"#<span class=subst>#u</span>"</span>, <span class=string>"#<span class=subst>#gs</span>"</span>]</span><br><span class=line>Corpus: (<span class=string>"h"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#g</span>"</span>, <span class=number>10</span>), (<span class=string>"p"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#g</span>"</span>, <span class=number>5</span>), (<span class=string>"p"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#n</span>"</span>, <span class=number>12</span>), (<span class=string>"b"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#n</span>"</span>, <span class=number>4</span>), (<span class=string>"h"</span> <span class=string>"#<span class=subst>#u</span>"</span> <span class=string>"#<span class=subst>#gs</span>"</span>, <span class=number>5</span>)</span><br></pre></table></figure><h5 id=Unigram><a class=headerlink href=#Unigram title=Unigram></a>Unigram</h5><p>Unigram 算法常用于 SentencePiece，而 SentencePiece 是 AlBERT、T5、mBART、Big Bird 和 XLNet 等模型使用的tokenization算法。<p>与 BPE 和 WordPiece 相比,Unigram 的工作方向相反:它从一个大词汇量开始，然后从中删除词组,直到达到所需的词汇量.有几种方法可以用来建立基本词库:例如,我们可以在预先标注的单词中提取最常见的子串,或者在初始语料库中应用 BPE,以获得较大的词汇量.<p>有几种方法可以用来建立基础词汇:例如,我们可以从预先标注的单词中选取最常见的子串,或者在初始语料库中应用 BPE,使其具有较大的词汇量。<p>在训练的每一步,Unigram 算法都会根据当前的词汇量计算语料库的损失.然后,对于词汇表中的每个符号,算法都会计算如果删除该符号,整体损失会增加多少,并寻找损失增加最少的符号.这些符号对语料库总体损失的影响较小,因此从某种意义上说,它们 “不那么需要”,是删除的最佳候选。<h3 id=Retrieve-Augmented-Generation-RAG><a title="Retrieve Augmented Generation(RAG)" class=headerlink href=#Retrieve-Augmented-Generation-RAG></a>Retrieve Augmented Generation(RAG)</h3><p>由 LLM 驱动的聊天机器人可处理用户提示并生成回复.该聊天机器人旨在与用户进行互动,并就广泛的主题与用户进行交流.<p>但是,它的回复仅限于所提供的上下文和基础培训数据.例如,GPT-4 的知识截止日期是 2021 年 9 月,这意味着它不了解这一时期之后发生的事件.此外,用于训练 LLM 的数据不包括个人笔记或公司产品手册等机密信息.<img alt="drawing showing how RAGs architecture" data-src=https://github.com/microsoft/generative-ai-for-beginners/raw/main/15-rag-and-vector-databases/images/encoder-decode.png?WT.mc_id=academic-105485-koreyst><h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://github.com/luban-agi/Awesome-AIGC-Tutorials?tab=readme-ov-file#-multimodal rel=noopener target=_blank>luban-agi/Awesome-AIGC-Tutorials: Curated tutorials and resources for Large Language Models, AI Painting, and more. (github.com)</a><li><a href=https://github.com/mlabonne/llm-course rel=noopener target=_blank>mlabonne/llm-course: Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks. (github.com)</a><li><a href=https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models rel=noopener target=_blank>BradyFU/Awesome-Multimodal-Large-Language-Models: :sparkles::sparkles:Latest Papers and Datasets on Multimodal Large Language Models, and Their Evaluation. (github.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/05/01/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ title=LLM论文阅读>https://www.sekyoro.top/2024/05/01/LLM论文阅读/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a href=/2024/05/01/%E8%AE%BA%E6%96%87%E5%BC%95%E7%94%A8%E8%BD%AC%E6%8D%A2%E5%B0%8F%E5%B7%A5%E5%85%B7/ rel=prev title=论文引用转换小工具> <i class="fa fa-chevron-left"></i> 论文引用转换小工具 </a></div><div class=post-nav-item><a href=/2024/05/02/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%80%E4%BA%9B%E7%89%B9%E6%80%A7/ rel=next title=编程语言的一些特性> 编程语言的一些特性 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#A-Survey-of-Large-Language-Models><span class=nav-number>1.</span> <span class=nav-text>A Survey of Large Language Models</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Scaling-laws><span class=nav-number>1.0.1.</span> <span class=nav-text>Scaling laws</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B><span class=nav-number>1.0.2.</span> <span class=nav-text>涌现能力</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#PEFT%E6%96%B9%E6%B3%95><span class=nav-number>1.1.</span> <span class=nav-text>PEFT方法</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Prefix-Tuning><span class=nav-number>1.1.1.</span> <span class=nav-text>Prefix Tuning</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>1.1.1.1.</span> <span class=nav-text>摘要</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E6%96%B9%E6%B3%95><span class=nav-number>1.1.1.2.</span> <span class=nav-text>方法</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#P-tuning-V1-amp-amp-V2><span class=nav-number>1.1.2.</span> <span class=nav-text>P-tuning V1&&V2</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#LoRA><span class=nav-number>1.1.3.</span> <span class=nav-text>LoRA</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Adapter><span class=nav-number>1.2.</span> <span class=nav-text>Adapter</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Prompt-Tuning><span class=nav-number>1.3.</span> <span class=nav-text>Prompt Tuning</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Quantization%E6%96%B9%E6%B3%95><span class=nav-number>1.4.</span> <span class=nav-text>Quantization方法</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Weight-packing><span class=nav-number>1.4.1.</span> <span class=nav-text>Weight packing</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86><span class=nav-number>1.5.</span> <span class=nav-text>数据处理</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Quality-Filtering><span class=nav-number>1.5.1.</span> <span class=nav-text>Quality Filtering</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#De-duplication><span class=nav-number>1.5.2.</span> <span class=nav-text>De-duplication</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Privacy-Reduction><span class=nav-number>1.5.3.</span> <span class=nav-text>Privacy Reduction</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Tokenization><span class=nav-number>1.5.4.</span> <span class=nav-text>Tokenization</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Byte-Pair-encoding><span class=nav-number>1.5.4.1.</span> <span class=nav-text>Byte-Pair encoding</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#SentencePiece><span class=nav-number>1.5.4.2.</span> <span class=nav-text>SentencePiece</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#WordPiece><span class=nav-number>1.5.4.3.</span> <span class=nav-text>WordPiece</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Unigram><span class=nav-number>1.5.4.4.</span> <span class=nav-text>Unigram</span></a></ol></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Retrieve-Augmented-Generation-RAG><span class=nav-number>1.6.</span> <span class=nav-text>Retrieve Augmented Generation(RAG)</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>2.</span> <span class=nav-text>参考资料</span></a></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>217</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>199</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>此文章目前无词云</h3></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>30:58</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>