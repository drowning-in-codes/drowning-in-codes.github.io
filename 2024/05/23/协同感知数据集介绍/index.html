<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=目前深度学习的方法都是数据驱动的,在协同感知方面,数据集目前不算特别多,主要存在1)真实数据较少2)不同数据集之间配置差异较大,需要自行修改等. name=description><meta content=article property=og:type><meta content=协同感知数据集和代码库介绍 property=og:title><meta content=https://www.sekyoro.top/2024/05/23/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=目前深度学习的方法都是数据驱动的,在协同感知方面,数据集目前不算特别多,主要存在1)真实数据较少2)不同数据集之间配置差异较大,需要自行修改等. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/05/28/XfzY8nQyI3adCNM.png property=og:image><meta content=https://s2.loli.net/2024/03/22/3dLmHN96vbuVBtY.png property=og:image><meta content=https://s2.loli.net/2024/05/24/43wQjzbeqUEdXTs.png property=og:image><meta content=https://s2.loli.net/2024/07/11/ldsiTMrox2hv6BQ.png property=og:image><meta content=https://s2.loli.net/2024/07/11/ChPesgXV5taR8BJ.png property=og:image><meta content=https://s2.loli.net/2024/07/12/TEs9YUIDhi7dkaR.png property=og:image><meta content=2024-05-23T02:10:14.000Z property=article:published_time><meta content=2024-09-03T07:31:26.385Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="collaborative perception" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/05/28/XfzY8nQyI3adCNM.png name=twitter:image><link href=https://www.sekyoro.top/2024/05/23/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>协同感知数据集和代码库介绍 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/05/23/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>协同感知数据集和代码库介绍</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-05-23 10:10:14" datetime=2024-05-23T10:10:14+08:00>2024-05-23</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-09-03 15:31:26" datetime=2024-09-03T15:31:26+08:00 itemprop=dateModified>2024-09-03</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>27k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>24 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>目前深度学习的方法都是数据驱动的,在协同感知方面,数据集目前不算特别多,主要存在1)真实数据较少2)不同数据集之间配置差异较大,需要自行修改等.</p><span id=more></span><h2 id=协同感知数据集><a class=headerlink href=#协同感知数据集 title=协同感知数据集></a>协同感知数据集</h2><p><img alt=image-20240528181153135 data-src=https://s2.loli.net/2024/05/28/XfzY8nQyI3adCNM.png><p>目前找到的数据集还是不少的.<h3 id=OPV2V-2022><a title="OPV2V 2022" class=headerlink href=#OPV2V-2022></a>OPV2V 2022</h3><p>推出了首个大规模开放式车对车感知模拟数据集。该数据集包含 <strong>70 多个场景、11,464 个帧和 232,913 个注释三维车辆边界框</strong>，<strong>收集自 CARLA 的 8 个城镇和洛杉矶卡尔弗城的一个数字城镇</strong>。然后构建了一个包含 16 个实施模型的综合基准，以评估几种信息融合策略（即早期、后期和中间融合）与最先进的激光雷达检测算法。<h3 id=V2XSet-2022><a title="V2XSet 2022" class=headerlink href=#V2XSet-2022></a>V2XSet 2022</h3><p>研究了如何应用 “车对物”（V2X）通信来提高自动驾驶汽车的感知性能。我们利用新颖的视觉转换器（Vision Transformer）提出了一个具有 V2X 通信功能的稳健合作感知框架。具体来说，我们建立了一个整体注意力模型，即 V2X-ViT，以有效融合道路代理（即车辆和基础设施）之间的信息。V2X-ViT 由异构多代理自我注意和多尺度窗口自我注意交替层组成，可捕捉代理间的交互和每个代理的空间关系。这些关键模块采用统一的 Transformer 架构设计，以应对常见的 V2X 挑战，包括异步信息共享、姿势错误和 V2X 组件的异构性。<p><img alt=image-20240322144729411 data-src=https://s2.loli.net/2024/03/22/3dLmHN96vbuVBtY.png><p>车与道路 CARLA和OPENCDA创建的模拟数据集<h3 id=DAIR-V2X-2022><a title="DAIR-V2X 2022" class=headerlink href=#DAIR-V2X-2022></a>DAIR-V2X 2022</h3><p>为了加速车辆-基础设施协同自动驾驶（VICAD）的计算机视觉研究和创新，我们发布了 DAIR-V2X 数据集,这是<strong>首个用于 VICAD 的大规模、多模态、多视角真实场景数据集</strong>。<h3 id=V2X-Sim-2022><a title="V2X-Sim 2022" class=headerlink href=#V2X-Sim-2022></a>V2X-Sim 2022</h3><p>车对物（V2X）通信技术实现了车辆与邻近环境中许多其他实体之间的协作，可以从根本上改善自动驾驶的感知系统。然而，公共数据集的缺乏极大地限制了协同感知的研究进展。为了填补这一空白，我们提出了 V2X-Sim—一个<strong>用于 V2X 辅助自动驾驶的综合模拟多代理感知数据集</strong>。V2XSim 提供：（1）来自路边装置（RSU）和多辆车的多代理传感器记录，可实现协同感知；（2）多模态传感器流，可促进多模态感知；以及（3）多种地面实况，可支持各种感知任务。同时，我们建立了一个开源测试平台，并在检测、跟踪和分割等三个任务上为最先进的协同感知算法提供了基准。V2X-Sim 试图在现实数据集广泛可用之前，促进自动驾驶的协同感知研究。<h3 id=V2V4Real-2023><a title="V2V4Real 2023" class=headerlink href=#V2V4Real-2023></a>V2V4Real 2023</h3><p>最近的研究表明，车对车（V2V）协同感知系统在彻底改变自动驾驶行业方面具有巨大潜力。然而，真实世界数据集的缺乏阻碍了这一领域的发展。为了促进协同感知的发展，我们提出了 V2V4Real，这是首个<strong>大规模真实世界多模态 V2V 感知数据集</strong>。<h4 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h4><p>现代自动驾驶感知协同对于遮挡敏感并且缺乏长范围感知能力,这是阻碍五级自动驾驶的关键瓶颈之一.<p>最近的研究表明，<strong>车对车（V2V）协作感知系统具有彻底改变自动驾驶行业的巨大潜力</strong>。然而，<strong>缺乏真实世界的数据集阻碍了该领域的进步。为了促进合作感知的发展，我们提出了 V2V4Real</strong>，这是第一个用于 V2V 感知的大规模现实世界多模态数据集。这些数据是由两辆配备多模态传感器的车辆在不同场景中一起行驶收集的。我们的 V2V4Real 数据集覆盖了 <strong>410 公里的驾驶区域，包括 20K LiDAR 帧、40K RGB 帧、240K 5 个类别的带注释 3D 边界框以及覆盖所有驾驶路线的 HDMap</strong>。<blockquote><p>自动驾驶技术在国际上有一个严格的分级标准，而美国交通部选择的是美国汽车工程师学会(Society of Automotive Engineers)给出的评定标准，其主要内容是：<p>0级：无自动驾驶，由人类驾驶员全权操控汽车，可以得到警告或干预系统的辅助;<p>1级：驾驶支援，通过驾驶环境对方向盘和加减速中的一项操作提供驾驶支持，其他的驾驶动作都由人类驾驶员进行操作;<p>2级：部分自动化，通过驾驶环境对方向盘和加减速中的多项操作提供驾驶支持，其他的驾驶动作都由人类驾驶员进行操作。<p>3级：有条件自动化，由自动驾驶系统完成所有的驾驶操作。根据系统要求，人类驾驶者需要在适当的时候提供应答。<p>4级：高度自动化，由自动驾驶系统完成所有的驾驶操作。根据系统要求，人类驾驶者不一定需要对所有的系统请求做出应答，包括限定道路和环境条件等。<p>5级：完全自动化，在所有人类驾驶者可以应付的道路和环境条件下，均可以由自动驾驶系统自主完成所有的驾驶操作。</blockquote><p>V2V4Real 引入了三个感知任务，包括协作 3D 对象检测、协作 3D 对象跟踪和用于协作感知的 Sim2Real 域自适应。我们提供了近期协作感知算法在三个任务上的综合基准。 V2V4Real 数据集和代码库可以在 Research.seas.ucla.edu/mobility-lab/v2v4real 上找到。<p>我们收集了 19 个小时的 310K 帧的驾驶数据。我们手动选择最具代表性的 67 个场景，每个场景时长 10-20 秒。我们以 10Hz 的频率对帧进行采样，得到总共 20K 帧的 LiDAR 点云和 40K 帧的 RGB 图像。对于每个场景，我们确保两辆车传感器系统之间的异步小于 50 毫秒。所有场景都与包含可行驶区域、道路边界以及虚线的地图对齐。<ul><li>协作者的相对位姿存在不可避免的误差，这在将数据转换到统一坐标系时会产生全局错位。<li>协作者的传感器测量通常不同步，这是由不同传感器系统的异步以及数据传输过程中的通信延迟造成的<li>典型的V2V通信技术需要有限的带宽，这限制了传输数据的大小[31,40,49]。因此，协作检测算法必须考虑精度和带宽要求之间的权衡</ul><p><strong>训练细节.</strong>对于所有三个任务，数据集分为训练/验证/测试集，分别具有 <strong>14,210/2,000/3,986 帧</strong>。所有检测模型均采用 PointPillar 作为骨干从点云中提取 2D 特征。我们用 60 个 epoch 训练所有模型，每个 GPU (RTX3090) 的批量大小为 4，学习率为 0.001，并通过余弦退火来衰减学习率。早期停止用于寻找最佳时期。我们还为所有实验添加了正常的点云数据增强，包括缩放、旋转和翻转。我们使用权重衰减为 1×10−2 的 AdamW来优化我们的模型。对于跟踪任务，我们将前 3 帧和当前帧一起作为输入.<h3 id=RCooper-2024><a title="RCooper  2024" class=headerlink href=#RCooper-2024></a>RCooper 2024</h3><p>近年来，路侧感知的价值逐渐凸显并得到认可，它可以延伸自动驾驶和交通管理的边界。<p>然而，现有的路侧感知方法仅针对单一基础设施的传感器系统，由于感知范围和盲区的限制，无法实现对交通区域的全面理解。面向高质量的路侧感知，我们需要路侧协同感知( Roadside Cooperative Perception，RCooper )来实现面向受限交通区域的实际区域覆盖路侧感知。Rcooper有其特定领域的挑战，但由于缺乏数据集而阻碍了进一步的探索。<p><img alt=image-20240524101039784 data-src=https://s2.loli.net/2024/05/24/43wQjzbeqUEdXTs.png><p>因此，我们发布了第一个真实世界的大规模RCooper数据集，以启动对实际路边协作感知的研究，包括检测和跟踪。人工标注的数据集包括50k幅图像和30k个点云，其中包含两个具有代表性的交通场景(即,交叉口和走廊)。所构建的基准证明了路侧合作感知的有效性，并展示了进一步研究的方向。<h3 id=TUMTraf-V2X-2024><a title="TUMTraf-V2X 2024" class=headerlink href=#TUMTraf-V2X-2024></a>TUMTraf-V2X 2024</h3><p><a href=https://tum-traffic-dataset.github.io/tumtraf-v2x/ rel=noopener target=_blank>TUMTraf V2X Cooperative Perception Dataset (tum-traffic-dataset.github.io)</a><p>协作感知为增强自动驾驶车辆的能力和改善道路安全提供了许多好处。除了车载传感器外，使用路边传感器增加了可靠性，并扩展了传感器的范围。<p>数据集包含2，000个标记点云和5，000张来自5个路侧和4个机载传感器的标记图像。它包括30k个带有轨道ID和精确的GPS和IMU数据的3D盒子。标注了八个类别，涵盖了具有挑战性的驾驶操作的遮挡场景，如交通违规、接近失误事件、超车和掉头。<h3 id=V2X-Real-2024><a title="V2X-Real 2024" class=headerlink href=#V2X-Real-2024></a>V2X-Real 2024</h3><p><a href=https://mobility-lab.seas.ucla.edu/v2x-real/ rel=noopener target=_blank>V2X-real | Jiaqi Ma | UCLA Mobility Lab</a><p>近年来，随着车联网( Vehicle-to-Ething，V2X )技术的发展，自动驾驶车辆能够共享感知信息以穿透遮挡，极大地提升了感知能力。<p>在本文中，我们提出了一个同时具有多种车辆和智能基础设施的混合数据集，以促进具有多模态感知数据的V2X协作感知开发。我们的V2X - Real使用两个连接的自动化车辆和两个智能基础设施进行采集，这些基础设施都配备了包括激光雷达传感器和多视角相机在内的多模态传感器。<p>整个数据集包含33K个LiDAR帧和171K个相机数据，在非常具有挑战性的城市场景中，有超过1.2 M的10个类别的注释边界框。根据协作模式和自我视角，我们推导出车辆中心、基础设施中心、车辆到车辆和基础设施到基础设施协作感知的四类数据集。 主要是包含了I2I的场景.<blockquote><p>上面三个数据集都是V2X的真实环境数据</blockquote><h2 id=CodeBase><a class=headerlink href=#CodeBase title=CodeBase></a>CodeBase</h2><p>我常用的就是OpenCOOD <a href=https://github.com/DerrickXuNu/OpenCOOD?tab=readme-ov-file rel=noopener target=_blank>ICRA 2022] An opensource framework for cooperative detection. Official implementation for OPV2V. (github.com)</a>的代码库,但也有其他看起来还行的,包括:<ul><li><a href=https://github.com/ucla-mobility/V2V4Real rel=noopener target=_blank>CVPR2023 Highlight] The official codebase for paper “V2V4Real: A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception” (github.com)</a><li><a href=https://github.com/yifanlu0227/HEAL rel=noopener target=_blank>ICLR2024] HEAL: An Extensible Framework for Open Heterogeneous Collaborative Perception ➡️ All You Need for Multi-Modality Collaborative Perception! (github.com)</a><li><a href=https://github.com/yifanlu0227/CoAlign/tree/main rel=noopener target=_blank>ICRA2023] CoAlign: Robust Collaborative 3D Object Detection in Presence of Pose Errors (github.com)</a><li><a href=https://github.com/AIR-THU/DAIR-V2X rel=noopener target=_blank>AIR-THU/DAIR-V2X (github.com)</a><li><a href=https://github.com/coperception/coperception rel=noopener target=_blank>coperception/coperception: An SDK for multi-agent collaborative perception. (github.com)</a> 这个仓库代码其实不太行 😅</ul><p>这里对其中一些代码做简单介绍.<h3 id=BaseDataset><a class=headerlink href=#BaseDataset title=BaseDataset></a>BaseDataset</h3><p>不同时期不同类型数据进行融合的父类.数据中通过idx得到对应的场景中数据<p>这个类会根据配置添加噪声并设置max_cav,添加root_dir中的文件夹scenario_folders.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br></pre><td class=code><pre><span class=line><span class=keyword>if</span> <span class=string>'wild_setting'</span> <span class=keyword>in</span> params:</span><br><span class=line>    self.seed = params[<span class=string>'wild_setting'</span>][<span class=string>'seed'</span>]</span><br><span class=line>    <span class=comment># whether to add time delay</span></span><br><span class=line>    self.async_flag = params[<span class=string>'wild_setting'</span>][<span class=string>'async'</span>]</span><br><span class=line>    self.async_mode = \</span><br><span class=line>        <span class=string>'sim'</span> <span class=keyword>if</span> <span class=string>'async_mode'</span> <span class=keyword>not</span> <span class=keyword>in</span> params[<span class=string>'wild_setting'</span>] \</span><br><span class=line>            <span class=keyword>else</span> params[<span class=string>'wild_setting'</span>][<span class=string>'async_mode'</span>]</span><br><span class=line>    self.async_overhead = params[<span class=string>'wild_setting'</span>][<span class=string>'async_overhead'</span>]</span><br><span class=line></span><br><span class=line>    <span class=comment># localization error</span></span><br><span class=line>    self.loc_err_flag = params[<span class=string>'wild_setting'</span>][<span class=string>'loc_err'</span>]</span><br><span class=line>    self.xyz_noise_std = params[<span class=string>'wild_setting'</span>][<span class=string>'xyz_std'</span>]</span><br><span class=line>    self.ryp_noise_std = params[<span class=string>'wild_setting'</span>][<span class=string>'ryp_std'</span>]</span><br><span class=line></span><br><span class=line>    <span class=comment># transmission data size</span></span><br><span class=line>    self.data_size = \</span><br><span class=line>        params[<span class=string>'wild_setting'</span>][<span class=string>'data_size'</span>] \</span><br><span class=line>            <span class=keyword>if</span> <span class=string>'data_size'</span> <span class=keyword>in</span> params[<span class=string>'wild_setting'</span>] <span class=keyword>else</span> <span class=number>0</span></span><br><span class=line>    self.transmission_speed = \</span><br><span class=line>        params[<span class=string>'wild_setting'</span>][<span class=string>'transmission_speed'</span>] \</span><br><span class=line>            <span class=keyword>if</span> <span class=string>'transmission_speed'</span> <span class=keyword>in</span> params[<span class=string>'wild_setting'</span>] <span class=keyword>else</span> <span class=number>27</span></span><br><span class=line>    self.backbone_delay = \</span><br><span class=line>        params[<span class=string>'wild_setting'</span>][<span class=string>'backbone_delay'</span>] \</span><br><span class=line>            <span class=keyword>if</span> <span class=string>'backbone_delay'</span> <span class=keyword>in</span> params[<span class=string>'wild_setting'</span>] <span class=keyword>else</span> <span class=number>0</span></span><br></pre></table></figure><p>遍历每个场景文件夹,每个场景对应scenario_database,每个场景下有若干agent,如果其中的infra在第一个,那就放在最后.对场景中所有agent遍历,如果多余max_cav跳出.<p>scenario_database中的一个场景的每个agent对应一个有序字典,然后读取对应的yaml和pcd文件. 一个场景下有多个时间点(timestamp),每个时间点上有许多数据,包括lidar,camera. 将第一个代理作为ego,设置其ego值为True并在len_record上添加对应的时间节点(一个场景的时间长度就是一个场景中采样了多少个时间点),其余代理ego属性设置为False. 这就是BaseData的初始化过程,它的其他类会用于子类进行调用.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> timestamp <span class=keyword>in</span> timestamps:</span><br><span class=line>    self.scenario_database[i][cav_id][timestamp] = \</span><br><span class=line>        OrderedDict()</span><br><span class=line></span><br><span class=line>    yaml_file = os.path.join(cav_path,</span><br><span class=line>                             timestamp + <span class=string>'.yaml'</span>)</span><br><span class=line>    lidar_file = os.path.join(cav_path,</span><br><span class=line>                              timestamp + <span class=string>'.pcd'</span>)</span><br><span class=line>    camera_files = self.load_camera_files(cav_path, timestamp)</span><br><span class=line></span><br><span class=line>    self.scenario_database[i][cav_id][timestamp][<span class=string>'yaml'</span>] = \</span><br><span class=line>        yaml_file</span><br><span class=line>    self.scenario_database[i][cav_id][timestamp][<span class=string>'lidar'</span>] = \</span><br><span class=line>        lidar_file</span><br><span class=line>    self.scenario_database[i][cav_id][timestamp][<span class=string>'camera0'</span>] = \</span><br><span class=line>        camera_files</span><br><span class=line><span class=comment># Assume all cavs will have the same timestamps length. Thus</span></span><br><span class=line><span class=comment># we only need to calculate for the first vehicle in the</span></span><br><span class=line><span class=comment># scene.</span></span><br><span class=line><span class=keyword>if</span> j == <span class=number>0</span>:</span><br><span class=line>    <span class=comment># we regard the agent with the minimum id as the ego</span></span><br><span class=line>    self.scenario_database[i][cav_id][<span class=string>'ego'</span>] = <span class=literal>True</span></span><br><span class=line>    <span class=keyword>if</span> <span class=keyword>not</span> self.len_record:</span><br><span class=line>        self.len_record.append(<span class=built_in>len</span>(timestamps))</span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        prev_last = self.len_record[-<span class=number>1</span>]</span><br><span class=line>        self.len_record.append(prev_last + <span class=built_in>len</span>(timestamps))</span><br><span class=line><span class=keyword>else</span>:</span><br><span class=line>    self.scenario_database[i][cav_id][<span class=string>'ego'</span>] = <span class=literal>False</span></span><br></pre></table></figure><p>BaseDataset有collate_batch_train,用于dataloader将多个batch进行处理.它会将每个batch中的数据放在一起,并将processed_lidar_list从一个长度为batch_size的每个元素为一个dict的列表转为一个dict,dict中每个value是一个np array,并将lable_dict转为如下<figure class="highlight sqf"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>{<span class=string>'targets'</span>: <span class=built_in>targets</span>,</span><br><span class=line>        <span class=string>'pos_equal_one'</span>: pos_equal_one,</span><br><span class=line>        <span class=string>'neg_equal_one'</span>: neg_equal_one}</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(batch)):</span><br><span class=line>        ego_dict = batch[i][<span class=string>'ego'</span>]</span><br><span class=line>        object_bbx_center.append(ego_dict[<span class=string>'object_bbx_center'</span>])</span><br><span class=line>        object_bbx_mask.append(ego_dict[<span class=string>'object_bbx_mask'</span>])</span><br><span class=line>        processed_lidar_list.append(ego_dict[<span class=string>'processed_lidar'</span>])</span><br><span class=line>        label_dict_list.append(ego_dict[<span class=string>'label_dict'</span>])</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> self.visualize:</span><br><span class=line>            origin_lidar.append(ego_dict[<span class=string>'origin_lidar'</span>])</span><br><span class=line></span><br><span class=line>    <span class=comment># convert to numpy, (B, max_num, 7)</span></span><br><span class=line>    object_bbx_center = torch.from_numpy(np.array(object_bbx_center))</span><br><span class=line>    object_bbx_mask = torch.from_numpy(np.array(object_bbx_mask))</span><br><span class=line></span><br><span class=line>    processed_lidar_torch_dict = \</span><br><span class=line>        self.pre_processor.collate_batch(processed_lidar_list)</span><br><span class=line>    label_torch_dict = \</span><br><span class=line>        self.post_processor.collate_batch(label_dict_list)</span><br><span class=line>    output_dict[<span class=string>'ego'</span>].update({<span class=string>'object_bbx_center'</span>: object_bbx_center,</span><br><span class=line>                               <span class=string>'object_bbx_mask'</span>: object_bbx_mask,</span><br><span class=line>                               <span class=string>'processed_lidar'</span>: processed_lidar_torch_dict,</span><br><span class=line>                               <span class=string>'label_dict'</span>: label_torch_dict})</span><br></pre></table></figure><p><img alt=Untitled-2024-07-11-1611 data-src=https://s2.loli.net/2024/07/11/ldsiTMrox2hv6BQ.png style=zoom:67%;><p><img style="zoom: 50%;" alt=Untitled-2024-07-11-1611 data-src=https://s2.loli.net/2024/07/11/ChPesgXV5taR8BJ.png><h3 id=不同数据类型融合><a class=headerlink href=#不同数据类型融合 title=不同数据类型融合></a>不同数据类型融合</h3><h4 id=早期融合><a class=headerlink href=#早期融合 title=早期融合></a>早期融合</h4><p>早期融合时每个代理直接传递点云数据,预处理步骤在ego代理.<p>子类在初始化时都会构建pre<em>processor和post<em>processor.在`__get_item</em></em><code>中拿到每个数据. 首先会调用</code>retrieve_base_data`,这就是父类的方法.根据传入的idx,从len_record中得到对应的scenario_index,从self.scenario_database列表中得到对应的scenario_database,其包括这个场景下所有的代理以及每个代理在每个timestamp下的yaml,lidar,camera0文件. 然后根据scenario_index获得timestamp_index,也就是在某个场景下的相对于这个场景的timestamp_index.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line>scenario_index = <span class=number>0</span></span><br><span class=line><span class=keyword>for</span> i, ele <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.len_record):</span><br><span class=line>    <span class=keyword>if</span> idx < ele:</span><br><span class=line>        scenario_index = i</span><br><span class=line>        <span class=keyword>break</span></span><br><span class=line>scenario_database = self.scenario_database[scenario_index]</span><br><span class=line></span><br><span class=line><span class=comment># check the timestamp index</span></span><br><span class=line>timestamp_index = idx <span class=keyword>if</span> scenario_index == <span class=number>0</span> <span class=keyword>else</span> \</span><br><span class=line>    idx - self.len_record[scenario_index - <span class=number>1</span>]</span><br><span class=line><span class=comment># retrieve the corresponding timestamp key</span></span><br><span class=line>timestamp_key = self.return_timestamp_key(scenario_database,</span><br><span class=line>                                          timestamp_index)</span><br></pre></table></figure><p>接着计算每个代理与ego代理的相对距离,首先遍历上面拿到的scenario_database拿到ego代理的数据,也就输它的yaml文件中的<code>lidar_pose</code>,分别表示代理的x,y,z,yaw,roll,pitch数据.然后更新了所有代理的cav_content,增加了<code>distance_to_ego</code>,然后遍历scenario_database,copy cav_content[‘ego’],计算delay,如果是ego就没有delay,否则根据配置信息,如果async_mode是real,从0-async_overhead均匀分布采样一个值得到一个时间加上data_size/speed的延迟.如果是sim就直接取async_overhead.如果async_flag是false就是0.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>calc_dist_to_ego</span>(<span class=params>self, scenario_database, timestamp_key</span>):</span></span><br><span class=line>    <span class=string>"""</span></span><br><span class=line><span class=string>    Calculate the distance to ego for each cav.</span></span><br><span class=line><span class=string>    """</span></span><br><span class=line>    ego_lidar_pose = <span class=literal>None</span></span><br><span class=line>    ego_cav_content = <span class=literal>None</span></span><br><span class=line>    <span class=comment># Find ego pose first</span></span><br><span class=line>    <span class=keyword>for</span> cav_id, cav_content <span class=keyword>in</span> scenario_database.items():</span><br><span class=line>        <span class=keyword>if</span> cav_content[<span class=string>'ego'</span>]:</span><br><span class=line>            ego_cav_content = cav_content</span><br><span class=line>            ego_lidar_pose = \</span><br><span class=line>                load_yaml(cav_content[timestamp_key][<span class=string>'yaml'</span>])[<span class=string>'lidar_pose'</span>]</span><br><span class=line>            <span class=keyword>break</span></span><br><span class=line></span><br><span class=line>    <span class=keyword>assert</span> ego_lidar_pose <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span></span><br><span class=line></span><br><span class=line>    <span class=comment># calculate the distance</span></span><br><span class=line>    <span class=keyword>for</span> cav_id, cav_content <span class=keyword>in</span> scenario_database.items():</span><br><span class=line>        cur_lidar_pose = \</span><br><span class=line>            load_yaml(cav_content[timestamp_key][<span class=string>'yaml'</span>])[<span class=string>'lidar_pose'</span>]</span><br><span class=line>        distance = \</span><br><span class=line>            math.sqrt((cur_lidar_pose[<span class=number>0</span>] -</span><br><span class=line>                       ego_lidar_pose[<span class=number>0</span>]) ** <span class=number>2</span> +</span><br><span class=line>                      (cur_lidar_pose[<span class=number>1</span>] - ego_lidar_pose[<span class=number>1</span>]) ** <span class=number>2</span>)</span><br><span class=line>        cav_content[<span class=string>'distance_to_ego'</span>] = distance</span><br><span class=line>        scenario_database.update({cav_id: cav_content})</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> ego_cav_content</span><br></pre></table></figure><p>计算出timestamp_delay之后根据延迟时间计算应该获取数据的timestamp并更新cav的time_delay. 然后进行数据转换<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br></pre><td class=code><pre><span class=line>data = OrderedDict()</span><br><span class=line><span class=comment># load files for all CAVs</span></span><br><span class=line><span class=keyword>for</span> cav_id, cav_content <span class=keyword>in</span> scenario_database.items():</span><br><span class=line>    data[cav_id] = OrderedDict()</span><br><span class=line>    data[cav_id][<span class=string>'ego'</span>] = cav_content[<span class=string>'ego'</span>]</span><br><span class=line></span><br><span class=line>    <span class=comment># calculate delay for this vehicle</span></span><br><span class=line>    timestamp_delay = \</span><br><span class=line>        self.time_delay_calculation(cav_content[<span class=string>'ego'</span>])</span><br><span class=line></span><br><span class=line>    <span class=keyword>if</span> timestamp_index - timestamp_delay <= <span class=number>0</span>:</span><br><span class=line>        timestamp_delay = timestamp_index</span><br><span class=line>    timestamp_index_delay = <span class=built_in>max</span>(<span class=number>0</span>, timestamp_index - timestamp_delay)</span><br><span class=line>    timestamp_key_delay = self.return_timestamp_key(scenario_database,</span><br><span class=line>                                                    timestamp_index_delay)</span><br><span class=line>    <span class=comment># add time delay to vehicle parameters</span></span><br><span class=line>    data[cav_id][<span class=string>'time_delay'</span>] = timestamp_delay</span><br><span class=line>    <span class=comment># load the corresponding data into the dictionary</span></span><br><span class=line>    data[cav_id][<span class=string>'params'</span>] = self.reform_param(cav_content,</span><br><span class=line>                                               ego_cav_content,</span><br><span class=line>                                               timestamp_key,</span><br><span class=line>                                               timestamp_key_delay,</span><br><span class=line>                                               cur_ego_pose_flag)</span><br><span class=line>    data[cav_id][<span class=string>'lidar_np'</span>] = \</span><br><span class=line>        pcd_utils.pcd_to_np(cav_content[timestamp_key_delay][<span class=string>'lidar'</span>])</span><br><span class=line><span class=keyword>return</span> data</span><br></pre></table></figure><p>根据cav_content,ego_content,timestamp_key以及delay时的key,还有一个cur_ego_pose_flag表示是否似乎用当前的ego位姿计算transformation matrix.<p>首先分别拿到cav、ego的yaml数据,以及延迟时的数据,如果不是ego代理并且设置了loc_err就加上噪声.如果设置了cur_ego_pose_flag为True,计算transformation_matrix<p>计算方式如下,就是将x1在6Dof下转换到x2. transformation_matrix其实就是延迟的cav数据转换到现在的ego数据上的转换矩阵. 这个矩阵无非是一个转换关系,spatial_correction_matrix是相等转换. 如果不使用现在的ego位姿,transformation_matrix就是延迟的cav位姿到延迟的ego位姿,此外spatial_correction_matrix需要负责将延迟的ego位姿转换到现在的ego位姿. 这些矩阵转换可以用来转换点云. 此外需要计算现在的cav到ego的位姿转换用于后期融合,将得到的其他cav的检测框转换到ego坐标系下.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=comment># This is only used for late fusion, as it did the transformation</span></span><br><span class=line><span class=comment># in the postprocess, so we want the gt object transformation use</span></span><br><span class=line><span class=comment># the correct one</span></span><br><span class=line>gt_transformation_matrix = x1_to_x2(cur_cav_lidar_pose,</span><br><span class=line>                                cur_ego_lidar_pose)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>x_to_world</span>(<span class=params>pose</span>):</span></span><br><span class=line>    x, y, z, roll, yaw, pitch = pose[:]</span><br><span class=line>    <span class=comment># used for rotation matrix</span></span><br><span class=line>    c_y = np.cos(np.radians(yaw))</span><br><span class=line>    s_y = np.sin(np.radians(yaw))</span><br><span class=line>    c_r = np.cos(np.radians(roll))</span><br><span class=line>    s_r = np.sin(np.radians(roll))</span><br><span class=line>    c_p = np.cos(np.radians(pitch))</span><br><span class=line>    s_p = np.sin(np.radians(pitch))</span><br><span class=line>    matrix = np.identity(<span class=number>4</span>)</span><br><span class=line>    <span class=comment># translation matrix</span></span><br><span class=line>    matrix[<span class=number>0</span>, <span class=number>3</span>] = x</span><br><span class=line>    matrix[<span class=number>1</span>, <span class=number>3</span>] = y</span><br><span class=line>    matrix[<span class=number>2</span>, <span class=number>3</span>] = z</span><br><span class=line>    <span class=comment># rotation matrix</span></span><br><span class=line>    matrix[<span class=number>0</span>, <span class=number>0</span>] = c_p * c_y</span><br><span class=line>    matrix[<span class=number>0</span>, <span class=number>1</span>] = c_y * s_p * s_r - s_y * c_r</span><br><span class=line>    matrix[<span class=number>0</span>, <span class=number>2</span>] = -c_y * s_p * c_r - s_y * s_r</span><br><span class=line>    matrix[<span class=number>1</span>, <span class=number>0</span>] = s_y * c_p</span><br><span class=line>    matrix[<span class=number>1</span>, <span class=number>1</span>] = s_y * s_p * s_r + c_y * c_r</span><br><span class=line>    matrix[<span class=number>1</span>, <span class=number>2</span>] = -s_y * s_p * c_r + c_y * s_r</span><br><span class=line>    matrix[<span class=number>2</span>, <span class=number>0</span>] = s_p</span><br><span class=line>    matrix[<span class=number>2</span>, <span class=number>1</span>] = -c_p * s_r</span><br><span class=line>    matrix[<span class=number>2</span>, <span class=number>2</span>] = c_p * c_r</span><br><span class=line>    <span class=keyword>return</span> matrix</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>x1_to_x2</span>(<span class=params>x1, x2</span>):</span></span><br><span class=line>    x1 : <span class=built_in>list</span></span><br><span class=line>        The pose of x1 under world coordinates.</span><br><span class=line>    x2 : <span class=built_in>list</span></span><br><span class=line>        The pose of x2 under world coordinates.</span><br><span class=line></span><br><span class=line>    Returns</span><br><span class=line>    -------</span><br><span class=line>    transformation_matrix : np.ndarray</span><br><span class=line>        The transformation matrix.</span><br><span class=line></span><br><span class=line>    <span class=string>"""</span></span><br><span class=line><span class=string>    x1_to_world = x_to_world(x1)</span></span><br><span class=line><span class=string>    x2_to_world = x_to_world(x2)</span></span><br><span class=line><span class=string>    world_to_x2 = np.linalg.inv(x2_to_world)</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>    transformation_matrix = np.dot(world_to_x2, x1_to_world)</span></span><br><span class=line><span class=string>    return transformation_matrix</span></span><br></pre></table></figure><p>然后把上面数据存在delay_params中<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line><span class=comment># we always use current timestamp's gt bbx to gain a fair evaluation</span></span><br><span class=line>delay_params[<span class=string>'vehicles'</span>] = cur_params[<span class=string>'vehicles'</span>] <span class=comment>#周围cav的数据</span></span><br><span class=line>delay_params[<span class=string>'transformation_matrix'</span>] = transformation_matrix</span><br><span class=line>delay_params[<span class=string>'gt_transformation_matrix'</span>] = \</span><br><span class=line>gt_transformation_matrix</span><br><span class=line>delay_params[<span class=string>'spatial_correction_matrix'</span>] = spatial_correction_matrix</span><br></pre></table></figure><p>然后得到lidar点云数据lidar_np,只使用点云的第一个通道的值<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>pcd_to_np</span>(<span class=params>pcd_file</span>):</span></span><br><span class=line>    pcd = o3d.io.read_point_cloud(pcd_file)</span><br><span class=line>    xyz = np.asarray(pcd.points)</span><br><span class=line>    <span class=comment># we save the intensity in the first channel</span></span><br><span class=line>    intensity = np.expand_dims(np.asarray(pcd.colors)[:, <span class=number>0</span>], -<span class=number>1</span>)</span><br><span class=line>    pcd_np = np.hstack((xyz, intensity))</span><br><span class=line>    <span class=keyword>return</span> np.asarray(pcd_np, dtype=np.float32) </span><br><span class=line>	<span class=comment># shape like (56423, 4)</span></span><br></pre></table></figure><p>这下终于有了base_data_dict,其包括这个场景下的一个timestamp的所有代理的延迟time_delay,params(包括转换矩阵和vehicles)以及lidar数据(numpy类型).<p>然后再遍历base_data拿到ego_id,然后再遍历排除距离大于某个值的代理,再获取那个代理的信息<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> cav_id, selected_cav_base <span class=keyword>in</span> base_data_dict.items():</span><br><span class=line>        <span class=comment># check if the cav is within the communication range with ego</span></span><br><span class=line>        distance = \</span><br><span class=line>            math.sqrt((selected_cav_base[<span class=string>'params'</span>][<span class=string>'lidar_pose'</span>][<span class=number>0</span>] -</span><br><span class=line>                       ego_lidar_pose[<span class=number>0</span>]) ** <span class=number>2</span> + (</span><br><span class=line>                              selected_cav_base[<span class=string>'params'</span>][</span><br><span class=line>                                  <span class=string>'lidar_pose'</span>][<span class=number>1</span>] - ego_lidar_pose[</span><br><span class=line>                                  <span class=number>1</span>]) ** <span class=number>2</span>)</span><br><span class=line>        <span class=keyword>if</span> distance > opencood.data_utils.datasets.COM_RANGE:</span><br><span class=line>            <span class=keyword>continue</span></span><br><span class=line></span><br><span class=line>        selected_cav_processed = self.get_item_single_car(</span><br><span class=line>            selected_cav_base,</span><br><span class=line>            ego_lidar_pose)</span><br><span class=line>        <span class=comment># all these lidar and object coordinates are projected to ego</span></span><br><span class=line>        <span class=comment># already.</span></span><br><span class=line>        projected_lidar_stack.append(</span><br><span class=line>            selected_cav_processed[<span class=string>'projected_lidar'</span>])</span><br><span class=line>        object_stack.append(selected_cav_processed[<span class=string>'object_bbx_center'</span>])</span><br><span class=line>        object_id_stack += selected_cav_processed[<span class=string>'object_ids'</span>]</span><br></pre></table></figure><p>根据ego_pose,计算transformation_matrix,根据后处理器生成物体中心 ,利用project_world_objects将物体在世界坐标系投影到ego坐标系中,每个物体位姿计算如下,这里的每个物体都是每个代理的周围代理vehicles(surrounding vehicles that have at least one LiDAR point hit from the agent),将周围的车的lidar投影到ego上.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>location = object_content[<span class=string>'location'</span>]<span class=comment># 世界坐标系下frontal axis中心的坐标</span></span><br><span class=line>rotation = object_content[<span class=string>'angle'</span>]<span class=comment># roll,yaw,pitch 世界坐标系 </span></span><br><span class=line>center = object_content[<span class=string>'center'</span>] <span class=comment># bbx中心相对于frontal axis中心位置偏离</span></span><br><span class=line>extent = object_content[<span class=string>'extent'</span>]<span class=comment># half length,width and height</span></span><br><span class=line></span><br><span class=line>object_pose = [location[<span class=number>0</span>] + center[<span class=number>0</span>],</span><br><span class=line>           location[<span class=number>1</span>] + center[<span class=number>1</span>],</span><br><span class=line>           location[<span class=number>2</span>] + center[<span class=number>2</span>],</span><br><span class=line>           rotation[<span class=number>0</span>], rotation[<span class=number>1</span>], rotation[<span class=number>2</span>]]</span><br></pre></table></figure><p>所以其实就是同个场景下所有被lidar打到的周围车辆放在了output_dict,根据配置max_num(最多的anchors)数,遍历output_dict将物体坐标存到object_np中,以及mask和物体的id得到object_bbx_center, object_bbx_mask, object_ids.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>object_np = np.zeros((self.params[<span class=string>'max_num'</span>], <span class=number>7</span>))</span><br><span class=line>mask = np.zeros(self.params[<span class=string>'max_num'</span>])</span><br><span class=line>object_ids = []</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> i, (object_id, object_bbx) <span class=keyword>in</span> <span class=built_in>enumerate</span>(output_dict.items()):</span><br><span class=line>    object_np[i] = object_bbx[<span class=number>0</span>, :] <span class=comment># object_bbx [1,7]</span></span><br><span class=line>    mask[i] = <span class=number>1</span></span><br><span class=line>    object_ids.append(object_id)</span><br></pre></table></figure><p>然后进行random打乱以及mask掉打到自己的点.最后将点云投射到ego坐标系中.最后返回这个代理检测到的代理的坐标(N,7),检测到的代理id以及经过投影的点云.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>selected_cav_processed.update(</span><br><span class=line>    {<span class=string>'object_bbx_center'</span>: object_bbx_center[object_bbx_mask == <span class=number>1</span>],</span><br><span class=line>     <span class=string>'object_ids'</span>: object_ids,</span><br><span class=line>     <span class=string>'projected_lidar'</span>: lidar_np})</span><br></pre></table></figure><p>然后在遍历场景下所有代理的过程中,把这些数据加入列表,然后把目标物体去重,更新物体坐标和mask的列表.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line>   projected_lidar_stack.append(</span><br><span class=line>   selected_cav_processed[<span class=string>'projected_lidar'</span>])</span><br><span class=line>  object_stack.append(selected_cav_processed[<span class=string>'object_bbx_center'</span>])</span><br><span class=line>   object_id_stack += selected_cav_processed[<span class=string>'object_ids'</span>]</span><br><span class=line><span class=comment># exclude all repetitive objects</span></span><br><span class=line>       unique_indices = \</span><br><span class=line>           [object_id_stack.index(x) <span class=keyword>for</span> x <span class=keyword>in</span> <span class=built_in>set</span>(object_id_stack)]</span><br><span class=line>       object_stack = np.vstack(object_stack)</span><br><span class=line>       object_stack = object_stack[unique_indices]</span><br><span class=line>  <span class=comment># make sure bounding boxes across all frames have the same number</span></span><br><span class=line>       object_bbx_center = \</span><br><span class=line>           np.zeros((self.params[<span class=string>'postprocess'</span>][<span class=string>'max_num'</span>], <span class=number>7</span>))</span><br><span class=line>       mask = np.zeros(self.params[<span class=string>'postprocess'</span>][<span class=string>'max_num'</span>])</span><br><span class=line>       object_bbx_center[:object_stack.shape[<span class=number>0</span>], :] = object_stack</span><br><span class=line>       mask[:object_stack.shape[<span class=number>0</span>]] = <span class=number>1</span></span><br></pre></table></figure><p>接着做点云做增强,包括flip,rotation和scale.然后去掉范围外的点云以及去掉在外面的bbox. 然后根据点云预处理得到体素,假设通过spvoxel预处理<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=comment># pre-process the lidar to voxel/bev/downsampled lidar</span></span><br><span class=line>lidar_dict = self.pre_processor.preprocess(projected_lidar_stack)</span><br><span class=line><span class=comment># generate the anchor boxes</span></span><br><span class=line>anchor_box = self.post_processor.generate_anchor_box()</span><br><span class=line><span class=comment># generate targets label</span></span><br><span class=line>label_dict = \</span><br><span class=line>    self.post_processor.generate_label(</span><br><span class=line>        gt_box_center=object_bbx_center,</span><br><span class=line>        anchors=anchor_box,</span><br><span class=line>        mask=mask)</span><br><span class=line></span><br></pre></table></figure><figure class="highlight yaml"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=attr>args:</span></span><br><span class=line><span class=attr>max_points_per_voxel:</span> <span class=number>32</span></span><br><span class=line><span class=attr>max_voxel_test:</span> <span class=number>70000</span></span><br><span class=line><span class=attr>max_voxel_train:</span> <span class=number>32000</span></span><br><span class=line><span class=attr>voxel_size:</span> [<span class=number>0.4</span>,<span class=number>0.4</span>,<span class=number>4</span>]</span><br><span class=line><span class=attr>cav_lidar_range:</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>-140.8</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>-38.4</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>-3</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>140.8</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>38.4</span></span><br><span class=line>  <span class=bullet>-</span> <span class=number>1</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>data_dict[<span class=string>'voxel_features'</span>] = voxels</span><br><span class=line>data_dict[<span class=string>'voxel_coords'</span>] = coordinates</span><br><span class=line>data_dict[<span class=string>'voxel_num_points'</span>] = num_points</span><br></pre></table></figure><p>得到特征,坐标和每个体素内的点云数.<p>然后通过generate_anchor_box得到anchor,generate_label生成label.通过配置知道anchor的长宽高,每个anchor是grid的倍数,一个grid就lidar_range/voxel_size. 每个位置的两个anchor长宽高固定,但是yaw反向. 生成label就要复杂一些了,需要知道anchor_box和box_center以及mask.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre><td class=code><pre><span class=line><span class=comment># (max_num, 7)</span></span><br><span class=line>gt_box_center = kwargs[<span class=string>'gt_box_center'</span>]</span><br><span class=line><span class=comment># (H, W, anchor_num, 7)</span></span><br><span class=line>anchors = kwargs[<span class=string>'anchors'</span>]</span><br><span class=line><span class=comment># (max_num)</span></span><br><span class=line>masks = kwargs[<span class=string>'mask'</span>]</span><br><span class=line></span><br><span class=line><span class=comment># (H, W)</span></span><br><span class=line>feature_map_shape = anchors.shape[:<span class=number>2</span>]</span><br><span class=line><span class=comment># (H*W*anchor_num, 7)</span></span><br><span class=line>anchors = anchors.reshape(-<span class=number>1</span>, <span class=number>7</span>)</span><br><span class=line><span class=comment># normalization factor, (H * W * anchor_num)</span></span><br><span class=line><span class=comment># 计算anchor的长宽对角线长度</span></span><br><span class=line>anchors_d = np.sqrt(anchors[:, <span class=number>4</span>] ** <span class=number>2</span> + anchors[:, <span class=number>5</span>] ** <span class=number>2</span>)</span><br><span class=line></span><br><span class=line><span class=comment># (H, W, 2)</span></span><br><span class=line>pos_equal_one = np.zeros((*feature_map_shape, self.anchor_num))</span><br><span class=line>neg_equal_one = np.zeros((*feature_map_shape, self.anchor_num))</span><br><span class=line><span class=comment># (H, W, self.anchor_num * 7) bbox 回归结果</span></span><br><span class=line>targets = np.zeros((*feature_map_shape, self.anchor_num * <span class=number>7</span>))</span><br><span class=line></span><br><span class=line><span class=comment># (n, 7)</span></span><br><span class=line>gt_box_center_valid = gt_box_center[masks == <span class=number>1</span>]</span><br><span class=line><span class=comment># (n, 8, 3)</span></span><br><span class=line>gt_box_corner_valid = \</span><br><span class=line>    box_utils.boxes_to_corners_3d(gt_box_center_valid,</span><br><span class=line>                                  self.params[<span class=string>'order'</span>])</span><br><span class=line><span class=comment># (H*W*anchor_num, 8, 3)</span></span><br><span class=line>anchors_corner = \</span><br><span class=line>    box_utils.boxes_to_corners_3d(anchors,</span><br><span class=line>                                  order=self.params[<span class=string>'order'</span>])</span><br></pre></table></figure><p>将gt_box和anchor_box从7转为8,然后转为二维box,忽略z轴,后面求overlaps用cython写的,它会求出每个anchor_box对每个gt_box的iou score,然后得到对于每个gt_box,iou最高的anchor box的索引. 然后根据iou的值得到id_pos和id_neg.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br></pre><td class=code><pre><span class=line><span class=comment># (H*W*anchor_num, 4)</span></span><br><span class=line>anchors_standup_2d = \</span><br><span class=line>box_utils.corner2d_to_standup_box(anchors_corner)</span><br><span class=line><span class=comment># (n, 4)</span></span><br><span class=line>gt_standup_2d = \</span><br><span class=line>box_utils.corner2d_to_standup_box(gt_box_corner_valid)</span><br><span class=line></span><br><span class=line><span class=comment># (H*W*anchor_n)</span></span><br><span class=line>iou = bbox_overlaps(</span><br><span class=line>np.ascontiguousarray(anchors_standup_2d).astype(np.float32),</span><br><span class=line>np.ascontiguousarray(gt_standup_2d).astype(np.float32),</span><br><span class=line>)</span><br><span class=line></span><br><span class=line><span class=comment># the anchor boxes has the largest iou across</span></span><br><span class=line><span class=comment># shape: (n)</span></span><br><span class=line>id_highest = np.argmax(iou.T, axis=<span class=number>1</span>)</span><br><span class=line><span class=comment># [0, 1, 2, ..., n-1]</span></span><br><span class=line>id_highest_gt = np.arange(iou.T.shape[<span class=number>0</span>])</span><br><span class=line><span class=comment># make sure all highest iou is larger than 0</span></span><br><span class=line>mask = iou.T[id_highest_gt, id_highest] > <span class=number>0</span></span><br><span class=line>id_highest, id_highest_gt = id_highest[mask], id_highest_gt[mask]</span><br><span class=line></span><br><span class=line><span class=comment># find anchors iou > params['pos_iou']</span></span><br><span class=line>id_pos, id_pos_gt = \</span><br><span class=line>np.where(iou ></span><br><span class=line>         self.params[<span class=string>'target_args'</span>][<span class=string>'pos_threshold'</span>])</span><br><span class=line><span class=comment>#  find anchors iou < params['neg_iou']</span></span><br><span class=line>id_neg = np.where(np.<span class=built_in>sum</span>(iou <</span><br><span class=line>                     self.params[<span class=string>'target_args'</span>][<span class=string>'neg_threshold'</span>],</span><br><span class=line>                     axis=<span class=number>1</span>) == iou.shape[<span class=number>1</span>])[<span class=number>0</span>]</span><br><span class=line>id_pos = np.concatenate([id_pos, id_highest])</span><br><span class=line>id_pos_gt = np.concatenate([id_pos_gt, id_highest_gt])</span><br><span class=line>id_pos, index = np.unique(id_pos, return_index=<span class=literal>True</span>)</span><br><span class=line>id_pos_gt = id_pos_gt[index]</span><br><span class=line>id_neg.sort()</span><br></pre></table></figure><p>然后计算需要回归的gt值,下面的值就是计算损失时的真值,包含七个值,针对anchor_box的中心三维坐标和长宽高以及yaw.</p><script type="math/tex; mode=display">
\frac{\Delta x}{\sqrt{w^{2}+l^{2}}} \\
\frac{\Delta y}{\sqrt{w^{2}+l^{2}}} \\
\frac{\Delta z}{h} \ where\  \Delta x=x^{gt}-x.    \quad y,z\  are\  same \\
\log\frac{h^{gt}}{h} \\
\log\frac{l^{gt}}{l} \\
\log\frac{w^{gt}}{w} \\
\Delta yaw</script><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=comment># calculate the targets</span></span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span>] = \</span><br><span class=line>       (gt_box_center[id_pos_gt, <span class=number>0</span>] - anchors[id_pos, <span class=number>0</span>]) / anchors_d[</span><br><span class=line>           id_pos]</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>1</span>] = \</span><br><span class=line>       (gt_box_center[id_pos_gt, <span class=number>1</span>] - anchors[id_pos, <span class=number>1</span>]) / anchors_d[</span><br><span class=line>           id_pos]</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>2</span>] = \</span><br><span class=line>       (gt_box_center[id_pos_gt, <span class=number>2</span>] - anchors[id_pos, <span class=number>2</span>]) / anchors[</span><br><span class=line>           id_pos, <span class=number>3</span>]</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>3</span>] = np.log(</span><br><span class=line>       gt_box_center[id_pos_gt, <span class=number>3</span>] / anchors[id_pos, <span class=number>3</span>])</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>4</span>] = np.log(</span><br><span class=line>       gt_box_center[id_pos_gt, <span class=number>4</span>] / anchors[id_pos, <span class=number>4</span>])</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>5</span>] = np.log(</span><br><span class=line>       gt_box_center[id_pos_gt, <span class=number>5</span>] / anchors[id_pos, <span class=number>5</span>])</span><br><span class=line>   targets[index_x, index_y, np.array(index_z) * <span class=number>7</span> + <span class=number>6</span>] = (</span><br><span class=line>           gt_box_center[id_pos_gt, <span class=number>6</span>] - anchors[id_pos, <span class=number>6</span>])</span><br></pre></table></figure><p>neg_label设置<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>index_x, index_y, index_z = np.unravel_index(</span><br><span class=line>        id_neg, (*feature_map_shape, self.anchor_num))</span><br><span class=line>    neg_equal_one[index_x, index_y, index_z] = <span class=number>1</span></span><br><span class=line></span><br><span class=line>    <span class=comment># to avoid a box be pos/neg in the same time</span></span><br><span class=line>    index_x, index_y, index_z = np.unravel_index(</span><br><span class=line>        id_highest, (*feature_map_shape, self.anchor_num))</span><br><span class=line>    neg_equal_one[index_x, index_y, index_z] = <span class=number>0</span></span><br></pre></table></figure><p>最后得到了三个数组 projected_lidar_stack,object_stack ,object_id_stack.分别是通过<code>project_world_objects</code>得到的bbx的中心,物体id以及lidar数据(投影到ego坐标系上),<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line>processed_data_dict[<span class=string>'ego'</span>].update(</span><br><span class=line>    {<span class=string>'object_bbx_center'</span>: object_bbx_center,</span><br><span class=line>     <span class=string>'object_bbx_mask'</span>: mask,</span><br><span class=line>     <span class=string>'object_ids'</span>: [object_id_stack[i] <span class=keyword>for</span> i <span class=keyword>in</span> unique_indices],</span><br><span class=line>     <span class=string>'anchor_box'</span>: anchor_box,</span><br><span class=line>     <span class=string>'processed_lidar'</span>: lidar_dict,</span><br><span class=line>     <span class=string>'label_dict'</span>: label_dict})</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> self.visualize:</span><br><span class=line>    processed_data_dict[<span class=string>'ego'</span>].update({<span class=string>'origin_lidar'</span>:</span><br><span class=line>                                           projected_lidar_stack})</span><br></pre></table></figure><p>得到的processed_data_dict在ego键上有gt_box,anchor_box,label以及lidar的数据,如果要可视化,还可以拿到projected_lidar_stack也就是通过体素预处理之前的点云数据(N,4)<h4 id=中期融合><a class=headerlink href=#中期融合 title=中期融合></a>中期融合</h4><p>中期融合时每个代理会对点云进行处理,传输处理后的特征.<p>中间融合会设置proj_first,使得先将其他代理的点云投射到ego坐标系上. 在获取数据时同样根据<code>retrieve_base_data</code>得到base_data_dict,获得<code>pairwise_t_matrix</code>,它表示代理i到代理j的转换矩阵. 然后年后依然是遍历base_data_dict(也就是通过retrieve_base_data得到的一个场景下的一个timestamp下的所有cav的数据). 然后在get_item_single_car中得到object_center,跟早期融合相同,根据transformation_matrix将其他代理点云投影到ego坐标系中(在此之前依旧有shuffle和mask). 然后将点云经过预处理,这里就是跟早期融合的差别,在每个代理上就进行了处理,得到processed_lidar,lidar_up依旧是点云<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br></pre><td class=code><pre><span class=line>transformation_matrix = \</span><br><span class=line>    selected_cav_base[<span class=string>'params'</span>][<span class=string>'transformation_matrix'</span>]</span><br><span class=line></span><br><span class=line><span class=comment># retrieve objects under ego coordinates</span></span><br><span class=line>object_bbx_center, object_bbx_mask, object_ids = \</span><br><span class=line>    self.post_processor.generate_object_center([selected_cav_base],</span><br><span class=line>                                               ego_pose)</span><br><span class=line></span><br><span class=line><span class=comment># filter lidar</span></span><br><span class=line>lidar_np = selected_cav_base[<span class=string>'lidar_np'</span>]</span><br><span class=line>lidar_np = shuffle_points(lidar_np)</span><br><span class=line><span class=comment># remove points that hit itself</span></span><br><span class=line>lidar_np = mask_ego_points(lidar_np)</span><br><span class=line><span class=comment># project the lidar to ego space</span></span><br><span class=line><span class=keyword>if</span> self.proj_first:</span><br><span class=line>    lidar_np[:, :<span class=number>3</span>] = \</span><br><span class=line>        box_utils.project_points_by_matrix_torch(lidar_np[:, :<span class=number>3</span>],</span><br><span class=line>                                                 transformation_matrix)</span><br><span class=line>lidar_np = mask_points_by_range(lidar_np,</span><br><span class=line>                                self.params[<span class=string>'preprocess'</span>][</span><br><span class=line>                                    <span class=string>'cav_lidar_range'</span>])</span><br><span class=line>processed_lidar = self.pre_processor.preprocess(lidar_np)</span><br><span class=line></span><br><span class=line><span class=comment># velocity</span></span><br><span class=line>velocity = selected_cav_base[<span class=string>'params'</span>][<span class=string>'ego_speed'</span>]</span><br><span class=line><span class=comment># normalize veloccity by average speed 30 km/h</span></span><br><span class=line>velocity = velocity / <span class=number>30</span></span><br><span class=line></span><br><span class=line>selected_cav_processed.update(</span><br><span class=line>    {<span class=string>'object_bbx_center'</span>: object_bbx_center[object_bbx_mask == <span class=number>1</span>],</span><br><span class=line>     <span class=string>'object_ids'</span>: object_ids,</span><br><span class=line>     <span class=string>'projected_lidar'</span>: lidar_np,</span><br><span class=line>     <span class=string>'processed_features'</span>: processed_lidar,</span><br><span class=line>     <span class=string>'velocity'</span>: velocity})</span><br></pre></table></figure><p>遍历所有base_data_dict,拿到bbx_center,ids,processed_features,等,此外还有spatial_correction_matrix,这是cur_ego_pose_flag为False时用来修正时延的. 因为processed_features是一个字典包括多个字段,同个场景中每个代理都有一个processed_features,要将所有代理的processed_features放在一个字典下,然后将其他数据补全,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br></pre><td class=code><pre><span class=line><span class=comment># merge preprocessed features from different cavs into the same dict</span></span><br><span class=line>cav_num = <span class=built_in>len</span>(processed_features)</span><br><span class=line>merged_feature_dict = self.merge_features_to_dict(processed_features)</span><br><span class=line></span><br><span class=line><span class=comment># generate the anchor boxes</span></span><br><span class=line>anchor_box = self.post_processor.generate_anchor_box()</span><br><span class=line></span><br><span class=line><span class=comment># generate targets label</span></span><br><span class=line>label_dict = \</span><br><span class=line>    self.post_processor.generate_label(</span><br><span class=line>        gt_box_center=object_bbx_center,</span><br><span class=line>        anchors=anchor_box,</span><br><span class=line>        mask=mask)</span><br><span class=line></span><br><span class=line><span class=comment># pad dv, dt, infra to max_cav</span></span><br><span class=line>velocity = velocity + (self.max_cav - <span class=built_in>len</span>(velocity)) * [<span class=number>0.</span>]</span><br><span class=line>time_delay = time_delay + (self.max_cav - <span class=built_in>len</span>(time_delay)) * [<span class=number>0.</span>]</span><br><span class=line>infra = infra + (self.max_cav - <span class=built_in>len</span>(infra)) * [<span class=number>0.</span>]</span><br><span class=line>spatial_correction_matrix = np.stack(spatial_correction_matrix)</span><br><span class=line>padding_eye = np.tile(np.eye(<span class=number>4</span>)[<span class=literal>None</span>],(self.max_cav - <span class=built_in>len</span>(</span><br><span class=line>                                       spatial_correction_matrix),<span class=number>1</span>,<span class=number>1</span>))</span><br><span class=line>spatial_correction_matrix = np.concatenate([spatial_correction_matrix,</span><br><span class=line>                                           padding_eye], axis=<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>processed_data_dict[<span class=string>'ego'</span>].update(</span><br><span class=line>    {<span class=string>'object_bbx_center'</span>: object_bbx_center,</span><br><span class=line>     <span class=string>'object_bbx_mask'</span>: mask,</span><br><span class=line>     <span class=string>'object_ids'</span>: [object_id_stack[i] <span class=keyword>for</span> i <span class=keyword>in</span> unique_indices],</span><br><span class=line>     <span class=string>'anchor_box'</span>: anchor_box,</span><br><span class=line>     <span class=string>'processed_lidar'</span>: merged_feature_dict,</span><br><span class=line>     <span class=string>'label_dict'</span>: label_dict,</span><br><span class=line>     <span class=string>'cav_num'</span>: cav_num,</span><br><span class=line>     <span class=string>'velocity'</span>: velocity,</span><br><span class=line>     <span class=string>'time_delay'</span>: time_delay,</span><br><span class=line>     <span class=string>'infra'</span>: infra,</span><br><span class=line>     <span class=string>'spatial_correction_matrix'</span>: spatial_correction_matrix,</span><br><span class=line>     <span class=string>'pairwise_t_matrix'</span>: pairwise_t_matrix})</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> self.visualize:</span><br><span class=line>    processed_data_dict[<span class=string>'ego'</span>].update({<span class=string>'origin_lidar'</span>:</span><br><span class=line>        np.vstack(</span><br><span class=line>            projected_lidar_stack)})</span><br><span class=line><span class=keyword>return</span> processed_data_dict</span><br></pre></table></figure><p><img alt=Untitled-2024-07-11-1611 data-src=https://s2.loli.net/2024/07/12/TEs9YUIDhi7dkaR.png><h4 id=晚期融合><a class=headerlink href=#晚期融合 title=晚期融合></a>晚期融合</h4><p>晚期融合用于在每个代理上检测得到confidence(N,anchor_num)和regression(N,anchor_num*7)结果然后再传给ego代理,其往往会利用bbox进行nms. 因为晚期融合传送的数据快,<p>在代码中首先通过retrice_base_data得到这个场景下某个timestamp的所有cav的数据,在训练时从数据集中随即返回一个数据,得到这个数据的<code>lidar_np</code>,object_center,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_item_train</span>(<span class=params>self, base_data_dict</span>):</span></span><br><span class=line>    processed_data_dict = OrderedDict()</span><br><span class=line></span><br><span class=line>    <span class=comment># during training, we return a random cav's data</span></span><br><span class=line>    <span class=keyword>if</span> <span class=keyword>not</span> self.visualize:</span><br><span class=line>        selected_cav_id, selected_cav_base = \</span><br><span class=line>            random.choice(<span class=built_in>list</span>(base_data_dict.items()))</span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        selected_cav_id, selected_cav_base = \</span><br><span class=line>            <span class=built_in>list</span>(base_data_dict.items())[<span class=number>0</span>]</span><br><span class=line></span><br><span class=line>    selected_cav_processed = self.get_item_single_car(selected_cav_base)</span><br><span class=line>    processed_data_dict.update({<span class=string>'ego'</span>: selected_cav_processed})</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> processed_data_dict</span><br></pre></table></figure><p>因为<h3 id=V2V4><a class=headerlink href=#V2V4 title=V2V4></a>V2V4</h3><p>相对来说增加了domain adaptation<h3 id=常用工具类><a class=headerlink href=#常用工具类 title=常用工具类></a>常用工具类</h3><p>从(N,C,H,W)也就是多个batch的cav数据放在一起,但是后面需要处理每个batch的数据,就通过regroup得到.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>regroup</span>(<span class=params>dense_feature, record_len, max_len</span>):</span></span><br><span class=line>    <span class=string>"""</span></span><br><span class=line><span class=string>    Regroup the data based on the record_len.</span></span><br><span class=line><span class=string>    Parameters</span></span><br><span class=line><span class=string>    ----------</span></span><br><span class=line><span class=string>    dense_feature : torch.Tensor</span></span><br><span class=line><span class=string>        N, C, H, W</span></span><br><span class=line><span class=string>    record_len : list</span></span><br><span class=line><span class=string>        [sample1_len, sample2_len, ...]</span></span><br><span class=line><span class=string>    max_len : int</span></span><br><span class=line><span class=string>        Maximum cav number</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>    Returns</span></span><br><span class=line><span class=string>    -------</span></span><br><span class=line><span class=string>    regroup_feature : torch.Tensor</span></span><br><span class=line><span class=string>        B, L, C, H, W</span></span><br><span class=line><span class=string>    """</span></span><br><span class=line>    cum_sum_len = <span class=built_in>list</span>(np.cumsum(torch_tensor_to_numpy(record_len)))</span><br><span class=line>    <span class=comment># [sample1_len, sample2_len+sample1_len, ...]</span></span><br><span class=line>    split_features = torch.tensor_split(dense_feature,</span><br><span class=line>                                        cum_sum_len[:-<span class=number>1</span>])</span><br><span class=line>    <span class=comment># 利用tensor_split划分每个batch的数据得到list</span></span><br><span class=line>    regroup_features = []</span><br><span class=line>    mask = []</span><br><span class=line></span><br><span class=line>    <span class=keyword>for</span> split_feature <span class=keyword>in</span> split_features:</span><br><span class=line>        <span class=comment># M, C, H, W</span></span><br><span class=line>        feature_shape = split_feature.shape</span><br><span class=line></span><br><span class=line>        <span class=comment># the maximum M is 5 as most 5 cavs</span></span><br><span class=line>        padding_len = max_len - feature_shape[<span class=number>0</span>]</span><br><span class=line>        mask.append([<span class=number>1</span>] * feature_shape[<span class=number>0</span>] + [<span class=number>0</span>] * padding_len)</span><br><span class=line></span><br><span class=line>        padding_tensor = torch.zeros(padding_len, feature_shape[<span class=number>1</span>],</span><br><span class=line>                                     feature_shape[<span class=number>2</span>], feature_shape[<span class=number>3</span>])</span><br><span class=line>        padding_tensor = padding_tensor.to(split_feature.device)</span><br><span class=line></span><br><span class=line>        split_feature = torch.cat([split_feature, padding_tensor],</span><br><span class=line>                                  dim=<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># 1, 5C, H, W</span></span><br><span class=line>        split_feature = split_feature.view(-<span class=number>1</span>,</span><br><span class=line>                                           feature_shape[<span class=number>2</span>],</span><br><span class=line>                                           feature_shape[<span class=number>3</span>]).unsqueeze(<span class=number>0</span>)</span><br><span class=line>        regroup_features.append(split_feature)</span><br><span class=line></span><br><span class=line>    <span class=comment># B, 5C, H, W</span></span><br><span class=line>    regroup_features = torch.cat(regroup_features, dim=<span class=number>0</span>)</span><br><span class=line>    <span class=comment># B, L, C, H, W</span></span><br><span class=line>    regroup_features = rearrange(regroup_features,</span><br><span class=line>                                 <span class=string>'b (l c) h w -> b l c h w'</span>,</span><br><span class=line>                                 l=max_len)</span><br><span class=line>    mask = torch.from_numpy(np.array(mask)).to(regroup_features.device)</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> regroup_features, mask</span><br></pre></table></figure><h2 id=相关论文><a class=headerlink href=#相关论文 title=相关论文></a>相关论文</h2><ol><li>arXiv:2404.14022</ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\30\协作感知算法-三\ rel=bookmark>协作感知算法:三</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\17\协同感知算法-二\ rel=bookmark>协同感知学习(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\11\30\协同感知算法-一\ rel=bookmark>协同感知学习(一)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/05/23/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/ title=协同感知数据集和代码库介绍>https://www.sekyoro.top/2024/05/23/协同感知数据集介绍/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag><i class="fa fa-tag"></i> collaborative perception</a></div><div class=post-nav><div class=post-nav-item><a title="Domain Adaptation Method" href=/2024/05/21/Domain-Adaptation-Method/ rel=prev> <i class="fa fa-chevron-left"></i> Domain Adaptation Method </a></div><div class=post-nav-item><a href=/2024/05/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E5%88%86%E6%9E%90/ rel=next title=深度学习可解释性分析> 深度学习可解释性分析 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>1.</span> <span class=nav-text>协同感知数据集</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#OPV2V-2022><span class=nav-number>1.1.</span> <span class=nav-text>OPV2V 2022</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2XSet-2022><span class=nav-number>1.2.</span> <span class=nav-text>V2XSet 2022</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#DAIR-V2X-2022><span class=nav-number>1.3.</span> <span class=nav-text>DAIR-V2X 2022</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2X-Sim-2022><span class=nav-number>1.4.</span> <span class=nav-text>V2X-Sim 2022</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2V4Real-2023><span class=nav-number>1.5.</span> <span class=nav-text>V2V4Real 2023</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>1.5.1.</span> <span class=nav-text>摘要</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#RCooper-2024><span class=nav-number>1.6.</span> <span class=nav-text>RCooper 2024</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#TUMTraf-V2X-2024><span class=nav-number>1.7.</span> <span class=nav-text>TUMTraf-V2X 2024</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2X-Real-2024><span class=nav-number>1.8.</span> <span class=nav-text>V2X-Real 2024</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#CodeBase><span class=nav-number>2.</span> <span class=nav-text>CodeBase</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#BaseDataset><span class=nav-number>2.1.</span> <span class=nav-text>BaseDataset</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%B8%8D%E5%90%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%9E%8D%E5%90%88><span class=nav-number>2.2.</span> <span class=nav-text>不同数据类型融合</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%97%A9%E6%9C%9F%E8%9E%8D%E5%90%88><span class=nav-number>2.2.1.</span> <span class=nav-text>早期融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%B8%AD%E6%9C%9F%E8%9E%8D%E5%90%88><span class=nav-number>2.2.2.</span> <span class=nav-text>中期融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%99%9A%E6%9C%9F%E8%9E%8D%E5%90%88><span class=nav-number>2.2.3.</span> <span class=nav-text>晚期融合</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#V2V4><span class=nav-number>2.3.</span> <span class=nav-text>V2V4</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB><span class=nav-number>2.4.</span> <span class=nav-text>常用工具类</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87><span class=nav-number>3.</span> <span class=nav-text>相关论文</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>214</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>197</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/collaborative-perception/ rel=tag>collaborative perception</a><span class=tag-list-count>4</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>30:39</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>