<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=这个方向的技术目前没有那么火了,但是能应用的场景非常之多. name=description><meta content=article property=og:type><meta content="Domain Adaptation Method" property=og:title><meta content=https://www.sekyoro.top/2024/05/21/Domain-Adaptation-Method/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=这个方向的技术目前没有那么火了,但是能应用的场景非常之多. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/05/21/yksHCLQcPdOrERa.png property=og:image><meta content=https://s2.loli.net/2024/05/21/R2xdyXHQJp38nNj.png property=og:image><meta content=https://s2.loli.net/2024/05/23/ZAvaHCuiyK78RpW.png property=og:image><meta content=https://s2.loli.net/2024/06/18/rWKxDdeUG4nSJRO.png property=og:image><meta content=https://s2.loli.net/2024/06/30/sP3IEinJu1cg2tm.png property=og:image><meta content=https://s2.loli.net/2024/06/30/5U4korDjOX1diuP.png property=og:image><meta content=https://s2.loli.net/2024/06/30/V3uX2QpSghyj8Bz.png property=og:image><meta content=https://s2.loli.net/2024/06/30/7wux3qMtXFE8AlS.png property=og:image><meta content=https://s2.loli.net/2024/06/30/6xZTH9OQfjRpunr.png property=og:image><meta content=https://s2.loli.net/2024/06/30/JREocqfp8jiwLeP.png property=og:image><meta content=https://s2.loli.net/2024/06/30/XtzFxWNyM5AC4ql.png property=og:image><meta content=https://s2.loli.net/2024/06/23/ym2ExsfAFX9cpP7.png property=og:image><meta content=https://s2.loli.net/2024/06/27/w4semziX5HcTZo9.png property=og:image><meta content=https://s2.loli.net/2024/06/27/ikL3uemU5nxNT7f.png property=og:image><meta content=https://s2.loli.net/2024/06/30/Uh35uECtq9zAWVm.png property=og:image><meta content=https://s2.loli.net/2024/06/30/5pCyk8FE2alwqhL.png property=og:image><meta content=https://s2.loli.net/2024/07/02/xOTivnD4tj2PrUG.png property=og:image><meta content=https://s2.loli.net/2024/06/27/tDwx9ZesFkz57yo.png property=og:image><meta content=https://s2.loli.net/2024/06/27/lgRXwOIv43etCjW.png property=og:image><meta content=https://s2.loli.net/2024/07/05/t2JurbfKWowqQEs.png property=og:image><meta content=https://s2.loli.net/2024/06/30/dm3lApv8HMux15G.png property=og:image><meta content=c:/Users/proanimer/AppData/Roaming/Typora/typora-user-images/image-20240702114426708.png property=og:image><meta content=https://s2.loli.net/2024/07/05/DNVsxMT7WhRHvX5.png property=og:image><meta content=https://s2.loli.net/2024/07/05/wEb6aNTnig8HWIt.png property=og:image><meta content=2024-05-21T01:47:55.000Z property=article:published_time><meta content=2024-07-25T02:17:54.472Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="domain adaptation" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/05/21/yksHCLQcPdOrERa.png name=twitter:image><link href=https://www.sekyoro.top/2024/05/21/Domain-Adaptation-Method/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>Domain Adaptation Method | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/05/21/Domain-Adaptation-Method/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Domain Adaptation Method</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-05-21 09:47:55" datetime=2024-05-21T09:47:55+08:00>2024-05-21</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-07-25 10:17:54" datetime=2024-07-25T10:17:54+08:00 itemprop=dateModified>2024-07-25</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>12k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>11 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>这个方向的技术目前没有那么火了,但是能应用的场景非常之多.<br><span id=more></span><h1 id=综述><a class=headerlink href=#综述 title=综述></a>综述</h1><h2 id=Deep-Unsupervised-Domain-Adaptation-A-Review-of-Recent-Advances-and-Perspectives><a title="Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives" class=headerlink href=#Deep-Unsupervised-Domain-Adaptation-A-Review-of-Recent-Advances-and-Perspectives></a>Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives</h2><p>深度学习已经成为解决不同领域现实问题的首选方法，部分原因是它能够从数据中学习,并在广泛的应用中取得了令人印象深刻的性能。<p>然而，它的成功通常依赖于两个假设：( i )<strong>精确的模型拟合需要大量有标签的数据集</strong>,( ii )<strong>训练和测试数据是独立同分布的.因此,它在看不见的目标域上的性能得不到保证,特别是在适应阶段遇到分布外数据时</strong>。<p>在目标域数据上的性能下降是部署在源域数据上成功训练的深度神经网络的一个关键问题.<p>无监督域适应( Unsupervised Domain Adaptation，UDA )正是针对这一问题提出的,通过同时利用已标记的源域数据和未标记的目标域数据,在目标域中执行各种任务<p><img alt=image-20240521102852946 data-src=https://s2.loli.net/2024/05/21/yksHCLQcPdOrERa.png><blockquote><p>Although supervised deep learning is the most prevalent and successful approach for a variety of tasks, its success hinges on (i) vast troves of labeled training data and (ii) the assumption of independent and identically distributed (i.i.d.) training and testing datasets.</blockquote><p>Because reliable labeling of massive datasets for various application domains is often expensive and prohibitive, for a task without sufficient labeled datasets in a target domain, there is strong demand to apply trained models, by leveraging rich labeled data from a source domain<p><img alt=image-20240521110351381 data-src=https://s2.loli.net/2024/05/21/R2xdyXHQJp38nNj.png></p><script type="math/tex; mode=display">
\begin{aligned}&\mathcal{L}_t(h)\leq\mathcal{L}_s(h)+d[p_\mathcal{S},p_\mathcal{T}]+\\&\min[\mathbb{E}_{x\sim p_s}|p_s(y|x)-p_t(y|x)|,\mathbb{E}_{x\sim p_t}|p_s(y|x)-p_t(y|x)|]\end{aligned}</script><p><img alt=image-20240523110706500 data-src=https://s2.loli.net/2024/05/23/ZAvaHCuiyK78RpW.png><p>常用的方法分类如图.<h4 id=Statistic-Divergence-Alignment><a title="Statistic Divergence Alignment" class=headerlink href=#Statistic-Divergence-Alignment></a>Statistic Divergence Alignment</h4><p>学习领域不变特征表示是许多深度UDA方法中应用最广泛的思想,其关键在于最小化潜在特征空间中的领域差异.<strong>为了实现这一目标,选择合适的divergence measure是这些方法的核心</strong>。<h2 id=Deep-visual-domain-adaptation-A-survey><a title="Deep visual domain adaptation: A survey" class=headerlink href=#Deep-visual-domain-adaptation-A-survey></a>Deep visual domain adaptation: A survey</h2><p>深度域适应已经成为一种新的学习技术，以解决缺乏大量标记数据的问题。与传统的学习共享特征子空间或重用具有浅层表示的重要源实例的方法相比，深度域适应方法通过将域适应嵌入到深度学习的流水线中，利用深度网络来学习更多的可迁移表示。已有针对浅域自适应的全面调查，但很少有人及时评论基于深度学习的新兴方法。在本文中，我们对计算机视觉应用中的深度域自适应方法进行了全面调查，主要有四大贡献。首先，我们根据定义两个域如何偏离的数据属性，提出了不同深度域适配场景的分类法。其次，我们根据训练损失将深域适应方法归纳为几个类别，并简要分析和比较了这些类别下的最先进方法。第三，我们概述了图像分类之外的计算机视觉应用，如人脸识别、语义分割和物体检测。第四，重点介绍当前方法的一些潜在不足和未来的几个发展方向。<p><img alt=image-20240618230157030 data-src=https://s2.loli.net/2024/06/18/rWKxDdeUG4nSJRO.png><p>域 D 由特征空间 X 和边际概率分布 P(X) 组成，其中 X = {x1, ., xn}∈ X。给定一个特定的域 D = {X , P(X )}，任务 T 由一个特征空间 Y 和一个客观预测函数 f( - ) 组成，从概率论的角度来看，f( - ) 也可以看作是条件概率分布 P(Y|X)。一般来说，我们可以从标注数据 {xi, yi}（其中 xi∈X 和 yi∈Y ）中以有监督的方式学习 P(Y|X)。<p>假设有两个域：有足够标注数据的训练数据集是源域 Ds = {X s, P(X )s}, 有<strong>少量标注数据或无标注数据的测试数据集是目标域 Dt = {X t , P(X )t }</strong>。部分标注的部分 Dtl 和未标注的部分 Dtu 构成了整个目标域，即 Dt = Dtl ∪ Dtu。每个域都有其任务：前者为 T s = {Ys, P(Y s|Xs )}, 后者为 T t = {Yt , P(Y t |Xt )} 。<p>同样，P(Ys|Xs) 可以从源标签数据 {xs i , ys i } 中学习，而 P(Yt|Xt) 可以从标签目标数据 {xtl i , ytl i } 和非标签数据 {xtu i } 中学习。<p>对DA方法进行分类<blockquote><p>在同质数据中，源域和目标域之间的特征空间是相同的（X s = X t），维度也相同（ds = dt）。因此，源数据集和目标数据集的数据分布一般是不同的（P(X)s = P(X)t）。<p>在异构数据中，源域和目标域之间的特征空间是不等同的（X s = X t），维度一般也可能不同（ds = dt）。</blockquote><p>此外，我们还可以将同质 DA 设置进一步分为三种情况：<ol><li><p>在有监督的 DA 中，存在少量有标签的目标数据（D^tl^）。然而，标签数据通常不足以完成任务。</p><li><p>在半监督 DA 中，训练阶段既有有限的目标域标记数据（D^tl^），也有冗余的非标记数据（D^tu^），这使得网络可以学习目标域的结构信息。</p><li><p>在无监督 DA 中，训练网络时无法观察到有标签但足够多的无标签目标域数据（D^tu^）</p></ol><p>与同质类似，异质也可分为有监督、半监督和无监督。<p>上述所有 DA 设置都假定源领域和目标领域直接相关，因此，知识转移只需一步即可完成。我们称其为一步式设计。<p><img alt=image-20240630194001836 data-src=https://s2.loli.net/2024/06/30/sP3IEinJu1cg2tm.png><p>然而，在现实中，这一假设偶尔也会落空。两个域之间几乎没有重叠，因此执行单步 DA 将不会有效。幸运的是，有一些中间域能够拉近源域和目标域之间的距离。使用一系列中间桥梁来连接两个看似不相关的域，然后通过该桥梁执行一步式 DA,这就是多步式（或反式）DA .例如,人脸图像和车辆图像由于形状或其他方面的不同而互不相似,因此单步检测会失败.但是,可以引入一些中间图像，如 “足球头盔”,作为中间域,从而顺利实现知识转移.<p>从广义上讲，深度 DA 是一种利用深度网络提高 DA 性能的方法。根据这一定义，具有深度特征的浅层方法可视为深度 DA 方法。浅层方法采用 DA，而深度网络只能提取矢量特征，无助于直接传递知识。例如，Lu 等人从 CNN 中提取卷积激活作为张量表示，然后进行张量对齐。<p>从狭义上讲，深度 DA 基于专为 DA 设计的深度学习架构，可以通过反向传播从深度网络中获得第一手效果。其直观思路是<strong>将 DA 嵌入到学习表征的过程中，学习一种既有语义意义又有领域不变性的深度特征表征。有了 “好 “的特征表示，目标任务的性能就会显著提高</strong>。<p>一步式数据采集的深度方法可归纳为三种情况.<p>第一种情况是<strong>基于差异(Discrepancy-based)</strong>的深度数据挖掘方法,该方法<strong>假定用标注或未标注的目标数据微调深度网络模型可以减少两个域之间的偏移</strong>.类判据(class criterion)、统计判据(statistic criterion)、架构判据(architecture criterion)和几何判据(geometric criterion)是进行微调的四种主要技术。<p><strong>class criterion</strong>:使用类标签信息作为在不同领域间转移知识的指南.在有监督的数据分析中,当目标领域的标签样本可用时,软标签和度量学习总是有效的.当无法获得此类样本时,可采用其他一些技术来替代类标签数据,如伪标签和属性表示法.<p><strong>statistic criterion</strong>:利用某些机制对源域和目标域之间的统计分布偏移进行配准。最常用的比较和减少分布偏移的方法有最大均值差异（MMD）、相关对齐（CORAL）、KullbackLeibler发散和 H 发散等。<p><strong>Architecture criterion</strong>:这些技术旨在通过调整深度网络的架构，提高学习更多可迁移特征的能力。已被证明具有成本效益的技术包括自适应批量归一化（BN）、弱相关权重、域引导剔除等。<p><strong>Geometric criterion</strong>:根据源域和目标域的几何特性将其连接起来。这一标准假定几何结构的关系可以减少域的偏移.<p>第二种情况可称为<strong>基于对抗(Adversarial-based)的深度 DA 方法</strong>。在这种情况下，<strong>会使用一个领域判别器来对数据点是来自源领域还是目标领域进行分类</strong>，通过对抗目标来鼓励领域混淆，从而使经验源映射分布和目标映射分布之间的距离最小化。<p>此外，根据是否存在生成模型，基于对抗的深度数据挖掘方法可分为两种情况。<p><strong>生成模型</strong>：将判别模型与一般基于生成对抗网络（GAN）的生成组件相结合。其中一种典型情况是使用源图像、噪声矢量或两者生成与目标样本相似的模拟样本，并保留源域的注释信息。<p><strong>非生成模型</strong>：特征提取器不是利用输入图像分布生成模型，而是利用源域中的标签学习判别表征，并通过域混淆损失将目标数据映射到同一空间，从而得到域不变表征.<p>第三种情况可称为<strong>基于重构(Reconstruction-based)</strong>的 DA 方法，它假定源样本或目标样本的数据重构有助于提高 DA 的性能。重构器既能确保域内表征的特异性，又能确保域间表征的不可分性。<ul><li>编码器-解码器重构：通过使用堆叠自动编码器（SAE），编码器-解码器重构方法将用于表征学习的编码器网络与用于数据重构的解码器网络结合起来.<li>对抗重构：重构误差是通过 GAN 识别器（如 dual GAN、cycle GAN和 disco GAN）获得的循环映射测量的每个图像域内重构图像与原始图像之间的差异。</ul><p>在多步骤 DA 中，<strong>我们首先要确定与源域和目标域的关系比直接联系更密切的中间域。其次，知识转移过程将在源域、中间域和目标域之间以较少信息损失的方式通过一步 DA 完成</strong>。因此，多步骤 DA 的关键在于如何选择和利用中间域；此外，根据文献，它可以分为三类：手工选择机制、基于特征的选择机制和基于表征的选择机制。<p><strong>Hand-crafted</strong>：用户根据经验确定中间域<p><strong>Instance-based</strong>：从辅助数据集中选择某些部分的数据来组成中间域，以训练深度网络<p><strong>Representation-based</strong>：通过冻结先前训练好的网络,并将其中间表征作为新网络的输入,可以实现转移<h4 id=Discrepancy-based-approaches><a title="Discrepancy-based approaches" class=headerlink href=#Discrepancy-based-approaches></a>Discrepancy-based approaches</h4><p>Yosinski 等人证明,由于脆弱的共适应性和表征特异性,深度网络学习到的可迁移特征具有局限性,而微调可以提高泛化性能.微调（也可视为基于差异的深度数据挖掘方法）是<strong>用源数据训练基础网络,然后直接重用前 n 层来构建目标网络</strong>.<strong>目标网络的其余层是随机初始化的,并根据差异损失进行训练</strong>.在训练过程中,可以根据目标数据集的大小及其与源数据集的相似度,对目标网络的前 n 层进行微调或冻结。<h5 id=class-criterion><a title="class criterion" class=headerlink href=#class-criterion></a>class criterion</h5><p>类标准是深度 DA 中最基本的训练损耗。<strong>在使用源数据对网络进行预训练后，目标模型的其余各层将类标签信息作为训练网络的指南</strong>。因此，假定可以从目标数据集中获得少量带标签的样本。理想情况下，类标签信息在有监督的数据分析中直接给出。大多数工作通常使用地面真实类的负对数似然与 softmax 作为训练损失，即 L = - ∑N i=0 yi log ˆ yi（ˆ yi 是模型的 softmax 预测，代表类概率）<p><img alt=image-20240630221825042 data-src=https://s2.loli.net/2024/06/30/5U4korDjOX1diuP.png><p>受 Tzeng 等人的启发， 通过同时最小化领域混淆损失（属于基于对抗的方法）和软标签损失，对网络进行了微调。使用软标签而不是硬标签可以保持跨域类之间的关系。Gebru 等人在的基础上修改了现有的适应算法，在细粒度的类级别 Lcsoft 和属性级别 Lasoft 上使用了软标签损失。<p>除了 softmax 损失外，还有其他方法可用作训练损失，以微调有监督 DA 中的目标模型。深度网络中的嵌入度量学习是另一种方法，它可以使来自不同领域、具有相同标签的样本距离更近，而具有不同标签的样本距离更远。基于这一思想构建了<strong>相应的语义对齐损失和分离损失</strong>。Hu 等人提出了深度转移度量学习，它应用了边际费雪分析准则和 MMD 准则.<p>然而，如果目标域中没有直接的类标签信息，我们该怎么办呢？众所周知，人类可以仅凭高层次的描述来识别未见过的类。例如，当我们得到 “高大的棕色长颈动物 “的描述时，我们就能识别出长颈鹿。为了模仿人类的能力，为每个类引入了高级语义属性。假设 a^c^ = (a^c^~1~, … , a^c^~m~ ) 是 c 类的属性表示，它具有固定长度的二进制值，在所有类中有 m 个属性.分类器为每个属性 a^m^ 提供 p(a^m^|x) 的估计值<p><strong>Statistic criterion</strong><p>虽然有些基于差异的方法会<strong>寻找伪标签、属性标签或其他标签目标数据的替代品</strong>，但更多的工作侧重于在<strong>无监督 DA 中通过最小化域分布差异来学习域不变表示</strong>。MMD 是通过核双样本测试比较两个数据集分布的有效指标。</p><script type="math/tex; mode=display">
MMD^{2}(s,t)=\sup_{\|\phi\|_{\mathcal{H}}\leq1}\left\|E_{x^{s}\sim s}[\phi(x^{s})]-E_{x^{t}\sim s}[\phi(x^{t})]\right\|_{\mathcal{H}}^{2}</script><p>在此基础上，Ghifary 等人提出了一种模型，在单隐层前馈神经网络中引入 MMD 指标。MMD 指标在每个域的表征之间进行计算，以减少潜在空间中的分布失配。</p><script type="math/tex; mode=display">
MMD^2(D_s,D_t)=\left\|\frac{1}{M}\sum_{i=1}^M\phi(x_i^s)-\frac{1}{N}\sum_{j=1}^N\phi(x_j^t)\right\|_H^2</script><p>Long 等人没有使用单层和线性 MMD，而是提出了深度适应网络 (DAN)，通过添加多个适应层和探索多个内核来匹配跨域边际分布的变化，同时假设条件分布保持不变（图 6）。然而，在实际应用中，这一假设相当强；换句话说，源分类器不能直接用于目标域。为了使其更具通用性，联合适应网络（JAN）基于联合最大均值差异（JMMD）准则，在多个特定领域层中调整输入特征和输出标签联合分布的偏移。Zhang 等人提出了 DTN，其中边际分布和条件分布都根据 MMD 进行匹配。<p>如果 φ 是特征核（即高斯核或拉普拉斯核），MMD 将比较所有阶次的统计量矩。与 MMD 不同，CORAL学习的是一种线性变换，它能使域之间的二阶统计量保持一致。Sun 和 Saenko 利用非线性变换将 CORAL 扩展到深度神经网络（深度 CORAL）。</p><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{CORAL}}=\frac{1}{4d^{2}}\|C_{S}-C_{T}\|_{F}^{2}</script><p>其中，‖-‖F^2^ 表示矩阵弗罗贝尼斯准则平方。C~S~ 和 C~T~ 分别表示源数据和目标数据的协方差矩阵。<p><strong>Architecture criterion</strong><p>其他一些方法则会优化网络架构，使分布差异最小化。这种适应行为可以在大多数深度 DA 模型中实现，如监督和非监督设置。Rozantsev 等人认为，对应层的权重不是共享的，而是通过权重正则化器 rw( - ) 进行关联，以考虑两个域的差异。<p><img alt=image-20240630223652513 data-src=https://s2.loli.net/2024/06/30/V3uX2QpSghyj8Bz.png><p><strong>Geometric criterion</strong><p>几何准则通过整合从源域到目标域的路径上的中间子空间来减轻域偏移。<h4 id=Adversarial-based-approaches><a title="Adversarial-based approaches" class=headerlink href=#Adversarial-based-approaches></a>Adversarial-based approaches</h4><p>GAN这里就不细说了.<h5 id=Non-generative-models><a title="Non-generative models" class=headerlink href=#Non-generative-models></a>Non-generative models</h5><p>深度DA的关键在于从源样本和目标样本中学习与领域无关的表征。有了这些表征，两个领域的分布就会足够相似，这样分类器就会被骗过，即使它是在源样本上训练出来的，也能直接用于目标领域。因此，表征是否存在领域混淆对于知识迁移至关重要。受 GAN 的启发引入了由判别器产生的领域混淆损失，以提高无生成器的深度 DA 性能。<p>域对抗神经网络（DANN）将梯度反转层（GRL）集成到标准架构中，以确保两个域的特征分布相似）。该网络由共享特征提取层和两个分类器组成。<p>DANN 通过使用 GRL 使域混淆损失（所有样本）和标签预测损失（源样本）最小化，同时使域混淆损失最大化。<p>与上述方法相比，对抗性判别域适应（ADDA） 通过取消权重来考虑独立的源和目标映射，目标模型的参数由预训练的源模型初始化。这种方法更加灵活，因为可以学习更多特定领域的特征提取。<p><img alt=image-20240630224001535 data-src=https://s2.loli.net/2024/06/30/7wux3qMtXFE8AlS.png><h4 id=Reconstruction-based-approaches><a title="Reconstruction-based approaches" class=headerlink href=#Reconstruction-based-approaches></a>Reconstruction-based approaches</h4><p>在 DA 中，源样本或目标样本的数据重构是一项辅助任务，它同时侧重于在两个域之间创建共享表征，并保持每个域的各自特点。<p><img alt=image-20240630224158266 data-src=https://s2.loli.net/2024/06/30/6xZTH9OQfjRpunr.png><p>这篇文章分了一个同构和异构的DA方法,个人感觉其实没有太大的必要,下面直接介绍.<p><img alt=image-20240630230147062 data-src=https://s2.loli.net/2024/06/30/JREocqfp8jiwLeP.png><p><strong>一些方法比较</strong><p><img alt=image-20240630230339375 data-src=https://s2.loli.net/2024/06/30/XtzFxWNyM5AC4ql.png><h2 id=一些具体工作><a class=headerlink href=#一些具体工作 title=一些具体工作></a>一些具体工作</h2><h3 id=Unsupervised-Domain-Adaptation-by-Backpropagation><a title="Unsupervised Domain Adaptation by Backpropagation" class=headerlink href=#Unsupervised-Domain-Adaptation-by-Backpropagation></a>Unsupervised Domain Adaptation by Backpropagation</h3><p><a href=https://arxiv.org/pdf/1409.7495与[1505.07818 rel=noopener target=_blank>https://arxiv.org/pdf/1409.7495与[1505.07818</a> (arxiv.org)](<a href=https://arxiv.org/pdf/1505.07818 rel=noopener target=_blank>https://arxiv.org/pdf/1505.07818</a>)<p>较早的域自适应工作,时间上跟GAN类似,本身也跟GAN很像.<p><img alt=image-20240623165143441 data-src=https://s2.loli.net/2024/06/23/ym2ExsfAFX9cpP7.png><h3 id=Simultaneous-Deep-Transfer-Across-Domains-and-Tasks><a title="Simultaneous Deep Transfer Across Domains and Tasks" class=headerlink href=#Simultaneous-Deep-Transfer-Across-Domains-and-Tasks></a>Simultaneous Deep Transfer Across Domains and Tasks</h3><p><a href=https://arxiv.org/pdf/1510.02192 rel=noopener target=_blank>https://arxiv.org/pdf/1510.02192</a><p>同样也是远古时期论文<h3 id=Minimum-Class-Confusion-for-Versatile-Domain-Adaptation><a title="Minimum Class Confusion for Versatile Domain Adaptation" class=headerlink href=#Minimum-Class-Confusion-for-Versatile-Domain-Adaptation></a>Minimum Class Confusion for Versatile Domain Adaptation</h3><p><img alt=image-20240627153353279 data-src=https://s2.loli.net/2024/06/27/w4semziX5HcTZo9.png><h3 id=Maximum-Classifier-Discrepancy-for-Unsupervised-Domain-Adaptation><a title="Maximum Classifier Discrepancy for Unsupervised Domain Adaptation" class=headerlink href=#Maximum-Classifier-Discrepancy-for-Unsupervised-Domain-Adaptation></a>Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</h3><p><img alt=image-20240627155801158 data-src=https://s2.loli.net/2024/06/27/ikL3uemU5nxNT7f.png><h3 id=Deep-Domain-Confusion-Maximizing-for-Domain-Invariance><a title="Deep Domain Confusion: Maximizing for Domain Invariance" class=headerlink href=#Deep-Domain-Confusion-Maximizing-for-Domain-Invariance></a>Deep Domain Confusion: Maximizing for Domain Invariance</h3><p><img alt=image-20240630160801199 data-src=https://s2.loli.net/2024/06/30/Uh35uECtq9zAWVm.png><h3 id=Taking-A-Closer-Look-at-Domain-Shift-Category-level-Adversaries-for-Semantics-Consistent-Domain-Adaptation><a title="Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation" class=headerlink href=#Taking-A-Closer-Look-at-Domain-Shift-Category-level-Adversaries-for-Semantics-Consistent-Domain-Adaptation></a>Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</h3><p><img alt=image-20240630231443263 data-src=https://s2.loli.net/2024/06/30/5pCyk8FE2alwqhL.png><p>提出catergory-level的对齐.<p>da的关键在于减少领域偏移，即强制两个领域的数据分布相似。常见的策略之一是通过对抗学习来对齐特征空间中的边际分布。然而，这种全局对齐策略并不考虑类别级的联合分布,这种全局移动的一个可能后果是，一些原本在源域和目标域之间对齐良好的类别可能会被错误地映射，从而导致目标域的分割结果变差。<p>为了解决这个问题，我们引入了一个类别级对抗网络，目的是在全局对齐趋势中执行局部语义一致性。我们的想法是仔细观察类别级联合分布，并通过自适应对抗损失来对齐每个类别。具体来说，我们降低了类别级对齐特征的对抗损失权重，同时增加了那些对齐不佳特征的对抗力。</p><script type="math/tex; mode=display">
\mathcal{L}_{seg}(G)=\sum_{i=1}^{H\times W}\sum_{c=1}^{C}-y_{ic}\log p_{ic}</script><script type="math/tex; mode=display">
\mathcal{L}_{weight}(G)=\frac{\vec{w_1}\cdot\vec{w_2}}{\|\vec{w_1}\|\|\vec{w_2}\|}</script><script type="math/tex; mode=display">
\begin{aligned}&\mathcal{L}_{adv}(G,D)=-E[\log(D(G(X_{S})))]-\\&E[(\lambda_{local}\mathcal{M}(p^{(1)},p^{(2)})+\epsilon)\log(1-D(G(X_{T})))] ,\end{aligned}</script><script type="math/tex; mode=display">
\begin{gathered}
\mathcal{L}_{CLAN}(G,D)= \mathcal{L}_{seg}(G)+\lambda_{weight}\mathcal{L}_{weight}(G)+ \\
\lambda_{adv}\mathcal{L}_{adv}(G,D) , 
\end{gathered}</script><script type="math/tex; mode=display">
G^*,D^*=\arg\min_G\max_D\mathcal{L}_{CLAN}(G,D).</script><h3 id=Cross-domain-adaptive-clustering-for-semisupervised-domain-adaptation><a title="Cross-domain adaptive clustering for semisupervised domain adaptation" class=headerlink href=#Cross-domain-adaptive-clustering-for-semisupervised-domain-adaptation></a>Cross-domain adaptive clustering for semisupervised domain adaptation</h3><p><img alt=image-20240702113906287 data-src=https://s2.loli.net/2024/07/02/xOTivnD4tj2PrUG.png><h3 id=Moment-Matching-for-Multi-Source-Domain-Adaptation><a title="Moment Matching for Multi-Source Domain Adaptation" class=headerlink href=#Moment-Matching-for-Multi-Source-Domain-Adaptation></a>Moment Matching for Multi-Source Domain Adaptation</h3><p><img alt=image-20240627164102546 data-src=https://s2.loli.net/2024/06/27/tDwx9ZesFkz57yo.png><h3 id=A-DIRT-T-APPROACH-TO-UNSUPERVISED-DOMAIN-ADAPTATION><a title="A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION" class=headerlink href=#A-DIRT-T-APPROACH-TO-UNSUPERVISED-DOMAIN-ADAPTATION></a>A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION</h3><p><img alt=image-20240627175801303 data-src=https://s2.loli.net/2024/06/27/lgRXwOIv43etCjW.png><h3 id=Reusing-the-Task-specific-Classifier-as-a-Discriminator-Discriminator-free-Adversarial-Domain-Adaptation><a title="Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation" class=headerlink href=#Reusing-the-Task-specific-Classifier-as-a-Discriminator-Discriminator-free-Adversarial-Domain-Adaptation></a>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation</h3><h4 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h4><p>对抗学习在无监督领域适应（UDA）方面取得了卓越的成绩。现有的对抗性 UDA 方法通常采<strong>用额外的判别器与特征提取器进行最小-最大博弈</strong>.<p>现有的对抗性 UDA 方法通<strong>常采用额外的判别器与特征提取器进行最小-最大博弈.然而,这些方法大多不能有效利用预测的判别信息</strong>，从而导致生成器的模式崩溃。<p>在这项工作中,我们从不同的角度来解决这个问题,设计了一种简单而有效的对抗范式，即<strong>无判别器对抗学习网络</strong>（DALN）,其中<strong>类别分类器被重新用作判别器</strong>，通过统一的目标实现明确的领域对齐和类别区分,使 DALN 能够利用预测的判别信息进行充分的特征对齐。<p>基本上，我们引入了一种Nuclear-norm Wasserstein discrepancy，NWD，它对进行判别具有明确的指导意义。这种 NWD 可以与分类器相结合，作为满足 K-Lipschitz 约束的判别器，而无需额外的权重剪切或梯度惩罚策略。<h4 id=引言><a class=headerlink href=#引言 title=引言></a>引言</h4><p>深度神经网络（DNN）在许多计算机视觉任务中取得了重大进展。然而，这些方法的成功在很大程度上取决于大量的注释数据，而获取这些数据极其耗时且昂贵。<p>此外，由于训练数据与真实世界测试数据之间存在差异，尽管进行了大量标注工作，但在标注数据上训练的 DNN 模型在测试集上的性能可能会急剧下降。为了解决这个问题，人们深入探讨了无监督领域适应（UDA）]，其目的是在领域转移的情况下，将知识从有标注的源领域转移到无标注的目标领域。<p>受 Ben-David 等人理论分析的启发，现有的 UDA 方法通常都在<strong>探索学习领域不变特征表征的思路</strong>。一般来说，这些方法可分为两个分支，即矩匹配方法和对抗学习方法。矩匹配方法通过匹配源域和目标域特征的明确分布差异，明确减少域偏移。<p>对抗性学习方法通过进行对抗性最小-最大双人博弈，隐含地减轻了领域转移的影响，这促使生成器提取不可区分的特征，以欺骗鉴别器<p>有一种方法利用两个特定任务分类器 C 和 C′的差异（可将其视为判别器）来隐式地实现对抗学习并提高特征的可转移性。这种范式使 UDA 方法<strong>能够减少类级域差异</strong>。但是，遵循这种范式的方法容易受到模糊预测的影响，从而阻碍适应性优化。<p>另一种方法直接构建了一个额外的域判别器 D，通过<strong>充分混淆跨域特征表示来提高特征的可转移性</strong>。然而，遵循这一范式的方法<strong>通常侧重于领域级特征混淆</strong>，这可能<strong>会损害类别级信息</strong>，从而导致模式崩溃问题<p><img alt=image-20240705114005812 data-src=https://s2.loli.net/2024/07/05/t2JurbfKWowqQEs.png><h3 id=Domain-Adaptation-in-Object-Detection><a title="Domain Adaptation in Object Detection" class=headerlink href=#Domain-Adaptation-in-Object-Detection></a>Domain Adaptation in Object Detection</h3><h4 id=Domain-Adaptive-Faster-R-CNN-for-Object-Detection-in-the-Wild><a title="Domain Adaptive Faster R-CNN for Object Detection in the Wild" class=headerlink href=#Domain-Adaptive-Faster-R-CNN-for-Object-Detection-in-the-Wild></a>Domain Adaptive Faster R-CNN for Object Detection in the Wild</h4><p><img alt=image-20240630231624714 data-src=https://s2.loli.net/2024/06/30/dm3lApv8HMux15G.png><p>较早的用于目标检测的domain adaptation方法,使用GRL和不同level的分类器.<h4 id=CDTRANS-CROSS-DOMAIN-TRANSFORMER-FOR-UNSUPERVISED-DOMAIN-ADAPTATION><a title="CDTRANS: CROSS-DOMAIN TRANSFORMER FOR UNSUPERVISED DOMAIN ADAPTATION" class=headerlink href=#CDTRANS-CROSS-DOMAIN-TRANSFORMER-FOR-UNSUPERVISED-DOMAIN-ADAPTATION></a>CDTRANS: CROSS-DOMAIN TRANSFORMER FOR UNSUPERVISED DOMAIN ADAPTATION</h4><p>无监督领域适应（UDA）旨在将从标注源领域学习到的知识转移到不同的无标注目标领域。大多数现有的无监督领域适应方法都侧重于学习领域不变的特征表征，无论是从<strong>领域层面还是从类别层面</strong>，都使用基于卷积神经网络（CNN）的框架。<strong>基于类别层的 UDA 面临的一个基本问题是，目标域中的样本会产生伪标签，而伪标签通常噪声过大，无法进行准确的域对齐，这不可避免地会影响 UDA 的性能</strong>。<p>随着 Transformer 在各种任务中的成功应用，我们发现 Transformer 中的交叉关注对噪声输入对具有鲁棒性，可以更好地进行特征对齐，因此本文采用 Transformer 来完成具有挑战性的 UDA 任务。<p>具体来说,为了生成准确的输入对,设计了<strong>一种双向中心感知标签算法,为目标样本生成伪标签</strong>。除了伪标签,我们还提出了一个<strong>权重共享的三重分支transformer框架</strong>,分别用于源/目标特征学习和源/目标域对齐的自我关注和交叉关注.这种设计明确地强化了该框架,使其能够同时学习具有区分性的特定领域表征和领域不变表征。所提出的方法被称为 CDTrans（跨域transformer）它是用纯transformer解决方案解决 UDA 任务的首次尝试之一.<h4 id=相关工作><a class=headerlink href=#相关工作 title=相关工作></a>相关工作</h4><p>UDA方法主要有两个层次：领域级和类别级。域级 UDA 通过将源域和目标域拉入不同尺度级别的相同分布中，来缓解源域和目标域之间的分布分歧。常用的分歧度量包括最大均值差异（MMD）和相关对齐（CORAL）。<p>最近，一些研究侧重于通过特征提取器和两个特定领域分类器之间的对抗方式进行细粒度类别级标签分布对齐。与领域尺度的粗粒度对齐不同，<strong>这种方法通过将目标样本推向源样本在每个类别中的分布，来对齐源和目标领域数据之间的每个类别分布。显然，细粒度配准能在同一标签空间内实现更精确的分布配准。</strong>尽管对抗方法通过在类别级别上融合源样本和目标样本的细粒度配准操作实现了新的改进，但它仍然无法解决噪声样本在错误类别中的问题。我们的方法采用了<strong>类别级 UDA</strong> 的 Transformers 来解决噪声问题。<p>伪标签法首次被引入半监督学习,并在领域适应任务中得到普及.它<strong>利用预测概率学习标记未标记数据,并与标记数据一起执行微调</strong>.在将伪标签用于领域适应任务方面，采用<strong>伪标签进行条件分布对齐</strong>；<strong>将伪标签作为领域适应的正则化</strong>；Zou 等通过交替求解伪标签设计了一种自训练框架；Caron 等提出了一种深度自监督方法，通过 k-means 聚类生成伪标签来逐步训练模型；Liang 等开发了一种自监督伪标签方法来减轻噪声伪标签的影响。在Liang 等人的基础上，本文提出了一种双向中心感知标注算法，以进一步过滤噪声伪标签对<p><img alt=image-20240702114426708 data-src=C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20240702114426708.png><h4 id=Uncertainty-Aware-Unsupervised-Domain-Adaptation-in-Object-Detection><a title="Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection" class=headerlink href=#Uncertainty-Aware-Unsupervised-Domain-Adaptation-in-Object-Detection></a>Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection</h4><p><img alt=image-20240705113637158 data-src=https://s2.loli.net/2024/07/05/DNVsxMT7WhRHvX5.png><h4 id=ConfMix-Unsupervised-Domain-Adaptation-for-Object-Detection-via-Confidence-based-Mixing><a title="ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing" class=headerlink href=#ConfMix-Unsupervised-Domain-Adaptation-for-Object-Detection-via-Confidence-based-Mixing></a>ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing</h4><p><img alt=image-20240705114702994 data-src=https://s2.loli.net/2024/07/05/wEb6aNTnig8HWIt.png><p>用于对象检测的无监督域自适应 （UDA） 旨在调整在源域上训练的模型，以检测来自注释不可用的新目标域的实例。与传统方法不同，我们提出了ConfMix，这是第一种<strong>引入基于区域级检测置信度的样本混合策略的方法，用于自适应目标检测器学习</strong>。我们将对应于最可靠伪检测的目标样本的局部区域与源图像混合，并应用额外的一致性损失项以逐渐适应目标数据分布。为了稳健地定义一个区域的置信度分数，我们利用了每次伪检测的置信度分数，该置信度同时考虑了检测器依赖的置信度和边界框的不确定性<h3 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h3><ol><li><a href=https://arxiv.org/pdf/2208.07422 rel=noopener target=_blank>2208.07422 (arxiv.org)</a><li><a href=https://www.youtube.com/watch?v=gvfLq4sPW4k&ab_channel=Hung-yiLee rel=noopener target=_blank>[TA 補充課] More about Domain Adaptation (1/2) (由助教趙崇皓同學講授) (youtube.com)</a></ol><p><strong>论文与代码库</strong><ul><li><p><a href=https://github.com/thuml/Transfer-Learning-Library rel=noopener target=_blank>thuml/Transfer-Learning-Library: Transfer Learning Library for Domain Adaptation, Task Adaptation, and Domain Generalization (github.com)</a></p><li><p><a href=https://github.com/adapt-python/adapt rel=noopener target=_blank>adapt-python/adapt: Awesome Domain Adaptation Python Toolbox (github.com)</a></p><li><p><a href=https://github.com/jindongwang/transferlearning rel=noopener target=_blank>jindongwang/transferlearning: Transfer learning / domain adaptation / domain generalization / multi-task learning etc. Papers, codes, datasets, applications, tutorials.-迁移学习 (github.com)</a></p><li><p><a href=https://github.com/zhaoxin94/awesome-domain-adaptation rel=noopener target=_blank>zhaoxin94/awesome-domain-adaptation: A collection of AWESOME things about domian adaptation (github.com)</a></p><li><p><a href=https://github.com/barebell/DA rel=noopener target=_blank>barebell/DA: Unsupervised Domain Adaptation Papers and Code (github.com)</a></p><li><p><a href=https://github.com/agrija9/Deep-Unsupervised-Domain-Adaptation rel=noopener target=_blank>agrija9/Deep-Unsupervised-Domain-Adaptation: Pytorch implementation of four neural network based domain adaptation techniques: DeepCORAL, DDC, CDAN and CDAN+E. Evaluated on benchmark dataset Office31. (github.com)</a></p><li><p><a href=https://github.com/jvanvugt/pytorch-domain-adaptation?tab=readme-ov-file rel=noopener target=_blank>jvanvugt/pytorch-domain-adaptation: A collection of implementations of adversarial domain adaptation algorithms (github.com)</a></p></ul><p>目标检测的域适应<ul><li><a href=https://github.com/kinredon/DA_Detection_Material rel=noopener target=_blank>kinredon/DA_Detection_Material: A Collection of Domain Adaptation for Object Detection Material (github.com)</a><li><a href=https://github.com/wangs311/awesome-domain-adaptation-object-detection?tab=readme-ov-file rel=noopener target=_blank>wangs311/awesome-domain-adaptation-object-detection: A collection of papers about domain adaptation object detection. Welcome to PR the works (papers, repositories) that are missed by the repo. (github.com)</a><li><a href=https://zhuanlan.zhihu.com/p/371721493 rel=noopener target=_blank>【领域自适应目标检测】论文及代码整理 - 知乎 (zhihu.com)</a></ul><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="Domain Adaptation Method" href=https://www.sekyoro.top/2024/05/21/Domain-Adaptation-Method/>https://www.sekyoro.top/2024/05/21/Domain-Adaptation-Method/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/domain-adaptation/ rel=tag><i class="fa fa-tag"></i> domain adaptation</a></div><div class=post-nav><div class=post-nav-item><a href=/2024/05/17/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%BA%8C/ rel=prev title=协同感知学习(二)> <i class="fa fa-chevron-left"></i> 协同感知学习(二) </a></div><div class=post-nav-item><a href=/2024/05/23/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/ rel=next title=协同感知数据集和代码库介绍> 协同感知数据集和代码库介绍 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E7%BB%BC%E8%BF%B0><span class=nav-number>1.</span> <span class=nav-text>综述</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Deep-Unsupervised-Domain-Adaptation-A-Review-of-Recent-Advances-and-Perspectives><span class=nav-number>1.1.</span> <span class=nav-text>Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Statistic-Divergence-Alignment><span class=nav-number>1.1.0.1.</span> <span class=nav-text>Statistic Divergence Alignment</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Deep-visual-domain-adaptation-A-survey><span class=nav-number>1.2.</span> <span class=nav-text>Deep visual domain adaptation: A survey</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Discrepancy-based-approaches><span class=nav-number>1.2.0.1.</span> <span class=nav-text>Discrepancy-based approaches</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#class-criterion><span class=nav-number>1.2.0.1.1.</span> <span class=nav-text>class criterion</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#Adversarial-based-approaches><span class=nav-number>1.2.0.2.</span> <span class=nav-text>Adversarial-based approaches</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Non-generative-models><span class=nav-number>1.2.0.2.1.</span> <span class=nav-text>Non-generative models</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#Reconstruction-based-approaches><span class=nav-number>1.2.0.3.</span> <span class=nav-text>Reconstruction-based approaches</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E4%B8%80%E4%BA%9B%E5%85%B7%E4%BD%93%E5%B7%A5%E4%BD%9C><span class=nav-number>1.3.</span> <span class=nav-text>一些具体工作</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Unsupervised-Domain-Adaptation-by-Backpropagation><span class=nav-number>1.3.1.</span> <span class=nav-text>Unsupervised Domain Adaptation by Backpropagation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Simultaneous-Deep-Transfer-Across-Domains-and-Tasks><span class=nav-number>1.3.2.</span> <span class=nav-text>Simultaneous Deep Transfer Across Domains and Tasks</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Minimum-Class-Confusion-for-Versatile-Domain-Adaptation><span class=nav-number>1.3.3.</span> <span class=nav-text>Minimum Class Confusion for Versatile Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Maximum-Classifier-Discrepancy-for-Unsupervised-Domain-Adaptation><span class=nav-number>1.3.4.</span> <span class=nav-text>Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Deep-Domain-Confusion-Maximizing-for-Domain-Invariance><span class=nav-number>1.3.5.</span> <span class=nav-text>Deep Domain Confusion: Maximizing for Domain Invariance</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Taking-A-Closer-Look-at-Domain-Shift-Category-level-Adversaries-for-Semantics-Consistent-Domain-Adaptation><span class=nav-number>1.3.6.</span> <span class=nav-text>Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Cross-domain-adaptive-clustering-for-semisupervised-domain-adaptation><span class=nav-number>1.3.7.</span> <span class=nav-text>Cross-domain adaptive clustering for semisupervised domain adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Moment-Matching-for-Multi-Source-Domain-Adaptation><span class=nav-number>1.3.8.</span> <span class=nav-text>Moment Matching for Multi-Source Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#A-DIRT-T-APPROACH-TO-UNSUPERVISED-DOMAIN-ADAPTATION><span class=nav-number>1.3.9.</span> <span class=nav-text>A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Reusing-the-Task-specific-Classifier-as-a-Discriminator-Discriminator-free-Adversarial-Domain-Adaptation><span class=nav-number>1.3.10.</span> <span class=nav-text>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>1.3.10.1.</span> <span class=nav-text>摘要</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%BC%95%E8%A8%80><span class=nav-number>1.3.10.2.</span> <span class=nav-text>引言</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Domain-Adaptation-in-Object-Detection><span class=nav-number>1.3.11.</span> <span class=nav-text>Domain Adaptation in Object Detection</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Domain-Adaptive-Faster-R-CNN-for-Object-Detection-in-the-Wild><span class=nav-number>1.3.11.1.</span> <span class=nav-text>Domain Adaptive Faster R-CNN for Object Detection in the Wild</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#CDTRANS-CROSS-DOMAIN-TRANSFORMER-FOR-UNSUPERVISED-DOMAIN-ADAPTATION><span class=nav-number>1.3.11.2.</span> <span class=nav-text>CDTRANS: CROSS-DOMAIN TRANSFORMER FOR UNSUPERVISED DOMAIN ADAPTATION</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C><span class=nav-number>1.3.11.3.</span> <span class=nav-text>相关工作</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Uncertainty-Aware-Unsupervised-Domain-Adaptation-in-Object-Detection><span class=nav-number>1.3.11.4.</span> <span class=nav-text>Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#ConfMix-Unsupervised-Domain-Adaptation-for-Object-Detection-via-Confidence-based-Mixing><span class=nav-number>1.3.11.5.</span> <span class=nav-text>ConfMix: Unsupervised Domain Adaptation for Object Detection via Confidence-based Mixing</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>1.3.12.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>238</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/domain-adaptation/ rel=tag>domain adaptation</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.6m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>39:30</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>