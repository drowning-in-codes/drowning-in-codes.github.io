<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="既text2img和大语言模型之后的热点,AI Agent和RAG,简单来说在既有的多模态大模型基础上打造应用,比如利用多个智能体以及外部数据进行搜索. 目前来看还是有一些发展潜力,这方面的资料多见于Hugging Face,LangChain,llama index等等" name=description><meta content=article property=og:type><meta content="Agentic RAG and LLMs-based assistant" property=og:title><meta content=https://www.sekyoro.top/2024/12/26/Agentic-RAG-and-LLMs-based-assistant/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="既text2img和大语言模型之后的热点,AI Agent和RAG,简单来说在既有的多模态大模型基础上打造应用,比如利用多个智能体以及外部数据进行搜索. 目前来看还是有一些发展潜力,这方面的资料多见于Hugging Face,LangChain,llama index等等" property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/12/26/4HhWUIlqwgAYEKJ.png property=og:image><meta content=https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rag-diagram.png property=og:image><meta content=https://s2.loli.net/2024/12/26/WSB5vpIPZzTwC31.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20241228204813028.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20241231183826980.png property=og:image><meta content=2024-12-26T12:25:13.000Z property=article:published_time><meta content=2024-12-31T10:38:35.705Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="AI Agent" property=article:tag><meta content=RAG property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/12/26/4HhWUIlqwgAYEKJ.png name=twitter:image><link href=https://www.sekyoro.top/2024/12/26/Agentic-RAG-and-LLMs-based-assistant/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>Agentic RAG and LLMs-based assistant | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/12/26/Agentic-RAG-and-LLMs-based-assistant/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Agentic RAG and LLMs-based assistant</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-12-26 20:25:13" datetime=2024-12-26T20:25:13+08:00>2024-12-26</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-12-31 18:38:35" datetime=2024-12-31T18:38:35+08:00 itemprop=dateModified>2024-12-31</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>14k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>13 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>既text2img和大语言模型之后的热点,AI Agent和RAG,简单来说在既有的多模态大模型基础上打造应用,比如利用多个智能体以及外部数据进行搜索. 目前来看还是有一些发展潜力,这方面的资料多见于Hugging Face,LangChain,llama index等等<br><span id=more></span><p>​ 过去一段时间涌现了一大堆大模型,参数量多达几十B,并且这些模型在许多benchmark上都似乎达到了很高的水平(甚至最近OpenAI的o3达到超越普通人的水平,AGI达没达到不知道,Hype是足够了). 普通人想免费下载的大模型就是贴心的llama了,可以在llama上进行微调等操作. 此外文生图等模型既Stable Diffusion v3.5之后,出走后的人员打造FLUX.<p>​ 种种迹象表明,Foundation Model的发展已经超出普通人能玩的范围了,因为背后耗费这成千上万张显卡以及大量人员精心修剪后的数据。而如何实现长期盈利才是关键,OpenAI早已打出了名声,作为行业标杆,不少人会去订阅ChatGPT Plus,即使再贵. 但其他公式就会遭殃了,毕竟一个人使用一到两种类似的AI服务已经足够,于是形成了类似冠军争夺,专业做AI的,只有前几家才能生成. 大厂可以背靠大量数据集和人力做模型以及服务,中小公司相对更困难. 而普通人貌似要么花钱使用Open(并不Open)AI更高质量服务,要么用开源的模型,事实上,在这个开源领域正在或者应该,在开发者之间流行,作为辅助工具抑或创造性的图片生成,即使无法盈利,但也逐渐变成计算器一样的工具成为普遍现象. 其中的一些有趣技术就包括T2I文生图以及相关的LoRA,ControlNet. 而多模态大语言模型就是RAG和AI Agent了.<p>目前国内大中厂以及个人开发者都在这些方向不断努力<p><img alt=image-20241226205431156 data-src=https://s2.loli.net/2024/12/26/4HhWUIlqwgAYEKJ.png><h2 id=For-any-API-provider><a title="For any API provider" class=headerlink href=#For-any-API-provider></a>For any API provider</h2><p>因为目前有许多大模型供应商,比如grok,gemini,openai,它们提供不同的大模型和接口. 如果为多个不同供应商分别写然后能够统一进行调用就更好了. HuggingFace提供了相关方法<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> os</span><br><span class=line><span class=keyword>from</span> openai <span class=keyword>import</span> OpenAI</span><br><span class=line></span><br><span class=line>openai_role_conversions = {</span><br><span class=line>    MessageRole.TOOL_RESPONSE: MessageRole.USER,</span><br><span class=line>}</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>OpenAIEngine</span>:</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, model_name=<span class=string>"gpt-4o"</span></span>):</span></span><br><span class=line>        self.model_name = model_name</span><br><span class=line>        self.client = OpenAI(</span><br><span class=line>            api_key=os.getenv(<span class=string>"OPENAI_API_KEY"</span>),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__call__</span>(<span class=params>self, messages, stop_sequences=[]</span>):</span></span><br><span class=line>        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)</span><br><span class=line></span><br><span class=line>        response = self.client.chat.completions.create(</span><br><span class=line>            model=self.model_name,</span><br><span class=line>            messages=messages,</span><br><span class=line>            stop=stop_sequences,</span><br><span class=line>            temperature=<span class=number>0.5</span>,</span><br><span class=line>        )</span><br><span class=line>        <span class=keyword>return</span> response.choices[<span class=number>0</span>].message.content</span><br></pre></table></figure><p>核心是构建一个llm_engine然后通过ReactCodeAgent或者其他Agent调用<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> anthropic <span class=keyword>import</span> Anthropic, AnthropicBedrock</span><br><span class=line><span class=comment># Cf this page for using Anthropic from Bedrock: https://docs.anthropic.com/en/api/claude-on-amazon-bedrock</span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>AnthropicEngine</span>:</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, model_name=<span class=string>"claude-3-5-sonnet-20240620"</span>, use_bedrock=<span class=literal>False</span></span>):</span></span><br><span class=line>        self.model_name = model_name</span><br><span class=line>        <span class=keyword>if</span> use_bedrock:</span><br><span class=line>            self.model_name = <span class=string>"anthropic.claude-3-5-sonnet-20240620-v1:0"</span></span><br><span class=line>            self.client = AnthropicBedrock(</span><br><span class=line>                aws_access_key=os.getenv(<span class=string>"AWS_BEDROCK_ID"</span>),</span><br><span class=line>                aws_secret_key=os.getenv(<span class=string>"AWS_BEDROCK_KEY"</span>),</span><br><span class=line>                aws_region=<span class=string>"us-east-1"</span>,</span><br><span class=line>            )</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            self.client = Anthropic(</span><br><span class=line>                api_key=os.getenv(<span class=string>"ANTHROPIC_API_KEY"</span>),</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__call__</span>(<span class=params>self, messages, stop_sequences=[]</span>):</span></span><br><span class=line>        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)</span><br><span class=line>        index_system_message, system_prompt = <span class=literal>None</span>, <span class=literal>None</span></span><br><span class=line>        <span class=keyword>for</span> index, message <span class=keyword>in</span> <span class=built_in>enumerate</span>(messages):</span><br><span class=line>            <span class=keyword>if</span> message[<span class=string>"role"</span>] == MessageRole.SYSTEM:</span><br><span class=line>                index_system_message = index</span><br><span class=line>                system_prompt = message[<span class=string>"content"</span>]</span><br><span class=line>        <span class=keyword>if</span> system_prompt <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>            <span class=keyword>raise</span> Exception(<span class=string>"No system prompt found!"</span>)</span><br><span class=line></span><br><span class=line>        filtered_messages = [message <span class=keyword>for</span> i, message <span class=keyword>in</span> <span class=built_in>enumerate</span>(messages) <span class=keyword>if</span> i != index_system_message]</span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>len</span>(filtered_messages) == <span class=number>0</span>:</span><br><span class=line>            <span class=built_in>print</span>(<span class=string>"Error, no user message:"</span>, messages)</span><br><span class=line>            <span class=keyword>assert</span> <span class=literal>False</span></span><br><span class=line></span><br><span class=line>        response = self.client.messages.create(</span><br><span class=line>            model=self.model_name,</span><br><span class=line>            system=system_prompt,</span><br><span class=line>            messages=filtered_messages,</span><br><span class=line>            stop_sequences=stop_sequences,</span><br><span class=line>            temperature=<span class=number>0.5</span>,</span><br><span class=line>            max_tokens=<span class=number>2000</span>,</span><br><span class=line>        )</span><br><span class=line>        full_response_text = <span class=string>""</span></span><br><span class=line>        <span class=keyword>for</span> content_block <span class=keyword>in</span> response.content:</span><br><span class=line>            <span class=keyword>if</span> content_block.<span class=built_in>type</span> == <span class=string>"text"</span>:</span><br><span class=line>                full_response_text += content_block.text</span><br><span class=line>        <span class=keyword>return</span> full_response_text</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>agent = ReactCodeAgent(tools=[], llm_engine=llm_engine)</span><br></pre></table></figure><h2 id=Intro-to-RAG><a title="Intro to RAG" class=headerlink href=#Intro-to-RAG></a>Intro to RAG</h2><p>RAG是一种流行的方法，用于解决LLM由于所述内容不在其训练数据中而无法意识到特定内容的问题，或者即使以前看到过该内容也会产生幻觉。这些特定内容可能是专有的、敏感的，或者是最近的和经常更新的。<p>如果数据是静态的，并且不定期更改，则可以考虑对大型模型进行微调。然而，在许多情况下，微调可能是昂贵的，并且，当重复进行时（例如，为了解决数据漂移），会导致“模型转移”。这是指模型的行为以一种没有改变的方式发生变化<p>RAG（检索增强生成）不需要模型微调。相反，RAG通过向LLM提供从相关数据中检索的附加上下文来工作，以便它可以生成更明智的响应。<p><img alt="RAG diagram" data-src=https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rag-diagram.png><p>这类应用可以非常方便写文档、报告的人,因为时常会有比较新的消息而大模型训练资料中没有,这个时候可以让大模型主动调用外部工具(比如搜索引擎),或者主动给它额外的工具从而补充大模型的能力.<h3 id=简易的RAG工作流><a class=headerlink href=#简易的RAG工作流 title=简易的RAG工作流></a>简易的RAG工作流</h3><h4 id=加载数据><a class=headerlink href=#加载数据 title=加载数据></a>加载数据</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> getpass <span class=keyword>import</span> getpass</span><br><span class=line><span class=keyword>from</span> langchain.document_loaders <span class=keyword>import</span> GitHubIssuesLoader</span><br><span class=line>ACCESS_TOKEN = getpass(<span class=string>"YOUR_GITHUB_PERSONAL_TOKEN"</span>)</span><br><span class=line>loader = GitHubIssuesLoader(repo=<span class=string>"huggingface/peft"</span>, access_token=ACCESS_TOKEN, include_prs=<span class=literal>False</span>, state=<span class=string>"all"</span>)</span><br><span class=line>docs = loader.load()</span><br></pre></table></figure><p>使用github加载数据,将数据分块加载，最常见和直接的分块方法是定义一个固定大小的块，以及它们之间是否应该有任何重叠。在块之间保持一些重叠可以让我们在块之间保留一些语义上下文。一般文本的推荐拆分器是RecursiveCharacterTextSplitter<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> langchain.text_splitter <span class=keyword>import</span> RecursiveCharacterTextSplitter</span><br><span class=line>splitter = RecursiveCharacterTextSplitter(chunk_size=<span class=number>512</span>, chunk_overlap=<span class=number>30</span>)</span><br><span class=line>chunked_docs = splitter.split_documents(docs)</span><br></pre></table></figure><h4 id=创建embedding和retriever><a class=headerlink href=#创建embedding和retriever title=创建embedding和retriever></a>创建embedding和retriever</h4><p>需要将文件转为特征,为了创建文档块嵌入,这里使用HuggingFaceEmbeddings和BAAI/ big -base-en-v1.5嵌入模型。Hub上还有许多其他的嵌入模型<a href=https://huggingface.co/spaces/mteb/leaderboard rel=noopener target=_blank>MTEB Leaderboard - a Hugging Face Space by mteb</a>，使用FAISS作为嵌入特征搜索库,相当于创建一个数据库,其中数据就是嵌入后的特征<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> langchain.vectorstores <span class=keyword>import</span> FAISS</span><br><span class=line><span class=keyword>from</span> langchain.embeddings <span class=keyword>import</span> HuggingFaceEmbeddings</span><br><span class=line></span><br><span class=line>db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=<span class=string>"BAAI/bge-base-en-v1.5"</span>))</span><br></pre></table></figure><p>设置检索方式,近邻搜索,返回最高的4个结果.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>retriever = db.as_retriever(search_type=<span class=string>"similarity"</span>, search_kwargs={<span class=string>"k"</span>: <span class=number>4</span>})</span><br></pre></table></figure><h4 id=LLM><a class=headerlink href=#LLM title=LLM></a>LLM</h4><p>刚才使用了嵌入模型,现在使用对输入prompt以及回复的llm,相关榜单<a href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard rel=noopener target=_blank>Open LLM Leaderboard - a Hugging Face Space by open-llm-leaderboard</a>.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class=line></span><br><span class=line>model_name = <span class=string>"HuggingFaceH4/zephyr-7b-beta"</span></span><br><span class=line></span><br><span class=line>bnb_config = BitsAndBytesConfig(</span><br><span class=line>    load_in_4bit=<span class=literal>True</span>, bnb_4bit_use_double_quant=<span class=literal>True</span>, bnb_4bit_quant_type=<span class=string>"nf4"</span>, bnb_4bit_compute_dtype=torch.bfloat16</span><br><span class=line>)</span><br><span class=line></span><br><span class=line>model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)</span><br><span class=line>tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br></pre></table></figure><p>此外对模型做了量化减小体积<h4 id=搭建LLM链><a class=headerlink href=#搭建LLM链 title=搭建LLM链></a>搭建LLM链</h4><p>设置pipeline,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> langchain.llms <span class=keyword>import</span> HuggingFacePipeline</span><br><span class=line><span class=keyword>from</span> langchain.prompts <span class=keyword>import</span> PromptTemplate</span><br><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> pipeline</span><br><span class=line><span class=keyword>from</span> langchain_core.output_parsers <span class=keyword>import</span> StrOutputParser</span><br><span class=line></span><br><span class=line>text_generation_pipeline = pipeline(</span><br><span class=line>    model=model,</span><br><span class=line>    tokenizer=tokenizer,</span><br><span class=line>    task=<span class=string>"text-generation"</span>,</span><br><span class=line>    temperature=<span class=number>0.2</span>,</span><br><span class=line>    do_sample=<span class=literal>True</span>,</span><br><span class=line>    repetition_penalty=<span class=number>1.1</span>,</span><br><span class=line>    return_full_text=<span class=literal>True</span>,</span><br><span class=line>    max_new_tokens=<span class=number>400</span>,</span><br><span class=line>)</span><br><span class=line></span><br><span class=line>llm = HuggingFacePipeline(pipeline=text_generation_pipeline)</span><br><span class=line></span><br><span class=line>prompt_template = <span class=string>"""</span></span><br><span class=line><span class=string><|system|></span></span><br><span class=line><span class=string>Answer the question based on your knowledge. Use the following context to help:</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>{context}</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>&LT/s></span></span><br><span class=line><span class=string><|user|></span></span><br><span class=line><span class=string>{question}</span></span><br><span class=line><span class=string>&LT/s></span></span><br><span class=line><span class=string><|assistant|></span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string> """</span></span><br><span class=line></span><br><span class=line>prompt = PromptTemplate(</span><br><span class=line>    input_variables=[<span class=string>"context"</span>, <span class=string>"question"</span>],</span><br><span class=line>    template=prompt_template,</span><br><span class=line>)</span><br><span class=line></span><br><span class=line>llm_chain = prompt | llm | StrOutputParser()</span><br></pre></table></figure><p>结合llm_chain搭配retriver<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> langchain_core.runnables <span class=keyword>import</span> RunnablePassthrough</span><br><span class=line></span><br><span class=line>retriever = db.as_retriever()</span><br><span class=line></span><br><span class=line>rag_chain = {<span class=string>"context"</span>: retriever, <span class=string>"question"</span>: RunnablePassthrough()} | llm_chain</span><br></pre></table></figure><h4 id=比较结果><a class=headerlink href=#比较结果 title=比较结果></a>比较结果</h4><p>在没有rag下<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>question = <span class=string>"How do you combine multiple adapters?"</span></span><br><span class=line>llm_chain.invoke({<span class=string>"context"</span>: <span class=string>""</span>, <span class=string>"question"</span>: question})</span><br></pre></table></figure><p>在有rag下<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>rag_chain.invoke(question)</span><br></pre></table></figure><h2 id=Agent-with-tool-calling><a title="Agent with tool-calling" class=headerlink href=#Agent-with-tool-calling></a>Agent with tool-calling</h2><p>大模型搭配tool calling(比如调用计算器,编程语言解析器,浏览器搜索等)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> load_tool, ReactCodeAgent, HfApiEngine</span><br><span class=line></span><br><span class=line><span class=comment># Import tool from Hub</span></span><br><span class=line>image_generation_tool = load_tool(<span class=string>"m-ric/text-to-image"</span>, cache=<span class=literal>False</span>)</span><br><span class=line></span><br><span class=line><span class=comment># Import tool from LangChain</span></span><br><span class=line><span class=keyword>from</span> transformers.agents.search <span class=keyword>import</span> DuckDuckGoSearchTool</span><br><span class=line></span><br><span class=line>search_tool = DuckDuckGoSearchTool()</span><br><span class=line></span><br><span class=line>llm_engine = HfApiEngine(<span class=string>"Qwen/Qwen2.5-72B-Instruct"</span>)</span><br><span class=line><span class=comment># Initialize the agent with both tools</span></span><br><span class=line>agent = ReactCodeAgent(tools=[image_generation_tool, search_tool], llm_engine=llm_engine)</span><br><span class=line></span><br><span class=line><span class=comment># Run it!</span></span><br><span class=line>result = agent.run(</span><br><span class=line>    <span class=string>"Generate me a photo of the car that James bond drove in the latest movie."</span>,</span><br><span class=line>)</span><br><span class=line>result</span><br></pre></table></figure><p>使用Qwen-2.5大模型,搭配文生图和搜索工具,相当于结合了多个模型和外部工具,让原本单一的模型具备多种功能.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> datasets</span><br><span class=line><span class=keyword>from</span> langchain.docstore.document <span class=keyword>import</span> Document</span><br><span class=line><span class=keyword>from</span> langchain.text_splitter <span class=keyword>import</span> RecursiveCharacterTextSplitter</span><br><span class=line><span class=keyword>from</span> langchain.vectorstores <span class=keyword>import</span> FAISS</span><br><span class=line><span class=keyword>from</span> langchain_community.embeddings <span class=keyword>import</span> HuggingFaceEmbeddings</span><br><span class=line>knowledge_base = datasets.load_dataset(<span class=string>"m-ric/huggingface_doc"</span>, split=<span class=string>"train"</span>)</span><br><span class=line></span><br><span class=line></span><br><span class=line>source_docs = [</span><br><span class=line>    Document(page_content=doc[<span class=string>"text"</span>], metadata={<span class=string>"source"</span>: doc[<span class=string>"source"</span>].split(<span class=string>"/"</span>)[<span class=number>1</span>]}) <span class=keyword>for</span> doc <span class=keyword>in</span> knowledge_base</span><br><span class=line>]</span><br><span class=line></span><br><span class=line>docs_processed = RecursiveCharacterTextSplitter(chunk_size=<span class=number>500</span>).split_documents(source_docs)[:<span class=number>1000</span>]</span><br><span class=line></span><br><span class=line>embedding_model = HuggingFaceEmbeddings(model_name=<span class=string>"thenlper/gte-small"</span>)</span><br><span class=line>vectordb = FAISS.from_documents(documents=docs_processed, embedding=embedding_model)</span><br><span class=line><span class=keyword>import</span> json</span><br><span class=line><span class=keyword>from</span> transformers.agents <span class=keyword>import</span> Tool</span><br><span class=line><span class=keyword>from</span> langchain_core.vectorstores <span class=keyword>import</span> VectorStore</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>RetrieverTool</span>(<span class=params>Tool</span>):</span></span><br><span class=line>    name = <span class=string>"retriever"</span></span><br><span class=line>    description = (</span><br><span class=line>        <span class=string>"Retrieves some documents from the knowledge base that have the closest embeddings to the input query."</span></span><br><span class=line>    )</span><br><span class=line>    inputs = {</span><br><span class=line>        <span class=string>"query"</span>: {</span><br><span class=line>            <span class=string>"type"</span>: <span class=string>"string"</span>,</span><br><span class=line>            <span class=string>"description"</span>: <span class=string>"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question."</span>,</span><br><span class=line>        },</span><br><span class=line>        <span class=string>"source"</span>: {<span class=string>"type"</span>: <span class=string>"string"</span>, <span class=string>"description"</span>: <span class=string>""</span>},</span><br><span class=line>        <span class=string>"number_of_documents"</span>: {</span><br><span class=line>            <span class=string>"type"</span>: <span class=string>"string"</span>,</span><br><span class=line>            <span class=string>"description"</span>: <span class=string>"the number of documents to retrieve. Stay under 10 to avoid drowning in docs"</span>,</span><br><span class=line>        },</span><br><span class=line>    }</span><br><span class=line>    output_type = <span class=string>"string"</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, vectordb: VectorStore, all_sources: <span class=built_in>str</span>, **kwargs</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__(**kwargs)</span><br><span class=line>        self.vectordb = vectordb</span><br><span class=line>        self.inputs[<span class=string>"source"</span>][<span class=string>"description"</span>] = (</span><br><span class=line>            <span class=string>f"The source of the documents to search, as a str representation of a list. Possible values in the list are: <span class=subst>{all_sources}</span>. If this argument is not provided, all sources will be searched."</span>.replace(</span><br><span class=line>                <span class=string>"'"</span>, <span class=string>"`"</span></span><br><span class=line>            )</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, query: <span class=built_in>str</span>, source: <span class=built_in>str</span> = <span class=literal>None</span>, number_of_documents=<span class=number>7</span></span>) -> <span class=built_in>str</span>:</span></span><br><span class=line>        <span class=keyword>assert</span> <span class=built_in>isinstance</span>(query, <span class=built_in>str</span>), <span class=string>"Your search query must be a string"</span></span><br><span class=line>        number_of_documents = <span class=built_in>int</span>(number_of_documents)</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> source:</span><br><span class=line>            <span class=keyword>if</span> <span class=built_in>isinstance</span>(source, <span class=built_in>str</span>) <span class=keyword>and</span> <span class=string>"["</span> <span class=keyword>not</span> <span class=keyword>in</span> <span class=built_in>str</span>(source):  <span class=comment># if the source is not representing a list</span></span><br><span class=line>                source = [source]</span><br><span class=line>            source = json.loads(<span class=built_in>str</span>(source).replace(<span class=string>"'"</span>, <span class=string>'"'</span>))</span><br><span class=line></span><br><span class=line>        docs = self.vectordb.similarity_search(</span><br><span class=line>            query,</span><br><span class=line>            <span class=built_in>filter</span>=({<span class=string>"source"</span>: source} <span class=keyword>if</span> source <span class=keyword>else</span> <span class=literal>None</span>),</span><br><span class=line>            k=number_of_documents,</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>len</span>(docs) == <span class=number>0</span>:</span><br><span class=line>            <span class=keyword>return</span> <span class=string>"No documents found with this filtering. Try removing the source filter."</span></span><br><span class=line>        <span class=keyword>return</span> <span class=string>"Retrieved documents:\n\n"</span> + <span class=string>"\n===Document===\n"</span>.join([doc.page_content <span class=keyword>for</span> doc <span class=keyword>in</span> docs])</span><br><span class=line></span><br><span class=line><span class=keyword>from</span> transformers.agents <span class=keyword>import</span> HfApiEngine, ReactJsonAgent</span><br><span class=line></span><br><span class=line>llm_engine = HfApiEngine(<span class=string>"Qwen/Qwen2.5-72B-Instruct"</span>)</span><br><span class=line></span><br><span class=line>retriever_tool = RetrieverTool(vectordb=vectordb, all_sources=all_sources)</span><br><span class=line>agent = ReactJsonAgent(tools=[retriever_tool], llm_engine=llm_engine, verbose=<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>agent_output = agent.run(<span class=string>"Please show me a LORA finetuning script"</span>)</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(<span class=string>"Final output:"</span>)</span><br><span class=line><span class=built_in>print</span>(agent_output)</span><br></pre></table></figure><p>通过RAG以及迭代的询问提升回答质量,首先还是加载数据,分块,编码为向量加载到一个向量数据库.<p><img alt=image-20241226215424509 data-src=https://s2.loli.net/2024/12/26/WSB5vpIPZzTwC31.png><p>huggingface的transformers库自带调用解释器<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> ReactCodeAgent</span><br><span class=line></span><br><span class=line>agent = ReactCodeAgent(tools=[], llm_engine=HfApiEngine(<span class=string>"Qwen/Qwen2.5-72B-Instruct"</span>))</span><br><span class=line></span><br><span class=line>code = <span class=string>"""</span></span><br><span class=line><span class=string>list=[0, 1, 2]</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>for i in range(4):</span></span><br><span class=line><span class=string>    print(list(i))</span></span><br><span class=line><span class=string>"""</span></span><br><span class=line></span><br><span class=line>final_answer = agent.run(</span><br><span class=line>    <span class=string>"I have some code that creates a bug: please debug it, then run it to make sure it works and return the final code"</span>,</span><br><span class=line>    code=code,</span><br><span class=line>)</span><br></pre></table></figure><p>结合OpenAI的接口制作agent.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> os</span><br><span class=line><span class=keyword>from</span> openai <span class=keyword>import</span> OpenAI</span><br><span class=line><span class=keyword>from</span> transformers.agents.llm_engine <span class=keyword>import</span> MessageRole, get_clean_message_list</span><br><span class=line></span><br><span class=line>openai_role_conversions = {</span><br><span class=line>    MessageRole.TOOL_RESPONSE: <span class=string>"user"</span>,</span><br><span class=line>}</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>OpenAIEngine</span>:</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, model_name=<span class=string>"gpt-4o-2024-05-13"</span></span>):</span></span><br><span class=line>        self.model_name = model_name</span><br><span class=line>        self.client = OpenAI(</span><br><span class=line>            api_key=os.getenv(<span class=string>"OPENAI_API_KEY"</span>),</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__call__</span>(<span class=params>self, messages, stop_sequences=[]</span>):</span></span><br><span class=line>        <span class=comment># Get clean message list</span></span><br><span class=line>        messages = get_clean_message_list(messages, role_conversions=openai_role_conversions)</span><br><span class=line></span><br><span class=line>        <span class=comment># Get LLM output</span></span><br><span class=line>        response = self.client.chat.completions.create(</span><br><span class=line>            model=self.model_name,</span><br><span class=line>            messages=messages,</span><br><span class=line>            stop=stop_sequences,</span><br><span class=line>        )</span><br><span class=line>        <span class=keyword>return</span> response.choices[<span class=number>0</span>].message.content</span><br><span class=line></span><br><span class=line></span><br><span class=line>openai_engine = OpenAIEngine()</span><br><span class=line>agent = ReactCodeAgent(llm_engine=openai_engine, tools=[])</span><br><span class=line></span><br><span class=line>code = <span class=string>"""</span></span><br><span class=line><span class=string>list=[0, 1, 2]</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>for i in range(4):</span></span><br><span class=line><span class=string>    print(list(i))</span></span><br><span class=line><span class=string>"""</span></span><br><span class=line></span><br><span class=line>final_answer = agent.run(</span><br><span class=line>    <span class=string>"I have some code that creates a bug: please debug it and return the final code"</span>,</span><br><span class=line>    code=code,</span><br><span class=line>)</span><br></pre></table></figure><h2 id=Agentic-RAG><a title="Agentic RAG" class=headerlink href=#Agentic-RAG></a>Agentic RAG</h2><p>单纯的RAG也有局限性，最重要的是以下两点：<ul><li>它只执行一个检索步骤：如果结果不好，那么生成的结果也会不好。<li>语义相似度是以用户查询作为参考来计算的，这可能是次优的：例如，用户查询通常是一个问题，而包含真实答案的文档就是确定句式，因此与疑问形式的其他源文档相比，其相似度得分将被降低，从而导致丢失相关信息的风险。</ul><p>但是可以通过创建一个RAG代理来缓解这些问题：<ol><li>不直接使用用户的问题作为查询,agent将结合用户输入<li>agent可以生成片段以便重新检索</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> datasets</span><br><span class=line><span class=keyword>from</span> tqdm <span class=keyword>import</span> tqdm</span><br><span class=line><span class=keyword>from</span> transformers <span class=keyword>import</span> AutoTokenizer</span><br><span class=line><span class=keyword>from</span> langchain.docstore.document <span class=keyword>import</span> Document</span><br><span class=line><span class=keyword>from</span> langchain.text_splitter <span class=keyword>import</span> RecursiveCharacterTextSplitter</span><br><span class=line><span class=keyword>from</span> langchain.vectorstores <span class=keyword>import</span> FAISS</span><br><span class=line><span class=keyword>from</span> langchain_community.embeddings <span class=keyword>import</span> HuggingFaceEmbeddings</span><br><span class=line><span class=keyword>from</span> langchain_community.vectorstores.utils <span class=keyword>import</span> DistanceStrategy</span><br><span class=line></span><br><span class=line>knowledge_base = datasets.load_dataset(<span class=string>"m-ric/huggingface_doc"</span>, split=<span class=string>"train"</span>)</span><br><span class=line>source_docs = [</span><br><span class=line>    Document(page_content=doc[<span class=string>"text"</span>], metadata={<span class=string>"source"</span>: doc[<span class=string>"source"</span>].split(<span class=string>"/"</span>)[<span class=number>1</span>]}) <span class=keyword>for</span> doc <span class=keyword>in</span> knowledge_base</span><br><span class=line>]</span><br><span class=line></span><br><span class=line>text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(</span><br><span class=line>    AutoTokenizer.from_pretrained(<span class=string>"thenlper/gte-small"</span>),</span><br><span class=line>    chunk_size=<span class=number>200</span>,</span><br><span class=line>    chunk_overlap=<span class=number>20</span>,</span><br><span class=line>    add_start_index=<span class=literal>True</span>,</span><br><span class=line>    strip_whitespace=<span class=literal>True</span>,</span><br><span class=line>    separators=[<span class=string>"\n\n"</span>, <span class=string>"\n"</span>, <span class=string>"."</span>, <span class=string>" "</span>, <span class=string>""</span>],</span><br><span class=line>)</span><br><span class=line></span><br><span class=line><span class=comment># Split docs and keep only unique ones</span></span><br><span class=line><span class=built_in>print</span>(<span class=string>"Splitting documents..."</span>)</span><br><span class=line>docs_processed = []</span><br><span class=line>unique_texts = {}</span><br><span class=line><span class=keyword>for</span> doc <span class=keyword>in</span> tqdm(source_docs):</span><br><span class=line>    new_docs = text_splitter.split_documents([doc])</span><br><span class=line>    <span class=keyword>for</span> new_doc <span class=keyword>in</span> new_docs:</span><br><span class=line>        <span class=keyword>if</span> new_doc.page_content <span class=keyword>not</span> <span class=keyword>in</span> unique_texts:</span><br><span class=line>            unique_texts[new_doc.page_content] = <span class=literal>True</span></span><br><span class=line>            docs_processed.append(new_doc)</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(<span class=string>"Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)"</span>)</span><br><span class=line>embedding_model = HuggingFaceEmbeddings(model_name=<span class=string>"thenlper/gte-small"</span>)</span><br><span class=line>vectordb = FAISS.from_documents(</span><br><span class=line>    documents=docs_processed,</span><br><span class=line>    embedding=embedding_model,</span><br><span class=line>    distance_strategy=DistanceStrategy.COSINE,</span><br><span class=line>)</span><br><span class=line></span><br><span class=line></span><br></pre></table></figure><p><img alt=image-20241228204813028 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20241228204813028.png><h2 id=Use-local-data><a title="Use local data" class=headerlink href=#Use-local-data></a>Use local data</h2><p><a href=https://huggingface.co/learn/cookbook/rag_with_unstructured_data rel=noopener target=_blank>Building RAG with Custom Unstructured Data</a><p>许多重要的知识都以各种格式存储，如pdf、电子邮件、Markdown文件、PowerPoint演示文稿、HTML页面、Word文档等。<p>如何预处理所有这些数据,因为这些文件的格式各不相同,要么搭配langchain等工具分别进行解析,要么直接使用现有工具库<a href=https://python.langchain.com/docs/introduction/ rel=noopener target=_blank>Introduction | 🦜️🔗 LangChain</a>,<a href=https://docs.unstructured.io/welcome rel=noopener target=_blank>Unstructured - Unstructured</a><h3 id=本地推理应用与库><a class=headerlink href=#本地推理应用与库 title=本地推理应用与库></a>本地推理应用与库</h3><p>目前有许多大模型本地推理应用,比如Ollama,LMStudio,GPT4All等等,而它们背后的推理库也有很多,比如<a href=https://github.com/ggerganov/llama.cpp rel=noopener target=_blank>llama.cpp</a>,<a href=https://huggingface.co/blog/introduction-to-ggml rel=noopener target=_blank>ggml</a>. 通过利用Ollama等工具搭配LangChain可以更高效及其定制化属于自己的LLM应用.<p>Ollama本身提供了API服务,通过这个服务与用户本地文件和prompt,可以搭建一个小应用<figure class="highlight shell"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line>curl http://localhost:11434/api/chat -d '{</span><br><span class=line>  "model": "llama3.2",</span><br><span class=line>  "messages": [</span><br><span class=line>    { "role": "user", "content": "why is the sky blue?" }</span><br><span class=line>  ]</span><br><span class=line>}'</span><br></pre></table></figure><p>注意这个返回值会是一个JSON流.具体来说, 通过<a href=https://python.langchain.com/docs/integrations/chat/ollama/ rel=noopener target=_blank>ChatOllama | 🦜️🔗 LangChain</a>获得更好的处理,再利用FastAPI或Sanic搭建web用户界面,结合上传用户文件等搭建一个应用.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> langchain_ollama <span class=keyword>import</span> OllamaLLM</span><br><span class=line></span><br><span class=line>model = OllamaLLM(model=<span class=string>"llama3"</span>)</span><br><span class=line>model.invoke(<span class=string>"Come up with 10 names for a song about parrots"</span>)</span><br></pre></table></figure><p><img alt=image-20241231183826980 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20241231183826980.png></p><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="Agentic RAG and LLMs-based assistant" href=https://www.sekyoro.top/2024/12/26/Agentic-RAG-and-LLMs-based-assistant/>https://www.sekyoro.top/2024/12/26/Agentic-RAG-and-LLMs-based-assistant/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/AI-Agent/ rel=tag><i class="fa fa-tag"></i> AI Agent</a><a href=/tags/RAG/ rel=tag><i class="fa fa-tag"></i> RAG</a></div><div class=post-nav><div class=post-nav-item><a href=/2024/12/25/glsl%E5%AD%A6%E4%B9%A0/ rel=prev title=OpenGL中不可忽视的部分:glsl、glm、assimp以及更多> <i class="fa fa-chevron-left"></i> OpenGL中不可忽视的部分:glsl、glm、assimp以及更多 </a></div><div class=post-nav-item><a title="Learn OpenGL(二):模型加载与高级OpenGL" href=/2024/12/27/Learn-OpenGL-%E4%BA%8C/ rel=next> Learn OpenGL(二):模型加载与高级OpenGL <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#For-any-API-provider><span class=nav-number>1.</span> <span class=nav-text>For any API provider</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Intro-to-RAG><span class=nav-number>2.</span> <span class=nav-text>Intro to RAG</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%AE%80%E6%98%93%E7%9A%84RAG%E5%B7%A5%E4%BD%9C%E6%B5%81><span class=nav-number>2.1.</span> <span class=nav-text>简易的RAG工作流</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE><span class=nav-number>2.1.1.</span> <span class=nav-text>加载数据</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%88%9B%E5%BB%BAembedding%E5%92%8Cretriever><span class=nav-number>2.1.2.</span> <span class=nav-text>创建embedding和retriever</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#LLM><span class=nav-number>2.1.3.</span> <span class=nav-text>LLM</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%90%AD%E5%BB%BALLM%E9%93%BE><span class=nav-number>2.1.4.</span> <span class=nav-text>搭建LLM链</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%AF%94%E8%BE%83%E7%BB%93%E6%9E%9C><span class=nav-number>2.1.5.</span> <span class=nav-text>比较结果</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Agent-with-tool-calling><span class=nav-number>3.</span> <span class=nav-text>Agent with tool-calling</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Agentic-RAG><span class=nav-number>4.</span> <span class=nav-text>Agentic RAG</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Use-local-data><span class=nav-number>5.</span> <span class=nav-text>Use local data</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%9C%AC%E5%9C%B0%E6%8E%A8%E7%90%86%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BA%93><span class=nav-number>5.1.</span> <span class=nav-text>本地推理应用与库</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>238</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/AI-Agent/ rel=tag>AI Agent</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/RAG/ rel=tag>RAG</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.6m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>39:21</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>