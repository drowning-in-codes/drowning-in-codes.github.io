<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="vqvae出自[1711.00937] Neural Discrete Representation Learning,用于无监督学习离散表征,目前在多模态生成领域还有使用. 这里学习一下代码" name=description><meta content=article property=og:type><meta content=vqvae及其变体代码学习 property=og:title><meta content=https://www.sekyoro.top/2024/11/18/vqvae%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="vqvae出自[1711.00937] Neural Discrete Representation Learning,用于无监督学习离散表征,目前在多模态生成领域还有使用. 这里学习一下代码" property=og:description><meta content=zh_CN property=og:locale><meta content=https://camo.githubusercontent.com/2b432c6d87633c75685c3703167c0a6b5a6d6592a7ca95540bf02f6de890052c/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f706978656c2d636e6e2e706e67 property=og:image><meta content=https://notesbylex.com/_media/vq.png property=og:image><meta content=https://s2.loli.net/2024/11/19/1EByprM63fZXdON.png property=og:image><meta content=https://s2.loli.net/2024/11/19/u2JyvB6W59j3PHC.png property=og:image><meta content=https://s2.loli.net/2024/11/19/LjYv1nK2odfCQe6.png property=og:image><meta content=https://notesbylex.com/_media/vq-for-audio.png property=og:image><meta content=https://notesbylex.com/_media/residual-vector-quantization-fig-2%201.png property=og:image><meta content=https://s2.loli.net/2024/11/19/O2APLos7bE9Fdgf.png property=og:image><meta content=https://s2.loli.net/2024/11/19/qWTtJIihjCaeVk9.png property=og:image><meta content=2024-11-18T10:13:55.000Z property=article:published_time><meta content=2024-11-22T14:58:48.324Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="deep learning" property=article:tag><meta content=vqvae property=article:tag><meta content=summary name=twitter:card><meta content=https://camo.githubusercontent.com/2b432c6d87633c75685c3703167c0a6b5a6d6592a7ca95540bf02f6de890052c/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f706978656c2d636e6e2e706e67 name=twitter:image><link href=https://www.sekyoro.top/2024/11/18/vqvae%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>vqvae及其变体代码学习 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/11/18/vqvae%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>vqvae及其变体代码学习</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-11-18 18:13:55" datetime=2024-11-18T18:13:55+08:00>2024-11-18</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-11-22 22:58:48" datetime=2024-11-22T22:58:48+08:00 itemprop=dateModified>2024-11-22</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>10k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>9 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>vqvae出自<a href=https://arxiv.org/abs/1711.00937 rel=noopener target=_blank>[1711.00937] Neural Discrete Representation Learning</a>,用于无监督学习离散表征,目前在多模态生成领域还有使用. 这里学习一下代码</p><span id=more></span><h2 id=VQVAE><a class=headerlink href=#VQVAE title=VQVAE></a>VQVAE</h2><p>vqvae道理本身很简单,它的提出与pixelcnn、自回归模型息息相关,像vae,gan这种生成式模型,它们更像是对整个数据进行估计,而自回归模型又与序列模型相关,更像是对数据生成分布的建模<blockquote><p>自回归模型以序列中的先前值为条件进行预测，而不是基于潜在随机变量。因此，他们试图对数据生成分布进行显式建模，而不是对其进行近似</blockquote><p>poixelcnn就是一个自回归模型,而其每次就是从vqvae得到的离散结果中进行采样序列性地生成结果,为了实现这种效果利用了一种masked convolution,将卷积权重后面部分置0,使得在卷积的时候不关注后面的结果<a href=https://github.com/pilipolio/learn-pytorch/blob/master/201708_ToyPixelCNN.ipynb rel=noopener target=_blank>ToyPixelCNN.ipynb at master · pilipolio/learn-pytorch</a><p><img alt=img data-src=https://camo.githubusercontent.com/2b432c6d87633c75685c3703167c0a6b5a6d6592a7ca95540bf02f6de890052c/68747470733a2f2f6c696c69616e77656e672e6769746875622e696f2f6c696c2d6c6f672f6173736574732f696d616765732f706978656c2d636e6e2e706e67><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MaskedConv</span>(<span class=params>nn.Conv2d</span>):</span></span><br><span class=line>	<span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, mask_type, *args, **kwargs</span>):</span></span><br><span class=line>		<span class=built_in>super</span>(MaskedConv, self).__init__(*args, **kwargs)</span><br><span class=line>		self.mask_type = mask_type</span><br><span class=line>		self.register_buffer(<span class=string>'mask'</span>, self.weight.data.clone())</span><br><span class=line></span><br><span class=line>		channels, depth, height, width = self.weight.size()</span><br><span class=line></span><br><span class=line>		self.mask.fill_(<span class=number>1</span>)</span><br><span class=line>		<span class=keyword>if</span> mask_type ==<span class=string>'A'</span>:</span><br><span class=line>			self.mask[:,:,height//<span class=number>2</span>,width//<span class=number>2</span>:] = <span class=number>0</span></span><br><span class=line>			self.mask[:,:,height//<span class=number>2</span>+<span class=number>1</span>:,:] = <span class=number>0</span></span><br><span class=line>		<span class=keyword>else</span>: </span><br><span class=line>			self.mask[:,:,height//<span class=number>2</span>,width//<span class=number>2</span>+<span class=number>1</span>:] = <span class=number>0</span></span><br><span class=line>			self.mask[:,:,height//<span class=number>2</span>+<span class=number>1</span>:,:] = <span class=number>0</span></span><br><span class=line></span><br><span class=line></span><br><span class=line>	<span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>		self.weight.data *= self.mask </span><br><span class=line>		<span class=keyword>return</span> <span class=built_in>super</span>(MaskedConv, self).forward(x)</span><br><span class=line></span><br></pre></table></figure><p>现在许多的模型,包括transformer都是auto-regressive的,而GAN与VAE并不是,它们的缺点就是难以建模离散数据.而vqvae就弥补了这一点.<p>而VQVAE中重点其实是设计好一个离散字典后,使用了一种技巧将梯度传导使得能够更新这个字典.<p>这种设计称作直通估计器,将decoder得到的梯度直接传到了encoder.假设codebook的shape是[codebook_size,codebook_dim],输入特征shape是[size,codebook_dim],通过一个指标得到它们的距离(可以使用<code>torch.cdist</code>)得到[size,codebook_size],这相当于得到了特征上每个位置在字典上对应的位置.<p><img alt="Vector Quantisation" data-src=https://notesbylex.com/_media/vq.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=comment># 写法1</span></span><br><span class=line>dist_manual = torch.sqrt(</span><br><span class=line>        torch.<span class=built_in>sum</span>(x ** <span class=number>2</span>, dim=<span class=number>1</span>, keepdim=<span class=literal>True</span>) +</span><br><span class=line>        torch.<span class=built_in>sum</span>(y ** <span class=number>2</span>, dim=<span class=number>1</span>, keepdim=<span class=literal>True</span>).t() -</span><br><span class=line>        <span class=number>2</span> * x @ y.t()</span><br><span class=line>    )</span><br><span class=line><span class=comment># 写法2 better readable and efficient since no gradient computation</span></span><br><span class=line>      <span class=keyword>with</span> torch.no_grad():</span><br><span class=line>            dist = torch.cdist(x, implicit_codebook)</span><br><span class=line>            indices = dist.argmin(dim = -<span class=number>1</span>)</span><br></pre></table></figure><p>根据最近的距离得到嵌入后的特征<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line><span class=comment># 写法1  </span></span><br><span class=line>min_encoding_indices = torch.argmin(d, dim=<span class=number>1</span>).unsqueeze(<span class=number>1</span>)  <span class=comment># (encoded_feat size,1)</span></span><br><span class=line>        min_encodings = torch.zeros(</span><br><span class=line>            min_encoding_indices.shape[<span class=number>0</span>], self.n_e, device=z.device)  <span class=comment># (encoded_feat size,embedding_size)</span></span><br><span class=line>        min_encodings.scatter_(<span class=number>1</span>, min_encoding_indices, <span class=number>1</span>)  <span class=comment># one-hot  like</span></span><br><span class=line><span class=comment># 写法2 dry and more clean</span></span><br><span class=line>min_encoding_indices = torch.argmin(d, dim=<span class=number>1</span>)</span><br><span class=line>    my_min_encodings = F.one_hot(min_encoding_indices.squeeze())</span><br></pre></table></figure><p><code>one-hot</code>的shape是[encode_size,embed_size],下面公式中第三项是commitment loss,用于更新encoder输出,第三项用于更新字典<blockquote><p>为了学习嵌入空间，使用最简单的字典学习算法之一，向量量化( VQ )。VQ目标使用l2误差将嵌入向量ei移动到编码器输出ze ( x )</blockquote><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)</span><br><span class=line>        loss =  self.beta * torch.mean((z_q - z.detach()) ** <span class=number>2</span>))+torch.mean(((z_q.detach() - z) ** <span class=number>2</span>)</span><br><span class=line>        z_q = z + (z_q - z).detach()</span><br><span class=line>                                                <span class=comment># torch.mean((z_q-z.detach())**2) 可以更简单地写为</span></span><br><span class=line> F.mse_loss(z_q,z_e.detach())</span><br></pre></table></figure><p><img alt=image-20241119192654741 data-src=https://s2.loli.net/2024/11/19/1EByprM63fZXdON.png><p>此外可以使用EMA更新字典<p><img alt=image-20241119202801862 data-src=https://s2.loli.net/2024/11/19/u2JyvB6W59j3PHC.png><p>这里的更新逻辑是,每次更新ema_cluster_size,针对每个嵌入的向量,得到与它最近的特征向量个数,通过ema更新,而权重就是每次嵌入的值通过ema更新<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre><td class=code><pre><span class=line><span class=comment># Update weights with EMA</span></span><br><span class=line>       <span class=keyword>if</span> self.training:</span><br><span class=line>           self._ema_cluster_size = self._ema_cluster_size * self._decay + (</span><br><span class=line>               <span class=number>1</span> - self._decay</span><br><span class=line>           ) * torch.<span class=built_in>sum</span>(encodings, <span class=number>0</span>)</span><br><span class=line></span><br><span class=line>           <span class=comment># Laplace smoothing</span></span><br><span class=line>           n = torch.<span class=built_in>sum</span>(self._ema_cluster_size.data)</span><br><span class=line>           self._ema_cluster_size = (</span><br><span class=line>               (self._ema_cluster_size + self._epsilon)</span><br><span class=line>               / (n + self._n_embeddings * self._epsilon)</span><br><span class=line>               * n</span><br><span class=line>           )</span><br><span class=line></span><br><span class=line>           dw = torch.matmul(encodings.t(), flat_z_e)</span><br><span class=line>           self._ema_w = nn.Parameter(</span><br><span class=line>               self._ema_w * self._decay + (<span class=number>1</span> - self._decay) * dw</span><br><span class=line>           )</span><br><span class=line></span><br><span class=line>           self._embedding.weight = nn.Parameter(</span><br><span class=line>               self._ema_w / self._ema_cluster_size.unsqueeze(<span class=number>1</span>)</span><br><span class=line>           )</span><br><span class=line></span><br></pre></table></figure><p><img alt=image-20241119222201269 data-src=https://s2.loli.net/2024/11/19/LjYv1nK2odfCQe6.png><h2 id=VQVAE-2><a class=headerlink href=#VQVAE-2 title=VQVAE-2></a>VQVAE-2</h2><p>简单来说就是多尺度的vqvae,设计了多个encoder-codelayer-decoder.<br>首先特征通过多个encoder降维,得到不同尺度的特征,再将不同尺度特征进行quantize,quantize后得到的特征进行上采样再decoder最终得到多尺度特征.<p>此外也有VQGAN论文在多尺度的基础上提出将codebook的维度从256到32,重建效果保持一致,同时将解码后的特征与codebook做l2-norm,使用cos相似度判断<h2 id=Residual-VQ><a title="Residual VQ" class=headerlink href=#Residual-VQ></a>Residual VQ</h2><p>道理非常简单——quantize(x-quantize(x-quantize(x-…)))<p><img alt="Vector Quantisation for Audio" data-src=https://notesbylex.com/_media/vq-for-audio.png><p><img alt="SoundStream architecture" data-src=https://notesbylex.com/_media/residual-vector-quantization-fig-2%201.png><h2 id=SIMVQ><a class=headerlink href=#SIMVQ title=SIMVQ></a>SIMVQ</h2><p><img alt=image-20241119222219261 data-src=https://s2.loli.net/2024/11/19/O2APLos7bE9Fdgf.png><p>据论文作者所说,在codebook上进行维度转换,提高编码表的利用率,使得在许多优化器上表现更好. 在具体代码上,我参考了<a href=https://github.com/lucidrains/vector-quantize-pytorch/tree/master rel=noopener target=_blank>lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch</a>的实现,其使用一个linear层变换codebook的维度,在进行计算距离时也使用这个转换后的codebook,量化也使用这个codebook,这样一来特征经过encoder后的维度需要与转换后的codebook的维度一致.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br></pre><td class=code><pre><span class=line>    <span class=keyword>return</span> inverse(rotated)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>SimVQ</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>            self,</span></span></span><br><span class=line><span class=params><span class=function>            dim,</span></span></span><br><span class=line><span class=params><span class=function>            codebook_size,</span></span></span><br><span class=line><span class=params><span class=function>            codebook_transform: Module | <span class=literal>None</span> = <span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>            init_fn: <span class=type>Callable</span> = identity,</span></span></span><br><span class=line><span class=params><span class=function>            channel_first=<span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>            rotation_trick=<span class=literal>True</span>,</span></span></span><br><span class=line><span class=params><span class=function>            input_to_quantize_commit_loss_weight=<span class=number>.25</span>,</span></span></span><br><span class=line><span class=params><span class=function>            commitment_weight=<span class=number>1.</span>,</span></span></span><br><span class=line><span class=params><span class=function>            frozen_codebook_dim=<span class=literal>None</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.codebook_size = codebook_size</span><br><span class=line>        self.channel_first = channel_first</span><br><span class=line></span><br><span class=line>        frozen_codebook_dim = default(frozen_codebook_dim, dim)</span><br><span class=line>        codebook = torch.randn(codebook_size, frozen_codebook_dim) * (frozen_codebook_dim ** -<span class=number>.5</span>)</span><br><span class=line></span><br><span class=line>        codebook = init_fn(codebook)</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> <span class=keyword>not</span> exists(codebook_transform):</span><br><span class=line>            codebook_transform = nn.Linear(frozen_codebook_dim, dim, bias=<span class=literal>False</span>)</span><br><span class=line>        self.code_transform = codebook_transform</span><br><span class=line>        self.register_buffer(<span class=string>"frozen_codebook"</span>, codebook)</span><br><span class=line></span><br><span class=line>        self.rotation_trick = rotation_trick</span><br><span class=line>        self.input_to_quantize_commit_loss_weight = input_to_quantize_commit_loss_weight</span><br><span class=line>        self.commitment_weight = commitment_weight</span><br><span class=line></span><br><span class=line><span class=meta>    @property</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>codebook</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.code_transform(self.frozen_codebook)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>indices_to_codes</span>(<span class=params>self, indices</span>):</span></span><br><span class=line>        frozen_codes = self.frozen_codebook(indices)</span><br><span class=line>        quantized = self.code_transform(frozen_codes)</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> self.channel_first:</span><br><span class=line>            quantized = rearrange(quantized, <span class=string>'b ... d -> b d ...'</span>)</span><br><span class=line>        <span class=keyword>return</span> quantized</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>if</span> self.channel_first:</span><br><span class=line>            x = rearrange(x, <span class=string>'b ... d -> b d ...'</span>)</span><br><span class=line>        x, inverse_pack = pack_one(x, <span class=string>'b * d'</span>)</span><br><span class=line>        implicit_codebook = self.codebook</span><br><span class=line>        <span class=keyword>with</span> torch.no_grad():</span><br><span class=line>            dist = torch.cdist(x, implicit_codebook)</span><br><span class=line>            indices = dist.argmin(dim=-<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        quantized = implicit_codebook[indices]</span><br><span class=line>        commit_loss = F.mse_loss(x.detach(), quantized)</span><br><span class=line>        <span class=keyword>if</span> self.rotation_trick:</span><br><span class=line>            quantized = rotate_to(x, quantized)</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            commit_loss = (commit_loss + F.mse_loss(x, quantized.detach()) * self.input_to_quantize_commit_loss_weight)</span><br><span class=line>            quantized = (quantized - x).detach() + x</span><br><span class=line>        quantized = inverse_pack(quantized)</span><br><span class=line>        indices = inverse_pack(indices, <span class=string>'b *'</span>)</span><br><span class=line>        <span class=keyword>if</span> self.channel_first:</span><br><span class=line>            quantized = rearrange(quantized, <span class=string>'b ... d-> b d...'</span>)</span><br><span class=line>        <span class=keyword>return</span> quantized, indices, commit_loss * self.commitment_weight</span><br></pre></table></figure><p>可以看到上面代码中经常用到einops和einx以及torch的einsum操作,这些都是非常方便的库或者函数.这里介绍一下<h2 id=einops中常用操作><a class=headerlink href=#einops中常用操作 title=einops中常用操作></a>einops中常用操作</h2><p><img alt=image-20241119201935141 data-src=https://s2.loli.net/2024/11/19/qWTtJIihjCaeVk9.png><h3 id=rearrange><a class=headerlink href=#rearrange title=rearrange></a>rearrange</h3><p>最常用的就是rearrange了,可以用来转换axis的顺序,composition,decomposition等<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>x = torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>10</span>,<span class=number>10</span>)</span><br><span class=line><span class=comment># order</span></span><br><span class=line>y = rearrange(x,<span class=string>'b c h w -> b h w c'</span>)</span><br><span class=line><span class=built_in>print</span>(y.shape)</span><br><span class=line><span class=comment># composition</span></span><br><span class=line>y = rearrange(x,<span class=string>'b c h w -> b c (h w)'</span>)</span><br><span class=line><span class=comment># decomposition</span></span><br><span class=line>y = rearrange(y,<span class=string>'b c (h w) -> b h w c'</span>)</span><br><span class=line>y = rearrange(y,<span class=string>'(b1 b2) h w c -> b1 b2 h w c'</span>,b1=<span class=number>2</span>)</span><br></pre></table></figure><h3 id=reduce><a class=headerlink href=#reduce title=reduce></a>reduce</h3><figure class="highlight apache"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line><span class=comment># yet another example. Can you compute result shape?</span></span><br><span class=line><span class=attribute>reduce</span>(ims, <span class=string>"(b1 b2) h w c -> (b2 h) (b1 w)"</span>, <span class=string>"mean"</span>, b<span class=number>1</span>=<span class=number>2</span>)</span><br></pre></table></figure><p>可以用于求均值,maxpooling等,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line>ims = torch.randn((<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>,<span class=number>30</span>))*<span class=number>10</span>-<span class=number>2</span></span><br><span class=line>b,c,h,w = ims.shape</span><br><span class=line>m_ims = reduce(ims,<span class=string>'b c h w -> b c'</span>,<span class=string>"min"</span>)</span><br><span class=line><span class=built_in>print</span>(m_ims.shape)</span><br><span class=line></span><br><span class=line>m_ims = reduce(ims,<span class=string>'b c h w -> b (h w) c'</span>,<span class=string>'min'</span>).transpose(<span class=number>1</span>,<span class=number>2</span>).reshape(b,c,h,w)</span><br><span class=line><span class=built_in>print</span>(m_ims.shape)</span><br><span class=line><span class=built_in>print</span>(ims == m_ims)</span><br><span class=line>min2_ims = reduce(ims,<span class=string>'b c (h h2) (w w2) -> b c h w'</span>,<span class=string>'mean'</span>,h2=<span class=number>2</span>,w2=<span class=number>2</span>)</span><br><span class=line>reduce(ims,<span class=string>'b (h h2) (w w2) c -> h (b w) c'</span>,<span class=string>"max"</span>,h2=<span class=number>2</span>,w2=<span class=number>2</span>)</span><br></pre></table></figure><p>通过使用<code>()</code>保持dim,或者也可以使用<code>1</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>data = torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>,<span class=number>40</span>)</span><br><span class=line>mean_ = reduce(data,<span class=string>'b c h w  -> b c () ()'</span>,<span class=string>'mean'</span>) <span class=comment># 求均值</span></span><br><span class=line>ans = data.mean(dim=[<span class=number>2</span>,<span class=number>3</span>],keepdim=<span class=literal>True</span>)</span><br><span class=line><span class=built_in>print</span>((((ans-mean_)<<span class=number>1e-6</span>).<span class=built_in>float</span>()).mean())</span><br><span class=line></span><br><span class=line>max_pool = reduce(data,<span class=string>'b c (2 h) (2 w) -> b c h w'</span>,<span class=string>'max'</span>) <span class=comment>#max pooling</span></span><br><span class=line>adaptive_max_pool = reduce(data,<span class=string>'b c h w -> b c ()'</span>,<span class=string>'max'</span>)</span><br></pre></table></figure><h3 id=stack-and-concatenation><a title="stack and concatenation" class=headerlink href=#stack-and-concatenation></a>stack and concatenation</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line><span class=comment># rearrange can also take care of lists of arrays with the same shape</span></span><br><span class=line>x = <span class=built_in>list</span>(ims)</span><br><span class=line><span class=built_in>print</span>(<span class=built_in>type</span>(x), <span class=string>"with"</span>, <span class=built_in>len</span>(x), <span class=string>"tensors of shape"</span>, x[<span class=number>0</span>].shape)</span><br><span class=line><span class=comment># that's how we can stack inputs</span></span><br><span class=line><span class=comment># "list axis" becomes first ("b" in this case), and we left it there</span></span><br><span class=line>rearrange(x, <span class=string>"b h w c -> b h w c"</span>).shape</span><br></pre></table></figure><p>将一个列表的tensor中的列表大小维度进行转换<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line>c = <span class=built_in>list</span>()</span><br><span class=line>c.append(torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>))</span><br><span class=line>c.append(torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>))</span><br><span class=line></span><br><span class=line>rearrange(c,<span class=string>'l c h w -> c l h w'</span>).shape</span><br><span class=line></span><br></pre></table></figure><p>或者求一个列表中的所有tensor和、max等<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>c = <span class=built_in>list</span>()</span><br><span class=line>c.append(torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>))</span><br><span class=line>c.append(torch.randn(<span class=number>10</span>,<span class=number>20</span>,<span class=number>30</span>))</span><br><span class=line></span><br><span class=line>rearrange(c,<span class=string>'l c h w -> c l h w'</span>).shape</span><br><span class=line>reduce(c,<span class=string>'c l h w -> l h w'</span>,<span class=string>'mean'</span>).shape</span><br><span class=line>reduce(c,<span class=string>'c l h w -> l h w'</span>,<span class=string>'sum'</span>).shape</span><br><span class=line>reduce(c,<span class=string>'c l h w -> l h w'</span>,<span class=string>'max'</span>).shape</span><br></pre></table></figure><h3 id=add-or-remove-axis><a title="add or remove axis" class=headerlink href=#add-or-remove-axis></a>add or remove axis</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>x = rearrange(x,<span class=string>'b h w c -> b 1 h w 1 c'</span>)</span><br><span class=line>y = rearrange(y,<span class=string>'b h w c - b h (w c)'</span>)</span><br></pre></table></figure><h3 id=channel-shuffle><a title="channel shuffle" class=headerlink href=#channel-shuffle></a>channel shuffle</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>c = torch.randn(<span class=number>10</span>,<span class=number>30</span>,<span class=number>10</span>,<span class=number>10</span>)</span><br><span class=line>rearrange(c,<span class=string>'b (g1 g2 c) h w -> b (g2 g1 c) h w'</span>,g1=<span class=number>3</span>,g2=<span class=number>5</span>).shape</span><br></pre></table></figure><h3 id=repeat><a class=headerlink href=#repeat title=repeat></a>repeat</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>repeat(x,<span class=string>'b h w c -> b (h 2) (w 2) c'</span>)</span><br><span class=line>repeat(x,<span class=string>'h w c -> h new_axis w c'</span>,new_axis=<span class=number>5</span>)</span><br><span class=line>repeat(x,<span class=string>'h w c -> h 5 w c'</span>)</span><br></pre></table></figure><h3 id=split-dimension><a title="split dimension" class=headerlink href=#split-dimension></a>split dimension</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>c = torch.randn(<span class=number>10</span>,<span class=number>30</span>,<span class=number>10</span>,<span class=number>10</span>)</span><br><span class=line>x,y,z = rearrange(c,<span class=string>'b (head c) h w -> head b c h w'</span>,head=<span class=number>3</span>)</span><br><span class=line><span class=built_in>print</span>(x.shape,y.shape,z.shape)</span><br></pre></table></figure><p>split有不同方法<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>y1, y2 = rearrange(x, <span class=string>'b (split c) h w -> split b c h w'</span>, split=<span class=number>2</span>)</span><br><span class=line>result = y2 * sigmoid(y2) <span class=comment># or tanh</span></span><br><span class=line>y1, y2 = rearrange(x, <span class=string>'b (c split) h w -> split b c h w'</span>, split=<span class=number>2</span>)</span><br></pre></table></figure><ul><li><code>y1 = x[:, :x.shape[1] // 2, :, :]</code><li><code>y1 = x[:, 0::2, :, :]</code></ul><h3 id=striding-anything><a title="striding anything" class=headerlink href=#striding-anything></a>striding anything</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line><span class=comment># each image is split into subgrids, each subgrid now is a separate "image"</span></span><br><span class=line>y = rearrange(x, <span class=string>"b c (h hs) (w ws) -> (hs ws b) c h w"</span>, hs=<span class=number>2</span>, ws=<span class=number>2</span>)</span><br><span class=line>y = convolve_2d(y)</span><br><span class=line><span class=comment># pack subgrids back to an image</span></span><br><span class=line>y = rearrange(y, <span class=string>"(hs ws b) c h w -> b c (h hs) (w ws)"</span>, hs=<span class=number>2</span>, ws=<span class=number>2</span>)</span><br><span class=line></span><br><span class=line><span class=keyword>assert</span> y.shape == x.shape</span><br></pre></table></figure><p>可以看到最常用的函数就是<code>rearrange</code>,<code>reduce</code>以及<code>repeat</code>,基本替代了原本的<code>sum</code>,<code>transpose</code>,<code>expand</code>,<code>reshape</code>等torch操作<h3 id=parse-shape><a class=headerlink href=#parse-shape title=parse_shape></a>parse_shape</h3><p>通过<code>parse_shape</code>,相当于更方便地获得了需要的维度大小<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>y = np.zeros([<span class=number>700</span>])</span><br><span class=line>rearrange(y, <span class=string>'(b c h w) -> b c h w'</span>, **parse_shape(x, <span class=string>'b _ h w'</span>)).shape</span><br></pre></table></figure><h3 id=pack-and-unpack><a title="pack and unpack" class=headerlink href=#pack-and-unpack></a>pack and unpack</h3><p>pack是将一些列数据中的的一些维度放在一起<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line>h,w = <span class=number>100</span>,<span class=number>200</span></span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line>img_rgb = np.random.random([h,w,<span class=number>3</span>])</span><br><span class=line>img_depth = np.random.random([h,w])</span><br><span class=line>img_rgbd,ps = pack([img_rgb,img_depth],<span class=string>'h w *'</span>)</span><br><span class=line><span class=built_in>print</span>(img_rgbd.shape,ps)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>unpacked_rgb,unpacked_depth = unpack(img_rgbd,ps,<span class=string>"h w *"</span>)</span><br><span class=line><span class=built_in>print</span>(unpacked_rgb.shape,unpacked_depth.shape)</span><br></pre></table></figure><h3 id=结合torch使用layers><a class=headerlink href=#结合torch使用layers title=结合torch使用layers></a>结合torch使用layers</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> einops.layers.torch <span class=keyword>import</span> Rearrange,Reduce</span><br></pre></table></figure><h3 id=Einx><a class=headerlink href=#Einx title=Einx></a>Einx</h3><p>一种类似<code>torch.einsum</code>的计算方式,einsum<a href=https://rockt.github.io/2018/04/30/einsum rel=noopener target=_blank>einsum tutorial</a>是一种方便计算多个tensor乘积的方式,而Einx方便了写MLP-based架构代码,通过weight_shape和bias_shape结合pattern构造mlp<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> einops.layers.torch <span class=keyword>import</span> EinMix <span class=keyword>as</span> Mix</span><br><span class=line>mlp = Mix(<span class=string>'t b c-> t b c_out'</span>,weight_shape=<span class=string>'c c_out'</span>,c=<span class=number>10</span>,c_out=<span class=number>20</span>)</span><br><span class=line>x = torch.randn(<span class=number>10</span>,<span class=number>30</span>,<span class=number>10</span>)</span><br><span class=line>y = mlp(x)</span><br></pre></table></figure><p>值得一提的是,einops也有einsum<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> einops <span class=keyword>import</span> einsum, pack, unpack</span><br><span class=line><span class=comment># einsum is like ... einsum, generic and flexible dot-product</span></span><br><span class=line><span class=comment># but 1) axes can be multi-lettered  2) pattern goes last 3) works with multiple frameworks</span></span><br><span class=line>C = einsum(A, B, <span class=string>'b t1 head c, b t2 head c -> b head t1 t2'</span>)</span><br></pre></table></figure><h2 id=相关资料><a class=headerlink href=#相关资料 title=相关资料></a>相关资料</h2><ol><li><a href=https://github.com/MishaLaskin/vqvae rel=noopener target=_blank>MishaLaskin/vqvae: A pytorch implementation of the vector quantized variational autoencoder (https://arxiv.org/abs/1711.00937)</a><li><a href=https://github.com/nadavbh12/VQ-VAE/blob/master/vq_vae/auto_encoder.py rel=noopener target=_blank>VQ-VAE/vq_vae/auto_encoder.py at master · nadavbh12/VQ-VAE</a><li><a href=https://github.com/AndrewBoessen/VQ-VAE/blob/main/vqvae.py rel=noopener target=_blank>VQ-VAE/vqvae.py at main · AndrewBoessen/VQ-VAE</a><li><a href=https://github.com/vvvm23/vqvae-2/blob/main/vqvae.py rel=noopener target=_blank>vqvae-2/vqvae.py at main · vvvm23/vqvae-2</a><li><a href=https://www.georgeho.org/deep-autoregressive-models/ rel=noopener target=_blank>Autoregressive Models in Deep Learning — A Brief Survey | George Ho</a><li><a href=https://github.com/lucidrains/vector-quantize-pytorch/tree/master rel=noopener target=_blank>lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch</a><li><a href=https://spaces.ac.cn/archives/6760 rel=noopener target=_blank>VQ-VAE的简明介绍：量子化自编码器 - 科学空间|Scientific Spaces</a><li><a href=https://spaces.ac.cn/archives/10489 rel=noopener target=_blank>VQ的旋转技巧：梯度直通估计的一般推广 - 科学空间|Scientific Spaces</a><li><a href=https://spaces.ac.cn/archives/10519 rel=noopener target=_blank>VQ的又一技巧：给编码表加一个线性变换 - 科学空间|Scientific Spaces</a><li><a href=https://einops.rocks/pytorch-examples.html rel=noopener target=_blank>Writing better code with pytorch+einops</a><li><a href=https://notesbylex.com/residual-vector-quantisation rel=noopener target=_blank>Residual Vector Quantisation - Notes by Lex</a><li><a href=https://github.com/rese1f/awesome-VQVAE?tab=readme-ov-file rel=noopener target=_blank>rese1f/Awesome-VQVAE: A collection of resources and papers on Vector Quantized Variational Autoencoder (VQ-VAE) and its application</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\11\03\文生图相关模型最新进展小结\ rel=bookmark>文生图相关模型最新进展小结</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\09\24\回看深度学习-经典网络学习\ rel=bookmark>回看深度学习:经典网络学习</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\07\30\profile-a-deep-learning-model\ rel=bookmark>profile a deep learning model</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\18\从论文中看AI绘画-二\ rel=bookmark>从论文中看AI绘画(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\12\myJourneyToAI-深度学习之旅\ rel=bookmark>myJourneyToAI:深度学习之旅</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/11/18/vqvae%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/ title=vqvae及其变体代码学习>https://www.sekyoro.top/2024/11/18/vqvae及其变体代码学习/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/deep-learning/ rel=tag><i class="fa fa-tag"></i> deep learning</a><a href=/tags/vqvae/ rel=tag><i class="fa fa-tag"></i> vqvae</a></div><div class=post-nav><div class=post-nav-item><a href=/2024/11/03/%E6%96%87%E7%94%9F%E5%9B%BE%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E5%B0%8F%E7%BB%93/ rel=prev title=文生图相关模型最新进展小结> <i class="fa fa-chevron-left"></i> 文生图相关模型最新进展小结 </a></div><div class=post-nav-item><a href=/2024/12/09/%E4%BB%8EC-%E6%A8%A1%E6%9D%BF%E8%B0%88%E8%B5%B7/ rel=next title=从C++模板谈起> 从C++模板谈起 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#VQVAE><span class=nav-number>1.</span> <span class=nav-text>VQVAE</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#VQVAE-2><span class=nav-number>2.</span> <span class=nav-text>VQVAE-2</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Residual-VQ><span class=nav-number>3.</span> <span class=nav-text>Residual VQ</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#SIMVQ><span class=nav-number>4.</span> <span class=nav-text>SIMVQ</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#einops%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C><span class=nav-number>5.</span> <span class=nav-text>einops中常用操作</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#rearrange><span class=nav-number>5.1.</span> <span class=nav-text>rearrange</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#reduce><span class=nav-number>5.2.</span> <span class=nav-text>reduce</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#stack-and-concatenation><span class=nav-number>5.3.</span> <span class=nav-text>stack and concatenation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#add-or-remove-axis><span class=nav-number>5.4.</span> <span class=nav-text>add or remove axis</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#channel-shuffle><span class=nav-number>5.5.</span> <span class=nav-text>channel shuffle</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#repeat><span class=nav-number>5.6.</span> <span class=nav-text>repeat</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#split-dimension><span class=nav-number>5.7.</span> <span class=nav-text>split dimension</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#striding-anything><span class=nav-number>5.8.</span> <span class=nav-text>striding anything</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#parse-shape><span class=nav-number>5.9.</span> <span class=nav-text>parse_shape</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#pack-and-unpack><span class=nav-number>5.10.</span> <span class=nav-text>pack and unpack</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BB%93%E5%90%88torch%E4%BD%BF%E7%94%A8layers><span class=nav-number>5.11.</span> <span class=nav-text>结合torch使用layers</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Einx><span class=nav-number>5.12.</span> <span class=nav-text>Einx</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99><span class=nav-number>6.</span> <span class=nav-text>相关资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>248</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>217</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/deep-learning/ rel=tag>deep learning</a><span class=tag-list-count>11</span><li class=tag-list-item><a class=tag-list-link href=/tags/vqvae/ rel=tag>vqvae</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>3.4m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>51:42</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>