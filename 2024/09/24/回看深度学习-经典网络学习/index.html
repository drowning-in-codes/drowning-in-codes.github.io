<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=传统深度学习,或者说在llm之前的深度学习,现在看来,还是有很多trick以及各种模块”缝合”的内容,这部分有很多提出来的方法其实都有一些共通点的,这里简单回顾总结一下. name=description><meta content=article property=og:type><meta content=回看深度学习:经典网络学习 property=og:title><meta content=https://www.sekyoro.top/2024/09/24/%E5%9B%9E%E7%9C%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=传统深度学习,或者说在llm之前的深度学习,现在看来,还是有很多trick以及各种模块”缝合”的内容,这部分有很多提出来的方法其实都有一些共通点的,这里简单回顾总结一下. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/09/26/GI83VzYUF4PaKg6.png property=og:image><meta content=https://s2.loli.net/2024/09/26/BEDZibGuALMsfUx.png property=og:image><meta content=https://s2.loli.net/2024/09/26/LoV7htDUYzE3TuI.png property=og:image><meta content=https://s2.loli.net/2024/09/26/qsz5JcBe1dC9Dak.png property=og:image><meta content=https://s2.loli.net/2024/09/26/j1EB3GYx5Kz9wJa.png property=og:image><meta content=https://s2.loli.net/2024/09/26/Q5ajrv8dZT6MoKS.png property=og:image><meta content=https://s2.loli.net/2024/09/24/B5C7huNZsTWfdGP.png property=og:image><meta content=https://raw.githubusercontent.com/zalandoresearch/pytorch-vq-vae/b98fde9a8f8f65486dbfaf12958049e8d33184a1//images/vq-vae.png property=og:image><meta content=https://s2.loli.net/2024/10/08/mZbQlPEJiKeTztn.png property=og:image><meta content=https://s2.loli.net/2024/09/24/eyoQUhlq24Ynxru.png property=og:image><meta content=https://s2.loli.net/2024/09/24/fPyLMHekCdGRWrT.png property=og:image><meta content=https://s2.loli.net/2024/09/24/l5AuBIpzqbGL2WO.png property=og:image><meta content=https://s2.loli.net/2024/11/16/zZne9Dv8XTjBGbc.png property=og:image><meta content=https://s2.loli.net/2024/11/16/62DyRXinWMfxJVd.png property=og:image><meta content=https://s2.loli.net/2024/09/25/dPITi2QMRE8u4Z3.png property=og:image><meta content=https://s2.loli.net/2024/09/25/bVSItlqa89kWpmj.png property=og:image><meta content=https://s2.loli.net/2024/11/13/IoSaJwE4RzcLWjB.png property=og:image><meta content=https://s2.loli.net/2024/11/13/MfEVeAO418buc7R.png property=og:image><meta content=https://s2.loli.net/2024/11/13/cpSPAv1gRCqd9iT.png property=og:image><meta content=https://s2.loli.net/2024/11/13/NVe1Qkb8cGsaCph.png property=og:image><meta content=https://s2.loli.net/2024/11/13/MowKEQIxsBk7Cq5.png property=og:image><meta content=https://s2.loli.net/2024/11/13/Zp76aA8Skv4joL2.png property=og:image><meta content=https://arxiv.org/html/2411.02038v1/x1.png property=og:image><meta content=2024-09-24T07:38:15.000Z property=article:published_time><meta content=2024-11-16T05:41:02.628Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="deep learning" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/09/26/GI83VzYUF4PaKg6.png name=twitter:image><link href=https://www.sekyoro.top/2024/09/24/%E5%9B%9E%E7%9C%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>回看深度学习:经典网络学习 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/09/24/%E5%9B%9E%E7%9C%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>回看深度学习:经典网络学习</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-09-24 15:38:15" datetime=2024-09-24T15:38:15+08:00>2024-09-24</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-11-16 13:41:02" datetime=2024-11-16T13:41:02+08:00 itemprop=dateModified>2024-11-16</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>8.8k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>8 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>传统深度学习,或者说在llm之前的深度学习,现在看来,还是有很多trick以及各种模块”缝合”的内容,这部分有很多提出来的方法其实都有一些共通点的,这里简单回顾总结一下.<br><span id=more></span><h2 id=Vision-Transformer-and-its-variants><a title="Vision Transformer and its variants" class=headerlink href=#Vision-Transformer-and-its-variants></a>Vision Transformer and its variants</h2><h3 id=AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE><a title="AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE" class=headerlink href=#AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE></a>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h3><p><img alt=image-20240926173222690 data-src=https://s2.loli.net/2024/09/26/GI83VzYUF4PaKg6.png><h3 id=Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows><a title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" class=headerlink href=#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h3><p><img alt=image-20240926173305765 data-src=https://s2.loli.net/2024/09/26/BEDZibGuALMsfUx.png><h2 id=Modernify-Conv><a title="Modernify Conv" class=headerlink href=#Modernify-Conv></a>Modernify Conv</h2><h3 id=A-ConvNet-for-the-2020s><a title="A ConvNet for the 2020s" class=headerlink href=#A-ConvNet-for-the-2020s></a>A ConvNet for the 2020s</h3><p>出发点是一个ResNet-50模型.首先使用类似的训练技术来训练vision transformer,并获得了比原始ResNet-50更好的结果<p>然后我们研究了一系列的设计决策,总结为<p>1 )marco design,2 ) ResNeXt,3 )inverted- bottleneck,4 )大核尺寸,5 )各种layer-wise的mirco design.<p><img alt=image-20240926153040226 data-src=https://s2.loli.net/2024/09/26/LoV7htDUYzE3TuI.png><h4 id=训练技术><a class=headerlink href=#训练技术 title=训练技术></a>训练技术</h4><p>最近的研究表明,一套现代的训练技术可以显著提高一个简单的ResNet-50模型的性能.在本研究中,使用了与Dei-T和Swin Transformer相近的训练方法.<p>对于深度残差网络,训练从原来的90次扩展到300次.<p>使用Adam W优化器,数据增强技术如Mixup、Cutmix、Rand Augment、Random Erasing,以及正则化方案包括Stochastic Depth和Label Smoothing<h4 id=Marco-Design><a title="Marco Design" class=headerlink href=#Marco-Design></a>Marco Design</h4><h5 id=修改阶段计算比例><a class=headerlink href=#修改阶段计算比例 title=修改阶段计算比例></a>修改阶段计算比例</h5><p>Swin-T阶段计算比例略有不同,为1：1：3：1.对于较大的Swin Transformer,比例为1：1：9：1.<p>根据设计将每个阶段的块数从ResNet-50中的( 3、4、6、3)调整为( 3、3、9、3),这也将FLOPs与Swin-T对齐<h4 id=修改stem为patchify><a class=headerlink href=#修改stem为patchify title=修改stem为patchify></a>修改stem为patchify</h4><p>标准ResNet中的stem包含一个7 × 7的步幅为2的卷积层,然后是一个max pooling,这导致对输入图像进行4 ×的下采样.<p>在Vision Transformers中,使用了patchify策略作为stem,对应于较大的核尺寸(如kernel大小= 14或16)和非重叠卷积.<p>Swin Transformer使用了类似的” Patchify “层,但是具有更小的Patch size 4以适应架构的多级设计<p>将ResNet风格的stem替换为使用4 × 4,stride为4的卷积层实现的patchify层.准确率从79.4 %变为79.5 %.这表明ResNet中的stem可能被更简单的” patchify “层ViT替代,这将导致类似的性能<h4 id=ResNext-ify><a class=headerlink href=#ResNext-ify title=ResNext-ify></a>ResNext-ify</h4><p>核心部件是group normalization,其中卷积滤波器被分成不同的组.在更高的层面上,ResNeXt的指导原则是”利用更多群体,拓展宽度”.更确切地说,ResNeXt对瓶颈块中的3 × 3 conv层使用分组卷积.由于这显著降低了FLOPs,因此扩大了网络宽度以补偿容量损失,<p>depth-wise卷积类似于自注意力中的加权和操作,它在每个通道的基础上操作,即只在空间维度上混合信息<p>深度卷积和1 × 1卷积的结合导致了空间和通道混合的分离,这是视觉转换器共有的特性,其中每个操作要么混合了空间或通道维度的信息,但不是两者都混合.<p>depth-wise卷积的使用有效地降低了网络的FLOPs,但会降低精度<p>根据ResNeXt提出的策略,将网络宽度增加到与Swin-T的( 64 ~ 96)相同的通道数.随着FLOPs ( 5.3G )的增加,网络性能达到80.5 %.<h4 id=Inverted-Bottleneck><a title="Inverted Bottleneck" class=headerlink href=#Inverted-Bottleneck></a>Inverted Bottleneck</h4><p>在每个Transformer模块中,一个重要的设计是它创建了一个反向瓶颈,即<strong>MLP模块的隐藏维度比输入维度宽4倍</strong><p><img alt=image-20240926163046594 data-src=https://s2.loli.net/2024/09/26/qsz5JcBe1dC9Dak.png><p>尽管depth-wise卷积层的FLOP值有所增加,但由于下采样残差块的捷径1 × 1 conv层的FLOP值显著降低,该变化使得整个网络的FLOP值降低到4.6 G<h4 id=更大的尺寸大小><a class=headerlink href=#更大的尺寸大小 title=更大的尺寸大小></a>更大的尺寸大小</h4><p>Vision Transformer最具有特色的一个方面是其非局部自注意力,这使得每一层都具有全局的感受野.过去,卷积神经网络使用大的内核尺寸,而(由VGGNet 推广)的金标准是堆叠小的内核尺寸( 3 × 3 )的conv层,它们在现代GPU上具有高效的硬件实现<p>虽然Swin Transformers将局部窗口重新引入到自注意力块中,但窗口大小至少为7 × 7,明显大于ResNe ( X ) t核大小3 × 3.在这里重新考虑了卷积神经网络中大核卷积的使用.<p>为了探索大核,<strong>一个先决条件是将depth-wise卷积放在前面</strong>.这在Transformers中也是显而易见的设计决策:MSA块( 大内核conv)放置在复杂/低效的模块的MLP层之前会有更少的通道,而高效、密集的1 × 1层会做繁重的提升.这一中间步骤将FLOPs降低到4.1 G,导致性能暂时下降到79.9 %.<p><img alt=image-20240926164338145 data-src=https://s2.loli.net/2024/09/26/j1EB3GYx5Kz9wJa.png><p>在所有这些准备工作中,采用更大的核卷积的好处是显著的.实验了几种不同的核尺寸,包括3、5、7、9和11.网络的性能从79.9 % ( 3 × 3 )提高到80.6 % ( 7 × 7 ),而网络的FLOPs基本保持不变.此外观察到<strong>较大的核尺寸带来的收益在7 × 7处达到饱和点</strong>.在大容量模型中也验证了这一行为:将内核大小增加到7 × 7以上时,ResNet - 200机制模型没有表现出进一步的增益<h4 id=Micro-Design><a title="Micro Design" class=headerlink href=#Micro-Design></a>Micro Design</h4><p><strong>使用GELU替换RELU</strong><p>NLP和视觉架构的一个不同之处在于使用的激活函数的具体形式.随着时间的推移,许多激活函数已经被开发出来,但ReLU 由于其简单和高效,仍然被广泛用于卷积神经网络中.<p>在ConvNet中,ReLU也可以用GELU代替,尽管精度保持不变<p><strong>更少的激活函数</strong><p>transfomer的激活函数较少.考虑一个具有key / query / value线性嵌入层的Transformer块,一个MLP块中的投影层和两个线性层.MLP块中只有一个激活函数.相比较而言,通常的做法是在每个卷积层中添加一个激活函数,包括1 × 1卷积层.<p>除了两个1 × 1层之间的GELU层外,从残差块中消除了所有GELU层,复制了Transformer块的风格.该过程使结果提高了0.7 % ~ 81.3 %,与Swin - T的性能基本匹配<p><strong>更少的normalization层</strong><p>transformer块通常也具有较少的归一化层数.这里去掉两个Batch Norm ( BN )层,在conv 1 × 1层之前只留下一个BN层.<p>每个块的归一化层数甚至比Transformers还要少,因为从经验上发现在块的开头增加一个BN层并不能提高性能<p><strong>替换BN为LN</strong><p>BatchNorm是卷积神经网络中的一个重要组成部分,它提高了收敛性并减少了过拟合<p>更简单的层归一化 在Transformer中得到了应用,在不同的应用场景中表现出良好的性能<p>直接将原始ResNet中的LN替换为BN会导致次优的性能.随着网络结构和训练技术的改变,这里重新使用LN代替BN的影响,观察到ConvNet模型在使用LN进行训练时没有任何困难；事实上,性能略好,获得了81.5 %的准确率<p><strong>单独的下采样层</strong><p>在ResNet中,空间下采样是通过每个阶段开始时的残差块来实现的,采用3×3的conv,stride为2 (short connection时使用stride为2的1 × 1 conv).<p>在Swin Transformers中,不同尺度之间增加了单独的下采样层.这里使用2×2的conv层和stride为2进行空间下采样.<p>进一步的研究表明,<strong>在空间分辨率变化的地方增加归一化层有助于稳定训练</strong>.其中包括Swin Transformers中也使用的几个LN层：每个下采样层之前的一个,stem之后的一个,最后全局平均池化之后的一个.可以将准确率提高到82.0%,明显超过Swin-T的81.3%<h3 id=Early-Convolutions-Help-Transformers-See-Better><a title="Early Convolutions Help Transformers See Better" class=headerlink href=#Early-Convolutions-Help-Transformers-See-Better></a>Early Convolutions Help Transformers See Better</h3><p><img alt=image-20240926160213575 data-src=https://s2.loli.net/2024/09/26/Q5ajrv8dZT6MoKS.png><p>Vision Transformer(ViT)模型表现出不达标的可优化性。特别地，它们对优化器( Adam W vs . SGD)、优化器超参数和训练调度长度的选择非常敏感。相比较而言，现代卷积神经网络更容易优化。为什么会出现这种情况?在这项工作中猜想问题在于ViT模型的patchify stem,它是通过应用于输入图像的stride-p p × p卷积(默认p = 16)来实现的<blockquote><p>也就是下采样p倍</blockquote><p>这种大核加大步长的卷积与神经网络中卷积层的典型设计选择背道而驰。为了检验这种非典型的设计选择是否会导致问题，分析了ViT模型的优化行为，其原始的Patchify词干与一个简单部分对应,将ViT patchify stem替换为少量堆叠的stride-2 3 × 3卷积。<p>虽然两种ViT设计的绝大多数计算量是相同的,但发现这种<strong>早期视觉处理的微小变化导致训练行为在对优化设置的敏感性以及最终的模型精度方面有明显的不同</strong>。在ViT中使用卷积树干显著地增加了优化的稳定性，并提高了峰值性能(在ImageNet - 1k上的top - 1准确率提高了1 - 2 %)，同时保持了触发器和运行时间<p>这种改进可以在模型复杂度(从1G到36G触发器)和数据集规模(从ImageNet - 1k到ImageNet - 21k)的宽光谱范围内观察到。这些发现促使我们推荐在该机制下为ViT模型使用一个标准的、轻量级的卷积主干，作为比原始ViT模型设计更稳健的架构选择。<h2 id=Vector-quantization-and-Codebook><a title="Vector quantization and Codebook" class=headerlink href=#Vector-quantization-and-Codebook></a>Vector quantization and Codebook</h2><h3 id=Neural-Discrete-Representation-Learning><a title="Neural Discrete Representation Learning" class=headerlink href=#Neural-Discrete-Representation-Learning></a>Neural Discrete Representation Learning</h3><p><a href=https://arxiv.org/abs/1711.00937 rel=noopener target=_blank>1711.00937] Neural Discrete Representation Learning (arxiv.org)</a><p><strong>在没有监督的情况下学习有用的表示</strong>(个人认为算是自监督->生成式学习)仍然是机器学习中的一个关键挑战.<p>VQ-VAE 两个关键方面与VAEs不同：<ul><li>encoder输出离散的编码,也就是中间嵌入是离散的<li>先验知识是学到的.</ul><p>为了学习一个离散的潜在表示,结合了向量量化( VQ )的思想.使用VQ方法可以使模型避免”后验崩溃”问题- -当潜在变量(中间嵌入)与decoder配对时被忽略<h4 id=离散潜变量-中间嵌入><a class=headerlink href=#离散潜变量-中间嵌入 title=离散潜变量(中间嵌入)></a>离散潜变量(中间嵌入)</h4><p>定义一个潜在嵌入空间e∈R^K×D^,其中K为离散潜在空间(即K - way范畴)的大小,D为每个潜在嵌入向量e^i^的维数.注意到存在K个嵌入向量e~i~∈R^D^,i∈1,2,..,K.该模型取一个输入x,通过编码器产生输出z~e~(x)<p>然后利用共享嵌入空间e通过最近邻查找计算离散潜变量z<p><img alt=image-20240924160115359 data-src=https://s2.loli.net/2024/09/24/B5C7huNZsTWfdGP.png><p>Z~e~(x)是编码器网络的输出,根据这个输出通过找到最近邻映射到给定的潜在空间中,得到z~q~(x)作为decoder的输入.</p><script type="math/tex; mode=display">
z_q(x)=e_k,\quad\text{where}\quad k=\text{argmin}_j\|z_e(x)-e_j\|_2</script><p>对于梯度,直接将z~q~(x)的梯度copy到z~e~(x)<p>由于encoder的输出表示和decoder的输入共享相同的D维空间<strong>,梯度包含了encoder如何改变其输出以降低重构损失的有用信息</strong><p>总损失如下,有三个组成部分,分别用于训练VQ-VAE的不同部分.<p>第一项是重构损失(或数据项),它优化了解码器和编码器<p>由于z~e~ (x)到z~q~( x)映射的直通梯度估计(梯度直接copy),嵌入e~i~没有从重构损失log~p~ ( z | z~q~ ( x ) )中获得梯度.因此,为了学习嵌入空间,使用最简单的字典学习算法之一,向量量化( VQ ).VQ目标使用l2误差将嵌入向量e~i~移动到编码器输出z~e~ (x).<p>最后,由于嵌入空间的体积是无量纲的,如果嵌入e~i~的训练速度没有编码器参数那么快,嵌入空间的体积可以任意增长.</p><script type="math/tex; mode=display">
L=\log p(x|z_q(x))+\|\mathrm{sg}[z_e(x)]-e\|_2^2+\beta\|z_e(x)-\mathrm{sg}[e]\|_2^2</script><p>Sg表示在前向计算时定义为恒等式且具有零偏导数的停止梯度算子,从而有效地约束其操作数为非更新常数</p><script type="math/tex; mode=display">
sg(x)=\left\{\begin{array}{l}x (in forward propagation)\\0 (in backward propagation)\end{array}\right.</script><p>decoder只优化第一个损失项,encoder优化第一个和最后一个损失项,嵌入由中间损失项优化.<p><img alt=img data-src=https://raw.githubusercontent.com/zalandoresearch/pytorch-vq-vae/b98fde9a8f8f65486dbfaf12958049e8d33184a1//images/vq-vae.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>VectorQuantizer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, num_embeddings, embedding_dim, commitment_cost</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(VectorQuantizer, self).__init__()</span><br><span class=line>        </span><br><span class=line>        self._embedding_dim = embedding_dim</span><br><span class=line>        self._num_embeddings = num_embeddings</span><br><span class=line>        </span><br><span class=line>        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)</span><br><span class=line>        self._embedding.weight.data.uniform_(-<span class=number>1</span>/self._num_embeddings, <span class=number>1</span>/self._num_embeddings)</span><br><span class=line>        self._commitment_cost = commitment_cost</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, inputs</span>):</span></span><br><span class=line>        <span class=comment># convert inputs from BCHW -> BHWC</span></span><br><span class=line>        inputs = inputs.permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>1</span>).contiguous()</span><br><span class=line>        input_shape = inputs.shape</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Flatten input</span></span><br><span class=line>        flat_input = inputs.view(-<span class=number>1</span>, self._embedding_dim)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Calculate distances</span></span><br><span class=line>        distances = (torch.<span class=built_in>sum</span>(flat_input**<span class=number>2</span>, dim=<span class=number>1</span>, keepdim=<span class=literal>True</span>) </span><br><span class=line>                    + torch.<span class=built_in>sum</span>(self._embedding.weight**<span class=number>2</span>, dim=<span class=number>1</span>)</span><br><span class=line>                    - <span class=number>2</span> * torch.matmul(flat_input, self._embedding.weight.t()))</span><br><span class=line>            </span><br><span class=line>        <span class=comment># Encoding</span></span><br><span class=line>        encoding_indices = torch.argmin(distances, dim=<span class=number>1</span>).unsqueeze(<span class=number>1</span>)</span><br><span class=line>        encodings = torch.zeros(encoding_indices.shape[<span class=number>0</span>], self._num_embeddings, device=inputs.device)</span><br><span class=line>        encodings.scatter_(<span class=number>1</span>, encoding_indices, <span class=number>1</span>)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Quantize and unflatten</span></span><br><span class=line>        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)</span><br><span class=line>        </span><br><span class=line>        <span class=comment># Loss</span></span><br><span class=line>        e_latent_loss = F.mse_loss(quantized.detach(), inputs)</span><br><span class=line>        q_latent_loss = F.mse_loss(quantized, inputs.detach())</span><br><span class=line>        loss = q_latent_loss + self._commitment_cost * e_latent_loss</span><br><span class=line>        </span><br><span class=line>        quantized = inputs + (quantized - inputs).detach()</span><br><span class=line>        avg_probs = torch.mean(encodings, dim=<span class=number>0</span>)</span><br><span class=line>        perplexity = torch.exp(-torch.<span class=built_in>sum</span>(avg_probs * torch.log(avg_probs + <span class=number>1e-10</span>)))</span><br><span class=line>        </span><br><span class=line>        <span class=comment># convert quantized from BHWC -> BCHW</span></span><br><span class=line>        <span class=keyword>return</span> loss, quantized.permute(<span class=number>0</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>2</span>).contiguous(), perplexity, encodings</span><br></pre></table></figure><ul><li><a href=https://zhuanlan.zhihu.com/p/640000410 rel=noopener target=_blank>VQVAE PyTorch 实现教程 - 知乎 (zhihu.com)</a><li><a href=https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/02_Vector_Quantized_Variational_AutoEncoder.ipynb rel=noopener target=_blank>Pytorch-VAE-tutorial/02_Vector_Quantized_Variational_AutoEncoder.ipynb at master · Jackson-Kang/Pytorch-VAE-tutorial (github.com)</a><li><a href=https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb rel=noopener target=_blank>pytorch-vq-vae/vq-vae.ipynb at master · zalandoresearch/pytorch-vq-vae (github.com)</a></ul><h3 id=Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2><a title="Generating Diverse High-Fidelity Images with VQ-VAE-2" class=headerlink href=#Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2></a>Generating Diverse High-Fidelity Images with VQ-VAE-2</h3><p><img alt=image-20241008113944530 data-src=https://s2.loli.net/2024/10/08/mZbQlPEJiKeTztn.png><h3 id=Taming-Transformers-for-High-Resolution-Image-Synthesis><a title="Taming Transformers for High-Resolution Image Synthesis" class=headerlink href=#Taming-Transformers-for-High-Resolution-Image-Synthesis></a>Taming Transformers for High-Resolution Image Synthesis</h3><p><a href=https://arxiv.org/pdf/2012.09841 rel=noopener target=_blank>2012.09841 (arxiv.org)</a><p><img alt=image-20240924210104616 data-src=https://s2.loli.net/2024/09/24/eyoQUhlq24Ynxru.png><p>为了学习序列数据上的长程交互,Transformer在各种各样的任务上不断地展示出最先进的结果.与卷积神经网络不同的是,它们不包含优先考虑局部交互的归纳偏差.这使得它们具有表达能力,<strong>但对于长序列,如高分辨率图像,在计算上也是不可行的.本文展示了如何将CNN的感应偏置的有效性与transformer的表达能力相结合,使其能够建模,从而合成高分辨率图像</strong><p>复杂度不是建立在单个像素上,而是需要一种方法,使用学习表示的离散码本,使得任何图像x∈R^H×W×3^都可以由码本项的空间集合z~q~∈R^h×w×nz^表示,其中nz是中间变量的维数<h4 id=学习高效codebook><a class=headerlink href=#学习高效codebook title=学习高效codebook></a>学习高效codebook</h4><p>首先学习一个由编码器E和解码器G组成的卷积模型,使得它们一起从一个学习的离散codebook $\mathcal{Z}={z<em>k}</em>{k=1}^K\subset\mathbb{R}^{n_z}$中学习用码表示图像,获得</p><script type="math/tex; mode=display">
z_{\mathbf{q}}=\mathbf{q}(\hat{z}):=\left(\underset{z_k\in\mathcal{Z}}{\operatorname*{\arg\min}}\|\hat{z}_{ij}-z_k\|\right)\in\mathbb{R}^{h\times w\times n_z}</script><script type="math/tex; mode=display">
\hat{x}=G(z_\mathbf{q})=G\left(\mathbf{q}(E(x))\right)</script><p>利用编码( z = E(x)∈R^h×w×nz^ )和每个空间码( z~ij~^^^∈R^nz^ )在其最近的codebook项z~k~上的后续逐元素量化q ( · )得到zq</p><script type="math/tex; mode=display">
\begin{aligned}\mathcal{L}_{\mathrm{VQ}}(E,G,\mathcal{Z})&=\|x-\hat{x}\|^{2}+\|\mathrm{sg}[E(x)]-z_{\mathbf{q}}\|_{2}^{2}\\&+\|\mathrm{sg}[z_{\mathbf{q}}]-E(x)\|_{2}^{2}.\end{aligned}</script><p>使用Transformer将图像表示为潜在图像成分上的分布<h4 id=生成视觉丰富的codebook><a class=headerlink href=#生成视觉丰富的codebook title=生成视觉丰富的codebook></a>生成视觉丰富的codebook</h4><script type="math/tex; mode=display">
\mathcal{L}_\mathrm{GAN}(\{E,G,\mathcal{Z}\},D)=[\log D(x)+\log(1-D(\hat{x}))]</script><script type="math/tex; mode=display">
\begin{aligned}\mathcal{Q}^{*}=\arg\min_{E,G,\mathcal{Z}}\max_{D}\mathbb{E}_{x\sim p(x)}\Big[\mathcal{L}_{\mathrm{VQ}}(E,G,\mathcal{Z})\\+\lambda\mathcal{L}_{\mathrm{GAN}}(\{E,G,\mathcal{Z}\},D)\Big]\end{aligned}</script><p>$\lambda$设置取</p><script type="math/tex; mode=display">
\lambda=\frac{\nabla_{G_L}[\mathcal{L}_{\mathrm{rec}}]}{\nabla_{G_L}[\mathcal{L}_{\mathrm{GAN}}]+\delta}</script><p>∂~GL~ [ · ]表示其梯度也就是在decoder最后一层L的梯度<h4 id=使用transformers学习图像生成><a class=headerlink href=#使用transformers学习图像生成 title=使用transformers学习图像生成></a>使用transformers学习图像生成</h4><p>在E和G可用的情况下可以根据它们编码的codebook索引来表示图像.图像x的量化编码是由z~q~ = q ( E ( x ) )∈R^h×w×nz^给出的,并且等价于一个序列从码本中得到的索引$s\in{0,\ldots,|\mathcal{Z}|-1}^{n\times w}$,它是由码本Z中的索引替换每个码得到的<p>通过将序列s的索引映射回其对应的码本项,z~q~ = ( z~sij~ )很容易恢复并解码成图像(x = G(z~q~ ) ).<p>在s中选择一些指标的排序后图像生成可以表示为自回归下一指标预测：给定指标s < i,转换器学习预测可能的下一指标的分布,即p(s~i~ | s < i)</p><script type="math/tex; mode=display">
\mathcal{L}_{\text{Transformer}}=\mathbb{E}_{x\sim p(x)}\left[-\log p(s)\right]</script><h4 id=约束的图像生成><a class=headerlink href=#约束的图像生成 title=约束的图像生成></a>约束的图像生成</h4><p>在许多图像合成任务中,用户需要通过提供额外的信息来控制生成过程,从而合成一个示例.这种信息,我们称之为c,可以是描述整体图像类别的单个标签,也可以是另一幅图像本身.然后任务是学习给定这个信息c的序列的似然</p><script type="math/tex; mode=display">
p(s|c)=\prod_ip(s_i|s_{<i},c)</script><h4 id=生成高分辨率图片><a class=headerlink href=#生成高分辨率图片 title=生成高分辨率图片></a>生成高分辨率图片</h4><p>为了生成百万像素级别的图像,我们必须在训练过程中对图像块和裁剪图像进行处理,将s的长度限制在最大可行尺寸.<p>为了对图像进行采样使用滑动窗口方式的transformers<p><img alt=image-20240924225113569 data-src=https://s2.loli.net/2024/09/24/fPyLMHekCdGRWrT.png><p><a href=https://github.com/Shubhamai/pytorch-vqgan rel=noopener target=_blank>Shubhamai/pytorch-vqgan: This repo contains the implementation of VQGAN, Taming Transformers for High-Resolution Image Synthesis in PyTorch from scratch. I have added support for custom datasets, testings, experiment tracking etc. (github.com)</a><h2 id=UDA><a class=headerlink href=#UDA title=UDA></a>UDA</h2><h3 id=Unsupervised-Domain-Adaptation-by-Backpropagation><a title="Unsupervised Domain Adaptation by Backpropagation" class=headerlink href=#Unsupervised-Domain-Adaptation-by-Backpropagation></a>Unsupervised Domain Adaptation by Backpropagation</h3><p><a href=https://arxiv.org/abs/1409.7495 rel=noopener target=_blank>1409.7495] Unsupervised Domain Adaptation by Backpropagation (arxiv.org)</a><p><img alt=image-20240924230029897 data-src=https://s2.loli.net/2024/09/24/l5AuBIpzqbGL2WO.png><p>第一篇提出GRL用于域适应的文章<h3 id=Domain-Adaptive-Object-Detection-for-Autonomous-Driving-under-Foggy-Weather><a title="Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather" class=headerlink href=#Domain-Adaptive-Object-Detection-for-Autonomous-Driving-under-Foggy-Weather></a>Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather</h3><p><img alt=image-20241116123259611 data-src=https://s2.loli.net/2024/11/16/zZne9Dv8XTjBGbc.png><p>本文提出了一种新颖的雾天环境下面向自动驾驶的域自适应目标检测框架。<p>我们的方法同时利用图像级和对象级的自适应来减少图像风格和对象外观的领域差异。<p>为了进一步增强模型在挑战性样本下的能力，我们还提出了一个新的对抗梯度反转层，与领域自适应一起对困难样本进行对抗挖掘。<p>此外，我们提出通过数据增强生成一个辅助域，以执行一个新的域级度量正则化。在公开数据集上的实验结果表明了所提方法的有效性和准确性。<h4 id=Image-level-adaptation><a title="Image-level adaptation" class=headerlink href=#Image-level-adaptation></a>Image-level adaptation</h4><p>域分类器只是一个具有两个卷积层的简单CNN，它将输出一个预测来识别特征域<h4 id=Object-level-Adaptation><a title="Object-level Adaptation" class=headerlink href=#Object-level-Adaptation></a>Object-level Adaptation</h4><p>除了不同领域的图像级全局差异外，不同领域的物体在外观、大小、颜色等方面也可能存在差异。将Faster R - CNN中ROI Pooling层之后的每个候选区域定义为一个潜在的对象。与图像级自适应模块类似，通过ROI池化获得目标级域表示后，我们实现了一个目标级域分类器来识别局部信息中的特征衍生。一个训练有素的对象级分类器，一个具有3个全连接层的神经网络，将有助于对齐对象级特征分布。<h3 id=Multi-adversarial-Faster-RCNN-for-Unrestricted-Object-Detection><a title="Multi-adversarial Faster-RCNN for Unrestricted Object Detection" class=headerlink href=#Multi-adversarial-Faster-RCNN-for-Unrestricted-Object-Detection></a>Multi-adversarial Faster-RCNN for Unrestricted Object Detection</h3><p><img alt=image-20241116123204164 data-src=https://s2.loli.net/2024/11/16/62DyRXinWMfxJVd.png><p>在多个模块上使用GRL与域判别器,相当于有个多尺度.<h3 id=CDTrans-Cross-domain-Transformer-for-Unsupervised-Domain-Adaptation><a title="CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation" class=headerlink href=#CDTrans-Cross-domain-Transformer-for-Unsupervised-Domain-Adaptation></a>CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation</h3><p><a href=https://arxiv.org/abs/2109.06165 rel=noopener target=_blank>2109.06165] CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation (arxiv.org)</a><p><img alt=image-20240925162052101 data-src=https://s2.loli.net/2024/09/25/dPITi2QMRE8u4Z3.png><p>它由3个权重共享的transformer组成,通过使用two-way center-aware labeling method方法选择成对输入.<p>源分支( HS )和目标分支( HT )采用交叉熵,源-目标分支( HS + T )和HT之间采用蒸馏损失$L_{dtl}=\sum_kq_k\log p_k$<p>针对来自不同域的特征使用权重相同的网络,将得到的结果使用交叉熵作为损失,相比于之前使用GRL的方法,这里更偏向使用一种指标作为损失优化网络<h3 id=TVT-Transferable-Vision-Transformer-for-Unsupervised-Domain-Adaptation><a title="TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation" class=headerlink href=#TVT-Transferable-Vision-Transformer-for-Unsupervised-Domain-Adaptation></a>TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation</h3><p><a href=https://openaccess.thecvf.com/content/WACV2023/papers/Yang_TVT_Transferable_Vision_Transformer_for_Unsupervised_Domain_Adaptation_WACV_2023_paper.pdf rel=noopener target=_blank>TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation (thecvf.com)</a><p>随着近年来Vision Transformer在视觉任务中的应用呈指数增长,然而,ViT在适应跨领域知识方面的能力在文献中仍未被探索.为了填补这一空白,本文首先全面考察了ViT在多种域适应任务上的表现.令人惊讶的是,ViT表现出优越的泛化能力,而通过结合对抗自适应可以进一步提高性能<p>尽管如此,直接使用基于CNNs的适应策略并没有利用ViT在知识转移中发挥重要作用的内在优势(例如,注意力机制和序列图像表示).为了弥补这一缺陷,我们提出了一个统一的框架,即可迁移视觉转换器( Transferable Vision Transformer,TVT ),以充分利用视觉里程计的可迁移性进行领域自适应.<p><img alt=image-20240925162302199 data-src=https://s2.loli.net/2024/09/25/bVSItlqa89kWpmj.png><p>与ViT一样,源图像和目标图像都被分割成固定大小的图像块,然后线性映射并嵌入位置信息.生成的补丁送入变压器编码器,最后一层由可转让性适配模块( TAM )替换.特征学习、对抗域适应和分类由ViT-akin骨干、两个域判别器(在path-level和global-level上)、判别聚类模块( Discriminative Clustering Module,DCM )和基于MLP的分类器完成<p>在transformer模型中插入了一个Transferable MSA模块(TMA)用于增强<p>在使用transformer时,考虑patch-level,global-level,spatial-level等等不同尺度上使用domain discriminator<p>除了进行域适应之外,还需要考虑本身特征对于检测的重要性<blockquote><p>针对利用无标签目标数据学习概率判别分类器的挑战性问题，需要最小化目标域上的期望分类误差。然而，如果不引入目标域的语义约束，通过TAM强迫两个域相似的跨域特征对齐可能会破坏学习到的表示的判别信息。</blockquote><h3 id=Safe-Self-Refinement-for-Transformer-based-Domain-Adaptation><a title="Safe Self-Refinement for Transformer-based Domain Adaptation" class=headerlink href=#Safe-Self-Refinement-for-Transformer-based-Domain-Adaptation></a>Safe Self-Refinement for Transformer-based Domain Adaptation</h3><h2 id=VQ-VAE><a class=headerlink href=#VQ-VAE title=VQ-VAE></a>VQ-VAE</h2><p><a href=https://github.com/lucidrains/vector-quantize-pytorch/tree/master rel=noopener target=_blank>lucidrains/vector-quantize-pytorch: Vector (and Scalar) Quantization, in Pytorch</a>非常不错,此外这个作者还有一个实现各种vit的仓库<a href=https://github.com/lucidrains/vit-pytorch rel=noopener target=_blank>lucidrains/vit-pytorch</a><h3 id=Neural-Discrete-Representation-Learning-1><a title="Neural Discrete Representation Learning" class=headerlink href=#Neural-Discrete-Representation-Learning-1></a>Neural Discrete Representation Learning</h3><p><img alt=image-20241113204329321 data-src=https://s2.loli.net/2024/11/13/IoSaJwE4RzcLWjB.png><h3 id=Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2-1><a title="Generating Diverse High-Fidelity Images with VQ-VAE-2" class=headerlink href=#Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2-1></a>Generating Diverse High-Fidelity Images with VQ-VAE-2</h3><p><img alt=image-20241113204123086 data-src=https://s2.loli.net/2024/11/13/MfEVeAO418buc7R.png><p>底层和顶层分别被压缩为大小为64 × 64和32 × 32的quantized latent图,相当于使用了不同尺度、多尺度的codelayer/codebook,再将不同尺度的codelayer通过多个decoder. 令分辨率最大的为bottom layer,不断降低分辨率并设置与encoder-decoder对数目对应的codelayer,得到多个H*W*1的量化结果,将上一层得到的量化结果通过decoder解码后的结果并上采样(增加特征图大小)与当前层量化的结果concat通过当前的decoder得到结果. 最后结果取bottom layer也就是分辨率最高那一层的解码结果,相当于实现了pyramid的多尺度,更新codelayer权重有两种方式,一种是使用stop gradient,在代码层面上使用.detach(),另一种通过ema.<p><img alt=image-20241113235501796 data-src=https://s2.loli.net/2024/11/13/cpSPAv1gRCqd9iT.png><h3 id=SoundStream-An-End-to-End-Neural-Audio-Codec><a title="SoundStream: An End-to-End Neural Audio Codec" class=headerlink href=#SoundStream-An-End-to-End-Neural-Audio-Codec></a>SoundStream: An End-to-End Neural Audio Codec</h3><p><img alt=image-20241113235655932 data-src=https://s2.loli.net/2024/11/13/NVe1Qkb8cGsaCph.png><p><img alt=image-20241113235621126 data-src=https://s2.loli.net/2024/11/13/MowKEQIxsBk7Cq5.png><h3 id=Hierarchical-Residual-Learning-Based-Vector-Quantized-Variational-Autoencoder-for-Image-Reconstruction-and-Generation><a title="Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation" class=headerlink href=#Hierarchical-Residual-Learning-Based-Vector-Quantized-Variational-Autoencoder-for-Image-Reconstruction-and-Generation></a>Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation</h3><p><img alt=image-20241113201457737 data-src=https://s2.loli.net/2024/11/13/Zp76aA8Skv4joL2.png><h3 id=Addressing-Representation-Collapse-in-Vector-Quantized-Models-with-One-Linear-Layer><a title="Addressing Representation Collapse in Vector Quantized Models with One Linear Layer" class=headerlink href=#Addressing-Representation-Collapse-in-Vector-Quantized-Models-with-One-Linear-Layer></a>Addressing Representation Collapse in Vector Quantized Models with One Linear Layer</h3><p><img alt="Refer to caption" data-src=https://arxiv.org/html/2411.02038v1/x1.png><h2 id=Variant-Transformer><a title="Variant Transformer" class=headerlink href=#Variant-Transformer></a>Variant Transformer</h2><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\11\18\vqvae及其变体代码学习\ rel=bookmark>vqvae及其变体代码学习</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\11\03\文生图相关模型最新进展小结\ rel=bookmark>文生图相关模型最新进展小结</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\07\30\profile-a-deep-learning-model\ rel=bookmark>profile a deep learning model</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\18\从论文中看AI绘画-二\ rel=bookmark>从论文中看AI绘画(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\12\myJourneyToAI-深度学习之旅\ rel=bookmark>myJourneyToAI:深度学习之旅</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/09/24/%E5%9B%9E%E7%9C%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/ title=回看深度学习:经典网络学习>https://www.sekyoro.top/2024/09/24/回看深度学习-经典网络学习/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/deep-learning/ rel=tag><i class="fa fa-tag"></i> deep learning</a></div><div class=post-nav><div class=post-nav-item><a href=/2024/09/16/%E5%AE%8C%E6%95%B4%E7%9A%84C-%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/ rel=prev title=完整的C++项目构建注意事项> <i class="fa fa-chevron-left"></i> 完整的C++项目构建注意事项 </a></div><div class=post-nav-item><a href=/2024/09/26/%E7%8E%B0%E4%BB%A3cpp%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E5%B9%B6%E5%8F%91%E5%88%9D%E6%8E%A2/ rel=next title=现代cpp多线程与并发初探> 现代cpp多线程与并发初探 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#Vision-Transformer-and-its-variants><span class=nav-number>1.</span> <span class=nav-text>Vision Transformer and its variants</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE><span class=nav-number>1.1.</span> <span class=nav-text>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows><span class=nav-number>1.2.</span> <span class=nav-text>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Modernify-Conv><span class=nav-number>2.</span> <span class=nav-text>Modernify Conv</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#A-ConvNet-for-the-2020s><span class=nav-number>2.1.</span> <span class=nav-text>A ConvNet for the 2020s</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF><span class=nav-number>2.1.1.</span> <span class=nav-text>训练技术</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Marco-Design><span class=nav-number>2.1.2.</span> <span class=nav-text>Marco Design</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E4%BF%AE%E6%94%B9%E9%98%B6%E6%AE%B5%E8%AE%A1%E7%AE%97%E6%AF%94%E4%BE%8B><span class=nav-number>2.1.2.1.</span> <span class=nav-text>修改阶段计算比例</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BF%AE%E6%94%B9stem%E4%B8%BApatchify><span class=nav-number>2.1.3.</span> <span class=nav-text>修改stem为patchify</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#ResNext-ify><span class=nav-number>2.1.4.</span> <span class=nav-text>ResNext-ify</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Inverted-Bottleneck><span class=nav-number>2.1.5.</span> <span class=nav-text>Inverted Bottleneck</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%9B%B4%E5%A4%A7%E7%9A%84%E5%B0%BA%E5%AF%B8%E5%A4%A7%E5%B0%8F><span class=nav-number>2.1.6.</span> <span class=nav-text>更大的尺寸大小</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Micro-Design><span class=nav-number>2.1.7.</span> <span class=nav-text>Micro Design</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Early-Convolutions-Help-Transformers-See-Better><span class=nav-number>2.2.</span> <span class=nav-text>Early Convolutions Help Transformers See Better</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Vector-quantization-and-Codebook><span class=nav-number>3.</span> <span class=nav-text>Vector quantization and Codebook</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Neural-Discrete-Representation-Learning><span class=nav-number>3.1.</span> <span class=nav-text>Neural Discrete Representation Learning</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%A6%BB%E6%95%A3%E6%BD%9C%E5%8F%98%E9%87%8F-%E4%B8%AD%E9%97%B4%E5%B5%8C%E5%85%A5><span class=nav-number>3.1.1.</span> <span class=nav-text>离散潜变量(中间嵌入)</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2><span class=nav-number>3.2.</span> <span class=nav-text>Generating Diverse High-Fidelity Images with VQ-VAE-2</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Taming-Transformers-for-High-Resolution-Image-Synthesis><span class=nav-number>3.3.</span> <span class=nav-text>Taming Transformers for High-Resolution Image Synthesis</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%AD%A6%E4%B9%A0%E9%AB%98%E6%95%88codebook><span class=nav-number>3.3.1.</span> <span class=nav-text>学习高效codebook</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90%E8%A7%86%E8%A7%89%E4%B8%B0%E5%AF%8C%E7%9A%84codebook><span class=nav-number>3.3.2.</span> <span class=nav-text>生成视觉丰富的codebook</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BD%BF%E7%94%A8transformers%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90><span class=nav-number>3.3.3.</span> <span class=nav-text>使用transformers学习图像生成</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%BA%A6%E6%9D%9F%E7%9A%84%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90><span class=nav-number>3.3.4.</span> <span class=nav-text>约束的图像生成</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%9B%BE%E7%89%87><span class=nav-number>3.3.5.</span> <span class=nav-text>生成高分辨率图片</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#UDA><span class=nav-number>4.</span> <span class=nav-text>UDA</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Unsupervised-Domain-Adaptation-by-Backpropagation><span class=nav-number>4.1.</span> <span class=nav-text>Unsupervised Domain Adaptation by Backpropagation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Domain-Adaptive-Object-Detection-for-Autonomous-Driving-under-Foggy-Weather><span class=nav-number>4.2.</span> <span class=nav-text>Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Image-level-adaptation><span class=nav-number>4.2.1.</span> <span class=nav-text>Image-level adaptation</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Object-level-Adaptation><span class=nav-number>4.2.2.</span> <span class=nav-text>Object-level Adaptation</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Multi-adversarial-Faster-RCNN-for-Unrestricted-Object-Detection><span class=nav-number>4.3.</span> <span class=nav-text>Multi-adversarial Faster-RCNN for Unrestricted Object Detection</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#CDTrans-Cross-domain-Transformer-for-Unsupervised-Domain-Adaptation><span class=nav-number>4.4.</span> <span class=nav-text>CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#TVT-Transferable-Vision-Transformer-for-Unsupervised-Domain-Adaptation><span class=nav-number>4.5.</span> <span class=nav-text>TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Safe-Self-Refinement-for-Transformer-based-Domain-Adaptation><span class=nav-number>4.6.</span> <span class=nav-text>Safe Self-Refinement for Transformer-based Domain Adaptation</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#VQ-VAE><span class=nav-number>5.</span> <span class=nav-text>VQ-VAE</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Neural-Discrete-Representation-Learning-1><span class=nav-number>5.1.</span> <span class=nav-text>Neural Discrete Representation Learning</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2-1><span class=nav-number>5.2.</span> <span class=nav-text>Generating Diverse High-Fidelity Images with VQ-VAE-2</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#SoundStream-An-End-to-End-Neural-Audio-Codec><span class=nav-number>5.3.</span> <span class=nav-text>SoundStream: An End-to-End Neural Audio Codec</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Hierarchical-Residual-Learning-Based-Vector-Quantized-Variational-Autoencoder-for-Image-Reconstruction-and-Generation><span class=nav-number>5.4.</span> <span class=nav-text>Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Addressing-Representation-Collapse-in-Vector-Quantized-Models-with-One-Linear-Layer><span class=nav-number>5.5.</span> <span class=nav-text>Addressing Representation Collapse in Vector Quantized Models with One Linear Layer</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Variant-Transformer><span class=nav-number>6.</span> <span class=nav-text>Variant Transformer</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>236</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/deep-learning/ rel=tag>deep learning</a><span class=tag-list-count>11</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.4m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>37:01</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>