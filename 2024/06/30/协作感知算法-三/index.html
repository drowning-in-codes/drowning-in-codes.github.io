<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=一些稍微新一点或者之前没看到的想法还不错的协同感知论文 name=description><meta content=article property=og:type><meta content=协作感知算法:三 property=og:title><meta content=https://www.sekyoro.top/2024/06/30/%E5%8D%8F%E4%BD%9C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%89/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=一些稍微新一点或者之前没看到的想法还不错的协同感知论文 property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/09/03/TPz1njuNZGi7Qbc.png property=og:image><meta content=https://s2.loli.net/2024/09/03/2oyQWEVrpMiRLhA.png property=og:image><meta content=https://s2.loli.net/2024/09/05/NxJjAighnW1tEVX.png property=og:image><meta content=https://s2.loli.net/2024/09/05/5dxPGSYCcNgRBbn.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909091549936.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909094342791.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909105358537.png property=og:image><meta content=https://s2.loli.net/2024/09/12/Q2bxeyYMCcFtz5K.png property=og:image><meta content=2024-06-30T05:46:08.000Z property=article:published_time><meta content=2024-09-12T13:39:58.742Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="collaborative perception" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/09/03/TPz1njuNZGi7Qbc.png name=twitter:image><link href=https://www.sekyoro.top/2024/06/30/%E5%8D%8F%E4%BD%9C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%89/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>协作感知算法:三 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/06/30/%E5%8D%8F%E4%BD%9C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%89/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>协作感知算法:三</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-06-30 13:46:08" datetime=2024-06-30T13:46:08+08:00>2024-06-30</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-09-12 21:39:58" datetime=2024-09-12T21:39:58+08:00 itemprop=dateModified>2024-09-12</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>11k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>10 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>一些稍微新一点或者之前没看到的想法还不错的协同感知论文<br><span id=more></span><h2 id=More-Robust><a title="More Robust" class=headerlink href=#More-Robust></a>More Robust</h2><p>提升检测精度,尤其是在位置噪声较大的情况下<h3 id=Self-Localized-Collaborative-Perception><a title="Self-Localized Collaborative Perception" class=headerlink href=#Self-Localized-Collaborative-Perception></a>Self-Localized Collaborative Perception</h3><h4 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h4><p>协作感知因其能够解决单智能体感知中的几个固有挑战（包括遮挡和超出范围问题）而受到广泛关注。然而，现有的<strong>协作感知系统严重依赖精确的定位系统来在智能体之间建立一致的空间坐标系</strong>。这种依赖使它们容易受到大型姿势错误或恶意攻击的影响，从而导致感知性能大幅降低。<p>为了解决这个问题，提出了 CoBEVGlue，这是一种新颖的自定位协作感知系统，无需使用外部定位系统即可实现更全面、更稳健的协作。CoBEVGlue 的核心是一个新颖的空间对齐模块，它通过有效匹配跨智能体的共可见对象来提供智能体之间的相对姿态。我们在真实数据集和模拟数据集上验证了我们的方法。<p>结果表明，i） CoBEVGlue 在<strong>任意定位噪声和攻击下实现了最先进的检测性能</strong>;ii） <strong>空间对齐模块可以与大多数以前的方法无缝集成</strong>，将它们的性能平均提高 57.7%<h4 id=介绍><a class=headerlink href=#介绍 title=介绍></a>介绍</h4><p>准确的感知对于自动驾驶汽车的导航和安全至关重要 。尽管大规模数据集 和强大的模型 推动了进步，但单智能体感知本身受到遮挡和远程问题 的限制，这可能导致灾难性的后果。利用现代通信技术，目前对协作感知的研究使多个智能体之间能够共享感知信息，从根本上提高了感知性能。<p>在高质量数据集和创新协作技术的推动下，协作感知系统有可能显著提高交通网络的安全性<p>在这个新兴的协作感知领域，大多数主流工作都做出了一个过于简化的假设：<strong>每个代理使用的全局定位系统（通常是 GPS 或 SLAM）足够精确，可以建立一个一致的协作空间坐标系</strong><p>然而，来自真实世界协作感知数据集V2V4Real和DAIR-V2X 的快照显示，即使经过细致和资源密集型的离线校准，地面实况定位仍然存在噪声。在<strong>计算限制和实时约束下，这些不准确之处在实际应用中可能会更加严重</strong>。<p>此外，定位系统容易受到长期存在但仍未解决的攻击。这些攻击允许攻击者随意操纵位置，进一步破坏定位系统的可靠性。这种显著噪声和恶意攻击的普遍挑战与早期工作所考虑的理想场景形成鲜明对比，这些工作主要关注轻微的姿态不准确，未能超越大噪声下无协作的基线.<p>为了消除对可能不可靠的外部定位系统的依赖，<strong>一种直接的解决方案是通过点云配准推断协作智能体的相对姿态，这种技术在多智能体协作系统中得到广泛应用。点云配准方法应用最近邻算法来识别广泛的3D点集之间的对应关系，然后是稳健的技术来计算这些假定对应关系的转换</strong>。尽管这些方法被证明对协作映射等延迟容忍应用有效，但对于带宽受限的协作感知系统来说，大量3D数据的实时传输是不切实际的。因此，在创建一个没有定位错误的系统，同时保持实际应用的通信效率方面，存在明显的差距。<p>为了填补这一空白，提出了 CoBEVGlue，这是一种自定位的协作感知系统，专为多个智能体设计，无需依赖外部定位系统即可实现更全面的感知，从而在降低通信成本的同时实现效率。CoBEVGlue 遵循以前的协作感知系统 的管道，并使用其关键的空间对齐模块 BEVGlue 来估计智能体与每个智能体检测和跟踪的物体之间的相对姿态。<p>BEVGlue <strong>背后的核心思想是从跨代理的鸟瞰感知数据中搜索共见对象，并计算与这些共见对象的相对变换，确保一致的协作空间坐标系</strong>。(The core idea behind BEVGlue is to search for the co-visible objects from the bird’s eye view perceptual data across agents and calculate the relative transformation with these co-visible objects, ensuring a consistent spatial coordinate system for collaboration.)确保一致的空间坐标系以进行协作。BEVGlue 包括三个关键组件：i） 对象图建模(object graph modeling)，将<strong>每个智能体的观察结果转换为具有丰富信息的对象图，包括对象形状、航向、跟踪 ID 和对象之间不变的空间关系</strong>;ii） 时间一致的最大子图检测，它有效地利用对象图中的空间和时间数据来检测最大的公共子图，遵循严格的空间同构约束和时间一致性;iii） 相对姿态计算，它<strong>使用检测到的公共子图计算代理之间的姿态关系</strong>，而无需使用耗时的异常值拒绝算法。<p>拟议的 CoBEVGlue 系统具有三个显着优势：<p>i） 它独立于外部定位设备运行，展示了其对噪音和恶意攻击的弹性<p>ii） 它带来的通信开销很小，因为 CoBEVGlue 仅使用带有跟踪 ID 的对象边界框来估计代理之间的相对姿势<p>;iii） 其核心模块 BEVGlue 通过在<strong>检测到的公共子图之间</strong>保持严格的空间同构约束和匹配结果之间随时间的时间一致性来确保高质量的匹配结果。<p>为了评估所提出的方法的有效性，考虑了三个数据集上的协作式3D目标检测任务：OPV2V、DAIR-V2X和V2V4Real，涵盖模拟和真实世界场景。结果表明，CoBEVGlue 赋予了强大的协作感知系统的性能与依赖精确定位信息的系统相当，并在存在定位噪声和攻击时实现了最先进的检测性能<p>为了获得对定位噪声的抵抗力，以前的工作考虑了两种主要方法：<strong>基于学习和基于匹配</strong>。基于学习的方法旨在构建健壮的网络架构，以减少姿态错误的影响。例如，V2VNet（稳健性） 设计了姿态回归、全局一致性和注意力聚合模块来纠正相对姿态并专注于姿态误差较小的邻居;V2X-ViT 使用多尺度窗口注意力来捕获各种范围内的特征。另一方面，基于匹配的方法<strong>寻求开发健壮的框架或网络架构。示例包括 FPV-RCNN和 CoAlign，它们使用基于 IoU 的匹配策略估计代理之间的相对姿势</strong>。但是，它们只能纠正外部定位中的微小不准确之处，因为这些方法依赖于基本精确的初始相对姿势。当噪声较大或存在攻击时，它们的性能会显著下降。<strong>相比之下，我们的工作认为协作感知独立于外部定位系统</strong><p>尽管本文的最终目标是提高检测能力，但<strong>点云配准方法的进步激发了我们提出新颖的自定位协作感知系统</strong>。传统的点云配准方法专注于改进迭代最近点 （ICP） 算法及其变体 导致了收敛和噪声弹性的改进。最近典型的点云配准工作流程包括提取<strong>本地 3D 特征描述符和进行配准。为了提取 3D 局部描述符，快速点特征直方图等传统方法利用了手工制作的特征</strong>。<p>最近的技术为此目的采用了基于学习的方法。在配准方面，<strong>传统方法通常采用最近邻算法进行匹配，并采用稳健优化来剔除异常值</strong>，而<strong>现代深度配准方法则利用自注意机制来确定对应关系</strong>。SGAligner率先使用预构建的 3D 场景图进行配准。然而，与前面的策略类似，它需要传输密集的点云和高维特征。这些方法广泛应用于容忍延迟的多智能体系统，如协同映射和 3D 场景图生成.但是，协作对象检测任务需要实时进行精确的相对姿态估计。遗憾的是，<strong>V2X 网络难以实时传输点云配准方法所需的密集点云和特征。为了克服这一限制</strong>，我们的方法优先考虑对象级注册，仅用 8 个 float 数字表示每个对象(To overcome this limitation, our approach prioritizes object-level registration, representing each object with just eight float numbers.)。这项创新显著降低了计算协作自动驾驶汽车之间相对姿态所需的带宽和计算成本，从而有效地解决了传输困境(This innovation markedly reduces the bandwidth necessary and computation cost for calculating relative poses among collaborative autonomous vehicles, thus efficiently resolving the transmission dilemma)<p>最大公共子图（Maximum Common Subgraph Detection,MCS）检测问题被归类为NPhard，在各个科学领域中都至关重要，需要平衡精度和计算效率的算法。传统方法主要采用分支定界算法和将 MCS 检测转化为最大团问题的技术. 机器学习的最新进展已经看到了图神经网络和强化学习在 MCS 检测中的应用，MCS 检测试图学习合适的启发式方法来进行图匹配。尽管他们进行了创新，但它们仍然受到搜索空间探索的启发式性质的限制，并且在最坏的情况下会受到指数级时间复杂度的影响。在这项工作中，我们<strong>使用几何不变对象姿态图对每个代理检测到的边界框进行建模，并利用空间约束和时间一致性来有效地解决问题。</strong><p><img alt=image-20240903155532374 data-src=https://s2.loli.net/2024/09/03/TPz1njuNZGi7Qbc.png></p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{F}_i^t,\mathbf{D}_i^t& =f_{\text{detection}\&\text{tracking}}\left(\mathbf{O}_i^t\right), \\
\xi_{j\to i}^t& =f_{\mathrm{BEVGlue}}\left(\mathrm{D}_i^t,\mathrm{D}_j^t\right), \\
\mathbf{F}_{j\to i}^t& =f_{\text{transform}}\left(\mathbf{F}_j^t,\xi_{j\to i}^t\right), \\
\mathbf{F}_{i}^{\prime}t& =f_{\mathrm{fusion}}\left(\{\mathbf{F}_{j\to i}^t\}_{j=1,2,\cdots,N}\right), \\
\mathbf{B}_i^t& =f_{\mathrm{decoder}}\left(\mathbf{F}_i^{'t}\right), 
\end{aligned}</script><p>精确的姿态信息<strong>要求每个代理利用外部定位系统来获取其全局位置并计算协作者之间的相对变换</strong>。这种对外部定位的依赖充满了挑战，包括容易受到噪声干扰和恶意攻击造成的潜在安全漏洞。<strong>BEVGlue 旨在通过利用感知数据来确保准确的相对姿态估计来解决这些问题</strong>，从而增强协作感知的弹性和有效性。<p>为了估计智能体之间的相对姿态 ξt j→i，BEVGlue 的主要思想是识别共可见的物体，然后根据这些共见物体计算变换。为了挖掘智能体之间的这种内部对应关系，BEVGlue 提出了三个模块：（i） 对象图建模，（ii） 时间一致的最大公共子图检测，以及 （iii） 相对姿态计算。<p><strong>Object Graph Modeling</strong><p>鉴于极点和参考方向在物理世界中具有清晰和单一的定义，因此可以实现不同对象图之间边缘特征计算的一致性。具体来说，如果在基于智能体 j 计算智能体图 Gt j 上的边缘特征 e^t^<em>j,mn</em> 时，节点 m 和 n 的检测结果对于第 i 个和第 j 个智能体都是准确的，则它与 e^t^<em>i,mn</em> 相同。<p>对象图提供了一种创新方法来对每个代理的观察进行建模：i） 节点属性包含时间跟踪数据，这有助于保持随时间推移的匹配一致性;ii） 边缘特征在从不同代理的角度得出的对象图中是一致的，这意味着应用于 D^t^<em>i</em> 的旋转和平移不会改变 e^t^ <em>i,mn</em> 的值。这意味着，当两个对象同时被不同的代理观察时，无论视角如何变化，边缘属性都保持一致。<p>对象图提供了一种创新方法来对每个代理的观察进行建模：i） <strong>节点属性包含时间跟踪数据，这有助于保持随时间推移的匹配一致性</strong>;ii） <strong>边缘特征在从不同代理的角度得出的对象图中是一致的</strong>，这意味着应用于 D^t^<em>i</em> 的旋转和平移不会改变 e^t^<em>i,mn</em> 的值。这意味着，当两个对象同时被不同的代理观察时，无论视角如何变化，边缘属性都保持一致<blockquote><p>每个代理根据自己的检测结构建模一个对象图,图中有一些节点和边.<p>每个节点属性包括bbox的宽高和id,边节点属性包括相对距离,相对航向角和航向角相对</blockquote><p><img alt=image-20240903170504372 data-src=https://s2.loli.net/2024/09/03/2oyQWEVrpMiRLhA.png></p><script type="math/tex; mode=display">
\mathcal{H}_{(i,j)}^t=f_{\mathrm{MCS}}\left(\mathcal{G}_i^t,\mathcal{G}_j^t,\mathcal{H}_{(i,j)}^{t-1}\right).</script><p>上面的MCS函数可以分为三个过程.具体我就不说了,这篇文章一般,在一些介绍上还有点含糊不清.<p><img alt=image-20240905150424806 data-src=https://s2.loli.net/2024/09/05/NxJjAighnW1tEVX.png><h3 id=Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network><a title="Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network" class=headerlink href=#Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network></a>Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network</h3><h2 id=More-domain-invariant><a title="More domain-invariant" class=headerlink href=#More-domain-invariant></a>More domain-invariant</h2><p>提升迁移性,在仿真数据集上训练能在真实数据集上保证良好的效果.<h3 id=V2X-DGW-Domain-Generalization-for-Multi-agent-Perception-under-Adverse-Weather-Conditions><a title="V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions" class=headerlink href=#V2X-DGW-Domain-Generalization-for-Multi-agent-Perception-under-Adverse-Weather-Conditions></a>V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions</h3><h2 id=More-Communication-efficient><a title="More Communication-efficient" class=headerlink href=#More-Communication-efficient></a>More Communication-efficient</h2><p>减少传输的数据大小,使得通信更高效.<h3 id=ERMVP-Communication-Efficient-and-Collaboration-Robust-Multi-Vehicle-Perception-in-Challenging-Environments><a title="ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments" class=headerlink href=#ERMVP-Communication-Efficient-and-Collaboration-Robust-Multi-Vehicle-Perception-in-Challenging-Environments></a>ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments</h3><p>​ 协作感知通过使自动驾驶汽车能够交换互补信息来提高感知性能。尽管它有可能彻底改变移动行业，但各种环境中的挑战，如<strong>通信带宽限制</strong>、定位错误和<strong>信息聚合效率低下</strong>，阻碍了它在实际应用中的实施。在这项工作中，我们提出了 ERMVP，这是一种在具有挑战性的环境中进行通信高效和协作的鲁棒多车辆感知方法。具体来说，ERMVP 具有三个明显的优势i） 它利用分层特征采样策略来抽象一组具有代表性的特征向量，使用更少的通信开销实现高效通信;ii） 它采用稀疏一致性特征来执行精确的空间位置校准，有效减轻车辆定位错误的影响;iii） 引入了一种开创性的特征融合和交互范式，以在不同车辆和数据源之间集成整体空间语义。<p>自动驾驶汽车被广泛认为是提高道路安全和交通效率的重要手段。这些车辆配备了激光雷达、摄像头和其他传感器，能够准确感知周围环境，以确保安全可靠的运行。<strong>然而，单车感知系统不可避免地存在缺点 ，例如传感器视野有限，容易被遮挡，以及由于数据稀疏和低分辨率而难以检测远处物体。</strong><p>最近，车对车 （V2V） 通信技术和深度学习的进步刺激了协作感知技术的创新和进步。这项技术允许互联自动驾驶汽车 （CAV） 共享传感数据，从而实现更全面的环境感知.<p>​ 尽管协作感知技术在移动出行行业转型方面显示出巨大潜力，但其实际应用面临一些挑战，包括通信带宽限制、<strong>定位错误和信息聚合效率低下</strong><p>在实际场景中，无线通信资源和可靠性的约束严重阻碍了延迟敏感协作感知的有效性。虽然最近的工作通过精心设计的机制实现了感知性能和通信带宽之间的平衡，但这些方法有其局限性，因为它们主要考虑信息压缩而不是空间冗余。这种狭窄的关注点会加剧高压缩比下的性能下降.<p>​ 此外,复杂的动态环境会导致定位错误，从而导致相对变换估计不准确和空间特征错位。这种相对姿势噪声会产生误导性特征，从而对协作感知的有效性产生不利影响。现有的方法试图通过密集的计算来优化整体姿态，但高延迟使其不适合实时动态感知.同时,协作方法只关注聚合信息，而忽视了自我载体固有的感知优势.此范例容易受到协作噪声引入的扰动的影响,包括异步运动模糊和不准确的投影.这样的缺点成为实现最佳感知性能的瓶颈.<p>相比之下，<strong>以自我为中心的特征可能包含不受协作噪声影响的局部准确空间位置信息</strong>。因此，建立务实的协作感知系统的首要任务是有效克服上述挑战.<p><img alt=image-20240905153336761 data-src=https://s2.loli.net/2024/09/05/5dxPGSYCcNgRBbn.png><p>基于这些观察提出了 ERMVP，这是一种在具有挑战性的环境中通信高效且协作稳健的多车辆感知方法。具体来说,（i） 首先设计了一种高级滤波器和合并特征采样策略来解决无线通信资源的局限性。此策略同时考虑类间和类内冗余关系，以从冗余特征中抽象出一组精炼的特征向量，从而使用更少的通信开销实现高效通信.(ii)其次，我们引入了一个即插即用功能空间校准模块，以减轻车辆定位错误的影响。该模块巧妙地利用共识稀疏前景特征来对齐自我车辆和合作者之间的相对姿态关系，而无需任何精确的姿态监督。(iii)此外提出了一种开创性的特征融合和交互范式,以整合整体空间语义。<p>该范式包括两个关键组件:第一个是基于注意力的特征融合模块，在本地和全局注意力之间交替,以融合来自不同车辆的异构信息.<p>第二种是准确性增强特征交互策略,它利用以自我为中心的特征中固有的准确位置信息来增强融合特征提供的丰富语义信息.<p>• 我们提出了 ERMVP，这是一种<strong>通信高效</strong>且<strong>协作稳健</strong>的多车辆感知方法，它<strong>解决了通信带宽限制、定位错误和信息聚合效率挑战</strong>。<p>• 我们开发了一个过滤和合并特征采样策略来提高通信效率，一个用<strong>于精确空间特征对齐的特征空间校准模块，以及两个信息聚合组件来优化融合过程。</strong><p>• 我们对真实世界和模拟数据集进行了广泛的实验。结果表明了我们方法的优越性和所提出组件的必要性。<h5 id=Filter-and-Merge-Feature-Sampling><a title="Filter and Merge Feature Sampling" class=headerlink href=#Filter-and-Merge-Feature-Sampling></a>Filter and Merge Feature Sampling</h5><p>以前的工作利用了精心设计的机制，如信息熵通信选择 [37] 和空间异质性映射来减少所需的传输带宽。<p>然而，这些方法主要关注前台和后台特征之间的类间冗余，而忽略了特征之间的类内冗余，从而导致次优压缩。为了解决这一差距，我们引入了一种高级过滤和合并特征采样策略 （FMS）。该策略同时考虑了类间和类内冗余关系，有效地从原始特征图中提取了一组简洁而独特的特征向量，从而更有效地减少了通信开销。FMS 由以下两个核心组件组成。<p>Filter Sampler(滤波器采样器).在对象检测中，包含对象的前景区域比背景区域更重要。因此，我们将减少空间冗余的想法实现到特征过滤器采样器模块中，旨在保留感知上重要但稀疏的特征向量集。由于显式学习二进制采样器是不可行的，因此我们开发了一种置信度过滤器策略.最初,为特征图生成检测置信度图。它反映了不同空间区域的感知重要性，较高的级别表示潜在的对象区域，较低的级别通常表示冗余的背景区域.</p><script type="math/tex; mode=display">
C_i=\Phi_{\mathrm{con_gen}}\left(F_i\right)\in[0,1]^{H\times W}</script><p>其中 Φcon_gen（·） 表示具有检测解码器结构的置信度生成网络。然后对置信度图进行阈值处理，然后进行非极大值抑制，从而产生二进制掩码 B。</p><script type="math/tex; mode=display">
\tilde{F}_{i}=B\odot F_{i}</script><p>Merge Sampler(合并采样器):在用滤波器采样器提取详细的前景特征向量集后，我们<strong>使用合并采样器进行额外的优化，通过加权合并来提炼相似或重复的前景特征向量</strong>。该过程分为三个阶段：信息驱动的<strong>特征分组</strong>、注意力启发的<strong>特征合并</strong>和基于索引的<strong>特征重建</strong>(information-driven feature grouping, attention-inspired feature merging, and index-based feature reconstruction.)<ol><li><p>应用最近邻聚类算法的变体对前景特征向量集进行分组.给定一组特征向量 $F^{~}_{i}$ =[x1，x2,…,xL] 和集群中心 $X_c$，我们计算每个特征向量的指标 δi。</p> <p>δi 的计算方法是最小特征距离减去到任何其他聚类中心向量的平均像素距离</p></ol><script type="math/tex; mode=display">
\delta_i=\min_{j:x_j\in X_c}\left(\left\|x_i-x_j\right\|_2^2-\gamma\left\|p(x_i),p(x_j)\right\|_2^2\right)</script><ol><li>合并特征向量的一种简单策略是平均集群中的每个特征向量。但是，此方案可能会受到异常值特征向量的严重影响。从注意力机制中汲取灵感，利<strong>用置信度分数作为指导来量化每个特征的重要性</strong>。因此，第 i 个簇 Gi 的合并特征向量 ̃ 习 计算为</ol><script type="math/tex; mode=display">
\widetilde{x}_i=\frac{\sum_{j\in G_i}c_jx_j}{\sum_{j\in G_i}c_j}</script><ol><li>在特征分组和合并过程中，每个特征向量都分配给一个集群，每个集群由一个合并的向量表示。<strong>维护原始特征向量和合并特征向量之间的索引对应关系的记录</strong>。利用这个索引记录，自我车辆确保合并的特征向量被映射到它们的相应位置，从而重建特征图</ol><p>定位错误可能导致车辆之间的特征图错位。这种错位会导致自我车辆误解物体的位置，从而导致感知能力下降，为了应对这一挑战，引入了特征空间校准模块 （FSC） 来促进精确的特征对齐<p><img alt=image-20240909091549936 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909091549936.png><p>涉及三个阶段：一致性匹配、几何验证和误差调制。<p>​ 将自我车辆的拟议匹配区域表示为 P，将协作车辆在嘈杂姿势条件下识别的区域表示为 Q。利用 P 和 Q，构建了一个加权二分图，其中每条边的权重由节点之间的距离决定，封装在成本矩阵中。然后将匹配过程转换为线性分配任务，目的是确定具有最低累积边缘权重的匹配结果。此过程将生成匹配的对 M<p>​ 由于对象位于专用区域和检测噪声，可能会出现无效匹配。为了解决这个问题，我们利用 RANSAC 来过滤和筛选一组与预期几何变换一致的匹配项。首先，选择一个随机匹配子集Ms ，然后利用奇异值分解计算变换矩阵Γ s .当Γ s应用于Ms内的所有对时，如果这些对之间的变换后距离保持在阈值η以下，则认为该集合是正确对齐的。阈值η反映了原始协作框架内允许的定位误差。这个过程是迭代进行的，以确定与最大正确匹配数相关的最佳变换矩阵。最终得到一个最优的精化变换矩阵Γ r，并将其应用于后续的空间标定操作中，得到对齐后的特征’ Zj = Γ rZj<p>​ 为了增强校准方法在各种环境下的适应性，我们融入了<strong>误差调制策略。该策略旨在实现定位误差与标定过程中产生的估计误差之间的平衡</strong>。它测量了自我和协作特征在调整状态和原始状态下的重叠比例。<h4 id=Attention-based-Feature-Fusion><a title="Attention-based Feature Fusion" class=headerlink href=#Attention-based-Feature-Fusion></a>Attention-based Feature Fusion</h4><p>在多车辆协作场景中，车辆能够捕获来自不同空间区域的异构信息。为了高效地融合来自多个车辆的感知特征，我们提出了一种注意力特征融合方法( AFF )。AFF利用交替的局部和全局注意力，在遮挡变化的交通场景中实现位置级别的精确匹配，并捕获道路拓扑和交通状态的全局语义注意力<p><img alt=image-20240909094342791 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909094342791.png><p>该模块允许系统从局部视图分析空间相关性,同时也捕获全局特征响应,确保在动态、复杂和遮挡变化的交通场景中实现高效和精确的感知.最后得到融合特征H~i~<h4 id=Accuracy-Enhanced-Feature-Interaction><a title="Accuracy Enhanced Feature Interaction" class=headerlink href=#Accuracy-Enhanced-Feature-Interaction></a>Accuracy Enhanced Feature Interaction</h4><p>​ 先前的工作已经证明了融合特征可以提供更丰富的语义信息，从而提高感知性能。然而，它们可能会受到协作噪声的影响，如异步运动模糊和不准确的投影,这会损害准确的位置信息,成为感知性能最优实现的瓶颈.以自我为中心的特征可能包含局部关键的空间位置信息,而不受协作噪声的影响.<p>​ 为此,我们提出了一种准确性增强的特征交互( Accuracy Enhanced Feature Interaction，AEI )策略,该策略利用自我中心特征固有的准确位置信息来增强协同融合特征提供的丰富语义信息.<p><img alt=image-20240909105358537 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240909105358537.png><h3 id=Pragmatic-Communication-in-Multi-Agent-Collaborative-Perception><a title="Pragmatic Communication in Multi-Agent Collaborative Perception" class=headerlink href=#Pragmatic-Communication-in-Multi-Agent-Collaborative-Perception></a>Pragmatic Communication in Multi-Agent Collaborative Perception</h3><p>​ 多智能体协同感知的目标是使智能体通过通信交换互补的感知信息，从而实现更全面的感知。协同感知保证了扩展的可视性，通过障碍物和识别小的、远距离的目标，从而实现对环境的彻底理解。它为从根本上克服单智能体感知的物理局限性提供了一个很有前途的方向，如视场受限、遮挡、远距离等问题。作为自主系统的最前沿，协作感知可以增强感知能力，并在各种现实世界的应用中进一步提高系统的功能和安全性，包括自动驾驶机器人技术和无人驾驶.<p>​ 为了应对这一挑战，关键在于在通信预算范围内优化消息以满足每个智能体的特定感知任务需求。一种直接的方法是在传统的香农通信范式中使用信源编码。该方法将原始数据编码为一系列代码，将较短的代码分配给频繁数据，将较长的代码分配给稀有数据，在不损失信息的情况下创建紧凑的表示。在协同感知的背景下，该方法在支持者端有效地将感知数据压缩为消息，并在接收者端确保无损复制.<p>​ 这种方法提高了通信效率，同时保留了对包括感知在内的一般下游任务的效用。然而，这种Shannon范式在需要为特定下游任务定制通信的场景中具有根本的局限性，因为它不可避免地浪费了无关的资源数据。例如，在相机-传感器-智能体协作的车辆检测任务中，香农范式对每个像素进行统一编码，而不区分非必要的背景和关键的车辆像素。这些背景像素在无助于检测性能的情况下极大地浪费了通信资源，从而影响了感知-通信的权衡。<h3 id=What-Makes-Good-Collaborative-Views-Contrastive-Mutual-Information-Maximization-for-Multi-Agent-Perception><a title="What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception" class=headerlink href=#What-Makes-Good-Collaborative-Views-Contrastive-Mutual-Information-Maximization-for-Multi-Agent-Perception></a>What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception</h3><h2 id=Towards-Label-Efficient><a title="Towards Label Efficient" class=headerlink href=#Towards-Label-Efficient></a>Towards Label Efficient</h2><h3 id=COˆ3-Cooperative-Unsupervised-3D-Representation-Learning-for-Autonomous-Driving><a title="COˆ3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving" class=headerlink href=#COˆ3-Cooperative-Unsupervised-3D-Representation-Learning-for-Autonomous-Driving></a>COˆ3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</h3><h4 id=摘要-1><a class=headerlink href=#摘要-1 title=摘要></a>摘要</h4><p>针对室内场景点云的无监督对比学习已经取得了巨大的成功。然而，室外场景点云的无监督表示学习仍然具有挑战性，因为以前的方法需要重建整个场景并捕获对比目标的部分视图。这在有运动物体、障碍物和传感器的室外场景中是不可行的。在本文中，我们提出了CO ( 3，即协同对比学习和上下文形状预测，以无监督的方式学习室外场景点云的三维表示。与现有方法相比，CO3有几个优点。( 1 )利用车载侧和基础设施侧的LiDAR点云<strong>构建足够差异但同时保持共同语义信息的视图进行对比学习，比以往方法构建的视图更合适</strong>。( 2 )在对比目标的基础上，<strong>提出了上下文形状预测作为预训练目标，为无监督的三维点云表示学习带来了更多与任务相关的信息，有利于将学习到的表示迁移到下游的检测任务中</strong>。( 3 )与以往的方法相比，CO ( 3 )学习到的表示可以迁移到不同类型的LiDAR传感器采集的室外场景数据集上。( 4 ) CO ( 3在Once和KITTI d上都改进了当前最先进的方法<h4 id=介绍-1><a class=headerlink href=#介绍-1 title=介绍></a>介绍</h4><p>​ 激光雷达作为室外环境中最可靠的传感器，能够精确地测量物体的三维位置，机器人和计算机视觉领域都对激光雷达点云的感知任务表现出强烈的兴趣，包括三维物体检测、分割和跟踪，这些任务对于自动驾驶系统至关重要。迄今为止，在详细的标注数据上从头开始随机初始化和直接训练仍然占据着该领域的主导地位。与此相反，最近图像领域的研究工作侧重于从图像构建具有不同视角对比目标的无监督表示学习<p>他们以无监督的方式使用ImageNet 等大规模数据集对2D骨干网络进行预训练，并使用预训练的骨干网络在不同的数据集上初始化下游神经网络，在2D目标检测中实现了从头开始训练的显著性能提升。受这些成功的启发，结合自动驾驶车辆中丰富的未 标记数据探索了室外场景点云的无监督表示学习，以提高三维目标检测任务的性能。在过去的十年中，从无标签数据中学习三维表示在单目标和室内场景点云中取得了巨大的成功。对于单个物体的点云，如CAD模型，先前的工作通过最小化对比损失来预训练3D编码器来预测全局表示，并针对包括物体分类和配准在内的低级下游任务。为了将这一思想扩展到室内场景点云的高层感知任务，Point Contrast 提出重建整个室内场景，从两个不同的姿态采集部分点云，并将其作为对比学习中的两个视图来学习稠密的(点级或三维像素水平)表示<p><img alt=image-20240912213954703 data-src=https://s2.loli.net/2024/09/12/Q2bxeyYMCcFtz5K.png><h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving/tree/main?tab=readme-ov-file rel=noopener target=_blank>CatOneTwo/Collaborative-Perception-in-Autonomous-Driving: (2023 ITSM) Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges (github.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\23\协同感知数据集介绍\ rel=bookmark>协同感知数据集和代码库介绍</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\17\协同感知算法-二\ rel=bookmark>协同感知学习(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\11\30\协同感知算法-一\ rel=bookmark>协同感知学习(一)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/06/30/%E5%8D%8F%E4%BD%9C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%89/ title=协作感知算法:三>https://www.sekyoro.top/2024/06/30/协作感知算法-三/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag><i class="fa fa-tag"></i> collaborative perception</a></div><div class=post-nav><div class=post-nav-item><a title="Rust learning:from germ to grave" href=/2024/06/29/Rust-learning-from-germ-to-grave/ rel=prev> <i class="fa fa-chevron-left"></i> Rust learning:from germ to grave </a></div><div class=post-nav-item><a href=/2024/07/06/i3wm%E3%80%81Neovim%E4%B8%8EAlacritty%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E9%85%8D%E7%BD%AE/ rel=next title=i3wm,Neovim与Alacritty的使用与配置> i3wm,Neovim与Alacritty的使用与配置 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#More-Robust><span class=nav-number>1.</span> <span class=nav-text>More Robust</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Self-Localized-Collaborative-Perception><span class=nav-number>1.1.</span> <span class=nav-text>Self-Localized Collaborative Perception</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>1.1.1.</span> <span class=nav-text>摘要</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BB%8B%E7%BB%8D><span class=nav-number>1.1.2.</span> <span class=nav-text>介绍</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network><span class=nav-number>1.2.</span> <span class=nav-text>Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#More-domain-invariant><span class=nav-number>2.</span> <span class=nav-text>More domain-invariant</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#V2X-DGW-Domain-Generalization-for-Multi-agent-Perception-under-Adverse-Weather-Conditions><span class=nav-number>2.1.</span> <span class=nav-text>V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#More-Communication-efficient><span class=nav-number>3.</span> <span class=nav-text>More Communication-efficient</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#ERMVP-Communication-Efficient-and-Collaboration-Robust-Multi-Vehicle-Perception-in-Challenging-Environments><span class=nav-number>3.1.</span> <span class=nav-text>ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Filter-and-Merge-Feature-Sampling><span class=nav-number>3.1.0.1.</span> <span class=nav-text>Filter and Merge Feature Sampling</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#Attention-based-Feature-Fusion><span class=nav-number>3.1.1.</span> <span class=nav-text>Attention-based Feature Fusion</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Accuracy-Enhanced-Feature-Interaction><span class=nav-number>3.1.2.</span> <span class=nav-text>Accuracy Enhanced Feature Interaction</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Pragmatic-Communication-in-Multi-Agent-Collaborative-Perception><span class=nav-number>3.2.</span> <span class=nav-text>Pragmatic Communication in Multi-Agent Collaborative Perception</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#What-Makes-Good-Collaborative-Views-Contrastive-Mutual-Information-Maximization-for-Multi-Agent-Perception><span class=nav-number>3.3.</span> <span class=nav-text>What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Towards-Label-Efficient><span class=nav-number>4.</span> <span class=nav-text>Towards Label Efficient</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#CO%CB%863-Cooperative-Unsupervised-3D-Representation-Learning-for-Autonomous-Driving><span class=nav-number>4.1.</span> <span class=nav-text>COˆ3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%91%98%E8%A6%81-1><span class=nav-number>4.1.1.</span> <span class=nav-text>摘要</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BB%8B%E7%BB%8D-1><span class=nav-number>4.1.2.</span> <span class=nav-text>介绍</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>5.</span> <span class=nav-text>参考资料</span></a></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>215</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>202</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/collaborative-perception/ rel=tag>collaborative perception</a><span class=tag-list-count>4</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.8m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>27:38</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>