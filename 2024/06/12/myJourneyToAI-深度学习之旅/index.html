<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="总结一下学习人工智能/深度学习过程中个人觉得重要的方法和经验.  主要关于模型." name=description><meta content=article property=og:type><meta content=myJourneyToAI:深度学习之旅 property=og:title><meta content=https://www.sekyoro.top/2024/06/12/myJourneyToAI-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="总结一下学习人工智能/深度学习过程中个人觉得重要的方法和经验.  主要关于模型." property=og:description><meta content=zh_CN property=og:locale><meta content=https://zh.d2l.ai/_images/lenet.svg property=og:image><meta content=https://zh.d2l.ai/_images/lenet-vert.svg property=og:image><meta content=https://zh.d2l.ai/_images/alexnet.svg property=og:image><meta content=https://guandi1995.github.io/images/classical_cnn/vgg-16-simplified.PNG property=og:image><meta content=https://zh.d2l.ai/_images/vgg.svg property=og:image><meta content=https://zh.d2l.ai/_images/nin.svg property=og:image><meta content=https://zh.d2l.ai/_images/inception.svg property=og:image><meta content=https://zh.d2l.ai/_images/inception-full.svg property=og:image><meta content=https://zh.d2l.ai/_images/residual-block.svg property=og:image><meta content=https://zh.d2l.ai/_images/resnet-block.svg property=og:image><meta content=https://zh.d2l.ai/_images/densenet-block.svg property=og:image><meta content=https://zh.d2l.ai/_images/densenet.svg property=og:image><meta content=https://s2.loli.net/2024/06/12/PIiVxGL71WYtqCy.png property=og:image><meta content=https://s2.loli.net/2024/06/12/XNfnA8pBt7cSZrw.png property=og:image><meta content=https://s2.loli.net/2024/06/12/vBQ5meLa21hNuTW.png property=og:image><meta content=https://upload-images.jianshu.io/upload_images/14932861-4a872d74db7a93ec.png?imageMogr2/auto-orient/strip|imageView2/2/w/760/format/webp property=og:image><meta content=https://img-blog.csdnimg.cn/img_convert/91f15dc3b7067e6ec693302399e05b0b.png property=og:image><meta content=https://zh.d2l.ai/_images/rnn.svg property=og:image><meta content=https://zh.d2l.ai/_images/gru-3.svg property=og:image><meta content=https://zh.d2l.ai/_images/lstm-3.svg property=og:image><meta content=https://zh.d2l.ai/_images/seq2seq-attention-details.svg property=og:image><meta content=https://zh.d2l.ai/_images/multi-head-attention.svg property=og:image><meta content=https://zh.d2l.ai/_images/transformer.svg property=og:image><meta content=https://github.com/facebookresearch/detr/raw/main/.github/DETR.png property=og:image><meta content=https://s2.loli.net/2024/06/12/UdvnyZqNjWB51SH.png property=og:image><meta content=https://s2.loli.net/2024/06/12/BpS6JWYKnGA5Rty.png property=og:image><meta content=https://s2.loli.net/2024/06/12/XmtBQZG31L4Wi89.png property=og:image><meta content=https://s2.loli.net/2024/06/12/RHxAKYM4NFjnTpu.png property=og:image><meta content=https://s2.loli.net/2024/06/12/pK6PTi7awbfCEBG.png property=og:image><meta content=2024-06-12T11:32:09.000Z property=article:published_time><meta content=2024-06-25T03:12:11.169Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="deep learning" property=article:tag><meta content=summary name=twitter:card><meta content=https://zh.d2l.ai/_images/lenet.svg name=twitter:image><link href=https://www.sekyoro.top/2024/06/12/myJourneyToAI-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>myJourneyToAI:深度学习之旅 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/06/12/myJourneyToAI-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>myJourneyToAI:深度学习之旅</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-06-12 19:32:09" datetime=2024-06-12T19:32:09+08:00>2024-06-12</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-06-25 11:12:11" datetime=2024-06-25T11:12:11+08:00 itemprop=dateModified>2024-06-25</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>23k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>21 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>总结一下学习人工智能/深度学习过程中个人觉得重要的方法和经验.<p>主要关于模型.<br><span id=more></span><h1 id=蛮荒时代><a class=headerlink href=#蛮荒时代 title=蛮荒时代></a>蛮荒时代</h1><blockquote><p>大约2010年开始，那些在计算上看起来不可行的神经网络算法变得热门起来，实际上是以下两点导致的： 其一，随着互联网的公司的出现，为数亿在线用户提供服务，大规模数据集变得触手可及； 另外，廉价又高质量的传感器、廉价的数据存储（克莱德定律）以及廉价计算（摩尔定律）的普及，特别是GPU的普及，使大规模算力唾手可得。</blockquote><h2 id=经典CNN模型><a class=headerlink href=#经典CNN模型 title=经典CNN模型></a>经典CNN模型</h2><h3 id=LeNet><a class=headerlink href=#LeNet title=LeNet></a>LeNet</h3><p><img alt=../_images/lenet.svg data-src=https://zh.d2l.ai/_images/lenet.svg>每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用5×5卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line></span><br><span class=line>net = nn.Sequential(</span><br><span class=line>    nn.Conv2d(<span class=number>1</span>, <span class=number>6</span>, kernel_size=<span class=number>5</span>, padding=<span class=number>2</span>), nn.Sigmoid(),</span><br><span class=line>    nn.AvgPool2d(kernel_size=<span class=number>2</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nn.Conv2d(<span class=number>6</span>, <span class=number>16</span>, kernel_size=<span class=number>5</span>), nn.Sigmoid(),</span><br><span class=line>    nn.AvgPool2d(kernel_size=<span class=number>2</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nn.Flatten(),</span><br><span class=line>    nn.Linear(<span class=number>16</span> * <span class=number>5</span> * <span class=number>5</span>, <span class=number>120</span>), nn.Sigmoid(),</span><br><span class=line>    nn.Linear(<span class=number>120</span>, <span class=number>84</span>), nn.Sigmoid(),</span><br><span class=line>    nn.Linear(<span class=number>84</span>, <span class=number>10</span>))</span><br></pre></table></figure><p><img alt=../_images/lenet-vert.svg data-src=https://zh.d2l.ai/_images/lenet-vert.svg><h3 id=AlexNet><a class=headerlink href=#AlexNet title=AlexNet></a>AlexNet</h3><p><img alt=../_images/alexnet.svg data-src=https://zh.d2l.ai/_images/alexnet.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line></span><br><span class=line>net = nn.Sequential(</span><br><span class=line>    <span class=comment># 这里使用一个11*11的更大窗口来捕捉对象。</span></span><br><span class=line>    <span class=comment># 同时，步幅为4，以减少输出的高度和宽度。</span></span><br><span class=line>    <span class=comment># 另外，输出通道的数目远大于LeNet</span></span><br><span class=line>    nn.Conv2d(<span class=number>1</span>, <span class=number>96</span>, kernel_size=<span class=number>11</span>, stride=<span class=number>4</span>, padding=<span class=number>1</span>), nn.ReLU(),</span><br><span class=line>    nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    <span class=comment># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class=line>    nn.Conv2d(<span class=number>96</span>, <span class=number>256</span>, kernel_size=<span class=number>5</span>, padding=<span class=number>2</span>), nn.ReLU(),</span><br><span class=line>    nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    <span class=comment># 使用三个连续的卷积层和较小的卷积窗口。</span></span><br><span class=line>    <span class=comment># 除了最后的卷积层，输出通道的数量进一步增加。</span></span><br><span class=line>    <span class=comment># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span></span><br><span class=line>    nn.Conv2d(<span class=number>256</span>, <span class=number>384</span>, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>), nn.ReLU(),</span><br><span class=line>    nn.Conv2d(<span class=number>384</span>, <span class=number>384</span>, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>), nn.ReLU(),</span><br><span class=line>    nn.Conv2d(<span class=number>384</span>, <span class=number>256</span>, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>), nn.ReLU(),</span><br><span class=line>    nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nn.Flatten(),</span><br><span class=line>    <span class=comment># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span></span><br><span class=line>    nn.Linear(<span class=number>6400</span>, <span class=number>4096</span>), nn.ReLU(),</span><br><span class=line>    nn.Dropout(p=<span class=number>0.5</span>),</span><br><span class=line>    nn.Linear(<span class=number>4096</span>, <span class=number>4096</span>), nn.ReLU(),</span><br><span class=line>    nn.Dropout(p=<span class=number>0.5</span>),</span><br><span class=line>    <span class=comment># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class=line>    nn.Linear(<span class=number>4096</span>, <span class=number>10</span>))</span><br></pre></table></figure><h3 id=VGG><a class=headerlink href=#VGG title=VGG></a>VGG</h3><p><img alt=img data-src=https://guandi1995.github.io/images/classical_cnn/vgg-16-simplified.PNG><p><img alt=../_images/vgg.svg data-src=https://zh.d2l.ai/_images/vgg.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>vgg</span>(<span class=params>conv_arch</span>):</span></span><br><span class=line>    conv_blks = []</span><br><span class=line>    in_channels = <span class=number>1</span></span><br><span class=line>    <span class=comment># 卷积层部分</span></span><br><span class=line>    <span class=keyword>for</span> (num_convs, out_channels) <span class=keyword>in</span> conv_arch:</span><br><span class=line>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class=line>        in_channels = out_channels</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> nn.Sequential(</span><br><span class=line>        *conv_blks, nn.Flatten(),</span><br><span class=line>        <span class=comment># 全连接层部分</span></span><br><span class=line>        nn.Linear(out_channels * <span class=number>7</span> * <span class=number>7</span>, <span class=number>4096</span>), nn.ReLU(), nn.Dropout(<span class=number>0.5</span>),</span><br><span class=line>        nn.Linear(<span class=number>4096</span>, <span class=number>4096</span>), nn.ReLU(), nn.Dropout(<span class=number>0.5</span>),</span><br><span class=line>        nn.Linear(<span class=number>4096</span>, <span class=number>10</span>))</span><br><span class=line></span><br><span class=line>net = vgg(conv_arch)</span><br></pre></table></figure><h3 id=NiN><a class=headerlink href=#NiN title=NiN></a>NiN</h3><p><img alt=../_images/nin.svg data-src=https://zh.d2l.ai/_images/nin.svg style=zoom:50%;><p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个<em>全局平均汇聚层</em>（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>nin_block</span>(<span class=params>in_channels, out_channels, kernel_size, strides, padding</span>):</span></span><br><span class=line>    <span class=keyword>return</span> nn.Sequential(</span><br><span class=line>        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class=line>        nn.ReLU(),</span><br><span class=line>        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=number>1</span>), nn.ReLU(),</span><br><span class=line>        nn.Conv2d(out_channels, out_channels, kernel_size=<span class=number>1</span>), nn.ReLU())</span><br><span class=line>net = nn.Sequential(</span><br><span class=line>    nin_block(<span class=number>1</span>, <span class=number>96</span>, kernel_size=<span class=number>11</span>, strides=<span class=number>4</span>, padding=<span class=number>0</span>),</span><br><span class=line>    nn.MaxPool2d(<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nin_block(<span class=number>96</span>, <span class=number>256</span>, kernel_size=<span class=number>5</span>, strides=<span class=number>1</span>, padding=<span class=number>2</span>),</span><br><span class=line>    nn.MaxPool2d(<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nin_block(<span class=number>256</span>, <span class=number>384</span>, kernel_size=<span class=number>3</span>, strides=<span class=number>1</span>, padding=<span class=number>1</span>),</span><br><span class=line>    nn.MaxPool2d(<span class=number>3</span>, stride=<span class=number>2</span>),</span><br><span class=line>    nn.Dropout(<span class=number>0.5</span>),</span><br><span class=line>    <span class=comment># 标签类别数是10</span></span><br><span class=line>    nin_block(<span class=number>384</span>, <span class=number>10</span>, kernel_size=<span class=number>3</span>, strides=<span class=number>1</span>, padding=<span class=number>1</span>),</span><br><span class=line>    nn.AdaptiveAvgPool2d((<span class=number>1</span>, <span class=number>1</span>)),</span><br><span class=line>    <span class=comment># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class=line>    nn.Flatten())</span><br></pre></table></figure><h3 id=GoogleNet><a class=headerlink href=#GoogleNet title=GoogleNet></a>GoogleNet</h3><p><img alt=../_images/inception.svg data-src=https://zh.d2l.ai/_images/inception.svg><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line><span class=keyword>from</span> torch.nn <span class=keyword>import</span> functional <span class=keyword>as</span> F</span><br><span class=line><span class=keyword>from</span> d2l <span class=keyword>import</span> torch <span class=keyword>as</span> d2l</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Inception</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=comment># c1--c4是每条路径的输出通道数</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(Inception, self).__init__(**kwargs)</span><br><span class=line>        <span class=comment># 线路1，单1x1卷积层</span></span><br><span class=line>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class=number>1</span>)</span><br><span class=line>        <span class=comment># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class=line>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class=number>0</span>], kernel_size=<span class=number>1</span>)</span><br><span class=line>        self.p2_2 = nn.Conv2d(c2[<span class=number>0</span>], c2[<span class=number>1</span>], kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>)</span><br><span class=line>        <span class=comment># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class=line>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class=number>0</span>], kernel_size=<span class=number>1</span>)</span><br><span class=line>        self.p3_2 = nn.Conv2d(c3[<span class=number>0</span>], c3[<span class=number>1</span>], kernel_size=<span class=number>5</span>, padding=<span class=number>2</span>)</span><br><span class=line>        <span class=comment># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class=line>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>1</span>, padding=<span class=number>1</span>)</span><br><span class=line>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        p1 = F.relu(self.p1_1(x))</span><br><span class=line>        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class=line>        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class=line>        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class=line>        <span class=comment># 在通道维度上连结输出</span></span><br><span class=line>        <span class=keyword>return</span> torch.cat((p1, p2, p3, p4), dim=<span class=number>1</span>)</span><br></pre></table></figure><p><img alt=../_images/inception-full.svg data-src=https://zh.d2l.ai/_images/inception-full.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br></pre><td class=code><pre><span class=line>b1 = nn.Sequential(nn.Conv2d(<span class=number>1</span>, <span class=number>64</span>, kernel_size=<span class=number>7</span>, stride=<span class=number>2</span>, padding=<span class=number>3</span>),</span><br><span class=line>                   nn.ReLU(),</span><br><span class=line>                   nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))</span><br><span class=line>b2 = nn.Sequential(nn.Conv2d(<span class=number>64</span>, <span class=number>64</span>, kernel_size=<span class=number>1</span>),</span><br><span class=line>                   nn.ReLU(),</span><br><span class=line>                   nn.Conv2d(<span class=number>64</span>, <span class=number>192</span>, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>),</span><br><span class=line>                   nn.ReLU(),</span><br><span class=line>                   nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))    </span><br><span class=line>b3 = nn.Sequential(Inception(<span class=number>192</span>, <span class=number>64</span>, (<span class=number>96</span>, <span class=number>128</span>), (<span class=number>16</span>, <span class=number>32</span>), <span class=number>32</span>),</span><br><span class=line>                   Inception(<span class=number>256</span>, <span class=number>128</span>, (<span class=number>128</span>, <span class=number>192</span>), (<span class=number>32</span>, <span class=number>96</span>), <span class=number>64</span>),</span><br><span class=line>                   nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))                </span><br><span class=line>b4 = nn.Sequential(Inception(<span class=number>480</span>, <span class=number>192</span>, (<span class=number>96</span>, <span class=number>208</span>), (<span class=number>16</span>, <span class=number>48</span>), <span class=number>64</span>),</span><br><span class=line>                   Inception(<span class=number>512</span>, <span class=number>160</span>, (<span class=number>112</span>, <span class=number>224</span>), (<span class=number>24</span>, <span class=number>64</span>), <span class=number>64</span>),</span><br><span class=line>                   Inception(<span class=number>512</span>, <span class=number>128</span>, (<span class=number>128</span>, <span class=number>256</span>), (<span class=number>24</span>, <span class=number>64</span>), <span class=number>64</span>),</span><br><span class=line>                   Inception(<span class=number>512</span>, <span class=number>112</span>, (<span class=number>144</span>, <span class=number>288</span>), (<span class=number>32</span>, <span class=number>64</span>), <span class=number>64</span>),</span><br><span class=line>                   Inception(<span class=number>528</span>, <span class=number>256</span>, (<span class=number>160</span>, <span class=number>320</span>), (<span class=number>32</span>, <span class=number>128</span>), <span class=number>128</span>),</span><br><span class=line>                   nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))           </span><br><span class=line>b5 = nn.Sequential(Inception(<span class=number>832</span>, <span class=number>256</span>, (<span class=number>160</span>, <span class=number>320</span>), (<span class=number>32</span>, <span class=number>128</span>), <span class=number>128</span>),</span><br><span class=line>                   Inception(<span class=number>832</span>, <span class=number>384</span>, (<span class=number>192</span>, <span class=number>384</span>), (<span class=number>48</span>, <span class=number>128</span>), <span class=number>128</span>),</span><br><span class=line>                   nn.AdaptiveAvgPool2d((<span class=number>1</span>,<span class=number>1</span>)),</span><br><span class=line>                   nn.Flatten())</span><br><span class=line></span><br><span class=line>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class=number>1024</span>, <span class=number>10</span>))            X = torch.rand(size=(<span class=number>1</span>, <span class=number>1</span>, <span class=number>96</span>, <span class=number>96</span>))</span><br><span class=line><span class=keyword>for</span> layer <span class=keyword>in</span> net:</span><br><span class=line>    X = layer(X)</span><br><span class=line>    <span class=built_in>print</span>(layer.__class__.__name__,<span class=string>'output shape:\t'</span>, X.shape)</span><br></pre></table></figure><h3 id=ResNet><a class=headerlink href=#ResNet title=ResNet></a>ResNet</h3><p><img alt=../_images/residual-block.svg data-src=https://zh.d2l.ai/_images/residual-block.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line><span class=keyword>from</span> torch.nn <span class=keyword>import</span> functional <span class=keyword>as</span> F</span><br><span class=line></span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Residual</span>(<span class=params>nn.Module</span>):</span>  <span class=comment>#@save</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, input_channels, num_channels,</span></span></span><br><span class=line><span class=params><span class=function>                 use_1x1conv=<span class=literal>False</span>, strides=<span class=number>1</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class=line>                               kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, stride=strides)</span><br><span class=line>        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class=line>                               kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>)</span><br><span class=line>        <span class=keyword>if</span> use_1x1conv:</span><br><span class=line>            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class=line>                                   kernel_size=<span class=number>1</span>, stride=strides)</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            self.conv3 = <span class=literal>None</span></span><br><span class=line>        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class=line>        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, X</span>):</span></span><br><span class=line>        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class=line>        Y = self.bn2(self.conv2(Y))</span><br><span class=line>        <span class=keyword>if</span> self.conv3:</span><br><span class=line>            X = self.conv3(X)</span><br><span class=line>        Y += X</span><br><span class=line>        <span class=keyword>return</span> F.relu(Y)</span><br></pre></table></figure><p><img alt=../_images/resnet-block.svg data-src=https://zh.d2l.ai/_images/resnet-block.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line>b1 = nn.Sequential(nn.Conv2d(<span class=number>1</span>, <span class=number>64</span>, kernel_size=<span class=number>7</span>, stride=<span class=number>2</span>, padding=<span class=number>3</span>),</span><br><span class=line>                   nn.BatchNorm2d(<span class=number>64</span>), nn.ReLU(),</span><br><span class=line>                   nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>resnet_block</span>(<span class=params>input_channels, num_channels, num_residuals,</span></span></span><br><span class=line><span class=params><span class=function>                 first_block=<span class=literal>False</span></span>):</span></span><br><span class=line>    blk = []</span><br><span class=line>    <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_residuals):</span><br><span class=line>        <span class=keyword>if</span> i == <span class=number>0</span> <span class=keyword>and</span> <span class=keyword>not</span> first_block:</span><br><span class=line>            blk.append(Residual(input_channels, num_channels,</span><br><span class=line>                                use_1x1conv=<span class=literal>True</span>, strides=<span class=number>2</span>))</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            blk.append(Residual(num_channels, num_channels))</span><br><span class=line>    <span class=keyword>return</span> blk</span><br><span class=line>b2 = nn.Sequential(*resnet_block(<span class=number>64</span>, <span class=number>64</span>, <span class=number>2</span>, first_block=<span class=literal>True</span>))</span><br><span class=line>b3 = nn.Sequential(*resnet_block(<span class=number>64</span>, <span class=number>128</span>, <span class=number>2</span>))</span><br><span class=line>b4 = nn.Sequential(*resnet_block(<span class=number>128</span>, <span class=number>256</span>, <span class=number>2</span>))</span><br><span class=line>b5 = nn.Sequential(*resnet_block(<span class=number>256</span>, <span class=number>512</span>, <span class=number>2</span>))</span><br><span class=line></span><br><span class=line>net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class=line>                    nn.AdaptiveAvgPool2d((<span class=number>1</span>,<span class=number>1</span>)),</span><br><span class=line>                    nn.Flatten(), nn.Linear(<span class=number>512</span>, <span class=number>10</span>))</span><br><span class=line>X = torch.rand(size=(<span class=number>1</span>, <span class=number>1</span>, <span class=number>224</span>, <span class=number>224</span>))</span><br><span class=line><span class=keyword>for</span> layer <span class=keyword>in</span> net:</span><br><span class=line>    X = layer(X)</span><br><span class=line>    <span class=built_in>print</span>(layer.__class__.__name__,<span class=string>'output shape:\t'</span>, X.shape)</span><br></pre></table></figure><h3 id=DenseNet><a class=headerlink href=#DenseNet title=DenseNet></a>DenseNet</h3><p><img alt=../_images/densenet-block.svg data-src=https://zh.d2l.ai/_images/densenet-block.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>conv_block</span>(<span class=params>input_channels, num_channels</span>):</span></span><br><span class=line>    <span class=keyword>return</span> nn.Sequential(</span><br><span class=line>        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=line>        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>))</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>DenseBlock</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, num_convs, input_channels, num_channels</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(DenseBlock, self).__init__()</span><br><span class=line>        layer = []</span><br><span class=line>        <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_convs):</span><br><span class=line>            layer.append(conv_block(</span><br><span class=line>                num_channels * i + input_channels, num_channels))</span><br><span class=line>        self.net = nn.Sequential(*layer)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, X</span>):</span></span><br><span class=line>        <span class=keyword>for</span> blk <span class=keyword>in</span> self.net:</span><br><span class=line>            Y = blk(X)</span><br><span class=line>            <span class=comment># 连接通道维度上每个块的输入和输出</span></span><br><span class=line>            X = torch.cat((X, Y), dim=<span class=number>1</span>)</span><br><span class=line>        <span class=keyword>return</span> X</span><br></pre></table></figure><p>稠密网络主要由2部分构成：<em>稠密块</em>（dense block）和<em>过渡层</em>（transition layer）<p><img alt=../_images/densenet.svg data-src=https://zh.d2l.ai/_images/densenet.svg><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>blk = DenseBlock(<span class=number>2</span>, <span class=number>3</span>, <span class=number>10</span>)</span><br><span class=line>X = torch.randn(<span class=number>4</span>, <span class=number>3</span>, <span class=number>8</span>, <span class=number>8</span>)</span><br><span class=line>Y = blk(X)</span><br><span class=line>Y.shape</span><br></pre></table></figure><p>由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过1×1卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>transition_block</span>(<span class=params>input_channels, num_channels</span>):</span></span><br><span class=line>    <span class=keyword>return</span> nn.Sequential(</span><br><span class=line>        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class=line>        nn.Conv2d(input_channels, num_channels, kernel_size=<span class=number>1</span>),</span><br><span class=line>        nn.AvgPool2d(kernel_size=<span class=number>2</span>, stride=<span class=number>2</span>))</span><br><span class=line>blk = transition_block(<span class=number>23</span>, <span class=number>10</span>)</span><br><span class=line>b1 = nn.Sequential(</span><br><span class=line>    nn.Conv2d(<span class=number>1</span>, <span class=number>64</span>, kernel_size=<span class=number>7</span>, stride=<span class=number>2</span>, padding=<span class=number>3</span>),</span><br><span class=line>    nn.BatchNorm2d(<span class=number>64</span>), nn.ReLU(),</span><br><span class=line>    nn.MaxPool2d(kernel_size=<span class=number>3</span>, stride=<span class=number>2</span>, padding=<span class=number>1</span>))</span><br><span class=line><span class=comment># num_channels为当前的通道数</span></span><br><span class=line>num_channels, growth_rate = <span class=number>64</span>, <span class=number>32</span></span><br><span class=line>num_convs_in_dense_blocks = [<span class=number>4</span>, <span class=number>4</span>, <span class=number>4</span>, <span class=number>4</span>]</span><br><span class=line>blks = []</span><br><span class=line><span class=keyword>for</span> i, num_convs <span class=keyword>in</span> <span class=built_in>enumerate</span>(num_convs_in_dense_blocks):</span><br><span class=line>    blks.append(DenseBlock(num_convs, num_channels, growth_rate))</span><br><span class=line>    <span class=comment># 上一个稠密块的输出通道数</span></span><br><span class=line>    num_channels += num_convs * growth_rate</span><br><span class=line>    <span class=comment># 在稠密块之间添加一个转换层，使通道数量减半</span></span><br><span class=line>    <span class=keyword>if</span> i != <span class=built_in>len</span>(num_convs_in_dense_blocks) - <span class=number>1</span>:</span><br><span class=line>        blks.append(transition_block(num_channels, num_channels // <span class=number>2</span>))</span><br><span class=line>        num_channels = num_channels // <span class=number>2</span></span><br><span class=line>net = nn.Sequential(</span><br><span class=line>    b1, *blks,</span><br><span class=line>    nn.BatchNorm2d(num_channels), nn.ReLU(),</span><br><span class=line>    nn.AdaptiveAvgPool2d((<span class=number>1</span>, <span class=number>1</span>)),</span><br><span class=line>    nn.Flatten(),</span><br><span class=line>    nn.Linear(num_channels, <span class=number>10</span>))</span><br></pre></table></figure><h3 id=U-Net><a class=headerlink href=#U-Net title=U-Net></a>U-Net</h3><p><img alt=image-20240612201025297 data-src=https://s2.loli.net/2024/06/12/PIiVxGL71WYtqCy.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br></pre><td class=code><pre><span class=line><span class=string>""" Parts of the U-Net model """</span></span><br><span class=line></span><br><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>DoubleConv</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>"""(convolution => [BN] => ReLU) * 2"""</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, out_channels, mid_channels=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        <span class=keyword>if</span> <span class=keyword>not</span> mid_channels:</span><br><span class=line>            mid_channels = out_channels</span><br><span class=line>        self.double_conv = nn.Sequential(</span><br><span class=line>            nn.Conv2d(in_channels, mid_channels, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, bias=<span class=literal>False</span>),</span><br><span class=line>            nn.BatchNorm2d(mid_channels),</span><br><span class=line>            nn.ReLU(inplace=<span class=literal>True</span>),</span><br><span class=line>            nn.Conv2d(mid_channels, out_channels, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, bias=<span class=literal>False</span>),</span><br><span class=line>            nn.BatchNorm2d(out_channels),</span><br><span class=line>            nn.ReLU(inplace=<span class=literal>True</span>)</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.double_conv(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Down</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>"""Downscaling with maxpool then double conv"""</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, out_channels</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.maxpool_conv = nn.Sequential(</span><br><span class=line>            nn.MaxPool2d(<span class=number>2</span>),</span><br><span class=line>            DoubleConv(in_channels, out_channels)</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.maxpool_conv(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Up</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>"""Upscaling then double conv"""</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, out_channels, bilinear=<span class=literal>True</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line></span><br><span class=line>        <span class=comment># if bilinear, use the normal convolutions to reduce the number of channels</span></span><br><span class=line>        <span class=keyword>if</span> bilinear:</span><br><span class=line>            self.up = nn.Upsample(scale_factor=<span class=number>2</span>, mode=<span class=string>'bilinear'</span>, align_corners=<span class=literal>True</span>)</span><br><span class=line>            self.conv = DoubleConv(in_channels, out_channels, in_channels // <span class=number>2</span>)</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            self.up = nn.ConvTranspose2d(in_channels, in_channels // <span class=number>2</span>, kernel_size=<span class=number>2</span>, stride=<span class=number>2</span>)</span><br><span class=line>            self.conv = DoubleConv(in_channels, out_channels)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x1, x2</span>):</span></span><br><span class=line>        x1 = self.up(x1)</span><br><span class=line>        <span class=comment># input is CHW</span></span><br><span class=line>        diffY = x2.size()[<span class=number>2</span>] - x1.size()[<span class=number>2</span>]</span><br><span class=line>        diffX = x2.size()[<span class=number>3</span>] - x1.size()[<span class=number>3</span>]</span><br><span class=line></span><br><span class=line>        x1 = F.pad(x1, [diffX // <span class=number>2</span>, diffX - diffX // <span class=number>2</span>,</span><br><span class=line>                        diffY // <span class=number>2</span>, diffY - diffY // <span class=number>2</span>])</span><br><span class=line></span><br><span class=line>        x = torch.cat([x2, x1], dim=<span class=number>1</span>)</span><br><span class=line>        <span class=keyword>return</span> self.conv(x)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>OutConv</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_channels, out_channels</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(OutConv, self).__init__()</span><br><span class=line>        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=<span class=number>1</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.conv(x)</span><br><span class=line> </span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>UNet</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, n_channels, n_classes, bilinear=<span class=literal>False</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>(UNet, self).__init__()</span><br><span class=line>        self.n_channels = n_channels</span><br><span class=line>        self.n_classes = n_classes</span><br><span class=line>        self.bilinear = bilinear</span><br><span class=line></span><br><span class=line>        self.inc = (DoubleConv(n_channels, <span class=number>64</span>))</span><br><span class=line>        self.down1 = (Down(<span class=number>64</span>, <span class=number>128</span>))</span><br><span class=line>        self.down2 = (Down(<span class=number>128</span>, <span class=number>256</span>))</span><br><span class=line>        self.down3 = (Down(<span class=number>256</span>, <span class=number>512</span>))</span><br><span class=line>        factor = <span class=number>2</span> <span class=keyword>if</span> bilinear <span class=keyword>else</span> <span class=number>1</span></span><br><span class=line>        self.down4 = (Down(<span class=number>512</span>, <span class=number>1024</span> // factor))</span><br><span class=line>        self.up1 = (Up(<span class=number>1024</span>, <span class=number>512</span> // factor, bilinear))</span><br><span class=line>        self.up2 = (Up(<span class=number>512</span>, <span class=number>256</span> // factor, bilinear))</span><br><span class=line>        self.up3 = (Up(<span class=number>256</span>, <span class=number>128</span> // factor, bilinear))</span><br><span class=line>        self.up4 = (Up(<span class=number>128</span>, <span class=number>64</span>, bilinear))</span><br><span class=line>        self.outc = (OutConv(<span class=number>64</span>, n_classes))</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x1 = self.inc(x)</span><br><span class=line>        x2 = self.down1(x1)</span><br><span class=line>        x3 = self.down2(x2)</span><br><span class=line>        x4 = self.down3(x3)</span><br><span class=line>        x5 = self.down4(x4)</span><br><span class=line>        x = self.up1(x5, x4)</span><br><span class=line>        x = self.up2(x, x3)</span><br><span class=line>        x = self.up3(x, x2)</span><br><span class=line>        x = self.up4(x, x1)</span><br><span class=line>        logits = self.outc(x)</span><br><span class=line>        <span class=keyword>return</span> logits</span><br></pre></table></figure><h3 id=SSD><a class=headerlink href=#SSD title=SSD></a>SSD</h3><p><img alt=image-20240612223108822 data-src=https://s2.loli.net/2024/06/12/XNfnA8pBt7cSZrw.png><p><a href=https://zh.d2l.ai/chapter_computer-vision/ssd.html#id9 rel=noopener target=_blank>13.7. 单发多框检测（SSD） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br></pre><td class=code><pre><span class=line><span class=comment>#  #!/usr/bin/env python</span></span><br><span class=line><span class=comment>#  -*- coding:utf-8 -*-</span></span><br><span class=line><span class=comment>#  Copyleft (C) 2024 proanimer, Inc. All Rights Reserved</span></span><br><span class=line><span class=comment>#   author:proanimer</span></span><br><span class=line><span class=comment>#   createTime:2024/6/12 下午10:42</span></span><br><span class=line><span class=comment>#   lastModifiedTime:2024/6/12 下午10:42</span></span><br><span class=line><span class=comment>#   file:SSD.py</span></span><br><span class=line><span class=comment>#   software: classicNets</span></span><br><span class=line><span class=comment>#</span></span><br><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>from</span> torchvision.models <span class=keyword>import</span> vgg19</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>SSD</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        vgg = vgg19(pretrained=<span class=literal>True</span>)</span><br><span class=line>        vgg.<span class=built_in>eval</span>()</span><br><span class=line>        self.conv = vgg.features</span><br><span class=line>        self.conv1 = nn.Conv2d(<span class=number>512</span>, <span class=number>1024</span>, <span class=number>3</span>)</span><br><span class=line>        self.conv2 = nn.Conv2d(<span class=number>1024</span>, <span class=number>1024</span>, <span class=number>1</span>)</span><br><span class=line>        self.conv3 = nn.Sequential(</span><br><span class=line>            nn.Conv2d(<span class=number>1024</span>, <span class=number>256</span>, <span class=number>1</span>),</span><br><span class=line>            nn.Conv2d(<span class=number>256</span>, <span class=number>512</span>, <span class=number>3</span>)</span><br><span class=line>        )</span><br><span class=line>        self.conv4 = nn.Sequential(</span><br><span class=line>            nn.Conv2d(<span class=number>512</span>, <span class=number>128</span>, <span class=number>1</span>),</span><br><span class=line>            nn.Conv2d(<span class=number>128</span>, <span class=number>256</span>, <span class=number>3</span>)</span><br><span class=line>        )</span><br><span class=line>        self.conv5 = nn.Sequential(</span><br><span class=line>            nn.Conv2d(<span class=number>256</span>, <span class=number>128</span>, <span class=number>1</span>),</span><br><span class=line>            nn.Conv2d(<span class=number>128</span>, <span class=number>256</span>, <span class=number>3</span>)</span><br><span class=line>        )</span><br><span class=line>        self.conv6 = nn.Sequential(</span><br><span class=line>            nn.Conv2d(<span class=number>256</span>, <span class=number>128</span>, <span class=number>1</span>),</span><br><span class=line>            nn.Conv2d(<span class=number>128</span>, <span class=number>256</span>, <span class=number>3</span>)</span><br><span class=line></span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        feat_1 = self.conv(x)</span><br><span class=line>        feat = self.conv1(feat_1)</span><br><span class=line>        feat_2 = self.conv2(feat)</span><br><span class=line>        feat_3 = self.conv3(feat_2)</span><br><span class=line>        feat_4 = self.conv4(feat_3)</span><br><span class=line>        feat_5 = self.conv5(feat_4)</span><br><span class=line>        feat_6 = self.conv6(feat_5)</span><br><span class=line>        <span class=keyword>return</span> feat_1, feat_2, feat_3, feat_4, feat_5, feat_6</span><br><span class=line></span><br></pre></table></figure><h3 id=Feature-Pyramid-Networks-for-Object-Detection><a title="Feature Pyramid Networks for Object Detection" class=headerlink href=#Feature-Pyramid-Networks-for-Object-Detection></a>Feature Pyramid Networks for Object Detection</h3><p><img alt=image-20240612201222802 data-src=https://s2.loli.net/2024/06/12/vBQ5meLa21hNuTW.png><p><img alt=img data-src=https://upload-images.jianshu.io/upload_images/14932861-4a872d74db7a93ec.png?imageMogr2/auto-orient/strip|imageView2/2/w/760/format/webp style=zoom:33%;><p>最后会将不同尺度得到的结果<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br></pre><td class=code><pre><span class=line><span class=comment>#  #!/usr/bin/env python</span></span><br><span class=line><span class=comment>#  -*- coding:utf-8 -*-</span></span><br><span class=line><span class=comment>#  Copyleft (C) 2024 proanimer, Inc. All Rights Reserved</span></span><br><span class=line><span class=comment>#   author:proanimer</span></span><br><span class=line><span class=comment>#   createTime:2024/6/12 下午9:13</span></span><br><span class=line><span class=comment>#   lastModifiedTime:2024/6/12 下午9:13</span></span><br><span class=line><span class=comment>#   file:fpn.py</span></span><br><span class=line><span class=comment>#   software: classicNets</span></span><br><span class=line><span class=comment>#</span></span><br><span class=line></span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line><span class=keyword>import</span> math</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=comment>##先定义ResNet基本类，或者可以说ResNet的基本砖块</span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Bottleneck</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    expansion = <span class=number>4</span>  <span class=comment>##通道倍增数</span></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_planes, planes, stride=<span class=number>1</span>, downsample=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>(Bottleneck, self).__init__()</span><br><span class=line>        self.bottleneck = nn.Sequential(</span><br><span class=line>            nn.Conv2d(in_planes, planes, <span class=number>1</span>, bias=<span class=literal>False</span>),</span><br><span class=line>            nn.BatchNorm2d(planes),</span><br><span class=line>            nn.ReLU(inplace=<span class=literal>True</span>),</span><br><span class=line>            nn.Conv2d(planes, planes, <span class=number>3</span>, stride, <span class=number>1</span>, bias=<span class=literal>False</span>),</span><br><span class=line>            nn.BatchNorm2d(planes),</span><br><span class=line>            nn.ReLU(inplace=<span class=literal>True</span>),</span><br><span class=line>            nn.Conv2d(planes, self.expansion * planes, <span class=number>1</span>, bias=<span class=literal>False</span>),</span><br><span class=line>            nn.BatchNorm2d(self.expansion * planes),</span><br><span class=line>        )</span><br><span class=line>        self.relu = nn.ReLU(inplace=<span class=literal>True</span>)</span><br><span class=line>        self.downsample = downsample</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        identity = x</span><br><span class=line>        out = self.bottleneck(x)</span><br><span class=line>        <span class=keyword>if</span> self.downsample <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>            identity = self.downsample(x)</span><br><span class=line>        out += identity</span><br><span class=line>        out = self.relu(out)</span><br><span class=line>        <span class=keyword>return</span> out</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=comment>##FPN类</span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>FPN</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, layers</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(FPN, self).__init__()</span><br><span class=line>        self.inplanes = <span class=number>64</span></span><br><span class=line>        <span class=comment>###下面四句代码代表处理输入的C1模块--对应博客中的图</span></span><br><span class=line>        self.conv1 = nn.Conv2d(<span class=number>3</span>, <span class=number>64</span>, <span class=number>7</span>, <span class=number>2</span>, <span class=number>3</span>, bias=<span class=literal>False</span>)</span><br><span class=line>        self.bn1 = nn.BatchNorm2d(<span class=number>64</span>)</span><br><span class=line>        self.relu = nn.ReLU(inplace=<span class=literal>True</span>)</span><br><span class=line>        self.maxpool = nn.MaxPool2d(<span class=number>3</span>, <span class=number>2</span>, <span class=number>1</span>)</span><br><span class=line>        <span class=comment>###搭建自下而上的C2,C3,C4,C5</span></span><br><span class=line>        self.layer1 = self._make_layer(<span class=number>64</span>, layers[<span class=number>0</span>])</span><br><span class=line>        self.layer2 = self._make_layer(<span class=number>128</span>, layers[<span class=number>1</span>], <span class=number>2</span>)</span><br><span class=line>        self.layer3 = self._make_layer(<span class=number>256</span>, layers[<span class=number>2</span>], <span class=number>2</span>)</span><br><span class=line>        self.layer4 = self._make_layer(<span class=number>512</span>, layers[<span class=number>3</span>], <span class=number>2</span>)</span><br><span class=line>        <span class=comment>###定义toplayer层，对C5减少通道数，得到P5</span></span><br><span class=line>        self.toplayer = nn.Conv2d(<span class=number>2048</span>, <span class=number>256</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>)</span><br><span class=line>        <span class=comment>###代表3*3的卷积融合，目的是消除上采样过程带来的重叠效应，以生成最终的特征图。</span></span><br><span class=line>        self.smooth1 = nn.Conv2d(<span class=number>256</span>, <span class=number>256</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>1</span>)</span><br><span class=line>        self.smooth2 = nn.Conv2d(<span class=number>256</span>, <span class=number>256</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>1</span>)</span><br><span class=line>        self.smooth3 = nn.Conv2d(<span class=number>256</span>, <span class=number>256</span>, <span class=number>3</span>, <span class=number>1</span>, <span class=number>1</span>)</span><br><span class=line>        <span class=comment>###横向连接，保证通道数目相同</span></span><br><span class=line>        self.latlayer1 = nn.Conv2d(<span class=number>1024</span>, <span class=number>256</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>)</span><br><span class=line>        self.latlayer2 = nn.Conv2d(<span class=number>512</span>, <span class=number>256</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>)</span><br><span class=line>        self.latlayer3 = nn.Conv2d(<span class=number>256</span>, <span class=number>256</span>, <span class=number>1</span>, <span class=number>1</span>, <span class=number>0</span>)</span><br><span class=line></span><br><span class=line>    <span class=comment>##作用：构建C2-C5砖块，注意stride为1和2的区别：得到C2没有经历下采样</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_make_layer</span>(<span class=params>self, planes, blocks, stride=<span class=number>1</span></span>):</span></span><br><span class=line>        downsample = <span class=literal>None</span></span><br><span class=line>        <span class=keyword>if</span> stride != <span class=number>1</span> <span class=keyword>or</span> self.inplanes != Bottleneck.expansion * planes:</span><br><span class=line>            downsample = nn.Sequential(</span><br><span class=line>                nn.Conv2d(self.inplanes, Bottleneck.expansion * planes, <span class=number>1</span>, stride, bias=<span class=literal>False</span>),</span><br><span class=line>                nn.BatchNorm2d(Bottleneck.expansion * planes)</span><br><span class=line>            )</span><br><span class=line>        <span class=comment>###初始化需要一个list，代表左侧网络ResNet每一个阶段的Bottleneck的数量</span></span><br><span class=line>        layers = []</span><br><span class=line>        layers.append(Bottleneck(self.inplanes, planes, stride, downsample))</span><br><span class=line>        self.inplanes = planes * Bottleneck.expansion</span><br><span class=line>        <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>1</span>, blocks):</span><br><span class=line>            layers.append(Bottleneck(self.inplanes, planes))</span><br><span class=line>        <span class=keyword>return</span> nn.Sequential(*layers)</span><br><span class=line></span><br><span class=line>    <span class=comment>###自上而下的上采样模块</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_upsample_add</span>(<span class=params>self, x, y</span>):</span></span><br><span class=line>        _, _, H, W = y.shape</span><br><span class=line>        <span class=keyword>return</span> F.upsample(x, size=(H, W), mode=<span class=string>'bilinear'</span>) + y</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=comment>###自下而上</span></span><br><span class=line>        c1 = self.maxpool(self.relu(self.bn1(self.conv1(x))))</span><br><span class=line>        c2 = self.layer1(c1)</span><br><span class=line>        c3 = self.layer2(c2)</span><br><span class=line>        c4 = self.layer3(c3)</span><br><span class=line>        c5 = self.layer4(c4)</span><br><span class=line>        <span class=comment>###自上而下</span></span><br><span class=line>        p5 = self.toplayer(c5)</span><br><span class=line>        p4 = self._upsample_add(p5, self.latlayer1(c4))</span><br><span class=line>        p3 = self._upsample_add(p4, self.latlayer2(c3))</span><br><span class=line>        p2 = self._upsample_add(p3, self.latlayer3(c2))</span><br><span class=line>        <span class=comment>###卷积融合，平滑处理</span></span><br><span class=line>        p4 = self.smooth1(p4)</span><br><span class=line>        p3 = self.smooth2(p3)</span><br><span class=line>        p2 = self.smooth3(p2)</span><br><span class=line>        <span class=keyword>return</span> p2, p3, p4, p5</span><br><span class=line></span><br></pre></table></figure><p><img alt=img data-src=https://img-blog.csdnimg.cn/img_convert/91f15dc3b7067e6ec693302399e05b0b.png><h3 id=Deformable-Conv><a title="Deformable Conv" class=headerlink href=#Deformable-Conv></a>Deformable Conv</h3><p>可变形的卷积,还有可变形的attention. 即插即用类型.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br><span class=line>132</span><br><span class=line>133</span><br><span class=line>134</span><br><span class=line>135</span><br><span class=line>136</span><br><span class=line>137</span><br><span class=line>138</span><br><span class=line>139</span><br><span class=line>140</span><br><span class=line>141</span><br><span class=line>142</span><br><span class=line>143</span><br><span class=line>144</span><br><span class=line>145</span><br><span class=line>146</span><br><span class=line>147</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>DeformConv2d</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, inc, outc, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, stride=<span class=number>1</span>, bias=<span class=literal>None</span>, modulation=<span class=literal>False</span></span>):</span></span><br><span class=line>        <span class=string>"""</span></span><br><span class=line><span class=string>        Args:</span></span><br><span class=line><span class=string>            modulation (bool, optional): If True, Modulated Defomable Convolution (Deformable ConvNets v2).</span></span><br><span class=line><span class=string>        """</span></span><br><span class=line>        <span class=built_in>super</span>(DeformConv2d, self).__init__()</span><br><span class=line>        self.kernel_size = kernel_size</span><br><span class=line>        self.padding = padding</span><br><span class=line>        self.stride = stride</span><br><span class=line>        self.zero_padding = nn.ZeroPad2d(padding)</span><br><span class=line>        <span class=comment># conv则是实际进行的卷积操作，注意这里步长设置为卷积核大小，因为与该卷积核进行卷积操作的特征图是由输出特征图中每个点扩展为其对应卷积核那么多个点后生成的。</span></span><br><span class=line>        self.conv = nn.Conv2d(inc, outc, kernel_size=kernel_size, stride=kernel_size, bias=bias)</span><br><span class=line>        <span class=comment># p_conv是生成offsets所使用的卷积，输出通道数为卷积核尺寸的平方的2倍，代表对应卷积核每个位置横纵坐标都有偏移量。</span></span><br><span class=line>        self.p_conv = nn.Conv2d(inc, <span class=number>2</span>*kernel_size*kernel_size, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, stride=stride)</span><br><span class=line>        nn.init.constant_(self.p_conv.weight, <span class=number>0</span>)</span><br><span class=line>        self.p_conv.register_backward_hook(self._set_lr)</span><br><span class=line> </span><br><span class=line>        self.modulation = modulation <span class=comment># modulation是可选参数,若设置为True,那么在进行卷积操作时,对应卷积核的每个位置都会分配一个权重。</span></span><br><span class=line>        <span class=keyword>if</span> modulation:</span><br><span class=line>            self.m_conv = nn.Conv2d(inc, kernel_size*kernel_size, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>, stride=stride)</span><br><span class=line>            nn.init.constant_(self.m_conv.weight, <span class=number>0</span>)</span><br><span class=line>            self.m_conv.register_backward_hook(self._set_lr)</span><br><span class=line> </span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_set_lr</span>(<span class=params>module, grad_input, grad_output</span>):</span></span><br><span class=line>        grad_input = (grad_input[i] * <span class=number>0.1</span> <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(grad_input)))</span><br><span class=line>        grad_output = (grad_output[i] * <span class=number>0.1</span> <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(grad_output)))</span><br><span class=line> </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        offset = self.p_conv(x)</span><br><span class=line>        <span class=keyword>if</span> self.modulation:</span><br><span class=line>            m = torch.sigmoid(self.m_conv(x))</span><br><span class=line> </span><br><span class=line>        dtype = offset.data.<span class=built_in>type</span>()</span><br><span class=line>        ks = self.kernel_size</span><br><span class=line>        N = offset.size(<span class=number>1</span>) // <span class=number>2</span></span><br><span class=line> </span><br><span class=line>        <span class=keyword>if</span> self.padding:</span><br><span class=line>            x = self.zero_padding(x)</span><br><span class=line> </span><br><span class=line>        <span class=comment># (b, 2N, h, w)</span></span><br><span class=line>        p = self._get_p(offset, dtype)</span><br><span class=line> </span><br><span class=line>        <span class=comment># (b, h, w, 2N)</span></span><br><span class=line>        p = p.contiguous().permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>1</span>)</span><br><span class=line>        q_lt = p.detach().floor()</span><br><span class=line>        q_rb = q_lt + <span class=number>1</span></span><br><span class=line> </span><br><span class=line>        q_lt = torch.cat([torch.clamp(q_lt[..., :N], <span class=number>0</span>, x.size(<span class=number>2</span>)-<span class=number>1</span>), torch.clamp(q_lt[..., N:], <span class=number>0</span>, x.size(<span class=number>3</span>)-<span class=number>1</span>)], dim=-<span class=number>1</span>).long()</span><br><span class=line>        q_rb = torch.cat([torch.clamp(q_rb[..., :N], <span class=number>0</span>, x.size(<span class=number>2</span>)-<span class=number>1</span>), torch.clamp(q_rb[..., N:], <span class=number>0</span>, x.size(<span class=number>3</span>)-<span class=number>1</span>)], dim=-<span class=number>1</span>).long()</span><br><span class=line>        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], dim=-<span class=number>1</span>)</span><br><span class=line>        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], dim=-<span class=number>1</span>)</span><br><span class=line> </span><br><span class=line>        <span class=comment># clip p</span></span><br><span class=line>        p = torch.cat([torch.clamp(p[..., :N], <span class=number>0</span>, x.size(<span class=number>2</span>)-<span class=number>1</span>), torch.clamp(p[..., N:], <span class=number>0</span>, x.size(<span class=number>3</span>)-<span class=number>1</span>)], dim=-<span class=number>1</span>)</span><br><span class=line> </span><br><span class=line>        <span class=comment># bilinear kernel (b, h, w, N)</span></span><br><span class=line>        g_lt = (<span class=number>1</span> + (q_lt[..., :N].type_as(p) - p[..., :N])) * (<span class=number>1</span> + (q_lt[..., N:].type_as(p) - p[..., N:]))</span><br><span class=line>        g_rb = (<span class=number>1</span> - (q_rb[..., :N].type_as(p) - p[..., :N])) * (<span class=number>1</span> - (q_rb[..., N:].type_as(p) - p[..., N:]))</span><br><span class=line>        g_lb = (<span class=number>1</span> + (q_lb[..., :N].type_as(p) - p[..., :N])) * (<span class=number>1</span> - (q_lb[..., N:].type_as(p) - p[..., N:]))</span><br><span class=line>        g_rt = (<span class=number>1</span> - (q_rt[..., :N].type_as(p) - p[..., :N])) * (<span class=number>1</span> + (q_rt[..., N:].type_as(p) - p[..., N:]))</span><br><span class=line> </span><br><span class=line>        <span class=comment># (b, c, h, w, N)</span></span><br><span class=line>        x_q_lt = self._get_x_q(x, q_lt, N)</span><br><span class=line>        x_q_rb = self._get_x_q(x, q_rb, N)</span><br><span class=line>        x_q_lb = self._get_x_q(x, q_lb, N)</span><br><span class=line>        x_q_rt = self._get_x_q(x, q_rt, N)</span><br><span class=line> </span><br><span class=line>        <span class=comment># (b, c, h, w, N)</span></span><br><span class=line>        x_offset = g_lt.unsqueeze(dim=<span class=number>1</span>) * x_q_lt + \</span><br><span class=line>                   g_rb.unsqueeze(dim=<span class=number>1</span>) * x_q_rb + \</span><br><span class=line>                   g_lb.unsqueeze(dim=<span class=number>1</span>) * x_q_lb + \</span><br><span class=line>                   g_rt.unsqueeze(dim=<span class=number>1</span>) * x_q_rt</span><br><span class=line> </span><br><span class=line>        <span class=comment># modulation</span></span><br><span class=line>        <span class=keyword>if</span> self.modulation:</span><br><span class=line>            m = m.contiguous().permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>1</span>)</span><br><span class=line>            m = m.unsqueeze(dim=<span class=number>1</span>)</span><br><span class=line>            m = torch.cat([m <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(x_offset.size(<span class=number>1</span>))], dim=<span class=number>1</span>)</span><br><span class=line>            x_offset *= m</span><br><span class=line> </span><br><span class=line>        x_offset = self._reshape_x_offset(x_offset, ks)</span><br><span class=line>        out = self.conv(x_offset)</span><br><span class=line> </span><br><span class=line>        <span class=keyword>return</span> out</span><br><span class=line> </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_get_p_n</span>(<span class=params>self, N, dtype</span>):</span></span><br><span class=line>        <span class=comment># 由于卷积核中心点位置是其尺寸的一半，于是中心点向左（上）方向移动尺寸的一半就得到起始点，向右（下）方向移动另一半就得到终止点</span></span><br><span class=line>        p_n_x, p_n_y = torch.meshgrid(</span><br><span class=line>            torch.arange(-(self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>, (self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>+<span class=number>1</span>),</span><br><span class=line>            torch.arange(-(self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>, (self.kernel_size-<span class=number>1</span>)//<span class=number>2</span>+<span class=number>1</span>))</span><br><span class=line>        <span class=comment># (2N, 1)</span></span><br><span class=line>        p_n = torch.cat([torch.flatten(p_n_x), torch.flatten(p_n_y)], <span class=number>0</span>)</span><br><span class=line>        p_n = p_n.view(<span class=number>1</span>, <span class=number>2</span>*N, <span class=number>1</span>, <span class=number>1</span>).<span class=built_in>type</span>(dtype)</span><br><span class=line> </span><br><span class=line>        <span class=keyword>return</span> p_n</span><br><span class=line> </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_get_p_0</span>(<span class=params>self, h, w, N, dtype</span>):</span></span><br><span class=line>        <span class=comment># p0_y、p0_x就是输出特征图每点映射到输入特征图上的纵、横坐标值。</span></span><br><span class=line>        p_0_x, p_0_y = torch.meshgrid(</span><br><span class=line>            torch.arange(<span class=number>1</span>, h*self.stride+<span class=number>1</span>, self.stride),</span><br><span class=line>            torch.arange(<span class=number>1</span>, w*self.stride+<span class=number>1</span>, self.stride))</span><br><span class=line>        </span><br><span class=line>        p_0_x = torch.flatten(p_0_x).view(<span class=number>1</span>, <span class=number>1</span>, h, w).repeat(<span class=number>1</span>, N, <span class=number>1</span>, <span class=number>1</span>)</span><br><span class=line>        p_0_y = torch.flatten(p_0_y).view(<span class=number>1</span>, <span class=number>1</span>, h, w).repeat(<span class=number>1</span>, N, <span class=number>1</span>, <span class=number>1</span>)</span><br><span class=line>        p_0 = torch.cat([p_0_x, p_0_y], <span class=number>1</span>).<span class=built_in>type</span>(dtype)</span><br><span class=line> </span><br><span class=line>        <span class=keyword>return</span> p_0</span><br><span class=line>    </span><br><span class=line>    <span class=comment># 输出特征图上每点（对应卷积核中心）加上其对应卷积核每个位置的相对（横、纵）坐标后再加上自学习的（横、纵坐标）偏移量。</span></span><br><span class=line>    <span class=comment># p0就是将输出特征图每点对应到卷积核中心，然后映射到输入特征图中的位置；</span></span><br><span class=line>    <span class=comment># pn则是p0对应卷积核每个位置的相对坐标；</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_get_p</span>(<span class=params>self, offset, dtype</span>):</span></span><br><span class=line>        N, h, w = offset.size(<span class=number>1</span>)//<span class=number>2</span>, offset.size(<span class=number>2</span>), offset.size(<span class=number>3</span>)</span><br><span class=line> </span><br><span class=line>        <span class=comment># (1, 2N, 1, 1)</span></span><br><span class=line>        p_n = self._get_p_n(N, dtype)</span><br><span class=line>        <span class=comment># (1, 2N, h, w)</span></span><br><span class=line>        p_0 = self._get_p_0(h, w, N, dtype)</span><br><span class=line>        p = p_0 + p_n + offset</span><br><span class=line>        <span class=keyword>return</span> p</span><br><span class=line> </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_get_x_q</span>(<span class=params>self, x, q, N</span>):</span></span><br><span class=line>        <span class=comment># 计算双线性插值点的4邻域点对应的权重</span></span><br><span class=line>        b, h, w, _ = q.size()</span><br><span class=line>        padded_w = x.size(<span class=number>3</span>)</span><br><span class=line>        c = x.size(<span class=number>1</span>)</span><br><span class=line>        <span class=comment># (b, c, h*w)</span></span><br><span class=line>        x = x.contiguous().view(b, c, -<span class=number>1</span>)</span><br><span class=line> </span><br><span class=line>        <span class=comment># (b, h, w, N)</span></span><br><span class=line>        index = q[..., :N]*padded_w + q[..., N:]  <span class=comment># offset_x*w + offset_y</span></span><br><span class=line>        <span class=comment># (b, c, h*w*N)</span></span><br><span class=line>        index = index.contiguous().unsqueeze(dim=<span class=number>1</span>).expand(-<span class=number>1</span>, c, -<span class=number>1</span>, -<span class=number>1</span>, -<span class=number>1</span>).contiguous().view(b, c, -<span class=number>1</span>)</span><br><span class=line> </span><br><span class=line>        x_offset = x.gather(dim=-<span class=number>1</span>, index=index).contiguous().view(b, c, h, w, N)</span><br><span class=line> </span><br><span class=line>        <span class=keyword>return</span> x_offset</span><br><span class=line> </span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>_reshape_x_offset</span>(<span class=params>x_offset, ks</span>):</span></span><br><span class=line>        b, c, h, w, N = x_offset.size()</span><br><span class=line>        x_offset = torch.cat([x_offset[..., s:s+ks].contiguous().view(b, c, h, w*ks) <span class=keyword>for</span> s <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>0</span>, N, ks)], dim=-<span class=number>1</span>)</span><br><span class=line>        x_offset = x_offset.contiguous().view(b, c, h*ks, w*ks)</span><br><span class=line> </span><br><span class=line>        <span class=keyword>return</span> x_offset</span><br></pre></table></figure><p>这些模型中NiN的1x1conv以及ResetNet的残差作为后面更复杂模型常用的方法,比如U-Net.而UNet,FPN这样的多尺度和残差连接又在许多目标检测等任务中使用.<h2 id=RNN-GRU-LSTM><a title="RNN GRU LSTM" class=headerlink href=#RNN-GRU-LSTM></a>RNN GRU LSTM</h2><p><img alt=../_images/rnn.svg data-src=https://zh.d2l.ai/_images/rnn.svg><p><img alt=../_images/gru-3.svg data-src=https://zh.d2l.ai/_images/gru-3.svg><p><img alt=../_images/lstm-3.svg data-src=https://zh.d2l.ai/_images/lstm-3.svg><h1 id=黑夜前的光明><a class=headerlink href=#黑夜前的光明 title=黑夜前的光明></a>黑夜前的光明</h1><blockquote><p>在transformer之前,我们认为泡沫即将吹破<p><a href=https://www.open-open.com/news/view/448d1219 rel=noopener target=_blank>李开复说2018年是AI泡沫破裂之年 LeCun点赞 - 李开复 - IT业界 - 深度开源 (open-open.com)</a><p><a href=https://www.thepaper.cn/newsDetail_forward_4103098 rel=noopener target=_blank>别吹了，AI的泡沫快被吹破了<em>澎湃号·湃客</em>澎湃新闻-The Paper</a></blockquote><h2 id=Attention><a class=headerlink href=#Attention title=Attention></a>Attention</h2><p><img alt=../_images/seq2seq-attention-details.svg data-src=https://zh.d2l.ai/_images/seq2seq-attention-details.svg><p><img alt=../_images/multi-head-attention.svg data-src=https://zh.d2l.ai/_images/multi-head-attention.svg><h2 id=Transformer><a class=headerlink href=#Transformer title=Transformer></a>Transformer</h2><p><img alt=../_images/transformer.svg data-src=https://zh.d2l.ai/_images/transformer.svg><p><a href=https://nlp.seas.harvard.edu/annotated-transformer/#prelims rel=noopener target=_blank>The Annotated Transformer (harvard.edu)</a><p>从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为sublayer）。第一个子层是<em>多头自注意力</em>（multi-head self-attention）汇聚；第二个子层是<em>基于位置的前馈网络</em>（positionwise feed-forward network）。具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受中残差网络的启发，每个子层都采用了<em>残差连接</em>（residual connection）。在Transformer中，对于序列中任何位置的任何输入𝑥∈𝑅𝑑，都要求满足sublayer(𝑥)∈𝑅𝑑，以便残差连接满足𝑥+sublayer(𝑥)∈𝑅𝑑。在残差连接的加法计算之后，紧接着应用<em>层规范化</em>（layer normalization）<p>因此，输入序列对应的每个位置，Transformer编码器都将输出一个𝑑维表示向量。<p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为<em>编码器－解码器注意力</em>（encoder-decoder attention）层。在编码器－解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。在解码器自注意力中，查询、键和值都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种<em>掩蔽</em>（masked）注意力保留了<em>自回归</em>（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。<blockquote><p>transformer目前是通吃的,在cv,nlp,speech等领域的各种任务上都有实践<p>下面是使用transformer的通用目标检测方法</blockquote><h3 id=DETR><a class=headerlink href=#DETR title=DETR></a>DETR</h3><p><img alt=DETR data-src=https://github.com/facebookresearch/detr/raw/main/.github/DETR.png><p>与传统的计算机视觉技术不同，DETR 将物体检测作为一个直接的集合预测问题(a direct set prediction problem)来处理。它由一个基于集合的全局损失和一个变换器编码器-解码器架构组成，前者通过两端匹配强制进行唯一预测。<p>给定一个固定的小范围已学对象查询集，DETR 会对对象关系和全局图像上下文进行推理，从而直接并行输出最终的预测集。由于这种并行性，DETR 非常快速高效。<p><a href=https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl rel=noopener target=_blank>detr_demo.ipynb - Colab (google.com)</a><h3 id=Deformable-DETR><a title="Deformable DETR" class=headerlink href=#Deformable-DETR></a>Deformable DETR</h3><p><img alt=image-20240612220634309 data-src=https://s2.loli.net/2024/06/12/UdvnyZqNjWB51SH.png><p><img alt=image-20240612220643631 data-src=https://s2.loli.net/2024/06/12/BpS6JWYKnGA5Rty.png><h2 id=Diffusion-Models><a title="Diffusion Models" class=headerlink href=#Diffusion-Models></a>Diffusion Models</h2><blockquote><p>在VAE,GAN之后的生成式之光.</blockquote><p><img alt=image-20240612212239385 data-src=https://s2.loli.net/2024/06/12/XmtBQZG31L4Wi89.png><p>具体代码参看<a href=https://huggingface.co/blog/annotated-diffusion rel=noopener target=_blank>The Annotated Diffusion Model (huggingface.co)</a><ul><li><a href=https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html rel=noopener target=_blank>Diffusion Models as a kind of VAE | Angus Turner</a><li><a href=https://github.com/yangqy1110/Diffusion-Models?tab=readme-ov-file rel=noopener target=_blank>yangqy1110/Diffusion-Models: 扩散模型原理和pytorch代码实现初学资料汇总 (github.com)</a><li><a href=https://github.com/mikonvergence/DiffusionFastForward/tree/master rel=noopener target=_blank>mikonvergence/DiffusionFastForward: DiffusionFastForward: a free course and experimental framework for diffusion-based generative models (github.com)</a><li><a href=https://github.com/diff-usion/Awesome-Diffusion-Models?tab=readme-ov-file rel=noopener target=_blank>diff-usion/Awesome-Diffusion-Models: A collection of resources and papers on Diffusion Models (github.com)</a></ul><h1 id=现代大模型><a class=headerlink href=#现代大模型 title=现代大模型></a>现代大模型</h1><blockquote><p>目前,它是正处于统治地位. 当然,人们也希望有其他方法.</blockquote><h2 id=LLM><a class=headerlink href=#LLM title=LLM></a>LLM</h2><p><img alt=image-20240612203226976 data-src=https://s2.loli.net/2024/06/12/RHxAKYM4NFjnTpu.png><p><img alt=image-20240612203251260 data-src=https://s2.loli.net/2024/06/12/pK6PTi7awbfCEBG.png><p>可以考虑参考Andrej Karpathy的nanoGPT以及GPT2复现.<h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><p>学习Pytorch<a href=https://www.learnpytorch.io/ rel=noopener target=_blank>Zero to Mastery Learn PyTorch for Deep Learning</a><p>学习经典<a href=https://zh.d2l.ai/ rel=noopener target=_blank>《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a><p>学习attention与transformer<ul><li><a href=https://nlp.seas.harvard.edu/annotated-transformer/#prelims rel=noopener target=_blank>The Annotated Transformer (harvard.edu)</a><li><p><a href=https://jalammar.github.io/illustrated-transformer/ rel=noopener target=_blank>The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></p><li><p><a href=https://github.com/lucidrains/vit-pytorch rel=noopener target=_blank>lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch (github.com)</a></p><li><p><a href=https://github.com/changzy00/pytorch-attention rel=noopener target=_blank>changzy00/pytorch-attention: 🦖Pytorch implementation of popular Attention Mechanisms, Vision Transformers, MLP-Like models and CNNs.🔥🔥🔥 (github.com)</a></p><li><a href=https://github.com/xmu-xiaoma666/External-Attention-pytorch rel=noopener target=_blank>xmu-xiaoma666/External-Attention-pytorch: 🍀 Pytorch implementation of various Attention Mechanisms, MLP, Re-parameter, Convolution, which is helpful to further understand papers.⭐⭐⭐ (github.com)</a></ul><p>学习llm<ul><li><p><a href=https://github.com/rasbt/LLMs-from-scratch rel=noopener target=_blank>https://github.com/rasbt/LLMs-from-scratch</a></p><li><p><a href=https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy rel=noopener target=_blank>Let’s build GPT: from scratch, in code, spelled out. (youtube.com)</a></p><li><p><a href=https://github.com/naklecha/llama3-from-scratch rel=noopener target=_blank>naklecha/llama3-from-scratch: llama3 implementation one matrix multiplication at a time (github.com)</a></p><li><a href=https://cyrilzakka.github.io/llm-playbook/index.html rel=noopener target=_blank>Introduction - The Large Language Model Playbook (cyrilzakka.github.io)</a></ul><p>学习扩散模型和大模型的个人博客<ul><li><a href=https://spaces.ac.cn/ rel=noopener target=_blank>科学空间|Scientific Spaces</a><li><a href=https://karpathy.github.io/ rel=noopener target=_blank>Andrej Karpathy blog</a><li><a href=https://lilianweng.github.io/ rel=noopener target=_blank>Lil’Log (lilianweng.github.io)</a></ul><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\11\18\vqvae及其变体代码学习\ rel=bookmark>vqvae及其变体代码学习</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\11\03\文生图相关模型最新进展小结\ rel=bookmark>文生图相关模型最新进展小结</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\09\24\回看深度学习-经典网络学习\ rel=bookmark>回看深度学习:经典网络学习</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\07\30\profile-a-deep-learning-model\ rel=bookmark>profile a deep learning model</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\18\从论文中看AI绘画-二\ rel=bookmark>从论文中看AI绘画(二)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/06/12/myJourneyToAI-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85/ title=myJourneyToAI:深度学习之旅>https://www.sekyoro.top/2024/06/12/myJourneyToAI-深度学习之旅/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/deep-learning/ rel=tag><i class="fa fa-tag"></i> deep learning</a></div><div class=post-nav><div class=post-nav-item><a title="modern cpp learning(三)" href=/2024/06/10/modern-cpp-learning-%E4%B8%89/ rel=prev> <i class="fa fa-chevron-left"></i> modern cpp learning(三) </a></div><div class=post-nav-item><a href=/2024/06/18/%E4%BB%8E%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9C%8BAI%E7%BB%98%E7%94%BB-%E4%BA%8C/ rel=next title=从论文中看AI绘画(二)> 从论文中看AI绘画(二) <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#%E8%9B%AE%E8%8D%92%E6%97%B6%E4%BB%A3><span class=nav-number>1.</span> <span class=nav-text>蛮荒时代</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#%E7%BB%8F%E5%85%B8CNN%E6%A8%A1%E5%9E%8B><span class=nav-number>1.1.</span> <span class=nav-text>经典CNN模型</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#LeNet><span class=nav-number>1.1.1.</span> <span class=nav-text>LeNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#AlexNet><span class=nav-number>1.1.2.</span> <span class=nav-text>AlexNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#VGG><span class=nav-number>1.1.3.</span> <span class=nav-text>VGG</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#NiN><span class=nav-number>1.1.4.</span> <span class=nav-text>NiN</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#GoogleNet><span class=nav-number>1.1.5.</span> <span class=nav-text>GoogleNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#ResNet><span class=nav-number>1.1.6.</span> <span class=nav-text>ResNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#DenseNet><span class=nav-number>1.1.7.</span> <span class=nav-text>DenseNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#U-Net><span class=nav-number>1.1.8.</span> <span class=nav-text>U-Net</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#SSD><span class=nav-number>1.1.9.</span> <span class=nav-text>SSD</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Feature-Pyramid-Networks-for-Object-Detection><span class=nav-number>1.1.10.</span> <span class=nav-text>Feature Pyramid Networks for Object Detection</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Deformable-Conv><span class=nav-number>1.1.11.</span> <span class=nav-text>Deformable Conv</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#RNN-GRU-LSTM><span class=nav-number>1.2.</span> <span class=nav-text>RNN GRU LSTM</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E9%BB%91%E5%A4%9C%E5%89%8D%E7%9A%84%E5%85%89%E6%98%8E><span class=nav-number>2.</span> <span class=nav-text>黑夜前的光明</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Attention><span class=nav-number>2.1.</span> <span class=nav-text>Attention</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Transformer><span class=nav-number>2.2.</span> <span class=nav-text>Transformer</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#DETR><span class=nav-number>2.2.1.</span> <span class=nav-text>DETR</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Deformable-DETR><span class=nav-number>2.2.2.</span> <span class=nav-text>Deformable DETR</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Diffusion-Models><span class=nav-number>2.3.</span> <span class=nav-text>Diffusion Models</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E7%8E%B0%E4%BB%A3%E5%A4%A7%E6%A8%A1%E5%9E%8B><span class=nav-number>3.</span> <span class=nav-text>现代大模型</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#LLM><span class=nav-number>3.1.</span> <span class=nav-text>LLM</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>3.2.</span> <span class=nav-text>参考资料</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>236</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/deep-learning/ rel=tag>deep learning</a><span class=tag-list-count>11</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.4m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>36:57</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>