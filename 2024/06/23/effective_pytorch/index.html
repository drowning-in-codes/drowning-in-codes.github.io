<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch." name=description><meta content=article property=og:type><meta content=使用pytorch时你可能需要注意的地方 property=og:title><meta content=https://www.sekyoro.top/2024/06/23/effective_pytorch/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch." property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png property=og:image><meta content=https://pytorch.org/tutorials/_images/mp_vs_rn.png property=og:image><meta content=2024-06-23T08:16:46.000Z property=article:published_time><meta content=2024-07-11T12:44:39.617Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content=pytorch property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png name=twitter:image><link href=https://www.sekyoro.top/2024/06/23/effective_pytorch/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>使用pytorch时你可能需要注意的地方 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/06/23/effective_pytorch/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>使用pytorch时你可能需要注意的地方</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-06-23 16:16:46" datetime=2024-06-23T16:16:46+08:00>2024-06-23</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-07-11 20:44:39" datetime=2024-07-11T20:44:39+08:00 itemprop=dateModified>2024-07-11</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>40k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>37 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch.<br><span id=more></span><blockquote><p>update:为了更好的理解pytorch,也许可以从零写点代码<a href=https://github.com/karpathy/micrograd rel=noopener target=_blank>karpathy/micrograd: A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API (github.com)</a><p><a href=https://nrehiew.github.io/blog/pytorch/ rel=noopener target=_blank>Taking PyTorch for Granted | wh (nrehiew.github.io)</a></blockquote><h2 id=tensor><a class=headerlink href=#tensor title=tensor></a>tensor</h2><h3 id=Tensor><a class=headerlink href=#Tensor title=Tensor></a>Tensor</h3><p>pytorch默认浮点类型是torch.float32,可以使用<code>torch.set_default_dtype</code>修改<p>torch.zeros等默认类型就是就是torch.float32,使用<code>torch.set_default_dtype</code>修改默认类型.<p>torch.tensor() 总是复制<code>data</code>(深拷贝,表示地址不相同).如果你有一个张量数据,<strong>只是想更改它的 requires<em>grad 标志,请使用 requires_grad</em>() 或 detach() 来避免复制</strong>.<p>如果你有一个 numpy 数组,并<strong>希望避免复制,请使用 torch.as_tensor()</strong>.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>torch.device(<span class=string>'cuda:0'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'cpu'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'mps'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'cuda'</span>)  <span class=comment># current cuda device</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([[<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>4</span>, <span class=number>5</span>], [<span class=number>6</span>, <span class=number>7</span>, <span class=number>8</span>, <span class=number>9</span>, <span class=number>10</span>]])</span><br><span class=line>x.stride() <span class=comment>#(5,1)</span></span><br><span class=line></span><br><span class=line>x.t().stride() <span class=comment>#(1,5)</span></span><br></pre></table></figure><h3 id=Views><a class=headerlink href=#Views title=Views></a>Views</h3><p>PyTorch 允许张量成为现有张量的 “views”.<strong>视图张量与其基础张量共享相同的底层数据</strong>.支持 “views “可以避免显式数据复制,从而使我们能够进行快速、高效的内存重塑、切片和元素操作.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>t = torch.rand(<span class=number>4</span>, <span class=number>4</span>)</span><br><span class=line>b = t.view(<span class=number>2</span>, <span class=number>8</span>)</span><br><span class=line>t.storage().data_ptr() == b.storage().data_ptr()  <span class=comment># `t` and `b` share the same underlying data.</span></span><br><span class=line>b[<span class=number>0</span>][<span class=number>0</span>] = <span class=number>3.14</span></span><br><span class=line>t[<span class=number>0</span>][<span class=number>0</span>]</span><br></pre></table></figure><p>由于views与其基础张量共享底层数据,因此如果修改views中的数据,也会反映在基础张量中.<p>通常,PyTorch 操作会返回一个新的张量作为输出,例如 add().但在视图操作中,输出是输入张量的视图,以避免不必要的数据复制.创建视图时不会发生数据移动,视图张量只是改变了解释相同数据的方式.<p><strong>对连续张量进行视图处理可能会产生非连续张量</strong>.transpose() 就是一个常见的例子.(包括view,transpose等操作都会返回view,也就是数据存储与输入相同)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>base = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>],[<span class=number>2</span>, <span class=number>3</span>]])</span><br><span class=line>base.is_contiguous()</span><br><span class=line>t = base.transpose(<span class=number>0</span>, <span class=number>1</span>)  <span class=comment># `t` is a view of `base`. No data movement happened here.</span></span><br><span class=line>t.is_contiguous()</span><br><span class=line>c = t.contiguous()</span><br></pre></table></figure><p><img alt=image-20240625212647753 data-src=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png><p><a href=https://zhuanlan.zhihu.com/p/342856639 rel=noopener target=_blank>通过公式判断张量是否连续 - 知乎 (zhihu.com)</a><h2 id=Extending-PyTorch><a title="Extending PyTorch" class=headerlink href=#Extending-PyTorch></a>Extending PyTorch</h2><p>原文<a href=https://pytorch.org/docs/stable/notes/extending.html rel=noopener target=_blank>Extending PyTorch — PyTorch 2.3 documentation</a><h3 id=extending-torch-autograd><a title="extending torch.autograd" class=headerlink href=#extending-torch-autograd></a>extending torch.autograd</h3><p>为 autograd 添加操作需要为每个操作实现一个新的 Function 子类.<h5 id=如何使用><a class=headerlink href=#如何使用 title=如何使用></a>如何使用</h5><p>一般来说,如果想在模型中<strong>执行不可微分的计算或依赖非 PyTorch 库</strong>（如 NumPy）,但仍希望您的操作能与其他操作连锁并与 autograd 引擎一起工作,那么请使用自定义函数.<p>在某些情况下,也可以<strong>使用自定义函数来提高性能和内存使用率</strong>: 如果您使用 C++ 扩展实现了前向和后向传递,您可以将它们封装在 Function 中,以便与 autograd 引擎对接.如果您想减少为后向传递保存的缓冲区数量,可以使用自定义函数将操作组合在一起.<p>如果想在后向传递过程中改变梯度或执行副作用，可以考虑register一个张量或模块hook<h5 id=什么时候不用><a class=headerlink href=#什么时候不用 title=什么时候不用></a>什么时候不用</h5><p>如果已经可以用 PyTorch 的内置操作来编写函数,那么它的反向图（很可能）已经可以被 autograd 记录下来.在这种情况下,不需要自己实现后向函数.可以考虑使用一个普通的 Python 函数。<p>如果需要维护状态,即可训练参数,则应（也可以）使用自定义模块torch.nn.<p>如果想在后向传递过程中改变梯度或执行副作用,可以考虑注册一个张量或模块钩子。<blockquote><p>注意,我在看pytorch2.3时 register_backward_hook已经deprecated了,使用register_full_backward_hook</blockquote><p>使用一个<code>register_full_backward_hook</code>将梯度变为相反数.<p><code>hook(module, grad_input, grad_output) -> tuple(Tensor) 或 None</code><br>grad_input 和 grad_output 是元组，分别包含<strong>相对于输入和输出的梯度</strong>。<strong>钩子不应修改其参数</strong>,但<strong>可以选择返回一个新的相对于输入的梯度，该梯度将在后续计算中代替 grad_input</strong>.对于所有非张量参数，grad_input 和 grad_output 中的条目均为 “None”.<p>如果想在后向传递过程中改变梯度或执行副作用，可以考虑注册一个张量或模块钩子.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>backward_hook</span>(<span class=params>module, grad_input, grad_output</span>):</span></span><br><span class=line>    output_grad_input = - grad_input[<span class=number>0</span>]</span><br><span class=line>    <span class=keyword>return</span> (output_grad_input,)</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>negGradient</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(negGradient, self).__init__()</span><br><span class=line>        self.register_full_backward_hook(backward_hook)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p>在domain adaptation的早期论文比如DANN中,一般会使用<code>Function</code>进行梯度变为负数,其实也可以注册backward的hook实现.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>_GradReverseLayer</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>ctx, x, constant</span>):</span></span><br><span class=line>        <span class=keyword>assert</span> <span class=built_in>isinstance</span>(constant, <span class=built_in>int</span>) <span class=keyword>and</span> constant > <span class=number>0</span></span><br><span class=line>        ctx.constant = constant</span><br><span class=line>        <span class=keyword>return</span> x.view_as(x)</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        <span class=keyword>return</span> grad_output.neg() * ctx.constant, <span class=literal>None</span></span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>GradReverseLayer</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, weight</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(GradReverseLayer, self).__init__()</span><br><span class=line>        self.weight = weight</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=keyword>return</span> _GradReverseLayer.apply(x, self.weight)</span><br></pre></table></figure><p>既然介绍了<code>register_full_backward_hook</code>,再说说<code>register_forward_hook</code>,每次 forward() 计算完输出后，都会调用该钩子。如果 with_kwargs 为 False 或未指定，输入将只包含给模块的位置参数。关键字参数不会传递给钩子，只会传递给 forward。<strong>钩子可以修改输出.钩子可以就地修改输入,但不会对 forward 产生影响</strong>,因为钩子是在调用 forward() 之后才调用的.<p>可以看看这篇文章<a href=https://blog.csdn.net/m0_51661400/article/details/135091359 rel=noopener target=_blank>深入理解PyTorch中的Hook机制：特征可视化的重要工具与实践-CSDN博客</a><h5 id=使用方法><a class=headerlink href=#使用方法 title=使用方法></a>使用方法</h5><p>采取以下步骤 1. 继承类 Function 并实现 forward()、（可选）setup_context() 和 backward() 方法。2. 在 ctx 参数上调用适当的方法。3. 声明您的函数是否支持 double backward。4. 使用 gradcheck 验证梯度是否正确。<p>step1:<ol><li>forward() 是执行操作的代码。它可以接受任意多个参数,如果指定默认值,其中一些参数是可选的.在调用之前,跟踪历史的张量参数（即 requires_grad=True）将被转换为不跟踪历史的参数,它们的使用将被记录在图中。请注意,此逻辑不会遍历列表/数据集/任何其他数据结构,只会考虑作为调用直接参数的张量.您可以返回一个张量输出，如果有多个输出，也可以返回一个张量元组。<li>setup_context()（可选）。可以编写一个接受 ctx 对象的 “组合 “forward()，或者（从 PyTorch 2.0 开始）编写一个不接受 ctx 的单独 forward()，以及一个用于修改 ctx 的 setup_context()方法。forward() 应该具有计算功能，而 <strong>setup_context() 应该只负责修改 ctx（而不具有任何计算功能</strong>）。一般来说，独立的 forward() 和 setup_context()更接近 PyTorch 本机操作的工作方式，因此更容易与各种 PyTorch 子系统兼容。<li>backward()（或 vjp()）定义梯度公式。<strong>输出有多少个张量参数，它就有多少个张量参数，每个张量参数都代表该输出的梯度</strong>。<strong>切勿就地修改这些参数。它应该返回与输入相同数量的张量，其中每个张量都包含对应输入的梯度。</strong>如果输入不需要梯度（needs_input_grad 是一个布尔元组，表示每个输入是否需要梯度计算）,或者是非张量对象，则可以返回 python:None。此外,如果 forward() 的参数是可选的,只要它们都是 None,返回的梯度值就会多于输入值。</ol><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Exp</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>ctx, i</span>):</span></span><br><span class=line>        result = i.exp()</span><br><span class=line>        ctx.save_for_backward(result)</span><br><span class=line>        <span class=keyword>return</span> result</span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        result, = ctx.saved_tensors</span><br><span class=line>        <span class=keyword>return</span> grad_output * result</span><br><span class=line><span class=comment># Use it by calling the apply method:</span></span><br><span class=line>output = Exp.apply(<span class=built_in>input</span>)</span><br></pre></table></figure><p>step 2：正确使用 ctx 中的函数，以确保新函数在 autograd 引擎中正常工作。<p>ctx上有许多方法可用于调用,比较多的就是<code>save_for_backward</code><blockquote><p>必须<strong>使用 save_for_backward()来保存要在后向传递中使用的张量</strong>。<strong>非张量应直接保存在 ctx 上</strong>。如果既不是输入也不是输出的张量被保存,那函数函数可能不支持double backward 。</blockquote><p>此外还有<code>set_materialize_grads</code><blockquote><p>set_materialize_grads()可以用来告诉 autograd 引擎，在输出不依赖于输入的情况下，<strong>通过不对后向函数中的梯度张量进行实体化来优化梯度计算</strong>。</blockquote><p>step3:如果函数不支持double backward ，则应通过使用 once_differentiable() 对逆运算进行装饰来明确声明这一点。使用此装饰器后，通过函数执行double backward 的尝试将产生错误。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>torch.autograd.gradcheck(Exp.apply, x)</span><br></pre></table></figure><p>step4:建议使用 torch.autograd.gradcheck() 检查后向函数是否能正确计算前向梯度,方法是使用后向函数计算雅各布矩阵,并将该值与使用有限差分法数值计算的雅各布值进行逐元素比较。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br></pre><td class=code><pre><span class=line><span class=comment># Inherit from Function</span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>LinearFunction</span>(<span class=params>Function</span>):</span></span><br><span class=line></span><br><span class=line>    <span class=comment># Note that forward, setup_context, and backward are @staticmethods</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params><span class=built_in>input</span>, weight, bias</span>):</span></span><br><span class=line>        output = <span class=built_in>input</span>.mm(weight.t())</span><br><span class=line>        <span class=keyword>if</span> bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>            output += bias.unsqueeze(<span class=number>0</span>).expand_as(output)</span><br><span class=line>        <span class=keyword>return</span> output</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=comment># inputs is a Tuple of all of the inputs passed to forward.</span></span><br><span class=line>    <span class=comment># output is the output of the forward().</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>setup_context</span>(<span class=params>ctx, inputs, output</span>):</span></span><br><span class=line>        <span class=built_in>input</span>, weight, bias = inputs</span><br><span class=line>        ctx.save_for_backward(<span class=built_in>input</span>, weight, bias)</span><br><span class=line></span><br><span class=line>    <span class=comment># This function has only a single output, so it gets only one gradient</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        <span class=comment># This is a pattern that is very convenient - at the top of backward</span></span><br><span class=line>        <span class=comment># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span></span><br><span class=line>        <span class=comment># None. Thanks to the fact that additional trailing Nones are</span></span><br><span class=line>        <span class=comment># ignored, the return statement is simple even when the function has</span></span><br><span class=line>        <span class=comment># optional inputs.</span></span><br><span class=line>        <span class=built_in>input</span>, weight, bias = ctx.saved_tensors</span><br><span class=line>        grad_input = grad_weight = grad_bias = <span class=literal>None</span></span><br><span class=line></span><br><span class=line>        <span class=comment># These needs_input_grad checks are optional and there only to</span></span><br><span class=line>        <span class=comment># improve efficiency. If you want to make your code simpler, you can</span></span><br><span class=line>        <span class=comment># skip them. Returning gradients for inputs that don't require it is</span></span><br><span class=line>        <span class=comment># not an error.</span></span><br><span class=line>        <span class=keyword>if</span> ctx.needs_input_grad[<span class=number>0</span>]:</span><br><span class=line>            grad_input = grad_output.mm(weight)</span><br><span class=line>        <span class=keyword>if</span> ctx.needs_input_grad[<span class=number>1</span>]:</span><br><span class=line>            grad_weight = grad_output.t().mm(<span class=built_in>input</span>)</span><br><span class=line>        <span class=keyword>if</span> bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span> <span class=keyword>and</span> ctx.needs_input_grad[<span class=number>2</span>]:</span><br><span class=line>            grad_bias = grad_output.<span class=built_in>sum</span>(<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> grad_input, grad_weight, grad_bias</span><br></pre></table></figure><p>上面这个例子已经写得很好了.为了更方便地使用这些自定义操作，<strong>建议将它们别名或封装在一个函数中。</strong>使用函数封装可以让我们支持默认参数和关键字参数<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line><span class=comment># Option 1: alias</span></span><br><span class=line>linear = LinearFunction.apply</span><br><span class=line></span><br><span class=line><span class=comment># Option 2: wrap in a function, to support default args and keyword args.</span></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>linear</span>(<span class=params><span class=built_in>input</span>, weight, bias=<span class=literal>None</span></span>):</span></span><br><span class=line>    <span class=keyword>return</span> LinearFunction.apply(<span class=built_in>input</span>, weight, bias)</span><br></pre></table></figure><p>此外还有输入没有tensor的情况,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MulConstant</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>tensor, constant</span>):</span></span><br><span class=line>        <span class=keyword>return</span> tensor * constant</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>setup_context</span>(<span class=params>ctx, inputs, output</span>):</span></span><br><span class=line>        <span class=comment># ctx is a context object that can be used to stash information</span></span><br><span class=line>        <span class=comment># for backward computation</span></span><br><span class=line>        tensor, constant = inputs</span><br><span class=line>        ctx.constant = constant <span class=comment># 注意这里直接使用ctx.xx = xx</span></span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        <span class=comment># We return as many input gradients as there were arguments.</span></span><br><span class=line>        <span class=comment># Gradients of non-Tensor arguments to forward must be None.</span></span><br><span class=line>        <span class=keyword>return</span> grad_output * ctx.constant, <span class=literal>None</span></span><br><span class=line>  <span class=comment># 上面代码可以改为 使用set_materialize_grads,因为计算梯度不需要tensor.</span></span><br><span class=line>  <span class=class><span class=keyword>class</span> <span class=title>MulConstant</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>tensor, constant</span>):</span></span><br><span class=line>        <span class=keyword>return</span> tensor * constant</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>setup_context</span>(<span class=params>ctx, inputs, output</span>):</span></span><br><span class=line>        tensor, constant = inputs</span><br><span class=line>        ctx.set_materialize_grads(<span class=literal>False</span>)</span><br><span class=line>        ctx.constant = constant</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        <span class=comment># Here we must handle None grad_output tensor. In this case we</span></span><br><span class=line>        <span class=comment># can skip unnecessary computations and just return None.</span></span><br><span class=line>        <span class=keyword>if</span> grad_output <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>            <span class=keyword>return</span> <span class=literal>None</span>, <span class=literal>None</span></span><br><span class=line></span><br><span class=line>        <span class=comment># We return as many input gradients as there were arguments.</span></span><br><span class=line>        <span class=comment># Gradients of non-Tensor arguments to forward must be None.</span></span><br><span class=line>        <span class=keyword>return</span> grad_output * ctx.constant, <span class=literal>None</span></span><br><span class=line> </span><br><span class=line>        </span><br></pre></table></figure><p>如果需要保存在 forward() 中计算的任何 “中间 “张量，必须将它们作为输出返回，或者将 forward 和 setup_context()合并.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyCube</span>(<span class=params>torch.autograd.Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>x</span>):</span></span><br><span class=line>        <span class=comment># We wish to save dx for backward. In order to do so, it must</span></span><br><span class=line>        <span class=comment># be returned as an output.</span></span><br><span class=line>        dx = <span class=number>3</span> * x ** <span class=number>2</span></span><br><span class=line>        result = x ** <span class=number>3</span></span><br><span class=line>        <span class=keyword>return</span> result, dx</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>setup_context</span>(<span class=params>ctx, inputs, output</span>):</span></span><br><span class=line>        x, = inputs</span><br><span class=line>        result, dx = output</span><br><span class=line>        ctx.save_for_backward(x, dx)</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output, grad_dx</span>):</span></span><br><span class=line>        x, dx = ctx.saved_tensors</span><br><span class=line>        <span class=comment># In order for the autograd.Function to work with higher-order</span></span><br><span class=line>        <span class=comment># gradients, we must add the gradient contribution of `dx`,</span></span><br><span class=line>        <span class=comment># which is grad_dx * 6 * x.</span></span><br><span class=line>        result = grad_output * dx + grad_dx * <span class=number>6</span> * x</span><br><span class=line>        <span class=keyword>return</span> result</span><br><span class=line></span><br><span class=line><span class=comment># Wrap MyCube in a function so that it is clearer what the output is</span></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>my_cube</span>(<span class=params>x</span>):</span></span><br><span class=line>    result, dx = MyCube.apply(x)</span><br><span class=line>    <span class=keyword>return</span> result</span><br></pre></table></figure><p>将forward和setup_context合在一起<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>LinearFunction</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=comment># ctx is the first argument to forward</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>ctx, <span class=built_in>input</span>, weight, bias=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=comment># The forward pass can use ctx.</span></span><br><span class=line>        ctx.save_for_backward(<span class=built_in>input</span>, weight, bias)</span><br><span class=line>        output = <span class=built_in>input</span>.mm(weight.t())</span><br><span class=line>        <span class=keyword>if</span> bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>            output += bias.unsqueeze(<span class=number>0</span>).expand_as(output)</span><br><span class=line>        <span class=keyword>return</span> output</span><br><span class=line></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        <span class=built_in>input</span>, weight, bias = ctx.saved_tensors</span><br><span class=line>        grad_input = grad_weight = grad_bias = <span class=literal>None</span></span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> ctx.needs_input_grad[<span class=number>0</span>]:</span><br><span class=line>            grad_input = grad_output.mm(weight)</span><br><span class=line>        <span class=keyword>if</span> ctx.needs_input_grad[<span class=number>1</span>]:</span><br><span class=line>            grad_weight = grad_output.t().mm(<span class=built_in>input</span>)</span><br><span class=line>        <span class=keyword>if</span> bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span> <span class=keyword>and</span> ctx.needs_input_grad[<span class=number>2</span>]:</span><br><span class=line>            grad_bias = grad_output.<span class=built_in>sum</span>(<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> grad_input, grad_weight, grad_bias</span><br></pre></table></figure><h3 id=extending-torch-nn><a title="extending torch.nn" class=headerlink href=#extending-torch-nn></a>extending torch.nn</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Linear</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, input_features, output_features, bias=<span class=literal>True</span></span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.input_features = input_features</span><br><span class=line>        self.output_features = output_features</span><br><span class=line></span><br><span class=line>        <span class=comment># nn.Parameter is a special kind of Tensor, that will get</span></span><br><span class=line>        <span class=comment># automatically registered as Module's parameter once it's assigned</span></span><br><span class=line>        <span class=comment># as an attribute. Parameters and buffers need to be registered, or</span></span><br><span class=line>        <span class=comment># they won't appear in .parameters() (doesn't apply to buffers), and</span></span><br><span class=line>        <span class=comment># won't be converted when e.g. .cuda() is called. You can use</span></span><br><span class=line>        <span class=comment># .register_buffer() to register buffers.</span></span><br><span class=line>        <span class=comment># nn.Parameters require gradients by default.</span></span><br><span class=line>        self.weight = nn.Parameter(torch.empty(output_features, input_features))</span><br><span class=line>        <span class=keyword>if</span> bias:</span><br><span class=line>            self.bias = nn.Parameter(torch.empty(output_features))</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            <span class=comment># You should always register all possible parameters, but the</span></span><br><span class=line>            <span class=comment># optional ones can be None if you want.</span></span><br><span class=line>            self.register_parameter(<span class=string>'bias'</span>, <span class=literal>None</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># Not a very smart way to initialize weights</span></span><br><span class=line>        nn.init.uniform_(self.weight, -<span class=number>0.1</span>, <span class=number>0.1</span>)</span><br><span class=line>        <span class=keyword>if</span> self.bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span>:</span><br><span class=line>            nn.init.uniform_(self.bias, -<span class=number>0.1</span>, <span class=number>0.1</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, <span class=built_in>input</span></span>):</span></span><br><span class=line>        <span class=comment># See the autograd section for explanation of what happens here.</span></span><br><span class=line>        <span class=keyword>return</span> LinearFunction.apply(<span class=built_in>input</span>, self.weight, self.bias)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>extra_repr</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=comment># (Optional)Set the extra information about this module. You can test</span></span><br><span class=line>        <span class=comment># it by printing an object of this class.</span></span><br><span class=line>        <span class=keyword>return</span> <span class=string>'input_features={}, output_features={}, bias={}'</span>.<span class=built_in>format</span>(</span><br><span class=line>            self.input_features, self.output_features, self.bias <span class=keyword>is</span> <span class=keyword>not</span> <span class=literal>None</span></span><br><span class=line>        )</span><br></pre></table></figure><p>可以通过定义具有与 Tensor 匹配的方法的自定义类来创建模拟 Tensor 的自定义类型。如果自定义 Python 类型定义了名为<code>__torch_function__</code>的方法，当您的自定义类的实例<strong>被传递给 torch 命名空间中的函数时</strong>，PyTorch 将调用您的 <code>__torch_function__</code>实现。这使得为 torch 命名空间中的任何函数定义自定义实现成为可能，<code>__torch_function__</code>实现可以调用这些函数，从而允许您的用户在他们已经为 Tensor 编写的现有 PyTorch 工作流中使用您的自定义类型。<p>这适用于<strong>与 Tensor 无关的 “duck “类型</strong>以及 <strong>Tensor 子类</strong>。<h5 id=Extending-torch-Tensor-like-type><a title="Extending torch Tensor-like type" class=headerlink href=#Extending-torch-Tensor-like-type></a>Extending torch Tensor-like type</h5><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line>HANDLED_FUNCTIONS = {}</span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ScalarTensor</span>(<span class=params><span class=built_in>object</span></span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, N, value</span>):</span></span><br><span class=line>        self._N = N</span><br><span class=line>        self._value = value</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__repr__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> <span class=string>"ScalarTensor(N={}, value={})"</span>.<span class=built_in>format</span>(self._N, self._value)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>tensor</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self._value * torch.eye(self._N)</span><br><span class=line></span><br><span class=line><span class=meta>    @classmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__torch_function__</span>(<span class=params>cls, func, types, args=(<span class=params></span>), kwargs=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=keyword>if</span> kwargs <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>            kwargs = {}</span><br><span class=line>        <span class=keyword>if</span> func <span class=keyword>not</span> <span class=keyword>in</span> HANDLED_FUNCTIONS <span class=keyword>or</span> <span class=keyword>not</span> <span class=built_in>all</span>(</span><br><span class=line>            <span class=built_in>issubclass</span>(t, (torch.Tensor, ScalarTensor))</span><br><span class=line>            <span class=keyword>for</span> t <span class=keyword>in</span> types</span><br><span class=line>        ):</span><br><span class=line>            <span class=keyword>return</span> <span class=literal>NotImplemented</span></span><br><span class=line>        <span class=keyword>return</span> HANDLED_FUNCTIONS[func](*args, **kwargs)</span><br></pre></table></figure><p>为 ScalarTensor 添加 <code>__torch_function__</code> 实现后,上述操作就有可能成功.这次添加一个<code>__torch_function__</code> 实现<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line>HANDLED_FUNCTIONS = {}</span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ScalarTensor</span>(<span class=params><span class=built_in>object</span></span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, N, value</span>):</span></span><br><span class=line>        self._N = N</span><br><span class=line>        self._value = value</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__repr__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> <span class=string>"ScalarTensor(N={}, value={})"</span>.<span class=built_in>format</span>(self._N, self._value)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>tensor</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self._value * torch.eye(self._N)</span><br><span class=line></span><br><span class=line><span class=meta>    @classmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__torch_function__</span>(<span class=params>cls, func, types, args=(<span class=params></span>), kwargs=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=keyword>if</span> kwargs <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>            kwargs = {}</span><br><span class=line>        <span class=keyword>if</span> func <span class=keyword>not</span> <span class=keyword>in</span> HANDLED_FUNCTIONS <span class=keyword>or</span> <span class=keyword>not</span> <span class=built_in>all</span>(</span><br><span class=line>            <span class=built_in>issubclass</span>(t, (torch.Tensor, ScalarTensor))</span><br><span class=line>            <span class=keyword>for</span> t <span class=keyword>in</span> types</span><br><span class=line>        ):</span><br><span class=line>            <span class=keyword>return</span> <span class=literal>NotImplemented</span></span><br><span class=line>        <span class=keyword>return</span> HANDLED_FUNCTIONS[func](*args, **kwargs)</span><br></pre></table></figure><p><code>__torch_function__</code>方法需要四个参数:func,对要重载的 torch API 函数的引用；types，实现 <code>__torch_function__</code>的 Tensor-likes 类型列表；args,传递给函数的参数元组；kwargs,传递给函数的关键字参数 dict,它使用名为 HANDLED_FUNCTIONS 的全局调度表来存储自定义实现.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> functools</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>implements</span>(<span class=params>torch_function</span>):</span></span><br><span class=line>    <span class=string>"""Register a torch function override for ScalarTensor"""</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>decorator</span>(<span class=params>func</span>):</span></span><br><span class=line>        functools.update_wrapper(func, torch_function)</span><br><span class=line>        HANDLED_FUNCTIONS[torch_function] = func</span><br><span class=line>        <span class=keyword>return</span> func</span><br><span class=line>    <span class=keyword>return</span> decorator</span><br><span class=line></span><br><span class=line><span class=meta>@implements(<span class=params>torch.mean</span>)</span></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>mean</span>(<span class=params><span class=built_in>input</span></span>):</span></span><br><span class=line>    <span class=keyword>return</span> <span class=built_in>float</span>(<span class=built_in>input</span>._value) / <span class=built_in>input</span>._N</span><br><span class=line></span><br><span class=line>d = ScalarTensor(<span class=number>5</span>, <span class=number>2</span>)</span><br><span class=line>torch.mean(d)</span><br></pre></table></figure><p>从 1.7.0 版开始，应用于 torch.Tensor 子类的 <strong>torch.Tensor 方法</strong>和<strong>公共 torch.* 命名空间中的函数</strong>将返回子类实例，而不是 torch.Tensor 实例.<p>如果希望对所有张量方法进行全局覆盖,可以使用<code>__torch_function__</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>LoggingTensor</span>(<span class=params>torch.Tensor</span>):</span></span><br><span class=line><span class=meta>    @classmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__torch_function__</span>(<span class=params>cls, func, types, args=(<span class=params></span>), kwargs=<span class=literal>None</span></span>):</span></span><br><span class=line>        <span class=comment># <span class=doctag>NOTE:</span> Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion</span></span><br><span class=line>        <span class=keyword>if</span> func <span class=keyword>is</span> <span class=keyword>not</span> torch.Tensor.__repr__:</span><br><span class=line>            logging.info(<span class=string>f"func: <span class=subst>{func.__name__}</span>, args: <span class=subst>{args!r}</span>, kwargs: <span class=subst>{kwargs!r}</span>"</span>)</span><br><span class=line>        <span class=keyword>if</span> kwargs <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>            kwargs = {}</span><br><span class=line>        <span class=keyword>return</span> <span class=built_in>super</span>().__torch_function__(func, types, args, kwargs)</span><br></pre></table></figure><p>在子类的<code>__torch_function__</code> 中应注意始终调用 super().<strong>torch_function</strong>(func,…)，而不是直接调用 func。如果不这样做，可能会导致 func 返回到 <code>__torch_function__</code>中，从而引起无限递归。<h2 id=torch-autograd><a class=headerlink href=#torch-autograd title=torch.autograd></a>torch.autograd</h2><p>torch.autograd 提供了实现任意标量值函数自动微分的类和函数.<p>它只需对现有代码做极少的改动—你只需用 requires_grad=True 关键字声明需要计算梯度的张量.<p>目前只持浮点张量类型（半浮点、浮点、双浮点和 bfloat16）和复合张量类型（cfloat、cdouble）的 autograd.<h3 id=detach-计算图与leaf-tensor><a title="detach 计算图与leaf tensor" class=headerlink href=#detach-计算图与leaf-tensor></a>detach 计算图与leaf tensor</h3><p><code>Tensor.detach()</code>返回一个从当前计算图中分离出来的新张量,生成的张量永远不需要梯度,目前替代了<code>.data</code>方法.<p>PyTorch 中,计算图(Computation Graph)是一个非常重要的概念.它是一种用于表示和执行机器学习模型的数据结构.<p>具体来说,PyTorch 中的计算图由以下几个关键组件组成:<ol><li><strong>张量(Tensor)</strong>:计算图的基本单元,表示输入数据、中间结果和最终输出.<li><strong>操作(Operation)</strong>:在张量上执行的各种数学运算,如加法、乘法、卷积等.<li><strong>节点(Node)</strong>:表示张量和操作,计算图由这些节点组成.<li><strong>边(Edge)</strong>:表示节点之间的依赖关系,数据沿着边流动.</ol><p>当在 PyTorch 中定义和执行机器学习模型时,PyTorch 会自动构建一个计算图来表示模型的结构和数据流.这个计算图可以用于以下几个方面:<ol><li><strong>前向传播</strong>:通过计算图,PyTorch 可以自动计算模型的输出.<li><strong>反向传播</strong>:当您调用 <code>loss.backward()</code> 时,PyTorch 会沿着计算图反向传播梯度,从而更新模型参数.<li><strong>可视化</strong>:您可以使用 PyTorch 提供的工具(如 TensorBoard)来可视化计算图,更好地理解模型的结构.<li><strong>优化</strong>:PyTorch 的优化器会利用计算图的结构来提高优化效率.</ol><p>detach使得tensor从计算图中分离具体是什么含义?简单来说,使得它本身requires_grad=False,它之前的计算也被阻断了.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line><span class=built_in>input</span> = torch.randn(<span class=number>1</span>, <span class=number>20</span>, <span class=number>10</span>)</span><br><span class=line>model = nn.Linear(<span class=number>10</span>, <span class=number>3</span>)</span><br><span class=line>inter = model(<span class=built_in>input</span>)</span><br><span class=line>model2 = nn.Linear(<span class=number>3</span>, <span class=number>1</span>)</span><br><span class=line>output = model2(inter.detach())</span><br><span class=line>loss = torch.mean(output - <span class=number>1</span>)</span><br><span class=line>loss.backward()</span><br><span class=line><span class=built_in>print</span>(model.weight.grad) <span class=comment># None</span></span><br></pre></table></figure><p>按照惯例,所有<strong>requires_grad 为False的张量都是leaf tensor</strong>.<p>对于<strong>requires_grad 为 True 的张量,如果它们是由用户创建(没有经过计算,包括移动到GPU的操作)的,那么它们将是叶子张量</strong>.这意味着它们不是操作的结果,因此 grad_fn 为 None.<p>只有叶子张量才会在调用 backward() 时被填充梯度.要为非叶子张量填充阶值,可以使用 retain_grad().<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line>xx = torch.randn(<span class=number>1</span>, <span class=number>3</span>).requires_grad_(<span class=literal>True</span>)</span><br><span class=line><span class=built_in>print</span>(xx.grad_fn, xx.is_leaf) <span class=comment># None,True</span></span><br><span class=line>model = nn.Linear(<span class=number>3</span>, <span class=number>1</span>)</span><br><span class=line>output = model(xx)</span><br><span class=line>loss = torch.mean(output - <span class=number>1</span>)</span><br><span class=line>loss.backward()</span><br><span class=line><span class=built_in>print</span>(xx.grad_fn, xx.grad) <span class=comment># None,tensor([[.., ..,  ..]])</span></span><br><span class=line><span class=built_in>print</span>(model.weight.grad_fn, model.weight.grad) <span class=comment># None,,tensor([[.., ..,  ..]])</span></span><br><span class=line><span class=built_in>print</span>(model.bias.grad_fn, model.bias.grad) <span class=comment># None,tensor([1])</span></span><br><span class=line><span class=built_in>print</span>(model.weight.is_leaf, model.bias.is_leaf) <span class=comment># True,True</span></span><br></pre></table></figure><p>只能获取计算图中叶子节点的梯度属性,这些节点的 requires_grad 属性设置为 True.对于图中的所有其他节点,梯度属性将不可用.<p>出于性能考虑,我们只能在给定图形上使用一次后向操作执行梯度计算.如果我们需要在同一图形上执行多次 backward 调用,则需要向 backward 调用传递 retain_graph=True 属性.<p>几个问题:<p>leaf tensor的grad_fn一定为空吗? 不一定,用户创建的requires_grad为True的tensor的grad_fn不为空<p>leaf tensor一定是模型输入吗?不一定,事实上直接创建一个模型,它的weight和bias也是leaf tensor<h3 id=属于旧时代的Variable和data><a class=headerlink href=#属于旧时代的Variable和data title=属于旧时代的Variable和data></a>属于旧时代的Variable和data</h3><p>Variable API 已被弃用:使用张量时,不再需要Variable.如果 requires_grad 设置为 True,Autograd 将自动支持张量.<p>Variable(tensor) 和 Variable(tensor, requires_grad) 仍按预期工作,但它们返回的是张量而不是变量.<p>var.data 与 tensor.data 相同.<p>var.backward()、var.detach()、var.register_hook() 等方法现在可以在具有相同方法名的张量上运行.<p>此外,现在还可以使用 torch.randn()、torch.zeros()、torch.none() 等工厂方法创建 requires_grad=True 的张量:<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>autograd_tensor = torch.randn((<span class=number>2</span>, <span class=number>3</span>, <span class=number>4</span>), requires_grad=<span class=literal>True</span>)</span><br></pre></table></figure><div class=table-container><table><thead><tr><th>api<th>介绍<tbody><tr><td><code>torch.Tensor.grad</code><td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a href=https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward rel=noopener target=_blank><code>backward()</code></a> computes gradients for <code>self</code><tr><td><code>torch.Tensor.requires_grad</code><td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.<tr><td><code>torch.Tensor.is_leaf</code><td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.<tr><td><code>torch.Tensor.backward</code>([gradient, …])<td>Computes the gradient of current tensor wrt graph leaves.<tr><td><code>torch.Tensor.detach</code><td>Returns a new Tensor, detached from the current graph.<tr><td><code>torch.Tensor.detach_</code><td>Detaches the Tensor from the graph that created it, making it a leaf.<tr><td><code>torch.Tensor.register_hook</code>(hook)<td>Registers a backward hook.<tr><td><code>torch.Tensor.register_post_accumulate_grad_hook</code>(hook)<td>Registers a backward hook that runs after grad accumulation.<tr><td><code>torch.Tensor.retain_grad</code>()<td>Enables this Tensor to have their <a href=https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad rel=noopener target=_blank><code>grad</code></a> populated during <a href=https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward rel=noopener target=_blank><code>backward()</code></a>.</table></div><h3 id=Function><a class=headerlink href=#Function title=Function></a>Function</h3><p>要创建自定义 autograd.Function,请继承该类并实现 forward() 和 backward() 静态方法.然后,要在前向传递中使用自定义 op,调用类方法 apply.不要直接调用 forward().<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Exp</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>ctx, i</span>):</span></span><br><span class=line>        result = i.exp()</span><br><span class=line>        ctx.save_for_backward(result)</span><br><span class=line>        <span class=keyword>return</span> result</span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        result, = ctx.saved_tensors</span><br><span class=line>        <span class=keyword>return</span> grad_output * result</span><br><span class=line></span><br><span class=line>Use it by calling the apply method:</span><br><span class=line>output = Exp.apply(<span class=built_in>input</span>)</span><br></pre></table></figure><h2 id=ONNX格式><a class=headerlink href=#ONNX格式 title=ONNX格式></a>ONNX格式</h2><p>在实际部署时非常常用的模型格式,是屏蔽了框架的.<p><a href=https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html rel=noopener target=_blank>(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime — PyTorch Tutorials 2.3.0+cu121 documentation</a><h3 id=保存模型><a class=headerlink href=#保存模型 title=保存模型></a>保存模型</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torchvision</span><br><span class=line></span><br><span class=line>dummy_input = torch.randn(<span class=number>10</span>, <span class=number>3</span>, <span class=number>224</span>, <span class=number>224</span>, device=<span class=string>"cuda"</span>)</span><br><span class=line>model = torchvision.models.alexnet(pretrained=<span class=literal>True</span>).cuda()</span><br><span class=line></span><br><span class=line>input_names = [ <span class=string>"actual_input_1"</span> ] + [ <span class=string>"learned_%d"</span> % i <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>16</span>) ]</span><br><span class=line>output_names = [ <span class=string>"output1"</span> ]</span><br><span class=line></span><br><span class=line>torch.onnx.export(model, dummy_input, <span class=string>"alexnet.onnx"</span>, verbose=<span class=literal>True</span>, input_names=input_names, output_names=output_names)</span><br></pre></table></figure><p>生成的 <code>alexnet.onnx</code> 文件包含一个 <a href=https://developers.google.com/protocol-buffers/ rel=noopener target=_blank>protocol buffer</a>,其中包含了导出的模型(在本例中为 AlexNet)的网络结构和参数.<code>verbose=True</code> 参数会导致导出器打印出模型的人类可读表示.<h4 id=加载模型><a class=headerlink href=#加载模型 title=加载模型></a>加载模型</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>pip install onnx</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnx</span><br><span class=line><span class=comment># Load the ONNX model</span></span><br><span class=line>model = onnx.load(<span class=string>"alexnet.onnx"</span>)</span><br><span class=line></span><br><span class=line><span class=comment># Check that the model is well formed</span></span><br><span class=line>onnx.checker.check_model(model)</span><br><span class=line></span><br><span class=line><span class=comment># Print a human readable representation of the graph</span></span><br><span class=line><span class=built_in>print</span>(onnx.helper.printable_graph(model.graph))</span><br><span class=line>You can also run the exported model <span class=keyword>with</span> one of the many runtimes that support ONNX. For example after installing ONNX Runtime, you can load <span class=keyword>and</span> run the model:</span><br><span class=line></span><br><span class=line><span class=keyword>import</span> onnxruntime <span class=keyword>as</span> ort</span><br><span class=line></span><br><span class=line>ort_session = ort.InferenceSession(<span class=string>"alexnet.onnx"</span>)</span><br><span class=line></span><br><span class=line>outputs = ort_session.run(</span><br><span class=line>    <span class=literal>None</span>,</span><br><span class=line>    {<span class=string>"actual_input_1"</span>: np.random.randn(<span class=number>10</span>, <span class=number>3</span>, <span class=number>224</span>, <span class=number>224</span>).astype(np.float32)},</span><br><span class=line>)</span><br><span class=line><span class=built_in>print</span>(outputs[<span class=number>0</span>])</span><br><span class=line><span class=comment># Print a human readable representation of the graph</span></span><br><span class=line><span class=built_in>print</span>(onnx.helper.printable_graph(model.graph))</span><br></pre></table></figure><h3 id=流程><a class=headerlink href=#流程 title=流程></a>流程</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=comment># Input to the model</span></span><br><span class=line>x = torch.randn(batch_size, <span class=number>1</span>, <span class=number>224</span>, <span class=number>224</span>, requires_grad=<span class=literal>True</span>)</span><br><span class=line>torch_out = torch_model(x)</span><br><span class=line></span><br><span class=line><span class=comment># Export the model</span></span><br><span class=line>torch.onnx.export(torch_model,               <span class=comment># model being run</span></span><br><span class=line>                  x,                         <span class=comment># model input (or a tuple for multiple inputs)</span></span><br><span class=line>                  <span class=string>"super_resolution.onnx"</span>,   <span class=comment># where to save the model (can be a file or file-like object)</span></span><br><span class=line>                  export_params=<span class=literal>True</span>,        <span class=comment># store the trained parameter weights inside the model file</span></span><br><span class=line>                  opset_version=<span class=number>10</span>,          <span class=comment># the ONNX version to export the model to</span></span><br><span class=line>                  do_constant_folding=<span class=literal>True</span>,  <span class=comment># whether to execute constant folding for optimization</span></span><br><span class=line>                  input_names = [<span class=string>'input'</span>],   <span class=comment># the model's input names</span></span><br><span class=line>                  output_names = [<span class=string>'output'</span>], <span class=comment># the model's output names</span></span><br><span class=line>                  dynamic_axes={<span class=string>'input'</span> : {<span class=number>0</span> : <span class=string>'batch_size'</span>},    <span class=comment># variable length axes</span></span><br><span class=line>                                <span class=string>'output'</span> : {<span class=number>0</span> : <span class=string>'batch_size'</span>}})</span><br></pre></table></figure><p>在pytorch中直接使用<code>torch.onnx.export</code>即可.<p>因为导出运行了模型,我们需要提供一个输入张量 <code>x</code>.这个输入的值可以是随机的,只要它的类型和大小是正确的.请注意,除非指定为dynamic_axes,否则导出的 ONNX 图中输入的所有维度大小都会被固定下来.在这个示例中,使用批量大小为 1 的输入导出模型,但在 <code>torch.onnx.export()</code> 的 <code>dynamic_axes</code> 参数中指定了第一个维度为动态的.因此,导出的模型将接受大小为 <code>[batch_size, 1, 224, 224]</code> 的输入,其中 <code>batch_size</code> 可以是可变的.<p>同时还计算了模型输出 <code>torch_out</code>,我们将使用它来验证在 ONNX Runtime 中运行时导出的模型是否计算出相同的值.<p>但在使用 ONNX Runtime 验证模型输出之前,我们会先使用 ONNX API 检查 ONNX 模型.首先,<code>onnx.load("super_resolution.onnx")</code> 会加载保存的模型,并输出一个 <code>onnx.ModelProto</code> 结构.然后,<code>onnx.checker.check_model(onnx_model)</code> 会验证模型的结构,并确认模型具有有效的架构.通过检查模型的版本、图结构以及节点及其输入和输出,来验证 ONNX 图的有效性.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnx</span><br><span class=line>onnx_model = onnx.load(<span class=string>"super_resolution.onnx"</span>)</span><br><span class=line>onnx.checker.check_model(onnx_model)</span><br></pre></table></figure><p>使用 ONNX Runtime 的 Python API 计算输出,通常情况下,这一部分可以在单独的进程中或其他机器上完成,但我们将继续在同一进程中进行,这样我们就可以验证 ONNX Runtime 和 PyTorch 为该网络计算出的值是否相同.<p>为了使用 ONNX Runtime 运行模型,我们需要为模型创建一个InferenceSession,并设置所需的配置参数(这里我们使用默认配置).创建会话后,我们就可以使用 <code>run()</code> API 来评估模型了.该调用的输出是一个列表,包含 ONNX Runtime 计算得出的模型输出.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnxruntime</span><br><span class=line></span><br><span class=line>ort_session = onnxruntime.InferenceSession(<span class=string>"super_resolution.onnx"</span>, providers=[<span class=string>"CPUExecutionProvider"</span>])</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>to_numpy</span>(<span class=params>tensor</span>):</span></span><br><span class=line>    <span class=keyword>return</span> tensor.detach().cpu().numpy() <span class=keyword>if</span> tensor.requires_grad <span class=keyword>else</span> tensor.cpu().numpy()</span><br><span class=line></span><br><span class=line><span class=comment># compute ONNX Runtime output prediction</span></span><br><span class=line>ort_inputs = {ort_session.get_inputs()[<span class=number>0</span>].name: to_numpy(x)}</span><br><span class=line>ort_outs = ort_session.run(<span class=literal>None</span>, ort_inputs)</span><br><span class=line></span><br><span class=line><span class=comment># compare ONNX Runtime and PyTorch results</span></span><br><span class=line>np.testing.assert_allclose(to_numpy(torch_out), ort_outs[<span class=number>0</span>], rtol=<span class=number>1e-03</span>, atol=<span class=number>1e-05</span>)</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(<span class=string>"Exported model has been tested with ONNXRuntime, and the result looks good!"</span>)</span><br></pre></table></figure><h4 id=注意><a class=headerlink href=#注意 title=注意></a>注意</h4><p>在模型中避免使用numpy,tensor.data,tensor.shape不能使用in_place操作<h2 id=自动混合精度><a class=headerlink href=#自动混合精度 title=自动混合精度></a>自动混合精度</h2><p>torch.amp 为混合精度提供了方便的方法,其中一些操作使用 torch.float32 （浮点）数据类型,另一些操作使用较低精度的浮点数据类型 (lower_precision_fp):torch.float16（半精度）或 torch.bfloat16.一些操作,如线性层和卷积,在 lower_precision_fp 下速度更快.其他操作,如还原,通常需要 float32 的动态范围.混合精度试图将每个操作与相应的数据类型相匹配.<p>通常,数据类型为 torch.float16 的 “自动混合精度训练 “使用 torch.autocast 和 torch.cpu.amp.GradScaler 或 torch.cuda.amp.GradScaler.<p>torch.autocast 实例可对所选上下文进行自动casting.自动cast会自动选择 GPU 运算的精度,从而在保持精度的同时提高性能.<p>torch.cuda.amp.GradScaler 的实例有助于方便地执行梯度缩放步骤.<strong>梯度缩放可最大限度地减少梯度下溢</strong>,从而改善具有 float16 梯度的网络的收敛性.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=comment># Creates model and optimizer in default precision</span></span><br><span class=line>model = Net().cuda()</span><br><span class=line>optimizer = optim.SGD(model.parameters(), ...)</span><br><span class=line></span><br><span class=line><span class=comment># Creates a GradScaler once at the beginning of training.</span></span><br><span class=line>scaler = GradScaler()  <span class=comment># 1. 创建gradscaler</span></span><br><span class=line></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line></span><br><span class=line>        <span class=comment># Runs the forward pass with autocasting.</span></span><br><span class=line>        <span class=comment># 2.使得模型训练时相关参数类型自动转换</span></span><br><span class=line>        <span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>            output = model(<span class=built_in>input</span>)</span><br><span class=line>            loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class=line>        <span class=comment># Backward passes under autocast are not recommended.</span></span><br><span class=line>        <span class=comment># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span><br><span class=line>        scaler.scale(loss).backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># scaler.step() first unscales the gradients of the optimizer's assigned params.</span></span><br><span class=line>        <span class=comment># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span><br><span class=line>        <span class=comment># otherwise, optimizer.step() is skipped.</span></span><br><span class=line>        scaler.step(optimizer)   </span><br><span class=line>        <span class=comment># Updates the scale for next iteration.</span></span><br><span class=line>        scaler.update()</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=comment># Creates model and optimizer in default precision</span></span><br><span class=line>model = Net().cuda()</span><br><span class=line>optimizer = optim.SGD(model.parameters(), ...)</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>    optimizer.zero_grad()</span><br><span class=line></span><br><span class=line>    <span class=comment># Enables autocasting for the forward pass (model + loss)</span></span><br><span class=line>    <span class=keyword>with</span> torch.autocast(device_type=<span class=string>"cuda"</span>):</span><br><span class=line>        output = model(<span class=built_in>input</span>)</span><br><span class=line>        loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>    <span class=comment># Exits the context manager before backward()</span></span><br><span class=line>    loss.backward()</span><br><span class=line>    optimizer.step()</span><br></pre></table></figure><p>所有由 scaler.scale(loss).backward() 生成的梯度都是按比例缩放的.如果要在 backward() 和 scaler.step(optimizer) 之间修改或检查参数的 .grad 属性,应首先取消缩放.<p><strong>梯度惩罚</strong><p>梯度惩罚的实现通常使用 torch.autograd.grad() 创建梯度,将它们组合起来创建惩罚值,并将惩罚值添加到损失中.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line>        output = model(<span class=built_in>input</span>)</span><br><span class=line>        loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># Creates gradients</span></span><br><span class=line>        grad_params = torch.autograd.grad(outputs=loss,</span><br><span class=line>                                          inputs=model.parameters(),</span><br><span class=line>                                          create_graph=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># Computes the penalty term and adds it to the loss</span></span><br><span class=line>        grad_norm = <span class=number>0</span></span><br><span class=line>        <span class=keyword>for</span> grad <span class=keyword>in</span> grad_params:</span><br><span class=line>            grad_norm += grad.<span class=built_in>pow</span>(<span class=number>2</span>).<span class=built_in>sum</span>()</span><br><span class=line>        grad_norm = grad_norm.sqrt()</span><br><span class=line>        loss = loss + grad_norm</span><br><span class=line></span><br><span class=line>        loss.backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># clip gradients here, if desired</span></span><br><span class=line></span><br><span class=line>        optimizer.step()</span><br></pre></table></figure><p>要通过梯度缩放实现梯度惩罚,应缩放传递给 torch.autograd.grad() 的输出张量.因此,生成的梯度也将被缩放,在合并生成惩罚值之前应取消缩放.<p>此外,惩罚项的计算是前向传递的一部分,因此应在自动传递上下文中进行.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line>scaler = torch.cuda.amp.GradScaler()</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer0.zero_grad()</span><br><span class=line>        optimizer1.zero_grad()</span><br><span class=line>        <span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>            output0 = model0(<span class=built_in>input</span>)</span><br><span class=line>            output1 = model1(<span class=built_in>input</span>)</span><br><span class=line>            loss0 = loss_fn(<span class=number>2</span> * output0 + <span class=number>3</span> * output1, target)</span><br><span class=line>            loss1 = loss_fn(<span class=number>3</span> * output0 - <span class=number>5</span> * output1, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># (retain_graph here is unrelated to amp, it's present because in this</span></span><br><span class=line>        <span class=comment># example, both backward() calls share some sections of graph.)</span></span><br><span class=line>        scaler.scale(loss0).backward(retain_graph=<span class=literal>True</span>)</span><br><span class=line>        scaler.scale(loss1).backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># You can choose which optimizers receive explicit unscaling, if you</span></span><br><span class=line>        <span class=comment># want to inspect or modify the gradients of the params they own.</span></span><br><span class=line>        scaler.unscale_(optimizer0)</span><br><span class=line></span><br><span class=line>        scaler.step(optimizer0)</span><br><span class=line>        scaler.step(optimizer1)</span><br><span class=line></span><br><span class=line>        scaler.update()</span><br></pre></table></figure><p>如果网络有多个损失,则必须对每个损耗单独调用 scaler.scale.如果的网络有多个优化器,您可以在任何一个优化器上单独调用 scaler.unscale_,并且必须在每个优化器上单独调用 scaler.step.<p>autocast不在在 float64 或非浮点类型上进行转换.。为了获得最佳性能和稳定性,在autocast区域中使用out-of-place运算,也就是使用类似a.addmm(b, c)这种操作.<p>autocast会将with下的区域中的运算自动转换,自动转发应只包含网络的前向传递，包括损失计算。<p>不建议使用自动转发的后向传递. 后向操作的运算类型是autocast之前的类型.<h4 id=cuda上会转为float16的运算><a class=headerlink href=#cuda上会转为float16的运算 title=cuda上会转为float16的运算></a>cuda上会转为float16的运算</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>__matmul__`, `addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`, `chain_matmul`, `multi_dot`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`, `conv_transpose2d`, `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`, `matmul`, `mm`, `mv`, `prelu`, `RNNCell</span><br></pre></table></figure><h4 id=cuda上会转为float32的运算><a class=headerlink href=#cuda上会转为float32的运算 title=cuda上会转为float32的运算></a>cuda上会转为float32的运算</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `acos`, `asin`, `binary_cross_entropy_with_logits`, `cosh`, `cosine_embedding_loss`, `cdist`, `cosine_similarity`, `cross_entropy`, `cumprod`, `cumsum`, `dist`, `erfinv`, `exp`, `expm1`, `group_norm`, `hinge_embedding_loss`, `kl_div`, `l1_loss`, `layer_norm`, `log`, `log_softmax`, `log10`, `log1p`, `log2`, `margin_ranking_loss`, `mse_loss`, `multilabel_margin_loss`, `multi_margin_loss`, `nll_loss`, `norm`, `normalize`, `pdist`, `poisson_nll_loss`, `<span class=built_in>pow</span>`, `prod`, `reciprocal`, `rsqrt`, `sinh`, `smooth_l1_loss`, `soft_margin_loss`, `softmax`, `softmin`, `softplus`, `<span class=built_in>sum</span>`, `renorm`, `tan`, `triplet_margin_loss</span><br></pre></table></figure><p>还有一些运算需要多个输入,如果输入全是float32那输出就是float32.也就是promote to the widest input type<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>addcdiv`, `addcmul`, `atan2`, `bilinear`, `cross`, `dot`, `grid_sample`, `index_put`, `scatter_add`, `tensordot</span><br></pre></table></figure><h4 id=CPU上会转为bfloat16的运算><a class=headerlink href=#CPU上会转为bfloat16的运算 title=CPU上会转为bfloat16的运算></a>CPU上会转为bfloat16的运算</h4><p>bfloat是比较特殊的数据类型,是针对深度学习运算特别调整指数位和小数位,使得相对于同等位数的float,其精度更小,但能表示的值范围更大,而且针对显卡运算更快(显卡厂商调整了)<div class=table-container><table><thead><tr><th>Format<th>Bits<th>Exponent<th>Fraction<th>sign(符号)<tbody><tr><td>FP32<td>32<td>8<td>23<td>1<tr><td>FP16<td>16<td>5<td>10<td>1<tr><td>BF16<td>16<td>8<td>7<td>1</table></div><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>conv1d`, `conv2d`, `conv3d`, `bmm`, `mm`, `baddbmm`, `addmm`, `addbmm`, `linear`, `matmul`, `_convolution</span><br></pre></table></figure><h4 id=CPU上会转为bfloat32的运算><a class=headerlink href=#CPU上会转为bfloat32的运算 title=CPU上会转为bfloat32的运算></a>CPU上会转为bfloat32的运算</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>conv_transpose1d, conv_transpose2d, conv_transpose3d, avg_pool3d, binary_cross_entropy, grid_sampler, grid_sampler_2d, _grid_sampler_2d_cpu_fallback, grid_sampler_3d, polar, prod, quantile, nanquantile, stft, cdist, trace, view_as_complex, cholesky, cholesky_inverse, cholesky_solve, inverse, lu_solve, orgqr, inverse, ormqr, pinverse, max_pool3d, max_unpool2d, max_unpool3d, adaptive_avg_pool3d, reflection_pad1d, reflection_pad2d, replication_pad1d, replication_pad2d, replication_pad3d, mse_loss, ctc_loss, kl_div, multilabel_margin_loss, fft_fft, fft_ifft, fft_fft2, fft_ifft2, fft_fftn, fft_ifftn, fft_rfft, fft_irfft, fft_rfft2, fft_irfft2, fft_rfftn, fft_irfftn, fft_hfft, fft_ihfft, linalg_matrix_norm, linalg_cond, linalg_matrix_rank, linalg_solve, linalg_cholesky, linalg_svdvals, linalg_eigvals, linalg_eigvalsh, linalg_inv, linalg_householder_product, linalg_tensorinv, linalg_tensorsolve, fake_quantize_per_tensor_affine, eig, geqrf, lstsq, _lu_with_info, qr, solve, svd, symeig, triangular_solve, fractional_max_pool2d, fractional_max_pool3d, adaptive_max_pool3d, multilabel_margin_loss_forward, linalg_qr, linalg_cholesky_ex, linalg_svd, linalg_eig, linalg_eigh, linalg_lstsq, linalg_inv_ex</span><br></pre></table></figure><p>类似的,cpu上也有promote to the widest input type的运算.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>cat, stack, index_copy</span><br></pre></table></figure><h2 id=单机器-多GPU-训练最佳实践><a class=headerlink href=#单机器-多GPU-训练最佳实践 title=单机器(多GPU)训练最佳实践></a>单机器(多GPU)训练最佳实践</h2><p>如果有多个GPU,每个GPU上运行复制的权重相同的模型,数据分发到多个GPU上,这样就能加快训练.但是有些使用一个GPU上容不下一个完整的模型,这个时候,将一个模型拆到不同的GPU上就是一个可行的方案.<p>这里就要提到model parallel(模型并行),模型并行是将一个模型的不同子网络放到不同的设备上,并相应地forward,以便在设备间移动中间输出.<blockquote><p>如果是跨机器,可以通过RPC<a href=https://pytorch.org/tutorials/intermediate/rpc_tutorial.html rel=noopener target=_blank>Getting Started with Distributed RPC Framework — PyTorch Tutorials 2.3.0+cu121 documentation</a></blockquote><p>一个简单的例子<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.optim <span class=keyword>as</span> optim</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ToyModel</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(ToyModel, self).__init__()</span><br><span class=line>        self.net1 = torch.nn.Linear(<span class=number>10</span>, <span class=number>10</span>).to(<span class=string>'cuda:0'</span>)</span><br><span class=line>        self.relu = torch.nn.ReLU()</span><br><span class=line>        self.net2 = torch.nn.Linear(<span class=number>10</span>, <span class=number>5</span>).to(<span class=string>'cuda:1'</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.relu(self.net1(x.to(<span class=string>'cuda:0'</span>)))</span><br><span class=line>        <span class=keyword>return</span> self.net2(x.to(<span class=string>'cuda:1'</span>))</span><br><span class=line>model = ToyModel()</span><br><span class=line>loss_fn = nn.MSELoss()</span><br><span class=line>optimizer = optim.SGD(model.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line></span><br><span class=line>optimizer.zero_grad()</span><br><span class=line>outputs = model(torch.randn(<span class=number>20</span>, <span class=number>10</span>))</span><br><span class=line>labels = torch.randn(<span class=number>20</span>, <span class=number>5</span>).to(<span class=string>'cuda:1'</span>)</span><br><span class=line>loss_fn(outputs, labels).backward()</span><br><span class=line>optimizer.step()</span><br></pre></table></figure><p>把模型不同部分放在了不同GPU上,并且forward时把数据也放在对应位置.注意计算损失时,label也要放对应位置.<p>只需修改几行代码，就可以在多个 GPU 上运行现有的单 GPU 模块.继承现有的 ResNet 模块,并在构建过程中将各层拆分到两个 GPU.然后,覆盖forward方法,通过相应移动中间输出来缝合两个子网络.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre><td class=code><pre><span class=line><span class=keyword>from</span> torchvision.models.resnet <span class=keyword>import</span> ResNet, Bottleneck</span><br><span class=line></span><br><span class=line>num_classes = <span class=number>1000</span></span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ModelParallelResNet50</span>(<span class=params>ResNet</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, *args, **kwargs</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(ModelParallelResNet50, self).__init__(</span><br><span class=line>            Bottleneck, [<span class=number>3</span>, <span class=number>4</span>, <span class=number>6</span>, <span class=number>3</span>], num_classes=num_classes, *args, **kwargs)</span><br><span class=line></span><br><span class=line>        self.seq1 = nn.Sequential(</span><br><span class=line>            self.conv1,</span><br><span class=line>            self.bn1,</span><br><span class=line>            self.relu,</span><br><span class=line>            self.maxpool,</span><br><span class=line></span><br><span class=line>            self.layer1,</span><br><span class=line>            self.layer2</span><br><span class=line>        ).to(<span class=string>'cuda:0'</span>)</span><br><span class=line></span><br><span class=line>        self.seq2 = nn.Sequential(</span><br><span class=line>            self.layer3,</span><br><span class=line>            self.layer4,</span><br><span class=line>            self.avgpool,</span><br><span class=line>        ).to(<span class=string>'cuda:1'</span>)</span><br><span class=line></span><br><span class=line>        self.fc.to(<span class=string>'cuda:1'</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = self.seq2(self.seq1(x).to(<span class=string>'cuda:1'</span>))</span><br><span class=line>        <span class=keyword>return</span> self.fc(x.view(x.size(<span class=number>0</span>), -<span class=number>1</span>))</span><br></pre></table></figure><p>最后做了相关实验,发现在多个GPU上模型并行执行时间更长,因为多个 GPU 中只有一个在工作，而其他的也不做,同时数据从不同GPU中复制时也需要时间.<p><img alt=img data-src=https://pytorch.org/tutorials/_images/mp_vs_rn.png><h2 id=多GPU训练><a class=headerlink href=#多GPU训练 title=多GPU训练></a>多GPU训练</h2><h4 id=并行方案><a class=headerlink href=#并行方案 title=并行方案></a>并行方案</h4><p><code>DataParallel</code>是单进程、多线程的,只能在单台机器上(可以多GPU)运行,而 <code>DistributedDataParallel</code> 是多进程的,可以在单台和多台机器上运行.<p>即使在单台机器上,DataParallel 通常也比 DistributedDataParallel 慢,这是因为线程间的 GIL 竞争、每次迭代的复制模型，以及分散输入和收集输出所带来的额外开销.<blockquote><p>实际为了方便,完全可以仅使用DataParaller在多GPU上运行.</blockquote><div class=table-container><table><thead><tr><th>DataParallel<th>disc<tbody><tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel rel=noopener target=_blank><code>nn.DataParallel</code></a><td>Implements data parallelism at the module level.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel rel=noopener target=_blank><code>nn.parallel.DistributedDataParallel</code></a><td>Implement distributed data parallelism based on <code>torch.distributed</code> at module level.</table></div><p>模型并行:如果模型太大,无法在单个 GPU 上运行,就必须使用模型并行功能将其分割到多个 GPU 上.<p>分布式数据并行（DistributedDataParallel,DDP）可与模型并行一起使用，而数据并行（DataParallel）目前还不能。当 DDP 与模型并行相结合时,每个 DDP 进程都将使用模型并行,而所有进程都将使用数据并行。<p>多GPU训练有每个GPU一个线程<p><code>torch.nn.DataParallel</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>model = MyModel()</span><br><span class=line>dp_model = nn.DataParallel(model)</span><br><span class=line></span><br><span class=line><span class=comment># Sets autocast in the main thread</span></span><br><span class=line><span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>    <span class=comment># dp_model's internal threads will autocast.</span></span><br><span class=line>    output = dp_model(<span class=built_in>input</span>)</span><br><span class=line>    <span class=comment># loss_fn also autocast</span></span><br><span class=line>    loss = loss_fn(output)</span><br></pre></table></figure><p>上面方法是最简单的弊端是后续的loss计算只会在<code>cuda:0</code>上进行,没法并行,因此会导致负载不均衡的问题<p>文档推荐使用<code>DistributedDataParallel</code><blockquote><p>为什么尽管增加了复杂性,还是会考虑使用 DistributedDataParallel 而不是 DataParallel:<p>首先,DataParallel 是单进程、多线程的,只能在单机上运行,而 <strong>DistributedDataParallel 是多进程的,可以在单机和多机训练中运行.即使在单台机器上,DataParallel 通常也比 DistributedDataParallel 慢,这是由于线程间的 GIL 竞争、每次迭代的复制模型,以及分散输入和收集输出所带来的额外开销.</strong><p>分布式数据并行（DistributedDataParallel）可与模型并行一起使用,而数据并行（DataParallel）目前还不能.当 DDP 与模型并行相结合时,每个 DDP 进程都将使用模型并行,而所有进程都将使用数据并行.<p>如果模型需要跨越多台机器,或者您的用例不符合数据并行模式,请使用RPC API,以通用的分布式训练.</blockquote><p>在模块级基于 torch.distributed 实现分布式数据并行.<br>该容器通过在每个模型副本之间同步梯度来提供数据并行性.要同步的设备由输入 process_group 指定,默认情况下是整个世界.请注意,DistributedDataParallel 不会在参与的 GPU 之间对输入进行分块或分片；用户负责定义如何进行分块或分片,例如通过使用 DistributedSampler.<p>此外需要进行初始化 torch.distributed.init_process_group()<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.distributed <span class=keyword>as</span> dist</span><br><span class=line><span class=keyword>import</span> torch.multiprocessing <span class=keyword>as</span> mp</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.optim <span class=keyword>as</span> optim</span><br><span class=line><span class=keyword>import</span> os</span><br><span class=line><span class=keyword>from</span> torch.nn.parallel <span class=keyword>import</span> DistributedDataParallel <span class=keyword>as</span> DDP</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>example</span>(<span class=params>rank, world_size</span>):</span></span><br><span class=line>    <span class=comment># create default process group</span></span><br><span class=line>    dist.init_process_group(<span class=string>"gloo"</span>, rank=rank, world_size=world_size)</span><br><span class=line>    <span class=comment># create local model</span></span><br><span class=line>    model = nn.Linear(<span class=number>10</span>, <span class=number>10</span>).to(rank)</span><br><span class=line>    <span class=comment># construct DDP model</span></span><br><span class=line>    ddp_model = DDP(model, device_ids=[rank])</span><br><span class=line>    <span class=comment># define loss function and optimizer</span></span><br><span class=line>    loss_fn = nn.MSELoss()</span><br><span class=line>    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line></span><br><span class=line>    <span class=comment># forward pass</span></span><br><span class=line>    outputs = ddp_model(torch.randn(<span class=number>20</span>, <span class=number>10</span>).to(rank))</span><br><span class=line>    labels = torch.randn(<span class=number>20</span>, <span class=number>10</span>).to(rank)</span><br><span class=line>    <span class=comment># backward pass</span></span><br><span class=line>    loss_fn(outputs, labels).backward()</span><br><span class=line>    <span class=comment># update parameters</span></span><br><span class=line>    optimizer.step()</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>main</span>():</span></span><br><span class=line>    world_size = <span class=number>2</span></span><br><span class=line>    mp.spawn(example,</span><br><span class=line>        args=(world_size,),</span><br><span class=line>        nprocs=world_size,</span><br><span class=line>        join=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> __name__==<span class=string>"__main__"</span>:</span><br><span class=line>    <span class=comment># Environment variables which need to be</span></span><br><span class=line>    <span class=comment># set when using c10d's default "env"</span></span><br><span class=line>    <span class=comment># initialization mode.</span></span><br><span class=line>    os.environ[<span class=string>"MASTER_ADDR"</span>] = <span class=string>"localhost"</span></span><br><span class=line>    os.environ[<span class=string>"MASTER_PORT"</span>] = <span class=string>"29500"</span></span><br><span class=line>    main()                                  </span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>demo_model_parallel</span>(<span class=params>rank, world_size</span>):</span></span><br><span class=line>    <span class=built_in>print</span>(<span class=string>f"Running DDP with model parallel example on rank <span class=subst>{rank}</span>."</span>)</span><br><span class=line>    setup(rank, world_size)</span><br><span class=line></span><br><span class=line>    <span class=comment># setup mp_model and devices for this process</span></span><br><span class=line>    dev0 = rank * <span class=number>2</span></span><br><span class=line>    dev1 = rank * <span class=number>2</span> + <span class=number>1</span></span><br><span class=line>    mp_model = ToyMpModel(dev0, dev1)</span><br><span class=line>    ddp_mp_model = DDP(mp_model)</span><br><span class=line></span><br><span class=line>    loss_fn = nn.MSELoss()</span><br><span class=line>    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line></span><br><span class=line>    optimizer.zero_grad()</span><br><span class=line>    <span class=comment># outputs will be on dev1</span></span><br><span class=line>    outputs = ddp_mp_model(torch.randn(<span class=number>20</span>, <span class=number>10</span>))</span><br><span class=line>    labels = torch.randn(<span class=number>20</span>, <span class=number>5</span>).to(dev1)</span><br><span class=line>    loss_fn(outputs, labels).backward()</span><br><span class=line>    optimizer.step()</span><br><span class=line></span><br><span class=line>    cleanup()</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=keyword>if</span> __name__ == <span class=string>"__main__"</span>:</span><br><span class=line>    n_gpus = torch.cuda.device_count()</span><br><span class=line>    <span class=keyword>assert</span> n_gpus >= <span class=number>2</span>, <span class=string>f"Requires at least 2 GPUs to run, but got <span class=subst>{n_gpus}</span>"</span></span><br><span class=line>    world_size = n_gpus</span><br><span class=line>    run_demo(demo_basic, world_size)</span><br><span class=line>    run_demo(demo_checkpoint, world_size)</span><br><span class=line>    world_size = n_gpus//<span class=number>2</span></span><br><span class=line>    run_demo(demo_model_parallel, world_size)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>sampler = DistributedSampler(dataset) <span class=keyword>if</span> is_distributed <span class=keyword>else</span> <span class=literal>None</span></span><br><span class=line>loader = DataLoader(dataset, shuffle=(sampler <span class=keyword>is</span> <span class=literal>None</span>),</span><br><span class=line>                    sampler=sampler)</span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(start_epoch, n_epochs):</span><br><span class=line>     <span class=keyword>if</span> is_distributed:</span><br><span class=line>         sampler.set_epoch(epoch)</span><br><span class=line>     train(loader)</span><br></pre></table></figure><p>它与 torch.nn.parallel.DistributedDataParallel 结合使用尤其有用.在这种情况下,<strong>每个进程都可以传递一个 DistributedSampler 实例作为 DataLoader 采样器</strong>,并加载其独有的原始数据集子集.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch  </span><br><span class=line><span class=keyword>import</span> torch.distributed <span class=keyword>as</span> dist  </span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn  </span><br><span class=line><span class=keyword>import</span> torch.optim <span class=keyword>as</span> optim  </span><br><span class=line><span class=keyword>from</span> torch.utils.data <span class=keyword>import</span> DataLoader, Dataset, DistributedSampler  </span><br><span class=line><span class=keyword>from</span> torch.nn.parallel <span class=keyword>import</span> DistributedDataParallel <span class=keyword>as</span> DDP  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 自定义数据集和模型  </span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyDataset</span>(<span class=params>Dataset</span>):</span>  </span><br><span class=line>    <span class=comment># 实现__len__和__getitem__方法  </span></span><br><span class=line>    <span class=keyword>pass</span>  </span><br><span class=line>  </span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModel</span>(<span class=params>nn.Module</span>):</span>  </span><br><span class=line>    <span class=comment># 定义模型结构,可能需要考虑如何拆分模型  </span></span><br><span class=line>    <span class=keyword>pass</span>  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 初始化分布式环境  </span></span><br><span class=line>dist.init_process_group(backend=<span class=string>'nccl'</span>, init_method=<span class=string>'tcp://localhost:23456'</span>, rank=<span class=number>0</span>, world_size=torch.cuda.device_count())  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 初始化数据集和模型  </span></span><br><span class=line>dataset = MyDataset()  </span><br><span class=line>sampler = DistributedSampler(dataset)  </span><br><span class=line>dataloader = DataLoader(dataset, batch_size=<span class=number>32</span>, shuffle=<span class=literal>False</span>, sampler=sampler)  </span><br><span class=line>model = MyModel()  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 拆分模型（这通常需要根据模型的具体结构来手动完成）  </span></span><br><span class=line><span class=comment>#### 例如,如果模型有两个主要部分,可以将它们分别放到不同的设备上  </span></span><br><span class=line>model_part1 = model.part1.to(<span class=string>'cuda:0'</span>)  </span><br><span class=line>model_part2 = model.part2.to(<span class=string>'cuda:1'</span>)  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 使用DistributedDataParallel包装模型  </span></span><br><span class=line>model = DDP(model, device_ids=[torch.cuda.current_device()])  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 定义损失函数和优化器  </span></span><br><span class=line>criterion = nn.CrossEntropyLoss()  </span><br><span class=line>optimizer = optim.Adam(model.parameters(), lr=<span class=number>0.001</span>)  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 训练循环  </span></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(num_epochs):  </span><br><span class=line>    <span class=keyword>for</span> inputs, labels <span class=keyword>in</span> dataloader:  </span><br><span class=line>        inputs, labels = inputs.to(model.device), labels.to(model.device)  </span><br><span class=line>        optimizer.zero_grad()  </span><br><span class=line>        outputs = model(inputs)  </span><br><span class=line>        loss = criterion(outputs, labels)  </span><br><span class=line>        loss.backward()  </span><br><span class=line>        optimizer.step()  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 销毁分布式进程组  </span></span><br><span class=line>dist.destroy_process_group()</span><br></pre></table></figure><p><a href=https://github.com/jia-zhuang/pytorch-multi-gpu-training rel=noopener target=_blank>jia-zhuang/pytorch-multi-gpu-training: 整理 pytorch 单机多 GPU 训练方法与原理 (github.com)</a><h2 id=常用Container><a class=headerlink href=#常用Container title=常用Container></a>常用Container</h2><div class=table-container><table><thead><tr><th>Containers<th>介绍<tbody><tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module rel=noopener target=_blank><code>Module</code></a><td>Base class for all neural network modules.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential rel=noopener target=_blank><code>Sequential</code></a><td>A sequential container.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList rel=noopener target=_blank><code>ModuleList</code></a><td>Holds submodules in a list.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict rel=noopener target=_blank><code>ModuleDict</code></a><td>Holds submodules in a dictionary.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList rel=noopener target=_blank><code>ParameterList</code></a><td>Holds parameters in a list.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict rel=noopener target=_blank><code>ParameterDict</code></a><td>Holds parameters in a dictionary.</table></div><p><code>Module</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Model</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.conv1 = nn.Conv2d(<span class=number>1</span>, <span class=number>20</span>, <span class=number>5</span>)</span><br><span class=line>        self.conv2 = nn.Conv2d(<span class=number>20</span>, <span class=number>20</span>, <span class=number>5</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = F.relu(self.conv1(x))</span><br><span class=line>        <span class=keyword>return</span> F.relu(self.conv2(x))</span><br></pre></table></figure><p><code>Sequential</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line>model = nn.Sequential(</span><br><span class=line>          nn.Conv2d(<span class=number>1</span>,<span class=number>20</span>,<span class=number>5</span>),</span><br><span class=line>          nn.ReLU(),</span><br><span class=line>          nn.Conv2d(<span class=number>20</span>,<span class=number>64</span>,<span class=number>5</span>),</span><br><span class=line>          nn.ReLU()</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line><span class=comment># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class=line><span class=comment># same as the above code</span></span><br><span class=line>model = nn.Sequential(OrderedDict([</span><br><span class=line>          (<span class=string>'conv1'</span>, nn.Conv2d(<span class=number>1</span>,<span class=number>20</span>,<span class=number>5</span>)),</span><br><span class=line>          (<span class=string>'relu1'</span>, nn.ReLU()),</span><br><span class=line>          (<span class=string>'conv2'</span>, nn.Conv2d(<span class=number>20</span>,<span class=number>64</span>,<span class=number>5</span>)),</span><br><span class=line>          (<span class=string>'relu2'</span>, nn.ReLU())</span><br><span class=line>        ]))</span><br></pre></table></figure><p><code>ModuleList</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.linears = nn.ModuleList([nn.Linear(<span class=number>10</span>, <span class=number>10</span>) <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>10</span>)])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=comment># ModuleList can act as an iterable, or be indexed using ints</span></span><br><span class=line>        <span class=keyword>for</span> i, l <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.linears):</span><br><span class=line>            x = self.linears[i // <span class=number>2</span>](x) + l(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ModuleDict</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.choices = nn.ModuleDict({</span><br><span class=line>                <span class=string>'conv'</span>: nn.Conv2d(<span class=number>10</span>, <span class=number>10</span>, <span class=number>3</span>),</span><br><span class=line>                <span class=string>'pool'</span>: nn.MaxPool2d(<span class=number>3</span>)</span><br><span class=line>        })</span><br><span class=line>        self.activations = nn.ModuleDict([</span><br><span class=line>                [<span class=string>'lrelu'</span>, nn.LeakyReLU()],</span><br><span class=line>                [<span class=string>'prelu'</span>, nn.PReLU()]</span><br><span class=line>        ])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, choice, act</span>):</span></span><br><span class=line>        x = self.choices[choice](x)</span><br><span class=line>        x = self.activations[act](x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ParameterList</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class=number>10</span>, <span class=number>10</span>)) <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>10</span>)])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=comment># ParameterList can act as an iterable, or be indexed using ints</span></span><br><span class=line>        <span class=keyword>for</span> i, p <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.params):</span><br><span class=line>            x = self.params[i // <span class=number>2</span>].mm(x) + p.mm(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ParameterDict</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.params = nn.ParameterDict({</span><br><span class=line>                <span class=string>'left'</span>: nn.Parameter(torch.randn(<span class=number>5</span>, <span class=number>10</span>)),</span><br><span class=line>                <span class=string>'right'</span>: nn.Parameter(torch.randn(<span class=number>5</span>, <span class=number>10</span>))</span><br><span class=line>        })</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, choice</span>):</span></span><br><span class=line>        x = self.params[choice].mm(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><h2 id=容易混淆和遗忘的方法><a class=headerlink href=#容易混淆和遗忘的方法 title=容易混淆和遗忘的方法></a>容易混淆和遗忘的方法</h2><h3 id=torch-Tensor-scatter><a class=headerlink href=#torch-Tensor-scatter title=torch.Tensor.scatter_></a>torch.Tensor.scatter_</h3><p>torch.scatter的in-place操作<p><code>Tensor.scatter_(dim, index, src, *, reduce=None) → [Tensor</code>]<p><strong>按照 <code>index</code> 张量中指定的索引,将张量 <code>src</code> 中的所有值写入 <code>self</code> 中</strong>.<p>对于 <code>src</code> 中的每个值,其输出索引在 <code>dimension != dim</code> 时由 <code>src</code> 中的索引指定,在 <code>dimension = dim</code> 时由 <code>index</code> 中的相应值指定.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>self[index[i][j][k]][j][k] = src[i][j][k]  <span class=comment># if dim == 0</span></span><br><span class=line>self[i][index[i][j][k]][k] = src[i][j][k]  <span class=comment># if dim == 1</span></span><br><span class=line>self[i][j][index[i][j][k]] = src[i][j][k]  <span class=comment># if dim == 2</span></span><br></pre></table></figure><p>self、index 和 src（如果是张量）的<strong>维数应该相同</strong>.对于<strong>所有维度d,index.size(d) <= src.size(d)</strong>；对于所有<strong>维度 d != dim,index.size(d) <= self.size(d)</strong>,index 和 src 不会广播.<p>与gather逆操作,常用作写one-hot量<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line>index = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>]])</span><br><span class=line>value = <span class=number>2</span></span><br><span class=line>torch.zeros(<span class=number>3</span>, <span class=number>5</span>).scatter_(<span class=number>0</span>, index, value)</span><br><span class=line></span><br><span class=line>src = torch.arange(<span class=number>1</span>, <span class=number>11</span>).reshape((<span class=number>2</span>, <span class=number>5</span>))</span><br><span class=line>src</span><br><span class=line>index = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>, <span class=number>2</span>, <span class=number>0</span>]])</span><br><span class=line>torch.zeros(<span class=number>3</span>, <span class=number>5</span>, dtype=src.dtype).scatter_(<span class=number>0</span>, index, src)</span><br><span class=line>index = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>, <span class=number>2</span>], [<span class=number>0</span>, <span class=number>1</span>, <span class=number>4</span>]])</span><br><span class=line>torch.zeros(<span class=number>3</span>, <span class=number>5</span>, dtype=src.dtype).scatter_(<span class=number>1</span>, index, src)</span><br><span class=line></span><br><span class=line>torch.full((<span class=number>2</span>, <span class=number>4</span>), <span class=number>2.</span>).scatter_(<span class=number>1</span>, torch.tensor([[<span class=number>2</span>], [<span class=number>3</span>]]),</span><br><span class=line>           <span class=number>1.23</span>, reduce=<span class=string>'multiply'</span>)</span><br><span class=line>torch.full((<span class=number>2</span>, <span class=number>4</span>), <span class=number>2.</span>).scatter_(<span class=number>1</span>, torch.tensor([[<span class=number>2</span>], [<span class=number>3</span>]]),</span><br><span class=line>           <span class=number>1.23</span>, reduce=<span class=string>'add'</span>)</span><br></pre></table></figure><h3 id=torch-gather><a class=headerlink href=#torch-gather title=torch.gather></a>torch.gather</h3><p><code>torch.gather(input, dim, index, *, sparse_grad=False, out=None) → [Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)</code><p>输入和索引的维数必须相同. <strong>在 d != dim的维度 中，index.size(d) <= input.size(d)</strong>。输入和index不会相互广播<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>out[i][j][k] = <span class=built_in>input</span>[index[i][j][k]][j][k]  <span class=comment># if dim == 0</span></span><br><span class=line>out[i][j][k] = <span class=built_in>input</span>[i][index[i][j][k]][k]  <span class=comment># if dim == 1</span></span><br><span class=line>out[i][j][k] = <span class=built_in>input</span>[i][j][index[i][j][k]]  <span class=comment># if dim == 2</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>t = torch.tensor([[<span class=number>1</span>, <span class=number>2</span>], [<span class=number>3</span>, <span class=number>4</span>]])</span><br><span class=line>torch.gather(t, <span class=number>1</span>, torch.tensor([[<span class=number>0</span>, <span class=number>0</span>], [<span class=number>1</span>, <span class=number>0</span>]]))</span><br></pre></table></figure><p>上面的代码就是把t根据index torch.tensor([[0, 0], [1, 0]])重新得到一个tensor.<blockquote><p>scatter是通过index将src的数据放在input中<p>gather是通过index将input的数据取出来</blockquote><h3 id=torch-split><a class=headerlink href=#torch-split title=torch.split></a>torch.split</h3><p><code>torch.split(tensor, split_size_or_sections, dim=0</code><p>将张量分割成块.每个块都是原始张量的一个view.<p>如果 split_size_or_sections 是整数类型,那么张量将被分割成大小相等的块（如果可能）.如果张量在给定维度 dim 上的大小不能被 split_size 整除,则最后一个块的大小会变小.<p>如果 split_size_or_sections 是一个列表,那么张量将被分割成 len(split_size_or_sections)小块,其大小与 split_size_or_sections 一致.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line>a = torch.arange(<span class=number>10</span>).reshape(<span class=number>5</span>, <span class=number>2</span>)</span><br><span class=line>torch.split(a, <span class=number>2</span>)</span><br><span class=line>(tensor([[<span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>         [<span class=number>2</span>, <span class=number>3</span>]]),</span><br><span class=line> tensor([[<span class=number>4</span>, <span class=number>5</span>],</span><br><span class=line>         [<span class=number>6</span>, <span class=number>7</span>]]),</span><br><span class=line> tensor([[<span class=number>8</span>, <span class=number>9</span>]]))</span><br><span class=line>torch.split(a, [<span class=number>1</span>, <span class=number>4</span>])</span><br><span class=line>(tensor([[<span class=number>0</span>, <span class=number>1</span>]]),</span><br><span class=line> tensor([[<span class=number>2</span>, <span class=number>3</span>],</span><br><span class=line>         [<span class=number>4</span>, <span class=number>5</span>],</span><br><span class=line>         [<span class=number>6</span>, <span class=number>7</span>],</span><br><span class=line>         [<span class=number>8</span>, <span class=number>9</span>]]))</span><br></pre></table></figure><h3 id=torch-tensor-split><a class=headerlink href=#torch-tensor-split title=torch.tensor_split></a>torch.tensor_split</h3><p><code>torch.tensor_split(input, indices_or_sections, dim=0) → List of Tensors</code><p>根据 indices_or_sections 指定的索引或部分数,将张量沿维度 dim 分割成多个子张量,所有子张量都是输入的视图.<ul><li><p>如果 indices_or_sections 是一个整数 n 或一个数值为 n 的零维长张量，则输入会沿着维度 dim 被分割成 n 个部分。如果输入沿着维数 dim 被 n 整除，则每个部分的大小相等，即 input.size(dim) / n。如果输入不能被 n 整除，则第一个 int(input.size(dim) % n) 部分的大小为 int(input.size(dim) / n) + 1，其余部分的大小为 int(input.size(dim)/n)。</p><li><p>如果 indices_or_sections 是一个 ints 列表或元组，或者是一个一维长张量，那么输入将在列表、元组或张量中的每个索引处沿着维度 dim 分割。例如，如果 indices_or_sections=[2,3]，dim=0，则会产生张量 input[:2]、input[2:3] 和 input[3:]。</p><li><p>如果 indices_or_sections 是张量，在 CPU 上必须是零维或一维长张量。</p></ul><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line>x = torch.arange(<span class=number>8</span>)</span><br><span class=line>torch.tensor_split(x, <span class=number>3</span>)</span><br><span class=line></span><br><span class=line>x = torch.arange(<span class=number>7</span>)</span><br><span class=line>torch.tensor_split(x, <span class=number>3</span>)</span><br><span class=line>torch.tensor_split(x, (<span class=number>1</span>, <span class=number>6</span>))</span><br><span class=line></span><br><span class=line>x = torch.arange(<span class=number>14</span>).reshape(<span class=number>2</span>, <span class=number>7</span>)</span><br><span class=line>x</span><br><span class=line>torch.tensor_split(x, <span class=number>3</span>, dim=<span class=number>1</span>)</span><br><span class=line>torch.tensor_split(x, (<span class=number>1</span>, <span class=number>6</span>), dim=<span class=number>1</span>)</span><br></pre></table></figure><blockquote><p>split如果输入是整数,按照dim分成多段,每段dim上的大小等于这个整数(如果能除尽)<p>如果输入是list,每段大小就对应list中的值(list的长度也跟dim对应的大小相同).返回tuple[Tensor,…]<p>tensor_split如果输入是整数,能除尽的话结果就跟split类似,否则前<code>int(input.size(dim) % n)</code> 段 大小<code>int(input.size(dim) / n) + 1</code>, 后面的大小 为<code>int(input.size(dim) / n)</code><p>如果是list,每一段数据是list中的两个indices,也就是<p>For instance, <code>indices_or_sections=[2, 3]</code> and <code>dim=0</code> would result in the tensors <code>input[:2]</code>, <code>input[2:3]</code>, and <code>input[3:]</code><p>两者默认dim都是0</blockquote><h3 id=torch-Tensor-repeat><a class=headerlink href=#torch-Tensor-repeat title=torch.Tensor.repeat></a>torch.Tensor.repeat</h3><p><code>Tensor.repeat(*sizes)</code><p>进行拷贝数据,沿指定维度重复此张量。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>])</span><br><span class=line>x.repeat(<span class=number>4</span>, <span class=number>2</span>)</span><br><span class=line>x.repeat(<span class=number>4</span>, <span class=number>2</span>, <span class=number>1</span>).size()</span><br></pre></table></figure><h3 id=torch-repeat-interleave><a class=headerlink href=#torch-repeat-interleave title=torch.repeat_interleave></a>torch.repeat_interleave</h3><p><code>torch.repeat_interleave(input, repeats, dim=None, *, output_size=None)</code><ul><li><strong>input</strong> (<a href=https://pytorch.org/docs/stable/tensors.html#torch.Tensor rel=noopener target=_blank><em>Tensor</em></a>) – the input tensor.<li><strong>repeats</strong> (<a href=https://pytorch.org/docs/stable/tensors.html#torch.Tensor rel=noopener target=_blank><em>Tensor</em></a> <em>or</em> <a href=https://docs.python.org/3/library/functions.html#int rel=noopener target=_blank><em>int</em></a>) – The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.<li><strong>dim</strong> (<a href=https://docs.python.org/3/library/functions.html#int rel=noopener target=_blank><em>int</em></a><em>,</em> <em>optional</em>) – The dimension along which to repeat values. <strong>By default, use the flattened input array, and return a flat output array</strong>.</ul><p><strong>output_size</strong> (<a href=https://docs.python.org/3/library/functions.html#int rel=noopener target=_blank><em>int</em></a><em>,</em> <em>optional</em>) – Total output size for the given axis ( e.g. sum of repeats). If given, it will avoid stream synchronization needed to calculate output shape of the tensor.<p>重复的方式是每个值后重复一次<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>])</span><br><span class=line>x.repeat_interleave(<span class=number>2</span>)</span><br><span class=line><span class=comment># tensor([1, 1, 2, 2, 3, 3])</span></span><br><span class=line>y = torch.tensor([[<span class=number>1</span>, <span class=number>2</span>], [<span class=number>3</span>, <span class=number>4</span>]])</span><br><span class=line>torch.repeat_interleave(y, <span class=number>2</span>)</span><br><span class=line><span class=comment># tensor([1, 1, 2, 2, 3, 3, 4, 4])</span></span><br><span class=line>torch.repeat_interleave(y, <span class=number>3</span>, dim=<span class=number>1</span>)</span><br><span class=line>torch.repeat_interleave(y, torch.tensor([<span class=number>1</span>, <span class=number>2</span>]), dim=<span class=number>0</span>)</span><br><span class=line><span class=comment># tensor([[1, 2],</span></span><br><span class=line>        [<span class=number>3</span>, <span class=number>4</span>],</span><br><span class=line>        [<span class=number>3</span>, <span class=number>4</span>]])</span><br><span class=line>torch.repeat_interleave(y, torch.tensor([<span class=number>1</span>, <span class=number>2</span>]), dim=<span class=number>0</span>, output_size=<span class=number>3</span>)</span><br><span class=line><span class=comment># tensor([[1, 2],</span></span><br><span class=line>        [<span class=number>3</span>, <span class=number>4</span>],</span><br><span class=line>        [<span class=number>3</span>, <span class=number>4</span>]])</span><br></pre></table></figure><h3 id=torch-Tensor-expand><a class=headerlink href=#torch-Tensor-expand title=torch.Tensor.expand></a>torch.Tensor.expand</h3><p><code>Tensor.expand(*sizes)</code><p>返回输入张量的新视图,并将单维度扩展到更大尺寸.<p>将 -1 作为维度的大小意味着不改变该维度的大小.<p>张量也可以扩展到更多维数,新的维数将被添加到前面。对于新维度,大小不能设置为-1.<p>扩展张量不会分配新的内存,只会在现有张量上创建一个新的视图,其中大小为 1 的维度会通过设置跨距为 0 来扩展为更大的维度.<blockquote><p>扩展张量的一个以上元素可能指向一个内存位置。 因此，in-place操作（尤其是矢量化操作）可能会导致不正确的行为。 如果需要写入张量，请先克隆它们。</blockquote><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([[<span class=number>1</span>], [<span class=number>2</span>], [<span class=number>3</span>]])</span><br><span class=line>x.size() <span class=comment># torch.Size([3, 1])</span></span><br><span class=line>x.expand(<span class=number>3</span>, <span class=number>4</span>)</span><br><span class=line>x.expand(-<span class=number>1</span>, <span class=number>4</span>)   <span class=comment># -1 means not changing the size of that dimension</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([[<span class=number>1</span>], [<span class=number>2</span>], [<span class=number>3</span>]])</span><br><span class=line><span class=built_in>print</span>(x.expand(<span class=number>3</span>, <span class=number>4</span>).stride()) <span class=comment># (1, 0)</span></span><br><span class=line><span class=built_in>print</span>(x.expand(-<span class=number>1</span>, <span class=number>4</span>).stride()) <span class=comment># (1, 0)</span></span><br></pre></table></figure><h3 id=torch-Tensor-expand-as><a class=headerlink href=#torch-Tensor-expand-as title=torch.Tensor.expand_as></a>torch.Tensor.expand_as</h3><p><code>Tensor.expand_as(other)</code><p>将此张量展开为与其他张量相同的大小。self.expand_as(other) 相当于 self.expand(other.size())。<p>expand和repeat,前者返回view,后者返回数据.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>])</span><br><span class=line><span class=built_in>print</span>(x.repeat(<span class=number>6</span>, <span class=number>1</span>))</span><br><span class=line><span class=built_in>print</span>(x.expand(<span class=number>6</span>, <span class=number>3</span>)) <span class=comment># same</span></span><br></pre></table></figure><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\09\16\ZTM-pytorchForDL\ rel=bookmark>ZTM-pytorchForDL</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\09\12\pytorch学习——初探\ rel=bookmark>pytorch学习——初探</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2022\04\15\pytorch学习\ rel=bookmark>pytorch学习</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/06/23/effective_pytorch/ title=使用pytorch时你可能需要注意的地方>https://www.sekyoro.top/2024/06/23/effective_pytorch/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/pytorch/ rel=tag><i class="fa fa-tag"></i> pytorch</a></div><div class=post-nav><div class=post-nav-item><a title="A better C:from C++, Go,Rust to Zig" href=/2024/06/23/A-better-C-from-C-Go-Rust-to-Zig/ rel=prev> <i class="fa fa-chevron-left"></i> A better C:from C++, Go,Rust to Zig </a></div><div class=post-nav-item><a href=/2024/06/23/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%85%A5%E9%97%A8/ rel=next title=函数式编程介绍与入门> 函数式编程介绍与入门 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#tensor><span class=nav-number>1.</span> <span class=nav-text>tensor</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Tensor><span class=nav-number>1.1.</span> <span class=nav-text>Tensor</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Views><span class=nav-number>1.2.</span> <span class=nav-text>Views</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Extending-PyTorch><span class=nav-number>2.</span> <span class=nav-text>Extending PyTorch</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#extending-torch-autograd><span class=nav-number>2.1.</span> <span class=nav-text>extending torch.autograd</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8><span class=nav-number>2.1.0.1.</span> <span class=nav-text>如何使用</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E4%B8%8D%E7%94%A8><span class=nav-number>2.1.0.2.</span> <span class=nav-text>什么时候不用</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95><span class=nav-number>2.1.0.3.</span> <span class=nav-text>使用方法</span></a></ol></ol><li class="nav-item nav-level-3"><a class=nav-link href=#extending-torch-nn><span class=nav-number>2.2.</span> <span class=nav-text>extending torch.nn</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Extending-torch-Tensor-like-type><span class=nav-number>2.2.0.1.</span> <span class=nav-text>Extending torch Tensor-like type</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#torch-autograd><span class=nav-number>3.</span> <span class=nav-text>torch.autograd</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#detach-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8Eleaf-tensor><span class=nav-number>3.1.</span> <span class=nav-text>detach 计算图与leaf tensor</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%B1%9E%E4%BA%8E%E6%97%A7%E6%97%B6%E4%BB%A3%E7%9A%84Variable%E5%92%8Cdata><span class=nav-number>3.2.</span> <span class=nav-text>属于旧时代的Variable和data</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Function><span class=nav-number>3.3.</span> <span class=nav-text>Function</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#ONNX%E6%A0%BC%E5%BC%8F><span class=nav-number>4.</span> <span class=nav-text>ONNX格式</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B><span class=nav-number>4.1.</span> <span class=nav-text>保存模型</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B><span class=nav-number>4.1.1.</span> <span class=nav-text>加载模型</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%B5%81%E7%A8%8B><span class=nav-number>4.2.</span> <span class=nav-text>流程</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B3%A8%E6%84%8F><span class=nav-number>4.2.1.</span> <span class=nav-text>注意</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6><span class=nav-number>5.</span> <span class=nav-text>自动混合精度</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#cuda%E4%B8%8A%E4%BC%9A%E8%BD%AC%E4%B8%BAfloat16%E7%9A%84%E8%BF%90%E7%AE%97><span class=nav-number>5.0.1.</span> <span class=nav-text>cuda上会转为float16的运算</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#cuda%E4%B8%8A%E4%BC%9A%E8%BD%AC%E4%B8%BAfloat32%E7%9A%84%E8%BF%90%E7%AE%97><span class=nav-number>5.0.2.</span> <span class=nav-text>cuda上会转为float32的运算</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#CPU%E4%B8%8A%E4%BC%9A%E8%BD%AC%E4%B8%BAbfloat16%E7%9A%84%E8%BF%90%E7%AE%97><span class=nav-number>5.0.3.</span> <span class=nav-text>CPU上会转为bfloat16的运算</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#CPU%E4%B8%8A%E4%BC%9A%E8%BD%AC%E4%B8%BAbfloat32%E7%9A%84%E8%BF%90%E7%AE%97><span class=nav-number>5.0.4.</span> <span class=nav-text>CPU上会转为bfloat32的运算</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%95%E6%9C%BA%E5%99%A8-%E5%A4%9AGPU-%E8%AE%AD%E7%BB%83%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5><span class=nav-number>6.</span> <span class=nav-text>单机器(多GPU)训练最佳实践</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%9AGPU%E8%AE%AD%E7%BB%83><span class=nav-number>7.</span> <span class=nav-text>多GPU训练</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%B9%B6%E8%A1%8C%E6%96%B9%E6%A1%88><span class=nav-number>7.0.1.</span> <span class=nav-text>并行方案</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%B8%B8%E7%94%A8Container><span class=nav-number>8.</span> <span class=nav-text>常用Container</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%AE%B9%E6%98%93%E6%B7%B7%E6%B7%86%E5%92%8C%E9%81%97%E5%BF%98%E7%9A%84%E6%96%B9%E6%B3%95><span class=nav-number>9.</span> <span class=nav-text>容易混淆和遗忘的方法</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#torch-Tensor-scatter><span class=nav-number>9.1.</span> <span class=nav-text>torch.Tensor.scatter_</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-gather><span class=nav-number>9.2.</span> <span class=nav-text>torch.gather</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-split><span class=nav-number>9.3.</span> <span class=nav-text>torch.split</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-tensor-split><span class=nav-number>9.4.</span> <span class=nav-text>torch.tensor_split</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-Tensor-repeat><span class=nav-number>9.5.</span> <span class=nav-text>torch.Tensor.repeat</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-repeat-interleave><span class=nav-number>9.6.</span> <span class=nav-text>torch.repeat_interleave</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-Tensor-expand><span class=nav-number>9.7.</span> <span class=nav-text>torch.Tensor.expand</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-Tensor-expand-as><span class=nav-number>9.8.</span> <span class=nav-text>torch.Tensor.expand_as</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>216</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>203</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/pytorch/ rel=tag>pytorch</a><span class=tag-list-count>4</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.8m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>27:58</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>