<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch." name=description><meta content=article property=og:type><meta content=使用pytorch时你可能需要注意的地方 property=og:title><meta content=https://www.sekyoro.top/2024/06/23/pytorch%E6%93%8D%E4%BD%9Ctensor%E6%97%B6%E4%BD%A0%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch." property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png property=og:image><meta content=2024-06-23T08:16:46.000Z property=article:published_time><meta content=2024-06-25T14:59:08.108Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content=pytorch property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png name=twitter:image><link href=https://www.sekyoro.top/2024/06/23/pytorch%E6%93%8D%E4%BD%9Ctensor%E6%97%B6%E4%BD%A0%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>使用pytorch时你可能需要注意的地方 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/06/23/pytorch%E6%93%8D%E4%BD%9Ctensor%E6%97%B6%E4%BD%A0%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>使用pytorch时你可能需要注意的地方</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-06-23 16:16:46" datetime=2024-06-23T16:16:46+08:00>2024-06-23</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-06-25 22:59:08" datetime=2024-06-25T22:59:08+08:00 itemprop=dateModified>2024-06-25</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>19k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>18 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>Pytorch是很好的深度学习框架,但在使用时你可能仍然不清楚其中一些概念.这里我只以官方文档为依据尝试解释其中一些概念和方法. 我这里可以称作Effective Pytorch.<br><span id=more></span><h2 id=tensor><a class=headerlink href=#tensor title=tensor></a>tensor</h2><h3 id=Tensor><a class=headerlink href=#Tensor title=Tensor></a>Tensor</h3><p>pytorch默认浮点类型是torch.float32,可以使用<code>torch.set_default_dtype</code>修改<p>torch.zeros等默认类型就是就是torch.float32,使用<code>torch.set_default_dtype</code>修改默认类型.<p>torch.tensor() 总是复制<code>data</code>(深拷贝,表示地址不相同)。如果你有一个张量数据，<strong>只是想更改它的 requires<em>grad 标志，请使用 requires_grad</em>() 或 detach() 来避免复制</strong>。<p>如果你有一个 numpy 数组，并<strong>希望避免复制，请使用 torch.as_tensor()</strong>。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>torch.device(<span class=string>'cuda:0'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'cpu'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'mps'</span>)</span><br><span class=line></span><br><span class=line>torch.device(<span class=string>'cuda'</span>)  <span class=comment># current cuda device</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>x = torch.tensor([[<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>4</span>, <span class=number>5</span>], [<span class=number>6</span>, <span class=number>7</span>, <span class=number>8</span>, <span class=number>9</span>, <span class=number>10</span>]])</span><br><span class=line>x.stride() <span class=comment>#(5,1)</span></span><br><span class=line></span><br><span class=line>x.t().stride() <span class=comment>#(1,5)</span></span><br></pre></table></figure><h3 id=Views><a class=headerlink href=#Views title=Views></a>Views</h3><p>PyTorch 允许张量成为现有张量的 “views”。<strong>视图张量与其基础张量共享相同的底层数据</strong>。支持 “views “可以避免显式数据复制，从而使我们能够进行快速、高效的内存重塑、切片和元素操作。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>t = torch.rand(<span class=number>4</span>, <span class=number>4</span>)</span><br><span class=line>b = t.view(<span class=number>2</span>, <span class=number>8</span>)</span><br><span class=line>t.storage().data_ptr() == b.storage().data_ptr()  <span class=comment># `t` and `b` share the same underlying data.</span></span><br><span class=line>b[<span class=number>0</span>][<span class=number>0</span>] = <span class=number>3.14</span></span><br><span class=line>t[<span class=number>0</span>][<span class=number>0</span>]</span><br></pre></table></figure><p>由于views与其基础张量共享底层数据，因此如果修改views中的数据，也会反映在基础张量中。<p>通常，PyTorch 操作会返回一个新的张量作为输出，例如 add()。但在视图操作中，输出是输入张量的视图，以避免不必要的数据复制。创建视图时不会发生数据移动，视图张量只是改变了解释相同数据的方式。<p><strong>对连续张量进行视图处理可能会产生非连续张量</strong>。transpose() 就是一个常见的例子。(包括view,transpose等操作都会返回view,也就是数据存储与输入相同)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>base = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>],[<span class=number>2</span>, <span class=number>3</span>]])</span><br><span class=line>base.is_contiguous()</span><br><span class=line>t = base.transpose(<span class=number>0</span>, <span class=number>1</span>)  <span class=comment># `t` is a view of `base`. No data movement happened here.</span></span><br><span class=line>t.is_contiguous()</span><br><span class=line>c = t.contiguous()</span><br></pre></table></figure><p><img alt=image-20240625212647753 data-src=https://s2.loli.net/2024/06/25/vKDYZ7A1jb3g9kO.png><p><a href=https://zhuanlan.zhihu.com/p/342856639 rel=noopener target=_blank>通过公式判断张量是否连续 - 知乎 (zhihu.com)</a><h2 id=torch-autograd><a class=headerlink href=#torch-autograd title=torch.autograd></a>torch.autograd</h2><p>torch.autograd 提供了实现任意标量值函数自动微分的类和函数。<p>它只需对现有代码做极少的改动—你只需用 requires_grad=True 关键字声明需要计算梯度的张量。<p>目前只持浮点张量类型（半浮点、浮点、双浮点和 bfloat16）和复合张量类型（cfloat、cdouble）的 autograd。<h3 id=detach-计算图与leaf-tensor><a title="detach 计算图与leaf tensor" class=headerlink href=#detach-计算图与leaf-tensor></a>detach 计算图与leaf tensor</h3><p><code>Tensor.detach()</code>返回一个从当前计算图中分离出来的新张量,生成的张量永远不需要梯度,目前替代了<code>.data</code>方法.<p>PyTorch 中,计算图(Computation Graph)是一个非常重要的概念。它是一种用于表示和执行机器学习模型的数据结构。<p>具体来说,PyTorch 中的计算图由以下几个关键组件组成:<ol><li><strong>张量(Tensor)</strong>：计算图的基本单元,表示输入数据、中间结果和最终输出。<li><strong>操作(Operation)</strong>：在张量上执行的各种数学运算,如加法、乘法、卷积等。<li><strong>节点(Node)</strong>：表示张量和操作,计算图由这些节点组成。<li><strong>边(Edge)</strong>：表示节点之间的依赖关系,数据沿着边流动。</ol><p>当在 PyTorch 中定义和执行机器学习模型时,PyTorch 会自动构建一个计算图来表示模型的结构和数据流。这个计算图可以用于以下几个方面:<ol><li><strong>前向传播</strong>：通过计算图,PyTorch 可以自动计算模型的输出。<li><strong>反向传播</strong>：当您调用 <code>loss.backward()</code> 时,PyTorch 会沿着计算图反向传播梯度,从而更新模型参数。<li><strong>可视化</strong>：您可以使用 PyTorch 提供的工具(如 TensorBoard)来可视化计算图,更好地理解模型的结构。<li><strong>优化</strong>：PyTorch 的优化器会利用计算图的结构来提高优化效率。</ol><p>按照惯例，所有<strong>requires_grad 为False的张量都是leaf tensor</strong>。<p>对于<strong>requires_grad 为 True 的张量，如果它们是由用户创建(没有经过计算,包括移动到GPU的操作)的，那么它们将是叶子张量</strong>。这意味着它们不是操作的结果，因此 grad_fn 为 None。<p>只有叶子张量才会在调用 backward() 时被填充梯度。要为非叶子张量填充阶值，可以使用 retain_grad()。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line>xx = torch.randn(<span class=number>1</span>, <span class=number>3</span>).requires_grad_(<span class=literal>True</span>)</span><br><span class=line><span class=built_in>print</span>(xx.grad_fn, xx.is_leaf) <span class=comment># None,True</span></span><br><span class=line>model = nn.Linear(<span class=number>3</span>, <span class=number>1</span>)</span><br><span class=line>output = model(xx)</span><br><span class=line>loss = torch.mean(output - <span class=number>1</span>)</span><br><span class=line>loss.backward()</span><br><span class=line><span class=built_in>print</span>(xx.grad_fn, xx.grad) <span class=comment># None,tensor([[.., ..,  ..]])</span></span><br><span class=line><span class=built_in>print</span>(model.weight.grad_fn, model.weight.grad) <span class=comment># None,,tensor([[.., ..,  ..]])</span></span><br><span class=line><span class=built_in>print</span>(model.bias.grad_fn, model.bias.grad) <span class=comment># None,tensor([1])</span></span><br><span class=line><span class=built_in>print</span>(model.weight.is_leaf, model.bias.is_leaf) <span class=comment># True,True</span></span><br></pre></table></figure><p>只能获取计算图中叶子节点的梯度属性,这些节点的 requires_grad 属性设置为 True。对于图中的所有其他节点,梯度属性将不可用。<p>出于性能考虑,我们只能在给定图形上使用一次后向操作执行梯度计算。如果我们需要在同一图形上执行多次 backward 调用，则需要向 backward 调用传递 retain_graph=True 属性。<p>几个问题:<p>leaf tensor的grad_fn一定为空吗? 不一定,用户创建的requires_grad为True的tensor的grad_fn不为空<p>leaf tensor一定是模型输入吗?不一定,事实上直接创建一个模型,它的weight和bias也是leaf tensor<h3 id=属于旧时代的Variable和data><a class=headerlink href=#属于旧时代的Variable和data title=属于旧时代的Variable和data></a>属于旧时代的Variable和data</h3><p>Variable API 已被弃用:使用张量时，不再需要Variable。如果 requires_grad 设置为 True，Autograd 将自动支持张量。<p>Variable(tensor) 和 Variable(tensor, requires_grad) 仍按预期工作，但它们返回的是张量而不是变量。<p>var.data 与 tensor.data 相同。<p>var.backward()、var.detach()、var.register_hook() 等方法现在可以在具有相同方法名的张量上运行。<p>此外，现在还可以使用 torch.randn()、torch.zeros()、torch.none() 等工厂方法创建 requires_grad=True 的张量：<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>autograd_tensor = torch.randn((<span class=number>2</span>, <span class=number>3</span>, <span class=number>4</span>), requires_grad=<span class=literal>True</span>)</span><br></pre></table></figure><div class=table-container><table><thead><tr><th>api<th>介绍<tbody><tr><td><code>torch.Tensor.grad</code><td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a href=https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward rel=noopener target=_blank><code>backward()</code></a> computes gradients for <code>self</code><tr><td><code>torch.Tensor.requires_grad</code><td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.<tr><td><code>torch.Tensor.is_leaf</code><td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.<tr><td><code>torch.Tensor.backward</code>([gradient, …])<td>Computes the gradient of current tensor wrt graph leaves.<tr><td><code>torch.Tensor.detach</code><td>Returns a new Tensor, detached from the current graph.<tr><td><code>torch.Tensor.detach_</code><td>Detaches the Tensor from the graph that created it, making it a leaf.<tr><td><code>torch.Tensor.register_hook</code>(hook)<td>Registers a backward hook.<tr><td><code>torch.Tensor.register_post_accumulate_grad_hook</code>(hook)<td>Registers a backward hook that runs after grad accumulation.<tr><td><code>torch.Tensor.retain_grad</code>()<td>Enables this Tensor to have their <a href=https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad rel=noopener target=_blank><code>grad</code></a> populated during <a href=https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward rel=noopener target=_blank><code>backward()</code></a>.</table></div><h3 id=Function><a class=headerlink href=#Function title=Function></a>Function</h3><p>要创建自定义 autograd.Function，请继承该类并实现 forward() 和 backward() 静态方法。然后，要在前向传递中使用自定义 op，调用类方法 apply。不要直接调用 forward()。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Exp</span>(<span class=params>Function</span>):</span></span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>ctx, i</span>):</span></span><br><span class=line>        result = i.exp()</span><br><span class=line>        ctx.save_for_backward(result)</span><br><span class=line>        <span class=keyword>return</span> result</span><br><span class=line><span class=meta>    @staticmethod</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>backward</span>(<span class=params>ctx, grad_output</span>):</span></span><br><span class=line>        result, = ctx.saved_tensors</span><br><span class=line>        <span class=keyword>return</span> grad_output * result</span><br><span class=line></span><br><span class=line>Use it by calling the apply method:</span><br><span class=line>output = Exp.apply(<span class=built_in>input</span>)</span><br></pre></table></figure><h2 id=ONNX格式><a class=headerlink href=#ONNX格式 title=ONNX格式></a>ONNX格式</h2><p>在实际部署时非常常用的模型格式,是屏蔽了框架的.<p><a href=https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html rel=noopener target=_blank>(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime — PyTorch Tutorials 2.3.0+cu121 documentation</a><h3 id=保存模型><a class=headerlink href=#保存模型 title=保存模型></a>保存模型</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torchvision</span><br><span class=line></span><br><span class=line>dummy_input = torch.randn(<span class=number>10</span>, <span class=number>3</span>, <span class=number>224</span>, <span class=number>224</span>, device=<span class=string>"cuda"</span>)</span><br><span class=line>model = torchvision.models.alexnet(pretrained=<span class=literal>True</span>).cuda()</span><br><span class=line></span><br><span class=line>input_names = [ <span class=string>"actual_input_1"</span> ] + [ <span class=string>"learned_%d"</span> % i <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>16</span>) ]</span><br><span class=line>output_names = [ <span class=string>"output1"</span> ]</span><br><span class=line></span><br><span class=line>torch.onnx.export(model, dummy_input, <span class=string>"alexnet.onnx"</span>, verbose=<span class=literal>True</span>, input_names=input_names, output_names=output_names)</span><br></pre></table></figure><p>生成的 <code>alexnet.onnx</code> 文件包含一个 <a href=https://developers.google.com/protocol-buffers/ rel=noopener target=_blank>protocol buffer</a>,其中包含了导出的模型(在本例中为 AlexNet)的网络结构和参数。<code>verbose=True</code> 参数会导致导出器打印出模型的人类可读表示。<h4 id=加载模型><a class=headerlink href=#加载模型 title=加载模型></a>加载模型</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>pip install onnx</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnx</span><br><span class=line><span class=comment># Load the ONNX model</span></span><br><span class=line>model = onnx.load(<span class=string>"alexnet.onnx"</span>)</span><br><span class=line></span><br><span class=line><span class=comment># Check that the model is well formed</span></span><br><span class=line>onnx.checker.check_model(model)</span><br><span class=line></span><br><span class=line><span class=comment># Print a human readable representation of the graph</span></span><br><span class=line><span class=built_in>print</span>(onnx.helper.printable_graph(model.graph))</span><br><span class=line>You can also run the exported model <span class=keyword>with</span> one of the many runtimes that support ONNX. For example after installing ONNX Runtime, you can load <span class=keyword>and</span> run the model:</span><br><span class=line></span><br><span class=line><span class=keyword>import</span> onnxruntime <span class=keyword>as</span> ort</span><br><span class=line></span><br><span class=line>ort_session = ort.InferenceSession(<span class=string>"alexnet.onnx"</span>)</span><br><span class=line></span><br><span class=line>outputs = ort_session.run(</span><br><span class=line>    <span class=literal>None</span>,</span><br><span class=line>    {<span class=string>"actual_input_1"</span>: np.random.randn(<span class=number>10</span>, <span class=number>3</span>, <span class=number>224</span>, <span class=number>224</span>).astype(np.float32)},</span><br><span class=line>)</span><br><span class=line><span class=built_in>print</span>(outputs[<span class=number>0</span>])</span><br><span class=line><span class=comment># Print a human readable representation of the graph</span></span><br><span class=line><span class=built_in>print</span>(onnx.helper.printable_graph(model.graph))</span><br></pre></table></figure><h3 id=流程><a class=headerlink href=#流程 title=流程></a>流程</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=comment># Input to the model</span></span><br><span class=line>x = torch.randn(batch_size, <span class=number>1</span>, <span class=number>224</span>, <span class=number>224</span>, requires_grad=<span class=literal>True</span>)</span><br><span class=line>torch_out = torch_model(x)</span><br><span class=line></span><br><span class=line><span class=comment># Export the model</span></span><br><span class=line>torch.onnx.export(torch_model,               <span class=comment># model being run</span></span><br><span class=line>                  x,                         <span class=comment># model input (or a tuple for multiple inputs)</span></span><br><span class=line>                  <span class=string>"super_resolution.onnx"</span>,   <span class=comment># where to save the model (can be a file or file-like object)</span></span><br><span class=line>                  export_params=<span class=literal>True</span>,        <span class=comment># store the trained parameter weights inside the model file</span></span><br><span class=line>                  opset_version=<span class=number>10</span>,          <span class=comment># the ONNX version to export the model to</span></span><br><span class=line>                  do_constant_folding=<span class=literal>True</span>,  <span class=comment># whether to execute constant folding for optimization</span></span><br><span class=line>                  input_names = [<span class=string>'input'</span>],   <span class=comment># the model's input names</span></span><br><span class=line>                  output_names = [<span class=string>'output'</span>], <span class=comment># the model's output names</span></span><br><span class=line>                  dynamic_axes={<span class=string>'input'</span> : {<span class=number>0</span> : <span class=string>'batch_size'</span>},    <span class=comment># variable length axes</span></span><br><span class=line>                                <span class=string>'output'</span> : {<span class=number>0</span> : <span class=string>'batch_size'</span>}})</span><br></pre></table></figure><p>在pytorch中直接使用<code>torch.onnx.export</code>即可.<p>因为导出运行了模型,我们需要提供一个输入张量 <code>x</code>。这个输入的值可以是随机的,只要它的类型和大小是正确的。请注意,除非指定为dynamic_axes,否则导出的 ONNX 图中输入的所有维度大小都会被固定下来。在这个示例中,使用批量大小为 1 的输入导出模型,但在 <code>torch.onnx.export()</code> 的 <code>dynamic_axes</code> 参数中指定了第一个维度为动态的。因此,导出的模型将接受大小为 <code>[batch_size, 1, 224, 224]</code> 的输入,其中 <code>batch_size</code> 可以是可变的。<p>同时还计算了模型输出 <code>torch_out</code>,我们将使用它来验证在 ONNX Runtime 中运行时导出的模型是否计算出相同的值。<p>但在使用 ONNX Runtime 验证模型输出之前,我们会先使用 ONNX API 检查 ONNX 模型。首先,<code>onnx.load("super_resolution.onnx")</code> 会加载保存的模型,并输出一个 <code>onnx.ModelProto</code> 结构。然后,<code>onnx.checker.check_model(onnx_model)</code> 会验证模型的结构,并确认模型具有有效的架构。通过检查模型的版本、图结构以及节点及其输入和输出,来验证 ONNX 图的有效性。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnx</span><br><span class=line>onnx_model = onnx.load(<span class=string>"super_resolution.onnx"</span>)</span><br><span class=line>onnx.checker.check_model(onnx_model)</span><br></pre></table></figure><p>使用 ONNX Runtime 的 Python API 计算输出,通常情况下,这一部分可以在单独的进程中或其他机器上完成,但我们将继续在同一进程中进行,这样我们就可以验证 ONNX Runtime 和 PyTorch 为该网络计算出的值是否相同。<p>为了使用 ONNX Runtime 运行模型,我们需要为模型创建一个InferenceSession,并设置所需的配置参数(这里我们使用默认配置)。创建会话后,我们就可以使用 <code>run()</code> API 来评估模型了。该调用的输出是一个列表,包含 ONNX Runtime 计算得出的模型输出。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> onnxruntime</span><br><span class=line></span><br><span class=line>ort_session = onnxruntime.InferenceSession(<span class=string>"super_resolution.onnx"</span>, providers=[<span class=string>"CPUExecutionProvider"</span>])</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>to_numpy</span>(<span class=params>tensor</span>):</span></span><br><span class=line>    <span class=keyword>return</span> tensor.detach().cpu().numpy() <span class=keyword>if</span> tensor.requires_grad <span class=keyword>else</span> tensor.cpu().numpy()</span><br><span class=line></span><br><span class=line><span class=comment># compute ONNX Runtime output prediction</span></span><br><span class=line>ort_inputs = {ort_session.get_inputs()[<span class=number>0</span>].name: to_numpy(x)}</span><br><span class=line>ort_outs = ort_session.run(<span class=literal>None</span>, ort_inputs)</span><br><span class=line></span><br><span class=line><span class=comment># compare ONNX Runtime and PyTorch results</span></span><br><span class=line>np.testing.assert_allclose(to_numpy(torch_out), ort_outs[<span class=number>0</span>], rtol=<span class=number>1e-03</span>, atol=<span class=number>1e-05</span>)</span><br><span class=line></span><br><span class=line><span class=built_in>print</span>(<span class=string>"Exported model has been tested with ONNXRuntime, and the result looks good!"</span>)</span><br></pre></table></figure><h4 id=注意><a class=headerlink href=#注意 title=注意></a>注意</h4><p>在模型中避免使用numpy,tensor.data,tensor.shape不能使用in_place操作<h2 id=自动混合精度><a class=headerlink href=#自动混合精度 title=自动混合精度></a>自动混合精度</h2><p>torch.amp 为混合精度提供了方便的方法，其中一些操作使用 torch.float32 （浮点）数据类型，另一些操作使用较低精度的浮点数据类型 (lower_precision_fp)：torch.float16（半精度）或 torch.bfloat16。一些操作，如线性层和卷积，在 lower_precision_fp 下速度更快。其他操作，如还原，通常需要 float32 的动态范围。混合精度试图将每个操作与相应的数据类型相匹配。<p>通常，数据类型为 torch.float16 的 “自动混合精度训练 “使用 torch.autocast 和 torch.cpu.amp.GradScaler 或 torch.cuda.amp.GradScaler.<p>torch.autocast 实例可对所选上下文进行自动casting。自动cast会自动选择 GPU 运算的精度，从而在保持精度的同时提高性能。<p>torch.cuda.amp.GradScaler 的实例有助于方便地执行梯度缩放步骤。<strong>梯度缩放可最大限度地减少梯度下溢</strong>，从而改善具有 float16 梯度的网络的收敛性。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=comment># Creates model and optimizer in default precision</span></span><br><span class=line>model = Net().cuda()</span><br><span class=line>optimizer = optim.SGD(model.parameters(), ...)</span><br><span class=line></span><br><span class=line><span class=comment># Creates a GradScaler once at the beginning of training.</span></span><br><span class=line>scaler = GradScaler()  <span class=comment># 1. 创建gradscaler</span></span><br><span class=line></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line></span><br><span class=line>        <span class=comment># Runs the forward pass with autocasting.</span></span><br><span class=line>        <span class=comment># 2.使得模型训练时相关参数类型自动转换</span></span><br><span class=line>        <span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>            output = model(<span class=built_in>input</span>)</span><br><span class=line>            loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span><br><span class=line>        <span class=comment># Backward passes under autocast are not recommended.</span></span><br><span class=line>        <span class=comment># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span><br><span class=line>        scaler.scale(loss).backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># scaler.step() first unscales the gradients of the optimizer's assigned params.</span></span><br><span class=line>        <span class=comment># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span><br><span class=line>        <span class=comment># otherwise, optimizer.step() is skipped.</span></span><br><span class=line>        scaler.step(optimizer)   </span><br><span class=line>        <span class=comment># Updates the scale for next iteration.</span></span><br><span class=line>        scaler.update()</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line><span class=comment># Creates model and optimizer in default precision</span></span><br><span class=line>model = Net().cuda()</span><br><span class=line>optimizer = optim.SGD(model.parameters(), ...)</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>    optimizer.zero_grad()</span><br><span class=line></span><br><span class=line>    <span class=comment># Enables autocasting for the forward pass (model + loss)</span></span><br><span class=line>    <span class=keyword>with</span> torch.autocast(device_type=<span class=string>"cuda"</span>):</span><br><span class=line>        output = model(<span class=built_in>input</span>)</span><br><span class=line>        loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>    <span class=comment># Exits the context manager before backward()</span></span><br><span class=line>    loss.backward()</span><br><span class=line>    optimizer.step()</span><br></pre></table></figure><p>所有由 scaler.scale(loss).backward() 生成的梯度都是按比例缩放的.如果要在 backward() 和 scaler.step(optimizer) 之间修改或检查参数的 .grad 属性,应首先取消缩放.<p><strong>梯度惩罚</strong><p>梯度惩罚的实现通常使用 torch.autograd.grad() 创建梯度，将它们组合起来创建惩罚值，并将惩罚值添加到损失中。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer.zero_grad()</span><br><span class=line>        output = model(<span class=built_in>input</span>)</span><br><span class=line>        loss = loss_fn(output, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># Creates gradients</span></span><br><span class=line>        grad_params = torch.autograd.grad(outputs=loss,</span><br><span class=line>                                          inputs=model.parameters(),</span><br><span class=line>                                          create_graph=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># Computes the penalty term and adds it to the loss</span></span><br><span class=line>        grad_norm = <span class=number>0</span></span><br><span class=line>        <span class=keyword>for</span> grad <span class=keyword>in</span> grad_params:</span><br><span class=line>            grad_norm += grad.<span class=built_in>pow</span>(<span class=number>2</span>).<span class=built_in>sum</span>()</span><br><span class=line>        grad_norm = grad_norm.sqrt()</span><br><span class=line>        loss = loss + grad_norm</span><br><span class=line></span><br><span class=line>        loss.backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># clip gradients here, if desired</span></span><br><span class=line></span><br><span class=line>        optimizer.step()</span><br></pre></table></figure><p>要通过梯度缩放实现梯度惩罚，应缩放传递给 torch.autograd.grad() 的输出张量。因此，生成的梯度也将被缩放，在合并生成惩罚值之前应取消缩放。<p>此外，惩罚项的计算是前向传递的一部分，因此应在自动传递上下文中进行。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line>scaler = torch.cuda.amp.GradScaler()</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> epochs:</span><br><span class=line>    <span class=keyword>for</span> <span class=built_in>input</span>, target <span class=keyword>in</span> data:</span><br><span class=line>        optimizer0.zero_grad()</span><br><span class=line>        optimizer1.zero_grad()</span><br><span class=line>        <span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>            output0 = model0(<span class=built_in>input</span>)</span><br><span class=line>            output1 = model1(<span class=built_in>input</span>)</span><br><span class=line>            loss0 = loss_fn(<span class=number>2</span> * output0 + <span class=number>3</span> * output1, target)</span><br><span class=line>            loss1 = loss_fn(<span class=number>3</span> * output0 - <span class=number>5</span> * output1, target)</span><br><span class=line></span><br><span class=line>        <span class=comment># (retain_graph here is unrelated to amp, it's present because in this</span></span><br><span class=line>        <span class=comment># example, both backward() calls share some sections of graph.)</span></span><br><span class=line>        scaler.scale(loss0).backward(retain_graph=<span class=literal>True</span>)</span><br><span class=line>        scaler.scale(loss1).backward()</span><br><span class=line></span><br><span class=line>        <span class=comment># You can choose which optimizers receive explicit unscaling, if you</span></span><br><span class=line>        <span class=comment># want to inspect or modify the gradients of the params they own.</span></span><br><span class=line>        scaler.unscale_(optimizer0)</span><br><span class=line></span><br><span class=line>        scaler.step(optimizer0)</span><br><span class=line>        scaler.step(optimizer1)</span><br><span class=line></span><br><span class=line>        scaler.update()</span><br></pre></table></figure><p>如果网络有多个损失，则必须对每个损耗单独调用 scaler.scale。如果的网络有多个优化器，您可以在任何一个优化器上单独调用 scaler.unscale_，并且必须在每个优化器上单独调用 scaler.step。<h2 id=多GPU训练><a class=headerlink href=#多GPU训练 title=多GPU训练></a>多GPU训练</h2><p><code>torch.nn.DataParallel</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>model = MyModel()</span><br><span class=line>dp_model = nn.DataParallel(model)</span><br><span class=line></span><br><span class=line><span class=comment># Sets autocast in the main thread</span></span><br><span class=line><span class=keyword>with</span> autocast(device_type=<span class=string>'cuda'</span>, dtype=torch.float16):</span><br><span class=line>    <span class=comment># dp_model's internal threads will autocast.</span></span><br><span class=line>    output = dp_model(<span class=built_in>input</span>)</span><br><span class=line>    <span class=comment># loss_fn also autocast</span></span><br><span class=line>    loss = loss_fn(output)</span><br></pre></table></figure><p>上面方法是最简单的弊端是后续的loss计算只会在<code>cuda:0</code>上进行，没法并行，因此会导致负载不均衡的问题<p>文档推荐使用<code>DistributedDataParallel</code><blockquote><p>为什么尽管增加了复杂性，还是会考虑使用 DistributedDataParallel 而不是 DataParallel：<p>首先，DataParallel 是单进程、多线程的，只能在单机上运行，而 <strong>DistributedDataParallel 是多进程的，可以在单机和多机训练中运行。即使在单台机器上，DataParallel 通常也比 DistributedDataParallel 慢，这是由于线程间的 GIL 竞争、每次迭代的复制模型，以及分散输入和收集输出所带来的额外开销。</strong><p>分布式数据并行（DistributedDataParallel）可与模型并行一起使用，而数据并行（DataParallel）目前还不能。当 DDP 与模型并行相结合时，每个 DDP 进程都将使用模型并行，而所有进程都将使用数据并行。<p>如果您的模型需要跨越多台机器，或者您的用例不符合数据并行模式，请查看 RPC API，以获得更通用的分布式训练支持。</blockquote><p>在模块级基于 torch.distributed 实现分布式数据并行。<br>该容器通过在每个模型副本之间同步梯度来提供数据并行性。要同步的设备由输入 process_group 指定，默认情况下是整个世界。请注意，DistributedDataParallel 不会在参与的 GPU 之间对输入进行分块或分片；用户负责定义如何进行分块或分片，例如通过使用 DistributedSampler。<p>此外需要进行初始化 torch.distributed.init_process_group()<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torch.distributed <span class=keyword>as</span> dist</span><br><span class=line><span class=keyword>import</span> torch.multiprocessing <span class=keyword>as</span> mp</span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.optim <span class=keyword>as</span> optim</span><br><span class=line><span class=keyword>import</span> os</span><br><span class=line><span class=keyword>from</span> torch.nn.parallel <span class=keyword>import</span> DistributedDataParallel <span class=keyword>as</span> DDP</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>example</span>(<span class=params>rank, world_size</span>):</span></span><br><span class=line>    <span class=comment># create default process group</span></span><br><span class=line>    dist.init_process_group(<span class=string>"gloo"</span>, rank=rank, world_size=world_size)</span><br><span class=line>    <span class=comment># create local model</span></span><br><span class=line>    model = nn.Linear(<span class=number>10</span>, <span class=number>10</span>).to(rank)</span><br><span class=line>    <span class=comment># construct DDP model</span></span><br><span class=line>    ddp_model = DDP(model, device_ids=[rank])</span><br><span class=line>    <span class=comment># define loss function and optimizer</span></span><br><span class=line>    loss_fn = nn.MSELoss()</span><br><span class=line>    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line></span><br><span class=line>    <span class=comment># forward pass</span></span><br><span class=line>    outputs = ddp_model(torch.randn(<span class=number>20</span>, <span class=number>10</span>).to(rank))</span><br><span class=line>    labels = torch.randn(<span class=number>20</span>, <span class=number>10</span>).to(rank)</span><br><span class=line>    <span class=comment># backward pass</span></span><br><span class=line>    loss_fn(outputs, labels).backward()</span><br><span class=line>    <span class=comment># update parameters</span></span><br><span class=line>    optimizer.step()</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>main</span>():</span></span><br><span class=line>    world_size = <span class=number>2</span></span><br><span class=line>    mp.spawn(example,</span><br><span class=line>        args=(world_size,),</span><br><span class=line>        nprocs=world_size,</span><br><span class=line>        join=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line><span class=keyword>if</span> __name__==<span class=string>"__main__"</span>:</span><br><span class=line>    <span class=comment># Environment variables which need to be</span></span><br><span class=line>    <span class=comment># set when using c10d's default "env"</span></span><br><span class=line>    <span class=comment># initialization mode.</span></span><br><span class=line>    os.environ[<span class=string>"MASTER_ADDR"</span>] = <span class=string>"localhost"</span></span><br><span class=line>    os.environ[<span class=string>"MASTER_PORT"</span>] = <span class=string>"29500"</span></span><br><span class=line>    main()                                  </span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>demo_model_parallel</span>(<span class=params>rank, world_size</span>):</span></span><br><span class=line>    <span class=built_in>print</span>(<span class=string>f"Running DDP with model parallel example on rank <span class=subst>{rank}</span>."</span>)</span><br><span class=line>    setup(rank, world_size)</span><br><span class=line></span><br><span class=line>    <span class=comment># setup mp_model and devices for this process</span></span><br><span class=line>    dev0 = rank * <span class=number>2</span></span><br><span class=line>    dev1 = rank * <span class=number>2</span> + <span class=number>1</span></span><br><span class=line>    mp_model = ToyMpModel(dev0, dev1)</span><br><span class=line>    ddp_mp_model = DDP(mp_model)</span><br><span class=line></span><br><span class=line>    loss_fn = nn.MSELoss()</span><br><span class=line>    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=<span class=number>0.001</span>)</span><br><span class=line></span><br><span class=line>    optimizer.zero_grad()</span><br><span class=line>    <span class=comment># outputs will be on dev1</span></span><br><span class=line>    outputs = ddp_mp_model(torch.randn(<span class=number>20</span>, <span class=number>10</span>))</span><br><span class=line>    labels = torch.randn(<span class=number>20</span>, <span class=number>5</span>).to(dev1)</span><br><span class=line>    loss_fn(outputs, labels).backward()</span><br><span class=line>    optimizer.step()</span><br><span class=line></span><br><span class=line>    cleanup()</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=keyword>if</span> __name__ == <span class=string>"__main__"</span>:</span><br><span class=line>    n_gpus = torch.cuda.device_count()</span><br><span class=line>    <span class=keyword>assert</span> n_gpus >= <span class=number>2</span>, <span class=string>f"Requires at least 2 GPUs to run, but got <span class=subst>{n_gpus}</span>"</span></span><br><span class=line>    world_size = n_gpus</span><br><span class=line>    run_demo(demo_basic, world_size)</span><br><span class=line>    run_demo(demo_checkpoint, world_size)</span><br><span class=line>    world_size = n_gpus//<span class=number>2</span></span><br><span class=line>    run_demo(demo_model_parallel, world_size)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>sampler = DistributedSampler(dataset) <span class=keyword>if</span> is_distributed <span class=keyword>else</span> <span class=literal>None</span></span><br><span class=line>loader = DataLoader(dataset, shuffle=(sampler <span class=keyword>is</span> <span class=literal>None</span>),</span><br><span class=line>                    sampler=sampler)</span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(start_epoch, n_epochs):</span><br><span class=line>     <span class=keyword>if</span> is_distributed:</span><br><span class=line>         sampler.set_epoch(epoch)</span><br><span class=line>     train(loader)</span><br></pre></table></figure><p>它与 torch.nn.parallel.DistributedDataParallel 结合使用尤其有用。在这种情况下，<strong>每个进程都可以传递一个 DistributedSampler 实例作为 DataLoader 采样器</strong>，并加载其独有的原始数据集子集。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch  </span><br><span class=line><span class=keyword>import</span> torch.distributed <span class=keyword>as</span> dist  </span><br><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn  </span><br><span class=line><span class=keyword>import</span> torch.optim <span class=keyword>as</span> optim  </span><br><span class=line><span class=keyword>from</span> torch.utils.data <span class=keyword>import</span> DataLoader, Dataset, DistributedSampler  </span><br><span class=line><span class=keyword>from</span> torch.nn.parallel <span class=keyword>import</span> DistributedDataParallel <span class=keyword>as</span> DDP  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 自定义数据集和模型  </span></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyDataset</span>(<span class=params>Dataset</span>):</span>  </span><br><span class=line>    <span class=comment># 实现__len__和__getitem__方法  </span></span><br><span class=line>    <span class=keyword>pass</span>  </span><br><span class=line>  </span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModel</span>(<span class=params>nn.Module</span>):</span>  </span><br><span class=line>    <span class=comment># 定义模型结构，可能需要考虑如何拆分模型  </span></span><br><span class=line>    <span class=keyword>pass</span>  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 初始化分布式环境  </span></span><br><span class=line>dist.init_process_group(backend=<span class=string>'nccl'</span>, init_method=<span class=string>'tcp://localhost:23456'</span>, rank=<span class=number>0</span>, world_size=torch.cuda.device_count())  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 初始化数据集和模型  </span></span><br><span class=line>dataset = MyDataset()  </span><br><span class=line>sampler = DistributedSampler(dataset)  </span><br><span class=line>dataloader = DataLoader(dataset, batch_size=<span class=number>32</span>, shuffle=<span class=literal>False</span>, sampler=sampler)  </span><br><span class=line>model = MyModel()  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 拆分模型（这通常需要根据模型的具体结构来手动完成）  </span></span><br><span class=line><span class=comment>#### 例如，如果模型有两个主要部分，可以将它们分别放到不同的设备上  </span></span><br><span class=line>model_part1 = model.part1.to(<span class=string>'cuda:0'</span>)  </span><br><span class=line>model_part2 = model.part2.to(<span class=string>'cuda:1'</span>)  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 使用DistributedDataParallel包装模型  </span></span><br><span class=line>model = DDP(model, device_ids=[torch.cuda.current_device()])  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 定义损失函数和优化器  </span></span><br><span class=line>criterion = nn.CrossEntropyLoss()  </span><br><span class=line>optimizer = optim.Adam(model.parameters(), lr=<span class=number>0.001</span>)  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 训练循环  </span></span><br><span class=line><span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(num_epochs):  </span><br><span class=line>    <span class=keyword>for</span> inputs, labels <span class=keyword>in</span> dataloader:  </span><br><span class=line>        inputs, labels = inputs.to(model.device), labels.to(model.device)  </span><br><span class=line>        optimizer.zero_grad()  </span><br><span class=line>        outputs = model(inputs)  </span><br><span class=line>        loss = criterion(outputs, labels)  </span><br><span class=line>        loss.backward()  </span><br><span class=line>        optimizer.step()  </span><br><span class=line>  </span><br><span class=line><span class=comment>#### 销毁分布式进程组  </span></span><br><span class=line>dist.destroy_process_group()</span><br></pre></table></figure><p><a href=https://github.com/jia-zhuang/pytorch-multi-gpu-training rel=noopener target=_blank>jia-zhuang/pytorch-multi-gpu-training: 整理 pytorch 单机多 GPU 训练方法与原理 (github.com)</a><h2 id=常用Container><a class=headerlink href=#常用Container title=常用Container></a>常用Container</h2><div class=table-container><table><thead><tr><th>Containers<th>介绍<tbody><tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module rel=noopener target=_blank><code>Module</code></a><td>Base class for all neural network modules.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential rel=noopener target=_blank><code>Sequential</code></a><td>A sequential container.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList rel=noopener target=_blank><code>ModuleList</code></a><td>Holds submodules in a list.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict rel=noopener target=_blank><code>ModuleDict</code></a><td>Holds submodules in a dictionary.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList rel=noopener target=_blank><code>ParameterList</code></a><td>Holds parameters in a list.<tr><td><a href=https://pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict rel=noopener target=_blank><code>ParameterDict</code></a><td>Holds parameters in a dictionary.</table></div><p><code>Module</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch.nn <span class=keyword>as</span> nn</span><br><span class=line><span class=keyword>import</span> torch.nn.functional <span class=keyword>as</span> F</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>Model</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.conv1 = nn.Conv2d(<span class=number>1</span>, <span class=number>20</span>, <span class=number>5</span>)</span><br><span class=line>        self.conv2 = nn.Conv2d(<span class=number>20</span>, <span class=number>20</span>, <span class=number>5</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        x = F.relu(self.conv1(x))</span><br><span class=line>        <span class=keyword>return</span> F.relu(self.conv2(x))</span><br></pre></table></figure><p><code>Sequential</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br></pre><td class=code><pre><span class=line>model = nn.Sequential(</span><br><span class=line>          nn.Conv2d(<span class=number>1</span>,<span class=number>20</span>,<span class=number>5</span>),</span><br><span class=line>          nn.ReLU(),</span><br><span class=line>          nn.Conv2d(<span class=number>20</span>,<span class=number>64</span>,<span class=number>5</span>),</span><br><span class=line>          nn.ReLU()</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line><span class=comment># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class=line><span class=comment># same as the above code</span></span><br><span class=line>model = nn.Sequential(OrderedDict([</span><br><span class=line>          (<span class=string>'conv1'</span>, nn.Conv2d(<span class=number>1</span>,<span class=number>20</span>,<span class=number>5</span>)),</span><br><span class=line>          (<span class=string>'relu1'</span>, nn.ReLU()),</span><br><span class=line>          (<span class=string>'conv2'</span>, nn.Conv2d(<span class=number>20</span>,<span class=number>64</span>,<span class=number>5</span>)),</span><br><span class=line>          (<span class=string>'relu2'</span>, nn.ReLU())</span><br><span class=line>        ]))</span><br></pre></table></figure><p><code>ModuleList</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.linears = nn.ModuleList([nn.Linear(<span class=number>10</span>, <span class=number>10</span>) <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>10</span>)])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=comment># ModuleList can act as an iterable, or be indexed using ints</span></span><br><span class=line>        <span class=keyword>for</span> i, l <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.linears):</span><br><span class=line>            x = self.linears[i // <span class=number>2</span>](x) + l(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ModuleDict</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.choices = nn.ModuleDict({</span><br><span class=line>                <span class=string>'conv'</span>: nn.Conv2d(<span class=number>10</span>, <span class=number>10</span>, <span class=number>3</span>),</span><br><span class=line>                <span class=string>'pool'</span>: nn.MaxPool2d(<span class=number>3</span>)</span><br><span class=line>        })</span><br><span class=line>        self.activations = nn.ModuleDict([</span><br><span class=line>                [<span class=string>'lrelu'</span>, nn.LeakyReLU()],</span><br><span class=line>                [<span class=string>'prelu'</span>, nn.PReLU()]</span><br><span class=line>        ])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, choice, act</span>):</span></span><br><span class=line>        x = self.choices[choice](x)</span><br><span class=line>        x = self.activations[act](x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ParameterList</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class=number>10</span>, <span class=number>10</span>)) <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>10</span>)])</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        <span class=comment># ParameterList can act as an iterable, or be indexed using ints</span></span><br><span class=line>        <span class=keyword>for</span> i, p <span class=keyword>in</span> <span class=built_in>enumerate</span>(self.params):</span><br><span class=line>            x = self.params[i // <span class=number>2</span>].mm(x) + p.mm(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><p><code>ParameterDict</code><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>MyModule</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.params = nn.ParameterDict({</span><br><span class=line>                <span class=string>'left'</span>: nn.Parameter(torch.randn(<span class=number>5</span>, <span class=number>10</span>)),</span><br><span class=line>                <span class=string>'right'</span>: nn.Parameter(torch.randn(<span class=number>5</span>, <span class=number>10</span>))</span><br><span class=line>        })</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, choice</span>):</span></span><br><span class=line>        x = self.params[choice].mm(x)</span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><h2 id=容易混淆和遗忘的方法><a class=headerlink href=#容易混淆和遗忘的方法 title=容易混淆和遗忘的方法></a>容易混淆和遗忘的方法</h2><h3 id=torch-scatter><a class=headerlink href=#torch-scatter title=torch.scatter></a>torch.scatter</h3><p>Tensor.scatter_(<em>dim</em>, <em>index</em>, <em>src</em>, <em>**, </em>reduce=None*) → <a href=https://pytorch.org/docs/stable/tensors.html#torch.Tensor rel=noopener target=_blank>Tensor</a><p>按照 <code>index</code> 张量中指定的索引，将张量 <code>src</code> 中的所有值写入 <code>self</code> 中。对于 <code>src</code> 中的每个值，其输出索引在 <code>dimension != dim</code> 时由 <code>src</code> 中的索引指定，在 <code>dimension = dim</code> 时由 <code>index</code> 中的相应值指定。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>self[index[i][j][k]][j][k] = src[i][j][k]  <span class=comment># if dim == 0</span></span><br><span class=line>self[i][index[i][j][k]][k] = src[i][j][k]  <span class=comment># if dim == 1</span></span><br><span class=line>self[i][j][index[i][j][k]] = src[i][j][k]  <span class=comment># if dim == 2</span></span><br></pre></table></figure><p>常用作写one-hot量<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>index = torch.tensor([[<span class=number>0</span>, <span class=number>1</span>]])</span><br><span class=line>value = <span class=number>2</span></span><br><span class=line>torch.zeros(<span class=number>3</span>, <span class=number>5</span>).scatter_(<span class=number>0</span>, index, value)</span><br></pre></table></figure><h3 id=torch-gather><a class=headerlink href=#torch-gather title=torch.gather></a>torch.gather</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>out[i][j][k] = <span class=built_in>input</span>[index[i][j][k]][j][k]  <span class=comment># if dim == 0</span></span><br><span class=line>out[i][j][k] = <span class=built_in>input</span>[i][index[i][j][k]][k]  <span class=comment># if dim == 1</span></span><br><span class=line>out[i][j][k] = <span class=built_in>input</span>[i][j][index[i][j][k]]  <span class=comment># if dim == 2</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>t = torch.tensor([[<span class=number>1</span>, <span class=number>2</span>], [<span class=number>3</span>, <span class=number>4</span>]])</span><br><span class=line>torch.gather(t, <span class=number>1</span>, torch.tensor([[<span class=number>0</span>, <span class=number>0</span>], [<span class=number>1</span>, <span class=number>0</span>]]))</span><br></pre></table></figure><p>上面的代码就是把t根据index torch.tensor([[0, 0], [1, 0]])重新得到一个tensor.<h3 id=torch-split><a class=headerlink href=#torch-split title=torch.split></a>torch.split</h3><p>torch.split(<em>tensor</em>, <em>split_size_or_sections</em>, <em>dim=0</em><p>将张量分割成块。每个块都是原始张量的一个view。<p>如果 split_size_or_sections 是整数类型，那么张量将被分割成大小相等的块（如果可能）。如果张量在给定维度 dim 上的大小不能被 split_size 整除，则最后一个块的大小会变小。<p>如果 split_size_or_sections 是一个列表，那么张量将被分割成 len(split_size_or_sections)小块，其大小与 split_size_or_sections 一致。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line>a = torch.arange(<span class=number>10</span>).reshape(<span class=number>5</span>, <span class=number>2</span>)</span><br><span class=line>torch.split(a, <span class=number>2</span>)</span><br><span class=line>(tensor([[<span class=number>0</span>, <span class=number>1</span>],</span><br><span class=line>         [<span class=number>2</span>, <span class=number>3</span>]]),</span><br><span class=line> tensor([[<span class=number>4</span>, <span class=number>5</span>],</span><br><span class=line>         [<span class=number>6</span>, <span class=number>7</span>]]),</span><br><span class=line> tensor([[<span class=number>8</span>, <span class=number>9</span>]]))</span><br><span class=line>torch.split(a, [<span class=number>1</span>, <span class=number>4</span>])</span><br><span class=line>(tensor([[<span class=number>0</span>, <span class=number>1</span>]]),</span><br><span class=line> tensor([[<span class=number>2</span>, <span class=number>3</span>],</span><br><span class=line>         [<span class=number>4</span>, <span class=number>5</span>],</span><br><span class=line>         [<span class=number>6</span>, <span class=number>7</span>],</span><br><span class=line>         [<span class=number>8</span>, <span class=number>9</span>]]))</span><br></pre></table></figure><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\09\16\ZTM-pytorchForDL\ rel=bookmark>ZTM-pytorchForDL</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\09\12\pytorch学习——初探\ rel=bookmark>pytorch学习——初探</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2022\04\15\pytorch学习\ rel=bookmark>pytorch学习</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2024/06/23/pytorch%E6%93%8D%E4%BD%9Ctensor%E6%97%B6%E4%BD%A0%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%9C%B0%E6%96%B9/ title=使用pytorch时你可能需要注意的地方>https://www.sekyoro.top/2024/06/23/pytorch操作tensor时你可能需要注意的地方/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/pytorch/ rel=tag><i class="fa fa-tag"></i> pytorch</a></div><div class=post-nav><div class=post-nav-item><a title="A better C:from C++, Go,Rust to Zig" href=/2024/06/23/A-better-C-from-C-Go-Rust-to-Zig/ rel=prev> <i class="fa fa-chevron-left"></i> A better C:from C++, Go,Rust to Zig </a></div><div class=post-nav-item><a href=/2024/06/23/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%85%A5%E9%97%A8/ rel=next title=函数式编程介绍与入门> 函数式编程介绍与入门 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#tensor><span class=nav-number>1.</span> <span class=nav-text>tensor</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Tensor><span class=nav-number>1.1.</span> <span class=nav-text>Tensor</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Views><span class=nav-number>1.2.</span> <span class=nav-text>Views</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#torch-autograd><span class=nav-number>2.</span> <span class=nav-text>torch.autograd</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#detach-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8Eleaf-tensor><span class=nav-number>2.1.</span> <span class=nav-text>detach 计算图与leaf tensor</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%B1%9E%E4%BA%8E%E6%97%A7%E6%97%B6%E4%BB%A3%E7%9A%84Variable%E5%92%8Cdata><span class=nav-number>2.2.</span> <span class=nav-text>属于旧时代的Variable和data</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Function><span class=nav-number>2.3.</span> <span class=nav-text>Function</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#ONNX%E6%A0%BC%E5%BC%8F><span class=nav-number>3.</span> <span class=nav-text>ONNX格式</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B><span class=nav-number>3.1.</span> <span class=nav-text>保存模型</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B><span class=nav-number>3.1.1.</span> <span class=nav-text>加载模型</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%B5%81%E7%A8%8B><span class=nav-number>3.2.</span> <span class=nav-text>流程</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B3%A8%E6%84%8F><span class=nav-number>3.2.1.</span> <span class=nav-text>注意</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6><span class=nav-number>4.</span> <span class=nav-text>自动混合精度</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%9AGPU%E8%AE%AD%E7%BB%83><span class=nav-number>5.</span> <span class=nav-text>多GPU训练</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%B8%B8%E7%94%A8Container><span class=nav-number>6.</span> <span class=nav-text>常用Container</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%AE%B9%E6%98%93%E6%B7%B7%E6%B7%86%E5%92%8C%E9%81%97%E5%BF%98%E7%9A%84%E6%96%B9%E6%B3%95><span class=nav-number>7.</span> <span class=nav-text>容易混淆和遗忘的方法</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#torch-scatter><span class=nav-number>7.1.</span> <span class=nav-text>torch.scatter</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-gather><span class=nav-number>7.2.</span> <span class=nav-text>torch.gather</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#torch-split><span class=nav-number>7.3.</span> <span class=nav-text>torch.split</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>199</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>193</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-d46hvgfka-drowningincodes-projects.vercel.app/ rel=noopener target=_blank title=https://protool-d46hvgfka-drowningincodes-projects.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/pytorch/ rel=tag>pytorch</a><span class=tag-list-count>4</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.5m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>23:17</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>