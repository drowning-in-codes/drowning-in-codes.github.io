<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="inspired by karpathy/micrograd: A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API (github.com)and Taking PyTorch for Granted | wh (nrehiew.github.io).  感" name=description><meta content=article property=og:type><meta content="from grad to tensor" property=og:title><meta content=https://www.sekyoro.top/2024/07/08/from-grad-to-tensor/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="inspired by karpathy/micrograd: A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API (github.com)and Taking PyTorch for Granted | wh (nrehiew.github.io).  感" property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/07/10/Ns8SM3QlpoeB4Ix.png property=og:image><meta content=https://www.tomasbeuzen.com/python-programming-for-data-science/_images/numpy_paper.png property=og:image><meta content=https://s2.loli.net/2024/07/10/s7ZuEzmD3vnAoiH.png property=og:image><meta content=https://s2.loli.net/2024/07/13/XRp2T8JgGrNQBwY.png property=og:image><meta content=2024-07-08T02:09:09.000Z property=article:published_time><meta content=2024-07-16T14:40:36.858Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content=tensor property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/07/10/Ns8SM3QlpoeB4Ix.png name=twitter:image><link href=https://www.sekyoro.top/2024/07/08/from-grad-to-tensor/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>from grad to tensor | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2024/07/08/from-grad-to-tensor/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>from grad to tensor</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2024-07-08 10:09:09" datetime=2024-07-08T10:09:09+08:00>2024-07-08</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-07-16 22:40:36" datetime=2024-07-16T22:40:36+08:00 itemprop=dateModified>2024-07-16</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>7.5k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>7 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>inspired by <a href=https://github.com/karpathy/micrograd rel=noopener target=_blank>karpathy/micrograd: A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API (github.com)</a>and <a href=https://nrehiew.github.io/blog/pytorch/ rel=noopener target=_blank>Taking PyTorch for Granted | wh (nrehiew.github.io)</a>.<p>感谢Karpathy以及x上所有真心探讨技术的网友.这属于karpathy的<a href=https://github.com/karpathy/nn-zero-to-hero?tab=readme-ov-file rel=noopener target=_blank>karpathy/nn-zero-to-hero: Neural Networks: Zero to Hero ,(github.com)</a>课程.事实上他还有很多值得一看的课程和repos.</p><span id=more></span><p>tensor分成哪些部分?<blockquote><p>一个tensor可以分为元数据区和存储区（Storage）<p>信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）,storage_offset,layout等信息,而真正的<strong>数据则保存成连续数组,存储在存储区</strong></blockquote><h3 id=tensor的存储><a class=headerlink href=#tensor的存储 title=tensor的存储></a>tensor的存储</h3><p>tensor数据底层存储是<strong>连续的</strong>,<del>相对应的就是链表</del>. pytorch使用Storage类存储数据. 可以使用tensor.storage()访问存储的数据<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>data = torch.arange(<span class=number>9</span>)</span><br><span class=line><span class=built_in>print</span>(data.storage().dtype)</span><br><span class=line><span class=built_in>print</span>(data.storage().device)</span><br><span class=line><span class=built_in>print</span>(data.storage().data_ptr()) <span class=comment>#这里 存储数据也能访问数据的属性</span></span><br></pre></table></figure><blockquote><p>All storage classes except for <a href=https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage rel=noopener target=_blank><code>torch.UntypedStorage</code></a> will be removed in the future, and <a href=https://pytorch.org/docs/stable/storage.html#torch.UntypedStorage rel=noopener target=_blank><code>torch.UntypedStorage</code></a> will be used in all cases.</blockquote><p>但是在最新的python中除了untypedstorage类其他都已经deprecated了,而在untypedstorage中数据是字节类型,并且也无法调用<code>dtype</code>这些属性,变得更加纯粹了.<p><img alt=image-20240710221322389 data-src=https://s2.loli.net/2024/07/10/Ns8SM3QlpoeB4Ix.png><p>事实上,大多数类似的数据都是这样的,比如常用库numpy.<p><img alt=img data-src=https://www.tomasbeuzen.com/python-programming-for-data-science/_images/numpy_paper.png style=zoom:150%;><h3 id=tensor的访问><a class=headerlink href=#tensor的访问 title=tensor的访问></a>tensor的访问</h3><p>pytorch数据存储是一维的,但是会根据它的一些元数据改变对它的”解释”,而影响解释的元数据就是tride (<code>as_strided</code>可以使得两个tensor的size,stride和storage_offset一致)<blockquote><p><strong>stride</strong> stride是从一个元素到指定维度的另一个元素的间隔数,如果不指定维度,就返回在每个维度上的stride的tuple<p>Stride is the jump necessary to go from one element to the next one in the specified dimension <a href=https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim rel=noopener target=_blank><code>dim</code></a>.<p>A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension <a href=https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim rel=noopener target=_blank><code>dim</code></a>.<p><strong>storage_offset</strong> 返回tensor的第一个元素与storage的第一个元素的偏移量。<p>Returns <code>self</code> tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>>x = torch.tensor([<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>4</span>, <span class=number>5</span>])</span><br><span class=line>>x.storage_offset()</span><br><span class=line>>x[<span class=number>3</span>:].storage_offset()</span><br></pre></table></figure></blockquote><p>pytorch中tensor存储区的数据是连续的,而stride规定了如何访问.访问的方式就是<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>data = torch.randn(<span class=number>1</span>,<span class=number>20</span>,<span class=number>20</span>) </span><br><span class=line>stride = data.stride() <span class=comment># -> (400, 20, 1)</span></span><br><span class=line>data[<span class=number>0</span>][<span class=number>2</span>][<span class=number>3</span>] ->  <span class=number>0</span>*stride[<span class=number>0</span>]+<span class=number>2</span>*stride[<span class=number>2</span>]+<span class=number>3</span>*stride[<span class=number>3</span>]->也就是说这个数据在第<span class=number>2</span>*<span class=number>20</span>+<span class=number>3</span>*<span class=number>1</span>=<span class=number>43</span>个</span><br></pre></table></figure><p>注意torch存储数据是行优先,也就是说,像下面这样的数据,第二个是0.6960而不是-0.5163. 所以访问时就类似索引乘以对应的行/列数,从这个角度来看,stride就是一个映射函数.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>tensor([[ <span class=number>1.6427</span>,  <span class=number>0.6960</span>,  <span class=number>0.7865</span>,  <span class=number>0.9934</span>,  <span class=number>0.4952</span>],</span><br><span class=line>        [-<span class=number>0.5163</span>, -<span class=number>0.0823</span>, -<span class=number>1.2630</span>, -<span class=number>0.9474</span>,  <span class=number>1.1055</span>],</span><br><span class=line>        [ <span class=number>0.1538</span>,  <span class=number>1.0177</span>, -<span class=number>1.8064</span>,  <span class=number>0.6440</span>, -<span class=number>1.4661</span>],</span><br><span class=line>        [ <span class=number>0.3305</span>,  <span class=number>0.2681</span>,  <span class=number>0.2768</span>, -<span class=number>0.3924</span>,  <span class=number>0.1743</span>],</span><br><span class=line>        [-<span class=number>0.8965</span>, -<span class=number>0.5499</span>, -<span class=number>0.4545</span>, -<span class=number>1.1470</span>,  <span class=number>0.6883</span>]])</span><br></pre></table></figure><p><img alt=image-20240710220152997 data-src=https://s2.loli.net/2024/07/10/s7ZuEzmD3vnAoiH.png><h3 id=tensor操作><a class=headerlink href=#tensor操作 title=tensor操作></a>tensor操作</h3><p>可以对tensor的数据进行操作,比如下面运算,会改变tensor的存储数据.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>data = torch.randn((<span class=number>4</span>, <span class=number>2</span>))</span><br><span class=line>stride = data.stride()</span><br><span class=line><span class=built_in>print</span>(data, stride)</span><br><span class=line>data[<span class=number>0</span>, <span class=number>1</span>] = <span class=number>10</span></span><br><span class=line><span class=built_in>print</span>(data, stride)</span><br><span class=line>data.add_(torch.ones((<span class=number>4</span>, <span class=number>2</span>)))</span><br><span class=line><span class=built_in>print</span>(data, stride)</span><br></pre></table></figure><p>但是有些操作不会,其只会返回数据相同(指的是数据在底层存储上相同)的<strong>视图</strong>(view),这些操作包括<code>t()</code>,<code>expand</code>,<code>transpose</code>,<code>permute</code>,<code>view</code>,<code>squeeze</code>等等,操作后的tensor数据不变(也就是views),但stride<strong>可能</strong>会改变,也就是说解释数据的方式会变.<blockquote><p>底层存储并没有改变,只需将映射函数从旧形状的坐标系调整为新形状的坐标系。 如果映射函数已经将形状作为输入,那么只需更改形状属性即可。</blockquote><ul><li><code>reshape()</code>、<code>reshape_as()</code> 和 <code>flatten()</code> 可以返回视图或新张量,所以后续代码不要假定它返回的存储数据是否跟原本输入相同.<li>如果输入的张量已经连续,<code>contiguous()</code> 会返回自身,否则会通过复制数据返回一个新的连续张量.</ul><p>下面一个例子报错原因,就是stride的问题,具体来说,这里转置之后stride变了,size没变,使得后续的view操作不满足条件<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>x = torch.arange(<span class=number>9</span>).reshape(<span class=number>3</span>, <span class=number>3</span>) <span class=comment># 3 x 3 stride (3,1)  size(3,3)</span></span><br><span class=line>x.t().view(<span class=number>1</span>, -<span class=number>1</span>) <span class=comment># x.t() stride (1,3)  size(3,3)</span></span><br><span class=line><span class=comment># >> RuntimeError: view size is not compatible with input tensor's </span></span><br><span class=line><span class=comment># size and stride. Use .reshape() instead</span></span><br></pre></table></figure><p>首先,<code>view</code>作用是返回一个存储数据相同,但shape/size可能不同的视图,要求新的视图与输入数据<strong>相兼容</strong>( 1.<strong>新的视图的每个dimension必须是输入的子空间</strong>或者2.<strong>新的视图的维度满足下面条件</strong>（连续性),否则不能得到新的视图.</p><script type="math/tex; mode=display">
假设得到的新的tensor维度涉及d,..,d+k,对其中所有维度要求:\\
\text{stride}[i]=\text{stride}[i+1]\times\text{size}[i+1]</script><blockquote><p>如果不清楚是否可以执行 view()，建议使用 reshape()<p>reshape:如果形状兼容,则返回视图,否则返回拷贝(相当于调用 contiguous)</blockquote><p>view之后size变为(1,9),这符合条件1,也就是size相符,再看这里(1,9)表明第二个维度跨域(span across)了原本输入的两个维度,而原本输入的两个维度中的第一个维度需要满足连续性条件,但是 stride[0] = 1 , stride[1]*size[1] = 1*3=3 不符合,所以view操作失败.<p>更抽象地说,因为没有办法在不改变底层数据的情况下对张量进行flatten处理.<blockquote><p>我们能否从tensor的stride(3,1)推得tensor size是(3,3)?<p>答案是不能,它的size也完全可以是(4,3). 反过来size也不能推出stride.</blockquote><p>在上面的例子中,比如底层数据是[1,2,3,4,5,6,7,8,9],stride是[3,1]. 也就是说在第一个维度下,数据到相同维度的下个数据间隔为3,同理第二个维度间隔为1,<p>经过转置之后,因为解释数据的方式变了,因为需要改变解释数据的方式,所以stride需要改变为(1,3). 再进行view(1,-1),如果不报错的话,size就是(1,9),你可能会认为结果不就是[[1,4,7,2,5,8…]]吗,但这个tensor的stride为多少呢? 是(9,1)吗,并不是.为什么呢,<p>因为底层数据[1,2,3,4,5,6,7,8,9],要查找第二个维度上的数据,比如1到4,在底层数据中是stride是3,而4到7也是3,所以是stride是(9,3),然而这跟size(1,9)不匹配(stride乘起来应该跟size乘起来应该相同,这是最起码的保证).<figure class="highlight lua"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line>tensor(<span class=string>[[1, 2, 3],</span></span><br><span class=line><span class=string>        [4, 5, 6],</span></span><br><span class=line><span class=string>        [7, 8, 9]]</span>) </span><br><span class=line>        ⬇⬇</span><br><span class=line>        tensor(<span class=string>[[1, 4, 7],</span></span><br><span class=line><span class=string>        [2, 5, 8],</span></span><br><span class=line><span class=string>        [3, 6, 9]]</span>)</span><br></pre></table></figure><p><img style="zoom: 67%;" alt=在这里插入图片描述 data-src=https://s2.loli.net/2024/07/13/XRp2T8JgGrNQBwY.png><h3 id=tensor的广播><a class=headerlink href=#tensor的广播 title=tensor的广播></a>tensor的广播</h3><p>PyTorch 的广播规则:(如何说一个tensor是可广播的)<ul><li><p>两个张量必须至少有一个维度</p><li><p>从最右边的维开始,两个维必须大小相等,其中一个为 1 或者其中一个不存在。</p> <blockquote><p>Two tensors are “broadcastable” if the following rules hold:<ul><li>Each tensor has at least one dimension.<li>When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.</ul><p><a href=https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics rel=noopener target=_blank>Broadcasting semantics — PyTorch 2.3 documentation</a><p>If two tensors <code>x</code>, <code>y</code> are “broadcastable”, the resulting tensor size is calculated as follows:<ul><li>If the number of dimensions of <code>x</code> and <code>y</code> are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.<li>Then, for each dimension size, the resulting dimension size is the max of the sizes of <code>x</code> and <code>y</code> along that dimension.</ul></blockquote></ul><figure class="highlight bash"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line><span class=comment># from https://nrehiew.github.io/blog/pytorch/</span></span><br><span class=line><span class=keyword>for</span> each dimension, starting from the right:</span><br><span class=line>	<span class=keyword>if</span> both shapes have this dimension:</span><br><span class=line>		<span class=keyword>if</span> they are different:</span><br><span class=line>				neither is 1: error</span><br><span class=line>				<span class=keyword>else</span>: use larger dimension </span><br><span class=line>		<span class=keyword>else</span> they are the same: use dimension</span><br><span class=line>	<span class=keyword>else</span>:</span><br><span class=line>		use whichever dimension exists</span><br></pre></table></figure><p>广播,不存在数据copy.这意味着,如果将一个小张量广播到一个大得多的形状,就不会产生内存或性能开销。<p>其次,由于较小张量中使用的实际元素是相同的,因此梯度会沿着这个较小维度中的项目累积.这在调试梯度或执行涉及广播的自定义自动梯度函数时特别有用。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line>x=torch.empty(<span class=number>5</span>,<span class=number>1</span>,<span class=number>4</span>,<span class=number>1</span>)</span><br><span class=line>y=torch.empty(  <span class=number>3</span>,<span class=number>1</span>,<span class=number>1</span>)</span><br><span class=line>(x+y).size()</span><br><span class=line></span><br><span class=line>x=torch.empty(<span class=number>1</span>)</span><br><span class=line>y=torch.empty(<span class=number>3</span>,<span class=number>1</span>,<span class=number>7</span>)</span><br><span class=line>(x+y).size()</span><br><span class=line></span><br><span class=line>x=torch.empty(<span class=number>5</span>,<span class=number>2</span>,<span class=number>4</span>,<span class=number>1</span>)</span><br><span class=line>y=torch.empty(<span class=number>3</span>,<span class=number>1</span>,<span class=number>1</span>)</span><br><span class=line>(x+y).size()</span><br></pre></table></figure><p>如果一个 PyTorch 操作支持广播,那么它的张量数据就会自动扩展为大小相等的数据(无需复制数据),所以本质上也是返回一个视图.<h3 id=利用矩阵乘法进行广播><a class=headerlink href=#利用矩阵乘法进行广播 title=利用矩阵乘法进行广播></a>利用矩阵乘法进行广播</h3><p>矩阵是二维的,但是tensor是不限制的.<p><strong>多维tensor如何相乘的呢?</strong><ol><li>取两个张量的最后两个维度,检查它们是否可以相乘.如果不能,则出错<li>广播剩余维数.结果形状为 [广播后的维数] + [矩阵乘法的结果形状]。<li>将 [广播后的维数] 作为批处理维度，执行batched matrix multiplication(其实就是矩阵乘法,但是两个相乘的矩阵分别来自不同的batch中的相同的index)</ol><p>使用torch.matmul进行tensor相乘,它的计算方式如下<ul><li><p>如果都是一维,进行点乘</p><li><p>如果都是二维,进行矩阵乘,,如果是1维和二维,在一维度之前添加一个维度在进行矩阵乘,乘完之后再去掉.</p><li><p>如果是二维和1维,进行矩阵-向量乘法,得到向量.</p><li><p>如果两个参数都至少为 1 维,且至少一个参数为 N 维(N > 2),则返回一个batched matrix multiplication.</p> <p>如果第一个参数是一维的,那么在进行batched matrix multiplication,会在其维度前添加一维,然后删除.</p> <p>如果第二个参数是一维的,则在其维度后加上 1,以便进行batched matrix multiply,并在运算后删除.</p> <p>非矩阵(即批处理)维度将被广播(因此必须是<strong>可广播的</strong>)</p> <p>比如(jx1xnxn)和(kxnxn)得到(jxkxnxn),在batch上广播得到(jxk),简单来说就是最后两维(不够进行广播)进行相乘,除了后面两维,其他维度直接进行广播.</p></ul><blockquote><p>注意:广播逻辑在确定输入是否可广播时，只查看批次维度，而不查看矩阵维度。<p>这跟上面的广播逻辑不同.</blockquote><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br></pre><td class=code><pre><span class=line>a = torch.randn((<span class=number>3</span>, <span class=number>4</span>, <span class=number>1</span>, <span class=number>2</span>)) <span class=comment># 3 x 4 x 1 x 2</span></span><br><span class=line>b = torch.randn((<span class=number>1</span>, <span class=number>2</span>, <span class=number>3</span>)) <span class=comment># 1 x 2 x 3</span></span><br><span class=line></span><br><span class=line><span class=comment># Matrix Multiply Shape: 1x2 @ 2x3 -> 1x3</span></span><br><span class=line><span class=comment># Batch Shape: We broadcast (3, 4) and (1) -> (3, 4)</span></span><br><span class=line><span class=comment># Result shape: 3 x 4 x 1 x 3</span></span><br><span class=line>c = torch.zeros((<span class=number>3</span>, <span class=number>4</span>, <span class=number>1</span>, <span class=number>3</span>))</span><br><span class=line><span class=comment># iterate over the batch dimensions of (3, 4)</span></span><br><span class=line><span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>3</span>):</span><br><span class=line>	<span class=keyword>for</span> j <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>4</span>):</span><br><span class=line>		a_slice = a[i][j] <span class=comment># 1 x 2</span></span><br><span class=line>		b_slice = b[<span class=number>0</span>] <span class=comment># 2 x 3	</span></span><br><span class=line>		c[i][j] = a_slice @ b_slice <span class=comment># 1 x 3</span></span><br><span class=line><span class=keyword>assert</span> torch.equal(torch.matmul(a, b), c)</span><br></pre></table></figure><h3 id=反向传播><a class=headerlink href=#反向传播 title=反向传播></a>反向传播</h3><p>PyTorch 的核心是它的自动微分引擎.一般来说,<strong>每次在两个张量之间进行微分操作时,PyTorch 都会通过回调函数自动构建出整个计算图. 然后,当调用 .backward() 时,每个张量的梯度都会被更新</strong>. 这是 PyTorch 最大的抽象.<p>从标量的求导开始扩展到高维,这并不困难.首先需要理解标量的基本运算中的加/减法,乘法,幂、指以及对数.一个softmax操作就包含了加,幂指的操作.<p>可以将矩阵乘法看作是多个标量值的一系列乘法和加法运算,只需指定这些标量运算的后向运算,两个矩阵相乘的导数就自然而然地产生了.<p>从标量的角度来考虑梯度还有一个好处，就是可以直观地了解张量操作对梯度的影响。<p>例如，.reshape()、.transpose()、.cat 和 .split()等操作不会影响单个值及其在标量的梯度。 因此这些操作对张量梯度的影响自然就是操作梯度本身。<p>例如,使用 .reshape(-1) 对张量进行扁平化处理,对梯度的影响与调用 .reshape(-1) 对张量的梯度的影响相同。<h3 id=优化><a class=headerlink href=#优化 title=优化></a>优化</h3><h4 id=矩阵相乘><a class=headerlink href=#矩阵相乘 title=矩阵相乘></a>矩阵相乘</h4><p>不使用 GPU 也能实现的优化方法.<p>一种可能的优化方法是利用内存访问模式,而不是改变算法。 回想一下,在给定 A @ B 的情况下，我们正在重复计算 A 中的一行和 B 中的一列的点乘.<p>简单的解决方法是转置,将其转为列主模式(column-first),这样每次从内存加载时,我们就可以在同一缓存行中加载 B 列中的正确项目.<p>转置是一种O(N) 操作，因此只适用于较大的矩阵.<p>另一种无需缓存的算法是对矩阵块进行运算，而不是一次性对整个矩阵进行运算。 这就是所谓的<strong>块矩阵乘法</strong>。 其原理是将矩阵分解成较小的块，然后在这些块上执行矩阵乘法。 这样做的另一个好处是减少了高速缓存的读取次数，因为我们现在是在矩阵的较小块上进行运算。<h4 id=内存和中间值><a class=headerlink href=#内存和中间值 title=内存和中间值></a>内存和中间值</h4><blockquote><p>这里原文<a href=https://nrehiew.github.io/blog/pytorch/ rel=noopener target=_blank>Taking PyTorch for Granted | wh (nrehiew.github.io)</a>似乎有typos,我进行了修正</blockquote><p>在反向传播时,符合直觉的想法是保留中间值的梯度,以便后续计算leaf tensor的梯度.但是有些时候并不需要中间值的梯度<p>比如(a*b)+(c*d)=e,进行反向传播求e在a的梯度时如下<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>_t1 = a * b</span><br><span class=line>_t2 = c * d</span><br><span class=line>e = _t1 + _t2</span><br></pre></table></figure><p>其实就是求b,所以并不需要保留_t1和_t2的值.<h3 id=micrograd-from-scratch><a title="micrograd from scratch" class=headerlink href=#micrograd-from-scratch></a>micrograd from scratch</h3><p>此外还有个tinygrad的项目也是受此启发,在pytorch和micrograd中得到改进.<h3 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h3><ol><li><a href=https://github.com/karpathy/micrograd rel=noopener target=_blank>karpathy/micrograd: A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API (github.com)</a><li><a href=https://github.com/karpathy/nn-zero-to-hero?tab=readme-ov-file rel=noopener target=_blank>karpathy/nn-zero-to-hero: Neural Networks: Zero to Hero (github.com)</a><li><a href=https://github.com/karpathy/nanoGPT rel=noopener target=_blank>karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs. (github.com)</a><li><a href=https://blog.csdn.net/weixin_44008424/article/details/110764494 rel=noopener target=_blank>pytorch笔记（一）——tensor的storage()、stride()、storage_offset（）_pytorch storage-CSDN博客</a><li><a href=https://blog.csdn.net/m0_46653437/article/details/108742525 rel=noopener target=_blank>PyTorch中张量的shape和stride的关系_shape和strides-CSDN博客</a><li><a href=http://blog.ezyang.com/2019/05/pytorch-internals/ rel=noopener target=_blank>PyTorch internals : ezyang’s blog</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="from grad to tensor" href=https://www.sekyoro.top/2024/07/08/from-grad-to-tensor/>https://www.sekyoro.top/2024/07/08/from-grad-to-tensor/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/tensor/ rel=tag><i class="fa fa-tag"></i> tensor</a></div><div class=post-nav><div class=post-nav-item><a href=/2024/07/06/i3wm%E3%80%81Neovim%E4%B8%8EAlacritty%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E9%85%8D%E7%BD%AE/ rel=prev title=i3wm,Neovim与Alacritty的使用与配置> <i class="fa fa-chevron-left"></i> i3wm,Neovim与Alacritty的使用与配置 </a></div><div class=post-nav-item><a href=/2024/07/08/%E5%AD%A6%E4%B9%A0llama3/ rel=next title=学习llama3> 学习llama3 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-3"><a class=nav-link href=#tensor%E7%9A%84%E5%AD%98%E5%82%A8><span class=nav-number>1.</span> <span class=nav-text>tensor的存储</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#tensor%E7%9A%84%E8%AE%BF%E9%97%AE><span class=nav-number>2.</span> <span class=nav-text>tensor的访问</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#tensor%E6%93%8D%E4%BD%9C><span class=nav-number>3.</span> <span class=nav-text>tensor操作</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#tensor%E7%9A%84%E5%B9%BF%E6%92%AD><span class=nav-number>4.</span> <span class=nav-text>tensor的广播</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%88%A9%E7%94%A8%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%B9%BF%E6%92%AD><span class=nav-number>5.</span> <span class=nav-text>利用矩阵乘法进行广播</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD><span class=nav-number>6.</span> <span class=nav-text>反向传播</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BC%98%E5%8C%96><span class=nav-number>7.</span> <span class=nav-text>优化</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98><span class=nav-number>7.1.</span> <span class=nav-text>矩阵相乘</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%86%85%E5%AD%98%E5%92%8C%E4%B8%AD%E9%97%B4%E5%80%BC><span class=nav-number>7.2.</span> <span class=nav-text>内存和中间值</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#micrograd-from-scratch><span class=nav-number>8.</span> <span class=nav-text>micrograd from scratch</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>9.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>225</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>206</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/tensor/ rel=tag>tensor</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>33:22</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>