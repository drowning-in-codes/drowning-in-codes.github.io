<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="文生图和图生图应用已经出现一段时间了,目前常用的应用就是根据用户需求修改图片,事实上这就是一种自定义. 因为需要模型重新生成整张图或者部分图,要么通过prompt、reference image,也就是改变输入的方式;要么通过修改模型,也就是微调模型的方式. 因此诞生出了许多微调模型的方式,目前常用的微调库是huggingface/peft: 🤗 PEFT. 针对AI绘图应用的微调技术,倒是可以" name=description><meta content=article property=og:type><meta content="Dive into fine-tuning methods for SD" property=og:title><meta content=https://www.sekyoro.top/2025/01/02/Dive-into-fine-tuning-methods-for-SD/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="文生图和图生图应用已经出现一段时间了,目前常用的应用就是根据用户需求修改图片,事实上这就是一种自定义. 因为需要模型重新生成整张图或者部分图,要么通过prompt、reference image,也就是改变输入的方式;要么通过修改模型,也就是微调模型的方式. 因此诞生出了许多微调模型的方式,目前常用的微调库是huggingface/peft: 🤗 PEFT. 针对AI绘图应用的微调技术,倒是可以" property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2025/01/02/B3wYEMNog5KPLa7.png property=og:image><meta content=https://s2.loli.net/2025/01/02/CjIZsPOXFoLDYai.png property=og:image><meta content=https://s2.loli.net/2025/01/02/FogSW1IwGdLzfVK.png property=og:image><meta content=https://s2.loli.net/2025/01/02/KEoVxDuY3IiJOar.png property=og:image><meta content=https://s2.loli.net/2025/01/02/sItZgfVkolGeN4b.png property=og:image><meta content=https://s2.loli.net/2025/01/02/RAhWYImDw6H2cpM.png property=og:image><meta content=https://s2.loli.net/2025/01/02/pBcPXm7JDOZL6It.png property=og:image><meta content=https://s2.loli.net/2025/01/02/WEG5I47BVre3yA1.png property=og:image><meta content=2025-01-02T02:08:06.000Z property=article:published_time><meta content=2025-03-08T12:58:01.173Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="个人博客 技术学习 计算机 互联网 人工智能" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2025/01/02/B3wYEMNog5KPLa7.png name=twitter:image><link href=https://www.sekyoro.top/2025/01/02/Dive-into-fine-tuning-methods-for-SD/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>Dive into fine-tuning methods for SD | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2025/01/02/Dive-into-fine-tuning-methods-for-SD/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>Dive into fine-tuning methods for SD</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2025-01-02 10:08:06" datetime=2025-01-02T10:08:06+08:00>2025-01-02</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2025-03-08 20:58:01" datetime=2025-03-08T20:58:01+08:00 itemprop=dateModified>2025-03-08</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>9.2k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>8 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>文生图和图生图应用已经出现一段时间了,目前常用的应用就是根据用户需求修改图片,事实上这就是一种自定义. 因为需要模型重新生成整张图或者部分图,要么通过prompt、reference image,也就是改变输入的方式;要么通过修改模型,也就是微调模型的方式. 因此诞生出了许多微调模型的方式,目前常用的微调库是<a href=https://github.com/huggingface/peft rel=noopener target=_blank>huggingface/peft: 🤗 PEFT</a>. 针对AI绘图应用的微调技术,倒是可以推出一道清晰的发展线. 这里简单整理一下.</p><span id=more></span><h2 id=Before-LoRA><a title="Before LoRA" class=headerlink href=#Before-LoRA></a>Before LoRA</h2><p>在大名鼎鼎的LoRA之前,绘画相关的微调技术主要有Textual Inversion<a href=https://arxiv.org/pdf/2208.01618 rel=noopener target=_blank>2208.01618</a>和DreamBooth<a href=https://arxiv.org/pdf/2208.12242 rel=noopener target=_blank>2208.12242</a>. 此外还有改变输入以及嵌入向量的Prefix Tuning<a href=https://arxiv.org/abs/2101.00190 rel=noopener target=_blank>Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>和Prompt Tuning<a href=https://arxiv.org/abs/2104.08691 rel=noopener target=_blank>The Power of Scale for Parameter-Efficient Prompt Tuning</a>,由于与AI绘图相关微调联系不大,这里不深入.<h3 id=Textual-Inversion><a title="Textual Inversion" class=headerlink href=#Textual-Inversion></a>Textual Inversion</h3><p>​ 文本到图像的模型为通过自然语言指导创作提供了前所未有的自由。然而，如何行使这种自由来生成特定独特概念的图像，修改它们的外观，或将它们组成新的角色和新的场景，尚不清楚。<p>​ 只使用用户提供的一个概念的3 - 5张图像，<strong>比如一个对象或一个样式，在一个冻结的文本到图像模型的嵌入空间中，我们学习通过新的”词”来表示它</strong>。这些”词”可以组合成自然语言句子，以直观的方式指导个性化创作。作者发现单个词嵌入足以捕获独特和多样的概念。<p>​ 将新概念引入模型的难点: 将新概念引入大规模模型往往是困难的。为每个新概念重新训练一个具有扩展数据集的模型是非常昂贵的，<strong>对少数例子进行微调通常会导致灾难性的遗忘</strong>。<strong>更多的度量方法在面对新概念时冻结模型并训练转换模块以适应其输出。然而，这些方法仍然容易遗忘先验知识，或者与新学习的概念融合时面临困难。</strong><p>​ 提出<strong>通过在预训练的文本到图像模型的文本嵌入空间中寻找新词来克服这些挑战</strong>。考虑文本编码过程的第一阶段。在这里输入字符串首先被转换为一组token(词元,理解为词典中的不可再分的词)。然后将每个token替换为自己的嵌入向量，这些向量通过下游模型进行反馈。<strong>目标是寻找新的嵌入向量来表示新的、特定的概念</strong>。<p>​ 用一个新的词(pseudo-word)表示一个新的嵌入向量，我们用S <em>表示。然后这个词像其他任何词一样被处理，并且可以用于为生成模型合成新的文本查询。因此，可以要求”一张沙滩上的S </em>照片”、”一幅挂在墙上的S <em>油画”，甚至可以组成两个概念，如”一幅S </em> 1的S * 2的画”。<p>​ 重要的是，这个过程没有触及生成模型。在这样做的过程中，我们保留了在新任务上微调视觉和语言模型时通常会丢失的丰富的文本理解和泛化能力。<p>​ 为了找到这些词，将任务定为求逆运算,<strong>给出了一个固定的预训练文本-图像模型和一个描述概念的小( 3-5 )图像集。目标是找到一个单词嵌入，这样’ A photo of S * ‘形式的句子将导致从我们的小集合中重建图像。这种嵌入是通过一个优化过程找到的，我们称之为”文本倒置”。</strong><p><img alt=image-20250102112045813 data-src=https://s2.loli.net/2025/01/02/B3wYEMNog5KPLa7.png><p>具体来说,首先选取若干张相关概念的照片,假设pikachu,对应输入的prompt类似An image of pikachu, A photo of pikachu,然后就照着LDM训练的方式在原本的预训练大模型上继续训练即可.<h3 id=DreamBooth><a class=headerlink href=#DreamBooth title=DreamBooth></a>DreamBooth</h3><p><img alt=image-20250102121433867 data-src=https://s2.loli.net/2025/01/02/CjIZsPOXFoLDYai.png><p>​ DreamBooth的目的和Textual Inversion类似,扩展模型的语言-视觉词典，使其能够将新词与用户想要生成的特定主题绑定在一起。<p>​ 给定一个主体的几幅图像，目标是将主体植入模型的输出域，使其能够用唯一的标识符进行合成。为此提出一种技术，用稀有的标记标识符表示给定的主题，并微调一个预训练的、基于扩散的文本到图像框架。<p>​ 给定一个对象的3 - 5图像，微调一个文本到图像的扩散模型，输入图像与一个包含唯一标识符且对象所属类名为( e.g . , ‘ A [ V ] dog ‘)的文本提示配对，并行地应用一个特定于类的先验保存损失，该损失利用了模型在类上的语义先验，并鼓励它使用文本提示中的类名生成属于该对象类的多样实例。<p>​ 相比于textual inversion,它对模型而不只是embedding进行了微调,同时增加了新的损失, 对于同类但不同具体实例的图像进行训练.</p><script type="math/tex; mode=display">
\begin{array}{l} 
\mathbb{E}_{\mathbf{x}, \mathbf{c}, \epsilon, \epsilon^{\prime}, t}\left[w_{t}\left\|\hat{\mathbf{x}}_{\theta}\left(\alpha_{t} \mathbf{x}+\sigma_{t} \boldsymbol{\epsilon}, \mathbf{c}\right)-\mathbf{x}\right\|_{2}^{2}+\right. \\
\left.\lambda w_{t^{\prime}}\left\|\hat{\mathbf{x}}_{\theta}\left(\alpha_{t^{\prime}} \mathbf{x}_{\mathrm{pr}}+\sigma_{t^{\prime}} \epsilon^{\prime}, \mathbf{c}_{\mathrm{pr}}\right)-\mathbf{x}_{\mathrm{pr}}\right\|_{2}^{2}\right]
\end{array}</script><h2 id=LoRA-era><a title="LoRA era" class=headerlink href=#LoRA-era></a>LoRA era</h2><p>如果说上面几种方法都集中通过新的输入修改embedding,那么LoRA就是修改其中涉及重要计算模块的部分,比如Linear或Conv层 .<p>LoRA作为AI绘画模型微调技术不得不提的一环,一经提出就带来了一股LoRA潮<a href=https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725 rel=noopener target=_blank>An Overview of the LoRA Family. LoRA, DoRA, AdaLoRA, Delta-LoRA, and… | by Dorian Drost | Towards Data Science</a>.<p>此外LoRA也可以结合上面的修改embedding方法达到更好的效果.<h3 id=LoRA><a class=headerlink href=#LoRA title=LoRA></a>LoRA</h3><p><img alt=image-20250102132811471 data-src=https://s2.loli.net/2025/01/02/FogSW1IwGdLzfVK.png><p>神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。在适应特定任务时，Aghajanyan等研究表明，<strong>预训练的语言模型具有较低的”特征维度”(rank)，即使随机投影到较小的子空间，仍然可以高效地学习</strong>。受此启发，<strong>假设权重的更新在适应过程中也具有较低的”内在秩”</strong>。对于一个预训练的权重矩阵W^0^∈R^d×k^，用一个低秩分解W~0~ + $\Delta$W = W~0~ + BA来约束它的更新，其中B∈R^d×r^，A∈R^r×k^，秩为r=min( d , k)。在训练过程中，W^0^被冻结，不接受梯度更新，而A和B包含可训练参数。<p>​ 注意W~0~和∆W = BA都乘以相同的输入，并且它们各自的输出向量按位求和。<p>对于h = W~0~x，修正前向传递得到h = W~0~x +∆W x = W~0~x + BAx<p>​ 对A使用随机高斯初始化，对B使用零初始化，因此在训练开始时，W = BA为零。然后用α/r对∆W x进行缩放，其中α为r中的常数。<p>​ 在使用Adam进行优化时，如果对初始化进行适当的缩放，则调整α与调整学习率大致相同。因此，简单地将α设置为r，并不对其进行调整。这种缩放有助于减少当我们改变r时重新调整超参数的需要.<p>​ 在训练时,加上lora,冻结预训练模型,训练一个包含A和B的MLP,它更新参数时需要减去这个模型的参数<p>​ 测试时,模型的权重要加上LoRA的参数.<p>​ LoRA本身修改了微调时更新权重的方式,将更新的权重放到了一个可拆卸的模块中,同时由于只搭配预训练大模型中的某部分,使得微调过程更短,训练周期更短.<h4 id=代码实现><a class=headerlink href=#代码实现 title=代码实现></a>代码实现</h4><p>参考<a href=https://github.com/microsoft/LoRA/blob/main/loralib/layers.py rel=noopener target=_blank>LoRA/loralib/layers.py at main · microsoft/LoRA</a><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>LoRALayer</span>():</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self, </span></span></span><br><span class=line><span class=params><span class=function>        r: <span class=built_in>int</span>, </span></span></span><br><span class=line><span class=params><span class=function>        lora_alpha: <span class=built_in>int</span>, </span></span></span><br><span class=line><span class=params><span class=function>        lora_dropout: <span class=built_in>float</span>,</span></span></span><br><span class=line><span class=params><span class=function>        merge_weights: <span class=built_in>bool</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        self.r = r</span><br><span class=line>        self.lora_alpha = lora_alpha</span><br><span class=line>        <span class=comment># Optional dropout</span></span><br><span class=line>        <span class=keyword>if</span> lora_dropout > <span class=number>0.</span>:</span><br><span class=line>            self.lora_dropout = nn.Dropout(p=lora_dropout)</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            self.lora_dropout = <span class=keyword>lambda</span> x: x</span><br><span class=line>        <span class=comment># Mark the weight as unmerged</span></span><br><span class=line>        self.merged = <span class=literal>False</span></span><br><span class=line>        self.merge_weights = merge_weights</span><br></pre></table></figure><p>对于Embedding,Linear以及Conv层有不同的具体实现. 但总体来说,在训练时,通过减去B@A更新参数. 在测试时权重加上LoRA层.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Linear</span>(<span class=params>nn.Linear, LoRALayer</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self,</span></span></span><br><span class=line><span class=params><span class=function>                 in_features: <span class=built_in>int</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 out_features: <span class=built_in>int</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 r: <span class=built_in>int</span> = <span class=number>0</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 lora_alpha: <span class=built_in>int</span> = <span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 lora_dropout: <span class=built_in>float</span> = <span class=number>0.</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 fan_in_fan_out: <span class=built_in>bool</span> = <span class=literal>False</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 <span class=comment># Set this to True if the layer to replace stores weight like (fan_in, fan_out)</span></span></span></span><br><span class=line><span class=params><span class=function>                 merge_weights: <span class=built_in>bool</span> = <span class=literal>True</span>,</span></span></span><br><span class=line><span class=params><span class=function>                 **kwargs</span>):</span></span><br><span class=line>        nn.Linear.__init__(self, in_features, out_features, **kwargs)</span><br><span class=line>        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=<span class=number>0</span>, merge_weights=merge_weights)</span><br><span class=line>        self.fan_in_fan_out = fan_in_fan_out</span><br><span class=line>        <span class=keyword>if</span> r > <span class=number>0</span>:</span><br><span class=line>            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))</span><br><span class=line>            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))</span><br><span class=line>            self.scaling = self.lora_alpha / self.r</span><br><span class=line>            self.weight.requires_grad = <span class=literal>False</span> <span class=comment># 禁止梯度更新</span></span><br><span class=line>        self.reset_parameters()</span><br><span class=line>        <span class=keyword>if</span> fan_in_fan_out:</span><br><span class=line>            self.weight.data = self.weight.data.transpose(<span class=number>0</span>, <span class=number>1</span>)</span><br><span class=line></span><br></pre></table></figure><h3 id=LyCORIS><a class=headerlink href=#LyCORIS title=LyCORIS></a>LyCORIS</h3><p><a href=https://arxiv.org/abs/2309.14859 rel=noopener target=_blank>2309.14859] Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation</a><p>提出了一系列用在Stable Diffusion中基于LoRA的微调方式并设计了benchmark测试<p><img alt=image-20250102152745443 data-src=https://s2.loli.net/2025/01/02/KEoVxDuY3IiJOar.png><p>主要包括LoHA和LoKr,其实光看图就很容易明白. LoHA另外引入一套BA,并通过点乘得到最终的更新,论文解释这种方法得到的矩阵的秩大于一般的低秩分解. 而LoKr就直接使用矩阵直积了.<p>这方面还有很多魔改的各种微调,就不一一介绍了.<h2 id=Adapters-for-preserving-Identity><a title="Adapters for preserving Identity" class=headerlink href=#Adapters-for-preserving-Identity></a>Adapters for preserving Identity</h2><p>Adapter的实现和LoRA有类似点,但提出的目的不同与LoRA:LoRA强调用户拿几张个人照片让模型学习新的权重同时不过于遗忘已有知识,而Adapter的场景更偏向适应与融合.<h3 id=IP-Adapter><a class=headerlink href=#IP-Adapter title=IP-Adapter></a>IP-Adapter</h3><p><a href=https://github.com/tencent-ailab/IP-Adapter?tab=readme-ov-file rel=noopener target=_blank>tencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.</a><p><img alt=image-20250102155440556 data-src=https://s2.loli.net/2025/01/02/sItZgfVkolGeN4b.png><p>针对现有的文本到图像扩散模型，提出了一种基于解耦交叉注意力策略的轻量级图像提示自适应方法IP-Adapter。<p>在图像编码器之后加入新的权重参数进行编码,并将编码后的特征用于训练一个新的cross attention layer. 论文的关键创新就是引入cross attention将图像特征用来微调模型.<p>给定图像特征c~i~，新的交叉注意力Z′′的输出计算如下</p><script type="math/tex; mode=display">
\mathbf{Z}^{\prime \prime}=\operatorname{Attention}\left(\mathbf{Q}, \mathbf{K}^{\prime}, \mathbf{V}^{\prime}\right)=\operatorname{Softmax}\left(\frac{\mathbf{Q}\left(\mathbf{K}^{\prime}\right)^{\top}}{\sqrt{d}}\right) \mathbf{V}^{\prime}</script><p>其中Q=ZW~q~,Z是文本编码后的特征,K^’^=cW~k~,c是图像特征,V=cV~v~<p><img alt=image-20250102161435821 data-src=https://s2.loli.net/2025/01/02/RAhWYImDw6H2cpM.png><h3 id=InstantID><a class=headerlink href=#InstantID title=InstantID></a>InstantID</h3><p><a href=https://github.com/instantX-research/InstantID?tab=readme-ov-file rel=noopener target=_blank>instantX-research/InstantID: InstantID: Zero-shot Identity-Preserving Generation in Seconds 🔥</a><p><img alt=image-20250102155632207 data-src=https://s2.loli.net/2025/01/02/pBcPXm7JDOZL6It.png><p>首先，采用人脸编码器代替CLIP提取语义人脸特征，并使用可训练的投影层将其投影到文本特征空间。将投影后的特征作为人脸嵌入。然后，引入解耦交叉注意力的轻量级自适应模块，以支持图像作为提示。最后提出IdentityNet对参考人脸图像中的复杂特征进行编码，并附加弱空间控制。在IdentityNet中，生成过程完全由人脸嵌入引导，无需任何文本信息。只更新新增加的模块，而预训练的文本到图像模型保持冻结，以确保灵活性。经过训练，用户可以自由地生成任意风格的高保真度的ID保持图像。<h2 id=Towards-Video-Diffusion-Models><a title="Towards Video Diffusion Models" class=headerlink href=#Towards-Video-Diffusion-Models></a>Towards Video Diffusion Models</h2><p><a href=https://github.com/ChenHsing/Awesome-Video-Diffusion-Models rel=noopener target=_blank>CSUR] A Survey on Video Diffusion Models</a><p>生成图像已经不够了,可以通过prompt生成连续的多张图片,这样可以组成GIF甚至更长更高质量的视频.<p>比如<strong>AnimateDiff</strong><p><img alt=image-20250102140534169 data-src=https://s2.loli.net/2025/01/02/WEG5I47BVre3yA1.png><h2 id=训练LoRA><a class=headerlink href=#训练LoRA title=训练LoRA></a>训练LoRA</h2><p>线性层中的lora<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br></pre><td class=code><pre><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>LoraInjectedLinear</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self, in_features, out_features, bias=<span class=literal>False</span>, r=<span class=number>4</span>, dropout_p=<span class=number>0.1</span>, scale=<span class=number>1.0</span></span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> r > <span class=built_in>min</span>(in_features, out_features):</span><br><span class=line>            <span class=keyword>raise</span> ValueError(</span><br><span class=line>                <span class=string>f"LoRA rank <span class=subst>{r}</span> must be less or equal than <span class=subst>{<span class=built_in>min</span>(in_features,out_features)}</span>"</span></span><br><span class=line>            )</span><br><span class=line>        self.r = r</span><br><span class=line>        self.linear = nn.Linear(in_features, out_features, bias)</span><br><span class=line>        self.lora_down = nn.Linear(in_features, r, bias=<span class=literal>False</span>)</span><br><span class=line>        self.dropout = nn.Dropout(dropout_p)</span><br><span class=line>        self.lora_up = nn.Linear(r, out_features, bias=<span class=literal>False</span>)</span><br><span class=line>        self.scale = scale</span><br><span class=line>        self.selector = nn.Identity()</span><br><span class=line></span><br><span class=line>        <span class=comment>#  init ΔW = A*B</span></span><br><span class=line>        nn.init.normal_(self.lora_down.weight, std=<span class=number>1</span> / r)</span><br><span class=line>        nn.init.zeros_(self.lora_up.weight)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, <span class=built_in>input</span></span>):</span></span><br><span class=line>        <span class=keyword>return</span> (</span><br><span class=line>            self.linear(<span class=built_in>input</span>)</span><br><span class=line>            + self.dropout(self.lora_up(self.selector(self.lora_down(<span class=built_in>input</span>))))</span><br><span class=line>            * self.scale</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>realize_as_lora</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.lora_up.weight.data * self.scale, self.lora_down.weight.data</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>set_selector_from_diag</span>(<span class=params>self, diag: torch.Tensor</span>):</span></span><br><span class=line>        <span class=keyword>assert</span> diag.shape == (self.r,)</span><br><span class=line>        self.selector = nn.Linear(self.r, self.r, bias=<span class=literal>False</span>)</span><br><span class=line>        self.selector.weight.data = (</span><br><span class=line>            torch.diag(diag)</span><br><span class=line>            .to(self.lora_up.weight.device)</span><br><span class=line>            .to(self.lora_up.weight.dtype)</span><br><span class=line>        )</span><br></pre></table></figure><p>卷积中的lora<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>LoraInjectedConv2d</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        self,</span></span></span><br><span class=line><span class=params><span class=function>        in_channels: <span class=built_in>int</span>,</span></span></span><br><span class=line><span class=params><span class=function>        out_channels: <span class=built_in>int</span>,</span></span></span><br><span class=line><span class=params><span class=function>        kernel_size: <span class=built_in>int</span>,</span></span></span><br><span class=line><span class=params><span class=function>        stride=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        padding=<span class=number>0</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dilation=<span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        groups: <span class=built_in>int</span> = <span class=number>1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        bias: <span class=built_in>bool</span> = <span class=literal>True</span>,</span></span></span><br><span class=line><span class=params><span class=function>        r: <span class=built_in>int</span> = <span class=number>4</span>,</span></span></span><br><span class=line><span class=params><span class=function>        dropout_p: <span class=built_in>float</span> = <span class=number>0.1</span>,</span></span></span><br><span class=line><span class=params><span class=function>        scale: <span class=built_in>float</span> = <span class=number>1.0</span>,</span></span></span><br><span class=line><span class=params><span class=function>    </span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        <span class=keyword>if</span> r > <span class=built_in>min</span>(in_channels, out_channels):</span><br><span class=line>            <span class=keyword>raise</span> ValueError(</span><br><span class=line>                <span class=string>f"LoRA rank <span class=subst>{r}</span> must be less or equal than <span class=subst>{<span class=built_in>min</span>(in_channels, out_channels)}</span>"</span></span><br><span class=line>            )</span><br><span class=line>        self.r = r</span><br><span class=line>        self.conv = nn.Conv2d(</span><br><span class=line>            in_channels=in_channels,</span><br><span class=line>            out_channels=out_channels,</span><br><span class=line>            kernel_size=kernel_size,</span><br><span class=line>            stride=stride,</span><br><span class=line>            padding=padding,</span><br><span class=line>            dilation=dilation,</span><br><span class=line>            groups=groups,</span><br><span class=line>            bias=bias,</span><br><span class=line>        )</span><br><span class=line>        self.lora_down = nn.Conv2d(</span><br><span class=line>            in_channels=in_channels,</span><br><span class=line>            out_channels=r,</span><br><span class=line>            kernel_size=kernel_size,</span><br><span class=line>            stride=stride,</span><br><span class=line>            padding=padding,</span><br><span class=line>            dilation=dilation,</span><br><span class=line>            groups=groups,</span><br><span class=line>            bias=<span class=literal>False</span>,</span><br><span class=line>        )</span><br><span class=line>        self.dropout = nn.Dropout(dropout_p)</span><br><span class=line>        self.lora_up = nn.Conv2d(</span><br><span class=line>            in_channels=r,</span><br><span class=line>            out_channels=out_channels,</span><br><span class=line>            kernel_size=<span class=number>1</span>,</span><br><span class=line>            stride=<span class=number>1</span>,</span><br><span class=line>            padding=<span class=number>0</span>,</span><br><span class=line>            bias=<span class=literal>False</span>,</span><br><span class=line>        )</span><br><span class=line>        self.selector = nn.Identity()</span><br><span class=line>        self.scale = scale</span><br><span class=line></span><br><span class=line>        nn.init.normal_(self.lora_down.weight, std=<span class=number>1</span> / r)</span><br><span class=line>        nn.init.zeros_(self.lora_up.weight)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, <span class=built_in>input</span></span>):</span></span><br><span class=line>        <span class=keyword>return</span> (</span><br><span class=line>            self.conv(<span class=built_in>input</span>)</span><br><span class=line>            + self.dropout(self.lora_up(self.selector(self.lora_down(<span class=built_in>input</span>))))</span><br><span class=line>            * self.scale</span><br><span class=line>        )</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>realize_as_lora</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.lora_up.weight.data * self.scale, self.lora_down.weight.data</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>set_selector_from_diag</span>(<span class=params>self, diag: torch.Tensor</span>):</span></span><br><span class=line>        <span class=keyword>assert</span> diag.shape == (self.r,)</span><br><span class=line>        self.selector = nn.Conv2d(</span><br><span class=line>            in_channels=self.r,</span><br><span class=line>            out_channels=self.r,</span><br><span class=line>            kernel_size=<span class=number>1</span>,</span><br><span class=line>            stride=<span class=number>1</span>,</span><br><span class=line>            padding=<span class=number>0</span>,</span><br><span class=line>            bias=<span class=literal>False</span>,</span><br><span class=line>        )</span><br><span class=line>        self.selector.weight.data = torch.diag(diag)</span><br><span class=line>        self.selector.weight.data = self.selector.weight.data.to(</span><br><span class=line>            self.lora_up.weight.device</span><br><span class=line>        ).to(self.lora_up.weight.dtype)</span><br><span class=line></span><br></pre></table></figure><h2 id=相关资料><a class=headerlink href=#相关资料 title=相关资料></a>相关资料</h2><ol><li><a href=https://zhuanlan.zhihu.com/p/675231376 rel=noopener target=_blank>一文辨析清楚LORA、Prompt Tuning、P-Tuning、Adapter 、Prefix等大模型微调方法 - 知乎</a><li><a href=https://www.artvy.ai/resource/lora-vs-dreambooth-vs-textual-inversion-vs rel=noopener target=_blank>LoRA vs Dreambooth vs Textual Inversion vs Hypernetworks: Exploring the World of Stable Diffusion Fine-Tuning Methods</a><li><a href=https://openreview.net/forum?id=wfzXa8e783 rel=noopener target=_blank>Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation | OpenReview</a><li><a href=https://github.com/KohakuBlueleaf/LyCORIS?tab=readme-ov-file rel=noopener target=_blank>KohakuBlueleaf/LyCORIS: Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion.</a><li><a href=https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725 rel=noopener target=_blank>An Overview of the LoRA Family. LoRA, DoRA, AdaLoRA, Delta-LoRA, and… | by Dorian Drost | Towards Data Science</a><li><a href=https://arxiv.org/pdf/2106.09685 rel=noopener target=_blank>2106.09685</a><li><a href=https://arxiv.org/abs/2309.14859 rel=noopener target=_blank>[2309.14859] Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation</a><li><a href=https://arxiv.org/pdf/2307.04725 rel=noopener target=_blank>2307.04725</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="Dive into fine-tuning methods for SD" href=https://www.sekyoro.top/2025/01/02/Dive-into-fine-tuning-methods-for-SD/>https://www.sekyoro.top/2025/01/02/Dive-into-fine-tuning-methods-for-SD/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a title="An intro to Websocket and SSE" href=/2024/12/31/An-intro-to-Websocket-and-SSE/ rel=prev> <i class="fa fa-chevron-left"></i> An intro to Websocket and SSE </a></div><div class=post-nav-item><a href=/2025/01/09/%E5%89%8D%E7%AB%AF%E6%9E%84%E5%BB%BA%E4%B8%8E%E6%89%93%E5%8C%85%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/ rel=next title=前端运行时、打包与构建简单介绍> 前端运行时、打包与构建简单介绍 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#Before-LoRA><span class=nav-number>1.</span> <span class=nav-text>Before LoRA</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Textual-Inversion><span class=nav-number>1.1.</span> <span class=nav-text>Textual Inversion</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#DreamBooth><span class=nav-number>1.2.</span> <span class=nav-text>DreamBooth</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#LoRA-era><span class=nav-number>2.</span> <span class=nav-text>LoRA era</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#LoRA><span class=nav-number>2.1.</span> <span class=nav-text>LoRA</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0><span class=nav-number>2.1.1.</span> <span class=nav-text>代码实现</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#LyCORIS><span class=nav-number>2.2.</span> <span class=nav-text>LyCORIS</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Adapters-for-preserving-Identity><span class=nav-number>3.</span> <span class=nav-text>Adapters for preserving Identity</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#IP-Adapter><span class=nav-number>3.1.</span> <span class=nav-text>IP-Adapter</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#InstantID><span class=nav-number>3.2.</span> <span class=nav-text>InstantID</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Towards-Video-Diffusion-Models><span class=nav-number>4.</span> <span class=nav-text>Towards Video Diffusion Models</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%AE%AD%E7%BB%83LoRA><span class=nav-number>5.</span> <span class=nav-text>训练LoRA</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99><span class=nav-number>6.</span> <span class=nav-text>相关资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>236</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>此文章目前无词云</h3></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.4m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>37:01</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>