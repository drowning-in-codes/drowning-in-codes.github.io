<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="这半年要论AI哪些方向最火,那关键词必然包括多模态,AI Agent,RAG等等(事实上已经火了一轮开始冷饭热炒了),一些做之前基础大模型的公司基本开始转向做应用甚至其他方向了. 这里整理一些关于AI Agents的知识和相关基础框架,并结合多智能体协同感知看看有哪些能做的结合." name=description><meta content=article property=og:type><meta content=协同感知算法(四):大模型、多模态以及新趋势 property=og:title><meta content=https://www.sekyoro.top/2025/01/10/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E5%9B%9B-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E3%80%81%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%B6%8B%E5%8A%BF/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="这半年要论AI哪些方向最火,那关键词必然包括多模态,AI Agent,RAG等等(事实上已经火了一轮开始冷饭热炒了),一些做之前基础大模型的公司基本开始转向做应用甚至其他方向了. 这里整理一些关于AI Agents的知识和相关基础框架,并结合多智能体协同感知看看有哪些能做的结合." property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2025/01/11/rF7sJIdcoWnBZhv.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/Frozen-arch.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/ClipCap-arch.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/VisualGPT.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/VC-GPT.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/MERLOT.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/Flamingo.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/CoCa-arch.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/CoCa.png property=og:image><meta content=https://s2.loli.net/2025/01/12/Knk9qHblQgwALuv.png property=og:image><meta content=https://lilianweng.github.io/posts/2022-06-09-vlm/SM-caption-example.png property=og:image><meta content=https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png property=og:image><meta content=https://lilianweng.github.io/posts/2023-06-23-agent/reflexion.png property=og:image><meta content=https://s2.loli.net/2025/03/01/b3Rwruo2Nqjl9Qc.png property=og:image><meta content=https://s2.loli.net/2025/03/01/JaKvA2B4yOdxcSP.png property=og:image><meta content=https://s2.loli.net/2025/03/01/6Rit5JYmheqW82P.png property=og:image><meta content=https://s2.loli.net/2025/03/01/U5HNZpuxPMXJIzd.png property=og:image><meta content=https://s2.loli.net/2025/03/01/K8GL6CWQgoEzJXl.png property=og:image><meta content=https://s2.loli.net/2025/03/01/1IqhDjUavQlOLou.png property=og:image><meta content=https://s2.loli.net/2025/03/01/GZoEUBPHJ4lnSMR.png property=og:image><meta content=https://s2.loli.net/2025/03/01/7N1apvPSXFcDfOo.png property=og:image><meta content=https://s2.loli.net/2025/05/19/LUCgmobRie6r4YA.png property=og:image><meta content=https://s2.loli.net/2025/05/19/CjKxWzUFnSImG6Q.png property=og:image><meta content=https://s2.loli.net/2025/05/26/WYh3nLOkHo4DcbA.png property=og:image><meta content=https://s2.loli.net/2025/05/26/zsyQwZC9VNhJruf.png property=og:image><meta content=https://s2.loli.net/2025/05/28/YD1qVd5jlihTAuF.png property=og:image><meta content=https://s2.loli.net/2025/05/26/zBJ1wDiRO6rTdqA.png property=og:image><meta content=https://s2.loli.net/2025/05/27/Q2tnd7KxNgJH4Cl.png property=og:image><meta content=https://s2.loli.net/2025/05/27/KPhjznkL9WQs2cX.png property=og:image><meta content=https://s2.loli.net/2025/05/27/d8WJcQ3hzrxtF6g.png property=og:image><meta content=https://s2.loli.net/2025/05/26/KyiUGwp4s8xnMYR.png property=og:image><meta content=https://s2.loli.net/2025/05/26/zyYZQMXmxW2EvST.png property=og:image><meta content=https://s2.loli.net/2025/05/19/eptucRAXdNPwk7D.png property=og:image><meta content=https://s2.loli.net/2025/05/28/JtHQVc1n2v86UXC.png property=og:image><meta content=https://s2.loli.net/2025/05/28/3UwDiBq8ZRs94Pp.png property=og:image><meta content=https://s2.loli.net/2025/05/28/UHwDobJxyz32LhW.png property=og:image><meta content=https://s2.loli.net/2025/05/28/q6HJklWt9fOSIZc.png property=og:image><meta content=https://s2.loli.net/2025/05/28/heA1tbXIiVDYr8o.png property=og:image><meta content=https://s2.loli.net/2025/05/28/C3TP5RLomvNcBp2.png property=og:image><meta content=https://s2.loli.net/2025/05/28/L4zjETsIZkaouq3.png property=og:image><meta content=2025-01-10T15:08:12.000Z property=article:published_time><meta content=2025-06-06T07:29:34.474Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="collaborative perception" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2025/01/11/rF7sJIdcoWnBZhv.png name=twitter:image><link href=https://www.sekyoro.top/2025/01/10/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E5%9B%9B-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E3%80%81%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%B6%8B%E5%8A%BF/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>协同感知算法(四):大模型、多模态以及新趋势 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2025/01/10/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E5%9B%9B-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E3%80%81%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%B6%8B%E5%8A%BF/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>协同感知算法(四):大模型、多模态以及新趋势</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2025-01-10 23:08:12" datetime=2025-01-10T23:08:12+08:00>2025-01-10</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2025-06-06 15:29:34" datetime=2025-06-06T15:29:34+08:00 itemprop=dateModified>2025-06-06</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>18k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>17 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag># collaborative perception</a></div><p>这半年要论AI哪些方向最火,那关键词必然包括多模态,AI Agent,RAG等等(事实上已经火了一轮开始冷饭热炒了),一些做之前基础大模型的公司基本开始转向做应用甚至其他方向了. 这里整理一些关于AI Agents的知识和相关基础框架,并结合多智能体协同感知看看有哪些能做的结合.<br><span id=more></span><h2 id=视觉语言模型><a class=headerlink href=#视觉语言模型 title=视觉语言模型></a>视觉语言模型</h2><p>引用lilianweng的博客,视觉语言模型可以粗略分为四类:<br>1.将图像转换为可以与标记嵌入联合训练的嵌入特征(将图像转为可以与语言编码的特征一起训练的特征)<br>2.学习作为冻结、预训练语言模型前缀的良好图像嵌入(训练图像特征作为冻结的预训练语言模型的输入前缀)<br>3.使用专门设计的交叉注意力机制将视觉信息融合到语言模型的层中(使用交叉注意力融合视觉信息到大模型中)<br>4.无需训练即可结合视觉和语言模型<h3 id=同时训练图像和文本><a class=headerlink href=#同时训练图像和文本 title=同时训练图像和文本></a>同时训练图像和文本</h3><p>VisualBERT将文本输入和图像区域同时输入 BERT，使其能够<strong>通过自注意力机制发现图像和文本之间的内部对齐</strong>。<br>在训练时同时输入图像和文本,mask相关文本并加上图像的相关信息,任务是预测遮挡的信息同时提供两个标题区分哪个与图像相关.<br><img alt=image-20250111165850291 data-src=https://s2.loli.net/2025/01/11/rF7sJIdcoWnBZhv.png><br>与 BERT 中的文本嵌入类似，VisualBERT 中的每个视觉嵌入也总结了三种类型的嵌入，即<strong>分词特征</strong>、<strong>分割嵌入</strong>和<strong>位置嵌入</strong>，具体来说：<ol><li><p>一种通过卷积神经网络计算出的图像边界区域的视觉特征向量</p><li><p>一个表示嵌入是否用于视觉而非文本的段嵌入</p><li><p>一种用于对齐边界区域顺序的位置嵌入</p></ol><p>SimVLM是一种简单的前缀语言模型，其中前缀序列的处理方式类似于 BERT 的双向注意力，但主要的文本输入序列只有因果注意力类似于 GPT。图像被编码为前缀标记，以便模型可以完全消耗视觉信息，然后以自回归方式生成相关文本。<br>SimVLM 将图像分割成更小的块，形成一个平铺的 1D 块序列。他们使用由 ResNet 的前 3 个块组成的卷积阶段来提取上下文化的块，这种设置被发现比简单的线性投影效果更好。<br> 这种方法的学习目标更像是通过视觉信息前缀,通过transformer结构进行自回归学习,这也是它与前者较大的差距. 因此通过图像编码器编码图像特征然后通过掩码语言建模(Masked Language Modeling)(类似完形填空)或自回归学习训练原本的大语言模型使其拥有视觉能力.<h3 id=学习图像嵌入><a class=headerlink href=#学习图像嵌入 title=学习图像嵌入></a>学习图像嵌入</h3><p>如果不想在适应处理视觉信号时更改语言模型参数,那么可以学习一个与语言模型兼容的图像嵌入空间。<br>受前缀或提示调整的启发，Frozen和 ClipCap<strong>仅在训练期间更新视觉模块的参数</strong>，以生成可以与预训练的冻结语言模型一起工作的图像嵌入。两者都使用对齐的图像标题数据集进行训练，以根据图像和先前的文本标记生成下一个文本标记。通过冻结 LM 参数，保留了强大的语言能力。此外，尽管这种设置是在有限的图像标题数据上训练的，但它们在测试时也可以依赖语言模型的知识库。<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/Frozen-arch.png><p>从框架图看来就是只是用了一个视觉编码器将得到的embedding与预训练固定参数的text encoder和大模型同时训练更新视觉编码器<br>ClipCap 依靠 CLIP 进行视觉编码，但它需要由光映射网络处理，以便将图像嵌入向量转换为与预训练 LM 相同的语义空间。该网络将 CLIP 嵌入向量映射到一系列嵌入向量中，每个向量与 GPT2 中的单词嵌入具有相同的维度。增加前缀大小有助于提高性能。CLIP 视觉编码器和 LM 在训练期间都会被冻结，并且只学习映射网络。<br>ClipCap学习的是一个映射网络,它同时利用了预训练的CLIP和大模型<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/ClipCap-arch.png><h3 id=交叉注意力混合视觉和文本信息><a class=headerlink href=#交叉注意力混合视觉和文本信息 title=交叉注意力混合视觉和文本信息></a>交叉注意力混合视觉和文本信息</h3><p><strong>VisualGPT</strong>采用self-resurrecting的编码器-解码器注意力机制,用少量的域内图像文本数据快速适应预训练的LM<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/VisualGPT.png style=zoom:67%;><br>设I为视觉编码器的输入,H为LM解码器的隐藏状态. 引入激活单元通过两个互补门来控制预训练语言信息和视觉组件之间混合.</p><script type="math/tex; mode=display">
\begin{array}{c}
B^{\text {vs }} \otimes \operatorname{EncDecattn}(H, I)+B^{\text {in }} \otimes H \\
\text { where } B^{\text {vs }}[i, j]=\sigma(H[i, j])[\sigma(H[i, j])>\tau] \\
B^{\text {ban }}[i, j]=(1-\sigma(H[i, j])] \neq[1-\sigma(H[i, j])>\tau]
\end{array}</script><p>每层decoder通过门控制单元混合不同的输入.<br><strong>VC-GPT</strong> 将预训练的视觉转换器(CLIP-ViT)作为视觉编码器和预训练的 LM 作为语言解码器相结合。CLIP-ViT 将一系列图像块作为输入和输出，并输出每个块的表示。为避免灾难性的遗忘，<strong>VC-GPT 不是将视觉信息直接注入 GPT2，而是在视觉编码器和语言解码器的输出之上引入了额外的交叉注意力层</strong>。然后， <em>一个自集成</em> 模块线性组合单模型语言解码器 logits h^G^和跨模型视觉语言融合模块 logitsh^fuse^ 自集成模块对于性能很重要<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/VC-GPT.png><br>MERLOT接受了 600 万个 YouTube 视频的训练，并转录了语音 （YT-Temporal-180M），以学习空间（帧级）和时间（视频级）目标，并在微调时在 VQA 和视觉推理任务上表现出强大的表现。每个视频被拆分为多个片段,每个片段从中间的图像帧和关联的单词。图像由学习的图像编码器编码，单词使用学习的嵌入进行编码。然后，两者在一个联合的vision-language transformer中一起编码。<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/MERLOT.png><ol><li>遮罩语言建模(MLM)，因为在视频中有许多重复的关键字或填充词。<li>对比帧-标题匹配使用联合视觉-语言转换器中的纯语言部分。每个帧 I~t~和 caption w~t~ 的匹配表示形式被视为正面示例，而负面示例来自小批量中的所有其他帧-标题对。<li>时间重新排序学习时间推理:打乱随机i帧,并将段级位置嵌入替换为随机且唯一的位置嵌入。随机位置嵌入被学习,允许模型以正确排序的帧为条件取消这些 “’shuffled’” 帧。</ol><p>Flamingo （ Alayrac et al. 2022） 是一种视觉语言模型，它接受与图像/视频交错的文本并输出自由格式的文本。Flamingo 通过基于 transformer 的映射器连接预训练的 LM 和预训练的视觉编码器（即 CLIP 图像编码器）。为了更有效地整合视觉信号，Flamingo 采用基于 Perceiver 的架构，从大量视觉输入特征中生成数百个标记，然后使用与 LM 层交错的交叉注意力层将视觉信息融合到语言解码过程中。训练目标是自回归的 NLL 损失。Perceiver 重采样器从图像/视频输入的视觉编码器接收时空特征，以生成固定大小的视觉标记。冻结的 LM 配备了新初始化的交叉注意力层，这些层在预训练的 LM 层之间交错。因此LM 可以生成以上述视觉标记为条件的文本。<p>与 ClipCap 类似，两个预训练模型在训练期间都会被冻结，因此 Flamingo 仅经过训练才能将现有的强大语言和视觉模型和谐地连接在一起。ClipCap 和 Flamingo 的主要区别在于，前者将图像嵌入视为 LM 的简单前缀，而后者使用门控交叉注意力密集层来融合图像信息。此外，Flamingo 包含的训练数据比 ClipCap 多得多。<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/Flamingo.png><br>为了轻松处理带有交错图像的文本，Flamingo 中的遮罩设计为文本标记仅交叉关注与最后一个前图像对应的视觉标记，从而大大减少了某个文本标记可以看到的视觉标记的数量。他们发现这比允许文本标记直接处理所有前面的图像效果更好。文本仍然可以处理所有以前的图像，因为文本编码器中存在因果自我注意依赖关系。此设计可以处理上下文中任意数量的图像。从互联网上抓取了 4300 万个网页，名为 MultiModal MassiveWeb （M3W) 数据集，其中包含带有交错图像的文本。此外，Flamingo 还在配对的图像/文本和视频/文本数据集上进行了训练，包括 ALIGN、LTIP 和 VTP。<p>CoCa捕捉到了对比学习和图像到标题生成的优点。它是一个联合训练的模型，在 CLIP 风格的表示上具有对比损失，在图像描述上具有生成损失，在各种多模态评估任务上实现了 SoTA 零样本转移。<br><img style="zoom: 67%;" alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/CoCa-arch.png><br>CoCa 是从头开始预训练的,使用 Web 规模的替代文本数据 ALIGN 和注释图像，将所有标签视为 JTB-3B 中的文本<br><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/CoCa.png><h3 id=无训练方法><a class=headerlink href=#无训练方法 title=无训练方法></a>无训练方法</h3><p>可以通过将预训练的语言和视觉模型拼接在一起来解决视觉语言任务,而无需训练任何其他参数。<p>MAGiC 根据名为 magic score 的基于 CLIP 的分数进行引导解码，以对下一个标记进行采样，而无需微调。鼓励生成的文本与给定图像相关，同时仍与先前生成的文本保持一致.与其他无监督方法相比，MAGiC 具有不错的性能，但与有监督方法仍然存在很大差距. MAGiC核心就是更改了采样方式,第t步采样的token会通过基于CLIP的分数计算与图像编码相似的值.<p><img alt=image-20250112122938693 data-src=https://s2.loli.net/2025/01/12/Knk9qHblQgwALuv.png><p>​ 对于基于知识的 VQA 任务，PICa首先将图像转换为标题或标签，然后使用少数镜头示例提示 GPT3 提供答案。图像标题或标记由某些现有模型（例如 VinVL）或 Azure 标记 API 提取。GPT3 被认为是一个非结构化的隐式知识库.<p>​ 苏格拉底模型是一个框架,通过语言(提示)将不同模态的多个预训练模型组合成一个模型，而无需进一步训练。在这里，语言被认为是不同模型可以交换信息的中间表示。关键思想是使用多模型多模态提示，其中非语言模型的输出入到语言提示中，然后用于 LM 进行推理. SM 可以生成图像字幕，首先使用 VLM 对不同的地点类别、对象类别、图像类型和人数进行零拍摄预测;然后将 VLM 填充的语言提示输入到因果 LM 中以生成候选字幕。Socratic 方法在图像字幕方面与 ClipCap 的性能仍然存在差距，但考虑到它不涉及任何培训，因此相当不错。 简单来说就是将不同模态模型,比如vision-language model,language model,audio-language model结合在一起,通过一种相对固定的prompt template通信,SM 可以生成图像字幕，首先使用 VLM 对不同的地点类别、对象类别、图像类型和人数进行零拍摄预测;然后将 VLM 填充的语言提示输入到因果 LM 中以生成候选字幕<p><img alt=img data-src=https://lilianweng.github.io/posts/2022-06-09-vlm/SM-caption-example.png><h2 id=大模型驱动的Agents><a class=headerlink href=#大模型驱动的Agents title=大模型驱动的Agents></a>大模型驱动的Agents</h2><p>​ 多个大模型之间协作增强输出并设计一些类似模仿一个系统中负责不同功能的组件组成Agent. 目前许多大模型公司在做应用时都是往这个方向发展.在 LLM 驱动的自主代理系统中，LLM 充当代理的大脑，并辅以几个关键组件：<p>​ <strong>规划</strong>:子目标和分解：智能体将大型任务分解为较小的、可管理的子目标，从而能够高效处理复杂任务。反思和完善：智能体可以对过去的行为进行自我批评和自我反省，从错误中吸取教训并为未来的步骤进行改进，从而提高最终结果的质量。(任务分解和完善行为)<p>​ <strong>记忆</strong>: 短期记忆：所有的上下文学习（参见提示工程）都是利用模型的短期记忆来学习的。长期记忆：这为代理提供了在较长时间内保留和调用（无限）信息的能力，通常是通过利用外部向量存储和快速检索。<p><strong>工具使用</strong>:智能体学习调用外部 API 以获取模型权重中缺少的额外信息（通常在预训练后很难更改），包括当前信息、代码执行能力、对专有信息源的访问等<p><img alt=img data-src=https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png><h3 id=规划><a class=headerlink href=#规划 title=规划></a>规划</h3><h4 id=任务分解><a class=headerlink href=#任务分解 title=任务分解></a>任务分解</h4><p>Chain of Thoughts已成为提高模型在复杂任务上性能的标准提示技术。该模型被指示 “逐步思考”，以利用更多的测试时计算将困难的任务分解为更小、更简单的步骤。CoT 将大任务转化为多个可管理的任务，并阐明对模型思维过程的解释<br>Tree of Thoughts通过在每一步探索多种推理可能性来扩展 CoT。它首先<strong>将问题分解为多个思考步骤，每个步骤生成多个思考，从而创建一个树状结构。</strong>搜索过程可以是 BFS（广度优先搜索）或 DFS（深度优先搜索），每个状态都由分类器（通过提示）或多数投票进行评估。<br>任务分解可以通过以下方式完成：（1） 由 LLM 使用简单的提示，如 <code>"Steps for XYZ.\n1."</code> ， <code>"What are the subgoals for achieving XYZ?"</code> ， （2） 通过使用特定于任务的指令;例如 <code>"Write a story outline."</code> 用于写小说，或 （3）人工输入<br>另一种非常不同的方法 LLM+P （ Liu et al. 2023） 涉及依靠外部经典规划师进行长期规划。这种方法利用规划域定义语言 （PDDL） 作为中间接口来描述规划问题。在这个过程中， LLM （1） 将问题翻译成 “问题 PDDL”，然后 （2） 请求经典规划师基于现有的 “领域 PDDL” 生成 PDDL 计划，最后 （3） 将 PDDL 计划翻译回自然语言。从本质上讲，规划步骤外包给外部工具，假设特定领域的 PDDL 和合适的规划器可用，这在某些机器人设置中很常见，但在许多其他领域中并不常见<h4 id=自我反思><a class=headerlink href=#自我反思 title=自我反思></a>自我反思</h4><p>自我反省允许自主智能体通过改进过去的行动决策和纠正以前的错误来迭代改进。它在不可避免地需要试错的实际任务中起着至关重要的作用。<br> ReAct通过将动作空间扩展为特定于任务的离散动作和语言空间的组合，将推理和行动整合到 LLM 中。前者使 LLM 能够与环境交互（例如使用维基百科搜索 API），而后者则提示 LLM 以自然语言生成推理轨迹。<br> Reflexion 是一个框架，用于为智能体提供动态记忆和自我反思能力，以提高推理技能。Reflexion具有标准的 RL 设置，其中奖励模型提供简单的二进制奖励，操作空间遵循ReAct中的设置，其中特定于任务的操作空间通过语言进行扩充，以支持复杂的推理步骤。在每个操作a~t~之后，智能体会计算启发式 h~t~,并且可以根据自我反思结果选择性地决定重置环境以开始新的试用。<br><img alt=img data-src=https://lilianweng.github.io/posts/2023-06-23-agent/reflexion.png><br> 启发式函数确定轨迹何时效率低下或包含幻觉，何时应停止。低效的规划是指花费太长时间而没有成功的轨迹。幻觉被定义为遇到一系列连续的相同动作，这些动作导致在环境中进行相同的观察。<br> 自我反思是通过向 LLM 展示两张照片的例子来创建的，每个例子都是一对（失败的轨迹，指导计划中未来变化的理想反思）。然后将反射添加到 agent 的工作内存中，最多三个，用作查询 LLM 的上下文。<br><strong>Chain of Hindsight</strong>鼓励模型通过明确呈现一系列过去的输出来改进自己的输出，每个输出都带有反馈注释.CoH 的理念是在上下文中呈现连续改进的产出的历史，并训练模型顺应趋势以产生更好的产出。算法蒸馏将相同的想法应用于强化学习任务中的跨集轨迹，其中算法被封装在一个长期受历史条件限制的策略中。考虑到代理与环境交互多次，并且每次代理都会变得更好，AD 会将此学习历史记录连接起来，并将其馈送到模型中。<h3 id=记忆模块><a class=headerlink href=#记忆模块 title=记忆模块></a>记忆模块</h3><p>三种类型的记忆<ol><li>感官记忆：这是记忆的最早阶段，提供在原始刺激结束后保留感官信息（视觉、听觉等）印象的能力。感官记忆通常最多只持续几秒钟。子类别包括图标记忆（视觉）、回声记忆（听觉）和触觉记忆（触觉<li>短期记忆 （STM） 或工作记忆：它存储我们目前知道的和执行复杂认知任务（如学习和推理）所需的信息。短期记忆被认为具有大约 7 项的容量 （Miller 1956） 并持续 20-30 秒。<li>长期记忆 （LTM）：长期记忆可以存储信息非常长的时间，从几天到几十年不等，具有基本上无限的存储容量。LTM 有两种亚型：<br> 显性 / 陈述性记忆：这是对事实和事件的记忆，指的是那些可以被有意识地回忆起来的记忆，包括情景记忆（事件和经历）和语义记忆（事实和概念）<br> 内隐/程序记忆：这种类型的记忆是无意识的，涉及自动执行的技能和例程，例如骑自行车或在键盘上打字。<br> 感官记忆作为原始输入（包括文本、图像或其他模态）的学习嵌入表示;<br> 短期记忆作为上下文学习。它简短而有限，因为它受 Transformer 的有限上下文窗口长度的限制。<br> 长期内存作为代理在查询时可以处理的外部向量存储，可通过快速检索访问。<h4 id=最大内积搜索><a class=headerlink href=#最大内积搜索 title=最大内积搜索></a>最大内积搜索</h4>​ 外部存储器可以缓解有限注意力持续时间的限制。标准做法是将信息的嵌入表示保存到可以支持快速最大内积搜索 （MIPS） 的向量存储数据库中。为了优化检索速度，常见的选择是近似最近邻 （ANN） 算法，以返回大约 k 个前 k 个最近邻，以牺牲一点准确性损失来换取巨大的加速<br>​ LSH（Locality-Sensitive Hashing）：它引入了一个哈希函数，<strong>以便将相似的输入项以高概率映射到相同的存储桶</strong>，其中存储桶的数量远小于输入的数量<br>​ ANNOY （Approximate Nearest Neighbors Oh Yeah）：核心数据结构是随机投影树，这是一组二叉树，其中每个非叶节点代表一个将输入空间分成两半的超平面，每个叶子存储一个数据点。树是独立且随机构建的，因此在某种程度上，它模仿了哈希函数。ANNOY 搜索发生在所有树中，以迭代搜索最接近查询的一半，然后聚合结果。这个想法与 KD 树非常相关，但更具可扩展性。<br>​ HNSW（分层可导航小世界）：它的灵感来自小世界网络的思想，其中大多数节点可以在少量步骤内被任何其他节点到达;例如社交网络的“六度分离”功能。HNSW 构建了这些小世界图的分层，其中底层包含实际数据点。中间的图层创建快捷方式以加快搜索速度。在执行搜索时，HNSW 从顶层的随机节点开始，并导航到目标。当它无法更靠近时，它会向下移动到下一层，直到到达底层。上层的每次移动都可能覆盖数据空间中的很长一段距离，而下层的每一次移动都会提高搜索质量。<br><strong>FAISS</strong>（meta相似性搜索）：它的运行基于以下假设：在高维空间中，节点之间的距离遵循高斯分布，因此应该存在数据点的聚类。<strong>FAISS 通过将向量空间划分为多个聚类，然后在聚类内优化量化来应用向量量化。Search 首先查找具有粗略量化的候选集群，然后进一步查找具有更精细量化的每个集群。</strong></ol><h3 id=工具使用><a class=headerlink href=#工具使用 title=工具使用></a>工具使用</h3><p>工具的使用是人类的一个显著特征。我们创造、修改和利用外部物体来做超出我们身体和认知极限的事情。为 LLMs 配备外部工具可以显着扩展模型功能。<br>HuggingGPT 是一个以 ChatGPT 作为任务规划器的框架，根据模型描述选择 HuggingFace 平台中可用的模型，并根据执行结果总结响应. 系统调用包括任务规划,模型选择,任务执行,生成响应.<br>API-Bank 是评估工具增强 LLMs 性能的基准。它包含 53 个常用的 API 工具、一个完整的工具增强 LLM 工作流程，以及 264 个带注释的对话，涉及 568 个 API 调用。API 的选择非常多样化，包括搜索引擎、计算器、日历查询、智能家居控制、日程管理、健康数据管理、帐户身份验证工作流程等。因为 API 数量众多，所以 LLM 首先要有 API 搜索引擎，找到合适的 API 进行调用，然后用相应的文档进行调用。<h2 id=协同感知中的LLM><a class=headerlink href=#协同感知中的LLM title=协同感知中的LLM></a>协同感知中的LLM</h2><p>最近有一系列的工作使用LLM与协同感知、自动驾驶结合(也有使用VLM的,方法类似).这里简单介绍一些相关工作.<h3 id=V2V-LLM-Vehicle-to-Vehicle-Cooperative-Autonomous-Driving-with-Multi-Modal-Large-Language-Models><a title="V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with  Multi-Modal Large Language Models" class=headerlink href=#V2V-LLM-Vehicle-to-Vehicle-Cooperative-Autonomous-Driving-with-Multi-Modal-Large-Language-Models></a>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</h3><p>当前的自动驾驶车辆主要依靠其单独的传感器来了解周围的场景并规划未来的轨迹，当传感器发生故障或被遮挡时可能是不可靠的。为了解决这个问题，通过车辆到车辆( Vehicle-to-Vehicle，V2V )通信的协作感知方法已经被提出，但<strong>它们往往侧重于检测和跟踪。这些方法如何有助于整体的协作规划性能仍未得到充分的研究</strong>。<p>受最近使用大语言模型( Large Language Models，LLMs )构建自动驾驶系统的进展的启发,提出了一种新的问题设置,将LLM集成到协作自动驾驶中，并提出了车对车问答( Vehicle-to- Vehicle Question-Answering，V2V-QA )数据集和基准测试集.<p>还提出了基线方法Vehicle-to-Vehicle Large Language Model ( V2V-LLM )，它使用一个LLM来融合来自多个连接的自动驾驶车辆( CAV )的感知信息，并回答与驾驶相关的问题：grounding,notable objected identification, and planning.<p>实验结果表明,提出的V2VLLM可以作为一个有前途的统一模型架构来执行协作自动驾驶中的各种任务，并且优于使用不同融合方法的其他基线方法。工作也开创了一个新的研究方向，可以提高未来自动驾驶系统的安全性。<p>无人驾驶车辆在日常运行中的感知和规划系统严格依赖于其本地的LiDAR传感器和相机来探测附近的显著目标并进行规划.<strong>当传感器被附近的大型物体遮挡时，这种方法可能会遇到安全问题</strong>。在这种情况下，自动驾驶车辆无法准确地检测到所有附近的显著目标，使得后续的轨迹规划结果不可靠。<p><img alt=image-20250301190916782 data-src=https://s2.loli.net/2025/03/01/b3Rwruo2Nqjl9Qc.png><p>为了解决这个安全问题，最近的研究提出了通过车车通信( V2V )通信的协作感知算法。在协同驾驶场景中，多个邻近行驶的智能网联汽车( Connected Autonomous Vehicles，CAVs )通过V2V通信共享彼此的感知信息。然后将接收到的来自多个CAV的感知数据进行融合，以产生更好的整体检测结果。<p>所有CAV与LLM共享各自的感知信息。任何CAV都可以以自然语言的形式向LLM提问，以获取对驾驶安全有用的信息。<p>为了研究这个问题，首先创建了车辆到车辆问答( V2V-QA )数据集，该数据集基于V2V4Real的自动驾驶协作感知数据集。<p>车-车大语言模型( V2V-LLM )用于协同自动驾驶.每个CAV提取自己的感知特征，并与V2V-LLM共享。V2V-LLM融合场景级特征图和对象级特征向量，然后进行视觉和语言理解，为V2V-QA中输入的驾驶相关问题提供答案。<p>还将V2V - LLM与其他基线方法对应的不同特征融合方法：不融合、早期融合和中间融合进行了比较。结果表明，V2V - LLM在较重要的目标识别和规划任务中取得了最好的性能，在grounding任务中取得了次优的性能，<p><img alt=image-20250301204106243 data-src=https://s2.loli.net/2025/03/01/JaKvA2B4yOdxcSP.png><p>对于V2V4Real数据集的每一帧，创建了5种不同类型的问答对，包括3种类型的背景问题，1种类型的显著对象识别问题和1种类型的规划问题。这些QAs是针对协同驾驶场景设计的。为了生成这些问答对的实例，使用V2V4Real 的ground-truth边界框注释、每个CAV的ground - truth轨迹和个体检测结果作为源信息。然后根据前述实体和文本模板之间的几何关系使用不同的手动设计的规则来生成的问答对<p>作者设计了几种不同类型的问答对作为数据集,这个数据集也是多模态数据集.<p><strong>Q1</strong> Grounding at a reference location<p>在这种类型的问题中，要求LLM来识别是否存在一个对象占据了特定的查询2D位置。如果是，则期望LLM提供物体的中心位置。否则，LLM应该表示在参考位置处没有任何信息。为了生成这类问答对的实例，我们使用背景-真值框的中心位置和每个CAV的单个检测结果框作为问题中的查询位置。这样，我们可以更专注于评估各个模型对潜在的假阳性和假阴性检测结果的协同接地能力。<p><strong>Q2</strong> Grounding behind a reference object at a location<p>当一个CAV的视场被一个邻近的大物体遮挡时，该CAV可能希望根据所有CAV的融合感知信息，请求中心化的LLM来判断遮挡大物体后面是否存在物体。如果是这样的话，LLM预计将返回对象的位置，询问CAV可能需要更多的防御性驾驶或调整其规划。否则，LLM应该表明参照对象背后没有任何东西。为了生成这类问答对的实例，使用每个检测结果框的中心位置作为这些问题中的查询位置。根据询问的CAV与参考物体的相对位姿画出一个扇形区域，并在该区域中选择距离最近的真实物体作为答案。<p><strong>Q3</strong> Grounding behind a reference object in a direction<p>进一步在语言和空间理解能力上对LLM提出了挑战，将Q2的参考2D位置替换为参考方向关键字。为了生成这类QA对的实例，首先在一个CAV的6个方向中各得到一个最接近的检测结果框作为参考对象。然后在Q2中遵循相同的数据生成方法，在相应的扇形区域中得到最接近的ground-truth box作为答案。<p><strong>Q4</strong> Notable object identification<p>前述的真值任务可以看作自动驾驶管道中的中间任务。自动驾驶车辆更关键的能力包括识别规划的未来轨迹附近的显著目标和调整未来规划以避免潜在的碰撞。在显著性物体识别问题中，从地面-真值轨迹中提取了6个未来3秒的路标点作为问题中的参考未来路标点。然后，在参考未来轨迹的10米范围内最多得到3个最近的地面真实物体作为答案。<p><strong>Q5</strong> 规划<p>与上述QA类型相比，规划是自动驾驶系统最重要的输出，因为自动驾驶汽车的最终目标是安全地通过复杂的环境，避免未来的任何潜在碰撞。为了生成规划QAs，我们从每个CAV的真实未来轨迹中提取6个均匀分布在未来3秒内的未来航路点作为答案。由于一些原因，V2V - QA的规划任务也比其他基于NuScenes的LLM驱动的相关工作更具有挑战性。首先，我们在协同驾驶场景中支持多辆CAV。LLM模型需要提供不同的答案，这取决于哪个CAV要求其建议的未来轨迹，而先前的工作只需要生成单个自动驾驶车辆的规划结果。其次，V2V-QA是基于V2V4Real的，它包括城市和高速公路两种驾驶场景。在这两种不同的环境中，车辆的运动模式有很大的不同。相反，基于Nu Scenes的LLM驾驶研究只需要考虑城市驾驶场景。<p><img alt=image-20250301212424018 data-src=https://s2.loli.net/2025/03/01/6Rit5JYmheqW82P.png><h3 id=AGENTSCODRIVER-Large-Language-Model-Empowered-Collaborative-Driving-with-Lifelong-Learning><a title="AGENTSCODRIVER: Large Language Model  Empowered Collaborative Driving with Lifelong  Learning" class=headerlink href=#AGENTSCODRIVER-Large-Language-Model-Empowered-Collaborative-Driving-with-Lifelong-Learning></a>AGENTSCODRIVER: Large Language Model Empowered Collaborative Driving with Lifelong Learning</h3><p>智能网联汽车和无人驾驶近来发展迅速。然而，目前的自动驾驶系统主要基于数据驱动的方法，<strong>在可解释性、泛化性和持续学习能力方面表现出明显的不足。此外，单车自动驾驶系统缺乏与其他车辆协作和协商的能力，这对驾驶安全和效率至关重要</strong>。为了有效地解决这些问题，利用大型语言模型( LLMs )开发了一个新的框架，称为AGENTSCODRIVER，以使多车辆能够进行协同驾驶。<p>AGENTSCODRIVER由5个模块组成：<strong>观察模块、推理引擎、认知记忆模块、强化反射模块和通信模块</strong>。它可以通过与驾驶环境的不断交互，随时间推移积累知识、教训和经验，从而使实现终身学习成为可能。此外，通过利用通信模块，不同智能体可以交换信息，实现复杂驾驶环境下的协商与协作。进行了大量的实验，并显示了AGENTSCODRIVER相对于现有方法的优越性。<p><img alt=image-20250301212736505 data-src=https://s2.loli.net/2025/03/01/U5HNZpuxPMXJIzd.png><p>AGENTSCODRIVER的体系结构由5个模块组成：观测模块、推理引擎、存储模块、增强反射模块和通信模块。推理引擎、通信模块和增强反射模块利用LLMs生成消息和最终决策<p><strong>Observation Module</strong><p>为了使智能体能够进行协作，CAV感知其周围环境并提取必要的信息用于下游的高阶任务推理是很重要的，因此为智能体开发了一个观测模块来编码其周围的场景，并提取其有用的高层信息，如车道数和周围车辆的位置和速度。然后将这些观察结果输入到智能体的推理引擎中进行分析并做出决策。它们也被用来从记忆模块中回忆相关的记忆<p><strong>Reasoning Engine</strong><p>推理是人类最基本、最重要的能力之一，对于人类做出日常的、复杂的决策具有重要的意义。传统的数据驱动方法直接利用感知信息(例如,目标检测结果和语义分割结果)进行最终的驾驶决策(例如,左转或右转,加速和减速)，缺乏可解释性，无法处理复杂场景和长尾情况。受人类推理能力的启发，我们提出了一个CAV智能体的推理引擎，它由三个步骤组成：1 )提示生成，2 )推理过程，3 )运动规划。<p><strong>Memory Module</strong><p>记忆对于一个人来说是非常重要的。当一个人驾驶汽车时，他或她会使用常识，例如遵守交通规则和回忆过去的经验来做出决定。为了将这种能力灌输给智能体，我们提出了智能体的记忆模块，该模块由三部分组成：常识记忆、经验记忆和结构化文本存储的反思记忆。常识性记忆包含了驾驶的常识性知识，如交通规则等。经验记忆包含过去的驾驶场景和相应的决策。反射记忆包含反射模块的反馈。智能体可以从存储模块中检索相关的内存，使其可用于决策。<p><strong>Reinforcement Reflection</strong><p>一个人要想成为某一领域的专家，就必须从过去的经历中学习，这意味着他必须有能力反思自己过去的错误，并分析其背后的原因。对于智能体来说，拥有这样的反思能力对于保持正确的操作和安全驾驶也是至关重要的。基于这些观察，提出了强化反射，它有两个模块：一个评估器和一个反射器。评价者(记为E )像评判者一样对agent的输出进行打分，反射者(记为R )可以对agent的行为进行反思，并生成分析结果来改进agent的行为。<p><img alt=image-20250301214528445 data-src=https://s2.loli.net/2025/03/01/K8GL6CWQgoEzJXl.png><p><strong>Communication Module</strong><p>协作Agent之间的有效通信是至关重要的。通过相互之间的通信，将扩大智能体的观测范围。分别考虑两个智能体的观测值o1和o2，如果两个智能体相互交换观测信息，则两个智能体的观测值将扩展到o1∪o2。此外，沟通对于代理人之间相互协商并做出更好的决策也是至关重要的。例如，考虑一个智能体正在驾驶一辆汽车，并且想要超越前车。如果智能体与前车进行通信，则前车的智能体学习到后方车辆的意图，进而可以做出更好的决策，避免潜在的碰撞。<h3 id=Towards-Interactive-and-Learnable-Cooperative-Driving-Automation-a-Large-Language-Model-Driven-Decision-Making-Framework><a title="Towards Interactive and Learnable Cooperative Driving Automation: a Large Language Model-Driven Decision-Making Framework" class=headerlink href=#Towards-Interactive-and-Learnable-Cooperative-Driving-Automation-a-Large-Language-Model-Driven-Decision-Making-Framework></a>Towards Interactive and Learnable Cooperative Driving Automation: a Large Language Model-Driven Decision-Making Framework</h3><p>目前，智能网联汽车( Connected Autonomous Vehicles，CAVs )已经开始在世界各地进行开放道路测试，但其在复杂场景下的安全和效率表现仍不尽如人意。协同驾驶利用CAV的连通能力实现大于其部分之和的协同作用，使其成为提高复杂场景下CAV性能的一种有前途的方法。然而，缺乏交互和持续学习能力限制了当前的协同驾驶到单场景应用和特定的协同驾驶自动化( Cooperative Driving Automation，CDA )。<p>为了应对这些挑战，本文提出了一种可交互和学习的LLM驱动的协同驾驶框架CoDrivingLLM，以实现全场景和全CDA。<p><img alt=image-20250301215707384 data-src=https://s2.loli.net/2025/03/01/1IqhDjUavQlOLou.png><p>首先，由于大语言模型( Large Language Models，LLMs )不擅长处理数学计算，<strong>引入环境模块，基于语义决策更新车辆位置，从而避免直接LLM控制车辆位置可能带来的误差</strong>。其次，基于SAE J3216标准定义的CDA的四个层次，提出了基于思维链( Chain-of- Thought，COT )的推理模块，包括<strong>状态感知、意图共享、协商和决策</strong>，增强了LLMs在多步推理任务中的稳定性。<p>然后，在推理过程中，通过冲突协调者来管理集中的冲突解决。最后，通过引入记忆模块和使用提取增强生成，赋予CAVs从过去经验中学习的能力。<p><img alt=image-20250301215538196 data-src=https://s2.loli.net/2025/03/01/GZoEUBPHJ4lnSMR.png><p><img alt=image-20250301221105318 data-src=https://s2.loli.net/2025/03/01/7N1apvPSXFcDfOo.png><h3 id=Is-Intermediate-Fusion-All-You-Need-for-UAV-based-Collaborative-Perception><a title="Is Intermediate Fusion All You Need for UAV-based Collaborative  Perception?" class=headerlink href=#Is-Intermediate-Fusion-All-You-Need-for-UAV-based-Collaborative-Perception></a>Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?</h3><h4 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h4><p>协同感知通过Agent间通信增强环境感知，被认为是智能交通系统的一个有前途的解决方案。然而，现有的无人机协同方法<strong>忽略了无人机视角的独特特性，导致了大量的通信开销</strong>。为了解决这个问题，我们提出了一种新的<strong>基于后中间层融合的通信高效的协作感知框架</strong>，称为LIF。<p>其核心思想是<strong>交换信息量大且紧凑的检测结果，并将融合阶段转移到特征表示层面</strong>。特别地，<strong>利用视觉引导的位置嵌入( VPE )</strong>和<strong>基于框的虚拟增强特征( Bo BEV )来有效地整合来自不同代理的互补信息</strong>。此外，我们创新性地引入了一种<strong>不确定性驱动的通信机制，使用不确定性评估来选择高质量和可靠的共享区域</strong>。<p><img alt=image-20250519201823979 data-src=https://s2.loli.net/2025/05/19/LUCgmobRie6r4YA.png><h3 id=TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE-COOPERATIVEPERCEPTION><a title="TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE COOPERATIVEPERCEPTION" class=headerlink href=#TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE-COOPERATIVEPERCEPTION></a>TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE COOPERATIVEPERCEPTION</h3><p>车辆与基础设施的通信( V2I )协作感知在自动驾驶场景中起着至关重要的作用。尽管其具有提高感知精度和鲁棒性的潜力，但大量的原始传感器数据不可避免地导致了较高的通信开销。为了缓解这个问题，我们提出了一种面向任务的V2I协作感知通信框架TOCOM - V2I，它<strong>通过只传输与任务相关的信息而不是原始数据流来感知周围环境，从而减少带宽消耗</strong>。<p>我们的贡献有三。首先，我们提出了基于空间关系和感知先验的空间感知特征选择模块来过滤掉不相关的信息。其次，我们引入了分层熵模型来利用特征内的冗余，以实现高效的压缩和传输。最后，我们利用一个缩放的点积注意力架构来融合车辆端和基础设施端特征，以提高感知性能<p>空间感知特征选择模块旨在从基础设施侧特征FI中识别出对车辆来说既具有感知意义又是必要的特征F ~ I。在目标检测的背景下，FI中包含目标的区域比背景区域更具有任务相关性。在协作过程中，这些物体可以通过恢复车辆视野中被遮挡或丢失的物体来帮助提高感知质量。此外，距离车辆越近的物体越容易被车辆自身检测到。虽然从基础设施的角度来看，这些对象在感知上很重要，但没有必要传输这些特征。<p><img alt=image-20250519202216047 data-src=https://s2.loli.net/2025/05/19/CjKxWzUFnSImG6Q.png><h2 id=时延问题><a class=headerlink href=#时延问题 title=时延问题></a>时延问题</h2><p>引入时间信息的编码<p>V2VNet<p><img alt=image-20250526111749407 data-src=https://s2.loli.net/2025/05/26/WYh3nLOkHo4DcbA.png><p><img alt=image-20250526114903582 data-src=https://s2.loli.net/2025/05/26/zsyQwZC9VNhJruf.png><p>V2X-ViT<p><img alt=image-20250528213454183 data-src=https://s2.loli.net/2025/05/28/YD1qVd5jlihTAuF.png><p>增加时间编码<h3 id=Latency-Aware-Collaborative-Perception><a title="Latency-Aware Collaborative Perception" class=headerlink href=#Latency-Aware-Collaborative-Perception></a>Latency-Aware Collaborative Perception</h3><p><img alt=image-20250526225952639 data-src=https://s2.loli.net/2025/05/26/zBJ1wDiRO6rTdqA.png><p><img alt=image-20250527102534252 data-src=https://s2.loli.net/2025/05/27/Q2tnd7KxNgJH4Cl.png><p><img alt=image-20250527180538143 data-src=https://s2.loli.net/2025/05/27/KPhjznkL9WQs2cX.png><p><img alt=image-20250527200936722 data-src=https://s2.loli.net/2025/05/27/d8WJcQ3hzrxtF6g.png><h3 id=Co-MTP-A-Cooperative-Trajectory-Prediction-Framework-with-Multi-Temporal-Fusion-for-Autonomous-Driving><a title="Co-MTP: A Cooperative Trajectory Prediction Framework  with Multi-Temporal Fusion for Autonomous Driving" class=headerlink href=#Co-MTP-A-Cooperative-Trajectory-Prediction-Framework-with-Multi-Temporal-Fusion-for-Autonomous-Driving></a>Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving</h3><p>​ 车联网技术( V2X )已成为扩展感知范围、看穿遮挡物的理想范式。现有的研究主要集中在单帧协作感知，然而，<strong>如何利用V2X捕获帧与帧之间的时间线索，以促进预测任务甚至规划任务的进行，仍然缺乏研究</strong>。在本文中，介绍了Co - MTP，一种面向自动驾驶的多时态融合的通用协同轨迹预测框架，它利用V2X系统充分捕获历史和未来域中智能体之间的相互作用，从而有利于规划。<p>​ 在历史域中，V2X可以对单车感知中不完整的历史轨迹进行补充，我们设计了一个异构的图变换来学习来自多个智能体的历史特征的融合，并捕获历史交互。此外，预测的目的是支持未来的规划。<p>​<h3 id=Asynchrony-Robust-Collaborative-Perception-via-Bird’s-Eye-View-Flow><a title="Asynchrony-Robust Collaborative Perception via Bird’s Eye View Flow" class=headerlink href=#Asynchrony-Robust-Collaborative-Perception-via-Bird’s-Eye-View-Flow></a>Asynchrony-Robust Collaborative Perception via Bird’s Eye View Flow</h3><p>作为一个新兴的领域，协作感知的研究有许多挑战需要解决，例如高质量的数据集，模型不可知和任务不可知的表述以及对错误的鲁棒性和对抗性攻击。然而绝大多数现有工作并没有认真考虑智能体之间实际通信的严酷现实，如拥塞、计算量大、中断和缺乏校准等。这些因素引入了延迟或失调，严重影响了Agent之间信息交换的可靠性和质量。一些先前的工作已经涉及到了通信延迟的问题。例如，<strong>V2VNet和V2XViT 将延迟时间作为特征补偿的输入</strong>。然而，它们只考虑了单一帧，没有利用历史帧，对于高速场景( 20m / s以上)或高延迟场景( 0.3 s以上)是不充分的。同时，<strong>SyncNet使用历史特征来预测完整的特征</strong><p>V2VNet利用卷积神经网络，通过将时间信息和相对位姿作为输入，学习如何补偿通信延迟；<strong>V2X-ViT设计了一个时延感知的位置编码模块来学习时延带来的影响，但是这些方法没有考虑历史时序信息进行补偿</strong>。SyncNet利用历史多帧信息，通过Conv - LSTM对当前时刻进行补偿，但其对整个特征图的补偿导致特征通道存在噪声，基于RNN的框架无法处理时间上的不规则输入。<p><img alt=image-20250526144303233 data-src=https://s2.loli.net/2025/05/26/KyiUGwp4s8xnMYR.png><h3 id=Flow-Based-Feature-Fusion-for-Vehicle-Infrastructure-Cooperative-3D-Object-Detection><a title="Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection" class=headerlink href=#Flow-Based-Feature-Fusion-for-Vehicle-Infrastructure-Cooperative-3D-Object-Detection></a>Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</h3><p>特征流生成。采用特征流作为预测函数来描述未来基础设施特征随时间的变化。给定当前点云帧Pi ( ti )和基础设施特征提取器Fi ( · )，定义ti之后未来时刻t的特征流为</p><script type="math/tex; mode=display">
\widetilde{F}_{i}(t)\ =F_{i}(P_{i}(t))_{,}\,t\ge\,t_{i}</script><h3 id=Vehicle-Infrastructure-Cooperative-3D-Object-Detection-via-Feature-Flow-Prediction><a title="Vehicle-Infrastructure Cooperative 3D Object Detection via Feature Flow Prediction" class=headerlink href=#Vehicle-Infrastructure-Cooperative-3D-Object-Detection-via-Feature-Flow-Prediction></a>Vehicle-Infrastructure Cooperative 3D Object Detection via Feature Flow Prediction</h3><p><img alt=image-20250526221403766 data-src=https://s2.loli.net/2025/05/26/zyYZQMXmxW2EvST.png><h3 id=V2XPnP-Vehicle-to-Everything-Spatio-Temporal-Fusion-for-Multi-Agent-Perception-and-Prediction><a title="V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction" class=headerlink href=#V2XPnP-Vehicle-to-Everything-Spatio-Temporal-Fusion-for-Multi-Agent-Perception-and-Prediction></a>V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction</h3><p>车联网( V2X )技术提供了一个有前途的范例，以减轻单车辆系统中受限可观测性的限制。先前的工作主要集中在单帧合作感知，它<strong>融合了不同空间位置的智能体信息，但忽略了时间线索和时间任务</strong>(例如,时间知觉和预测)。<p>本文针对V2X场景下的时空融合问题，设计了一步式和多步式通信策略(何时传输)，并考察了其与早、晚、中3种融合策略(传输什么?)的融合情况，提供了11种融合模型(如何融合)的综合基准。进一步地，我们提出了V2XPnP，一种新颖的用于端到端感知和预测的一步通信中间层融合框架。我们的框架采用了基于Transformer的统一架构，有效地建模了跨多个代理、帧和高清地图的复杂时空关系。<p>自动驾驶系统需要准确感知周围道路用户并预测其未来轨迹，以确保安全和交互式驾驶。尽管最近在感知和预测方面取得了进展，但单车系统仍然面临有限的感知范围和遮挡问题，影响了驾驶性能和道路安全。因此，车联网( V2X )技术已经成为一种很有前途的范式,它使连接和自动化的车辆( CAVs )和基础设施能够共享互补信息并缓解遮挡，从而支持整体环境理解<p>尽管有它们的潜力，现有的工作集中在逐帧协作检测，它聚合了来自不同空间位置的代理的信息。然而，这些工作<strong>忽略了连续帧之间的时间线索，这对于定位以前可见但目前未被发现的对象</strong>和预测对象未来的轨迹很重要<h3 id=CoDynTrust-Robust-Asynchronous-Collaborative-Perception-via-Dynamic-Feature-Trust-Modulus><a title="CoDynTrust: Robust Asynchronous Collaborative Perception via  Dynamic Feature Trust Modulus" class=headerlink href=#CoDynTrust-Robust-Asynchronous-Collaborative-Perception-via-Dynamic-Feature-Trust-Modulus></a>CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus</h3><p>协同感知通过融合多个智能体的信息，可以扩展感知范围，从而提高感知性能。<strong>然而，在现实环境中，由于通信延迟、时钟失调或采样配置差异引起的时间异步会导致信息不匹配。</strong><p><strong>如果处理不当，那么协同性能就会参差不齐，更严重的可能会发生安全事故。为了应对这一挑战，我们提出了CoDynTrust，一种不确定性编码的异步融合感知框架，对时间不同步导致的信息不匹配具有鲁棒性。</strong><p>CoDynTrust通过对即兴和认知不确定性建模，选择性地抑制或保留单车辆特征，为每个感兴趣区域生成动态特征信任模( DFTM )，从而缓解信息不匹配问题。然后，我们设计了一个多尺度融合模块来处理DFTM处理后的多尺度特征图。<p>然后，我们设计了一个多尺度融合模块来处理DFTM处理后的多尺度特征图。与现有的考虑异步协同感知的工作相比，CoDynTrust在时间异步场景中对抗各种低质量信息，并允许不确定性传播到下游任务，如规划和控制。<h3 id=Leveraging-Temporal-Contexts-to-Enhance-Vehicle-Infrastructure-Cooperative-Perception><a title="Leveraging Temporal Contexts to Enhance Vehicle-Infrastructure  Cooperative Perception" class=headerlink href=#Leveraging-Temporal-Contexts-to-Enhance-Vehicle-Infrastructure-Cooperative-Perception></a>Leveraging Temporal Contexts to Enhance Vehicle-Infrastructure Cooperative Perception</h3><p>安装在高架位置的基础设施传感器提供了更广泛的感知范围，并遇到更少的遮挡。通过V2X通信整合基础设施和自车数据，称为车-基础设施合作，在增强感知能力和解决单车自动驾驶中遇到的转角情况方面显示出相当大的优势。<p>​ 然而,协作感知仍然面临着许多挑战，包括有限的通信带宽和实际通信中断。在本文中，提出了一种新的协同3D目标检测框架CTCE。该框架<strong>以时间上下文增强的方式传输查询，有效地平衡了传输效率和性能</strong>，以适应现实世界的通信条件。此外，我们还提出了一个时间引导的融合模块来进一步提高性能。<p>路侧时间增强和车侧时空融合共同构成了多层次的时间上下文融合机制，充分利用时间信息提升性能。<p><img alt=image-20250519202400933 data-src=https://s2.loli.net/2025/05/19/eptucRAXdNPwk7D.png><h3 id=Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network><a title="Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network" class=headerlink href=#Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network></a>Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network</h3><p><img alt=image-20250528105139615 data-src=https://s2.loli.net/2025/05/28/JtHQVc1n2v86UXC.png><p><img alt=image-20250528111736833 data-src=https://s2.loli.net/2025/05/28/3UwDiBq8ZRs94Pp.png><p><img alt=image-20250528111849168 data-src=https://s2.loli.net/2025/05/28/UHwDobJxyz32LhW.png><p><img alt=image-20250528111903710 data-src=https://s2.loli.net/2025/05/28/q6HJklWt9fOSIZc.png><p>将feature flow和lstm更新特征的想法结合<p>motivation: 多智能体的特征融合是提升协作感知性能的一个关键,过去的一些工作忽略的传输延迟导致时间上的特征不对齐，我们利用历史帧数据,考虑motion状态不断更新. 一个空间上不同位置的注意力和改造后的lstm,结合时序预测. flow generator,in-frame decoding<p><strong>STAM</strong> (Spatio-Temporal Aggregation Module)<p>transformer+lstm+feature flow+时序预测+帧内位置编码<p>每一帧保存一个q,k,v(融合后的)<p>相关工作 v2vnet v2x-vit lantancy-aware bevflow<h2 id=利用query与晚期检测结果、置信度降低带宽><a class=headerlink href=#利用query与晚期检测结果、置信度降低带宽 title=利用query与晚期检测结果、置信度降低带宽></a>利用query与晚期检测结果、置信度降低带宽</h2><h3 id=CoSDH-Communication-Efficient-Collaborative-Perception-via-Supply-Demand-Awareness-and-Intermediate-Late-Hybridization><a title="CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization" class=headerlink href=#CoSDH-Communication-Efficient-Collaborative-Perception-via-Supply-Demand-Awareness-and-Intermediate-Late-Hybridization></a>CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization</h3><p>​ 多智能体协同感知通过利用多个智能体的信息来增强感知能力，被认为是解决自动驾驶中单车感知能力弱问题的根本解决方案。然而，现有的协同感知方法面临着通信效率和感知精度之间的两难问题。<p>​ 为了解决这个问题，我们提出了一种新颖的基于供需感知和中晚期混合的高效通信协作感知框架，称为CoSDH。该框架通过对Agent之间的供需关系进行建模，细化了协作区域的选择，在保持准确性的同时减少了不必要的通信成本。<p>​ 此外创新性地引入了中-晚期混合协作模式，其中晚期协作弥补了低通信带宽下协作感知中的性能下降。<p><img alt=image-20250528141211346 data-src=https://s2.loli.net/2025/05/28/heA1tbXIiVDYr8o.png><p>首先在PointPillar编码点云时,对于agent i,考虑使用每个支柱中的点云数量来表示点云密度，并将其映射到范围[ 0、1 ]. 然后选择点云密度低于阈值ε a的区域来获得agent i的需求掩码，Di = Ai < εa∈{ 0，1 }^H×W^<p>同时使用detection header得到空间置信度图,利用一个供给阈值ε c得到供给掩码S ( l ) i = Ci > εc∈{ 0，1 } ^H×W^. 至此得到supply和demand两个空间mask. 通过调整阈值，我们可以动态地调整用于协作感知的带宽，以适应不断变化的通信条件。<p>在协作过程中，agent~j~基于其供应掩码S~j~和agent~i~的需求掩码D~i~生成一个二元供需选择掩码M~j→i~ = D~i~⊙S~j~∈{ 0,1 }^H×W^，并对其进行采样，将其与多尺度BEV特征{ F(l) ~j~ } ~l=1,2,..,L~进行逐元相乘,得到稀疏特征{ Z(l)~j→i~ }~l=1,2,..,L~<h3 id=CoopDETR-A-Unified-Cooperative-Perception-Framework-for-3D-Detection-via-Object-Query><a title="CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query" class=headerlink href=#CoopDETR-A-Unified-Cooperative-Perception-Framework-for-3D-Detection-via-Object-Query></a>CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query</h3><p><img alt=image-20250528151426743 data-src=https://s2.loli.net/2025/05/28/C3TP5RLomvNcBp2.png><h3 id=How2Com><a class=headerlink href=#How2Com title=How2Com></a>How2Com</h3><p>​ 多智能体协同感知作为一种新兴的驾驶场景应用，最近受到了广泛关注。尽管在先前的努力中取得了进展，但由于感知过程中的各种困境，包括<strong>通信冗余</strong>、<strong>传输延迟</strong>和协作异构性，挑战仍然存在。<p>​ 为了解决这些问题，提出了How2comm，一种在感知性能和通信带宽之间寻求平衡的协作感知框架。我们的新颖性体现在三个方面。<p>首先，设计了一种互信息感知的通信机制，以最大限度地保持合作者共享的信息特征。采用空间通道滤波进行有效的特征稀疏化，以实现高效通信。<p>其次，我们提出了一种流导向的延迟补偿策略来预测合作者的未来特征，并消除由于时间不同步导致的特征错位。<p>​ 最后，引入一个语用协作转换器来整合智能体之间的整体空间语义和时间上下文线索。我们的框架在真实世界和模拟场景中的多个基于LiDAR的协同探测数据集上进行了全面的评估<p>​ 减少通信开销的主要模式被总结为<strong>特征压缩和空间滤波</strong>。前者假设智能体无差别地共享所有的空间区域，这极大地浪费了带宽。后者过度依赖置信图来突出易受骗的位置，没有考虑空间上的整体信息。此外，这些方法总是会造成传输的有价值信息的损失。<p>​ 不可避免的传输延迟会导致快速移动物体在绿色圆内的位置错位，从而对后续的协作性能造成潜在的危害。虽然一些延迟感知策略被提出来解决这个问题，但它们要么<strong>受到性能瓶颈的影响</strong>，要么<strong>引入大量的计算开销</strong>，从而导致次优的解决方案。<p>​ 各Agent之间激光雷达配置差异(例如,不同的LiDAR密度、分布、反射率和噪声干扰)可能导致特征空间内的协作异构. 从这两个感知区域融合有价值的空间语义，有助于全面和务实的感知。然而,先前的大多数方法通过个体/位置信息融合来整合合作者共享的特征以增强自我表征，<strong>其协作过程可能是脆弱的，因为没有从整体上考虑来自异构智能体的不同感知区域的优势。而且，当前的单帧感知范式面临着三维点云稀疏性和定位误差的挑战</strong>，增加了构建鲁棒的多智能体感知系统的难度。<p><img alt=image-20250528161421934 data-src=https://s2.loli.net/2025/05/28/L4zjETsIZkaouq3.png><h4 id=Inspiration><a class=headerlink href=#Inspiration title=Inspiration></a>Inspiration</h4><ol><li>对于减少通信量,</ol><p>exclusive map,common map. decouple?<p>supply demand object query, confidence map,entropy?<ol><li>对于融合</ol><p>考虑时延<p>解决时延问题,引入feature flow思想,利用多层LSTM.<p>feature flow, flow generator,设计损失减少差异<p>使用范围编码考虑空间.<p>为了更好地关注/聚合历史数据,我们在lstm结构基础上进行了改进,引入注意力机制,关注.多层的query exclusive common query<h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://lilianweng.github.io/posts/2023-06-23-agent/ rel=noopener target=_blank>LLM Powered Autonomous Agents | Lil’Log</a><li><a href=https://lilianweng.github.io/posts/2022-06-09-vlm/ rel=noopener target=_blank>Generalized Visual Language Models | Lil’Log</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\30\协作感知算法-三\ rel=bookmark>协作感知算法:三</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\23\协同感知数据集介绍\ rel=bookmark>协同感知数据集和代码库介绍</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\17\协同感知算法-二\ rel=bookmark>协同感知学习(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\11\30\协同感知算法-一\ rel=bookmark>协同感知学习(一)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2025/01/10/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E5%9B%9B-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E3%80%81%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BB%A5%E5%8F%8A%E6%96%B0%E8%B6%8B%E5%8A%BF/ title=协同感知算法(四):大模型、多模态以及新趋势>https://www.sekyoro.top/2025/01/10/协同感知算法-四-大模型、多模态以及新趋势/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag><i class="fa fa-tag"></i> collaborative perception</a></div><div class=post-nav><div class=post-nav-item><a href=/2025/01/10/%E5%AD%A6%E4%B9%A0%E6%A1%8C%E9%9D%A2%E5%BC%80%E5%8F%91%E5%85%88%E8%BF%9B%E7%BB%8F%E9%AA%8C%EF%BC%8C%E4%BB%8EWPF%E7%9C%8B%E8%B5%B7/ rel=prev title=学习桌面开发先进经验,从WPF看起> <i class="fa fa-chevron-left"></i> 学习桌面开发先进经验,从WPF看起 </a></div><div class=post-nav-item><a href=/2025/01/18/%E7%BB%99neovim%E5%86%99%E6%8F%92%E4%BB%B6/ rel=next title=使用Lua给neovim写插件> 使用Lua给neovim写插件 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B><span class=nav-number>1.</span> <span class=nav-text>视觉语言模型</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%90%8C%E6%97%B6%E8%AE%AD%E7%BB%83%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC><span class=nav-number>1.1.</span> <span class=nav-text>同时训练图像和文本</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%AD%A6%E4%B9%A0%E5%9B%BE%E5%83%8F%E5%B5%8C%E5%85%A5><span class=nav-number>1.2.</span> <span class=nav-text>学习图像嵌入</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B7%B7%E5%90%88%E8%A7%86%E8%A7%89%E5%92%8C%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF><span class=nav-number>1.3.</span> <span class=nav-text>交叉注意力混合视觉和文本信息</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%97%A0%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95><span class=nav-number>1.4.</span> <span class=nav-text>无训练方法</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A9%B1%E5%8A%A8%E7%9A%84Agents><span class=nav-number>2.</span> <span class=nav-text>大模型驱动的Agents</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%A7%84%E5%88%92><span class=nav-number>2.1.</span> <span class=nav-text>规划</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3><span class=nav-number>2.1.1.</span> <span class=nav-text>任务分解</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%87%AA%E6%88%91%E5%8F%8D%E6%80%9D><span class=nav-number>2.1.2.</span> <span class=nav-text>自我反思</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AE%B0%E5%BF%86%E6%A8%A1%E5%9D%97><span class=nav-number>2.2.</span> <span class=nav-text>记忆模块</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%9C%80%E5%A4%A7%E5%86%85%E7%A7%AF%E6%90%9C%E7%B4%A2><span class=nav-number>2.2.1.</span> <span class=nav-text>最大内积搜索</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8><span class=nav-number>2.3.</span> <span class=nav-text>工具使用</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E4%B8%AD%E7%9A%84LLM><span class=nav-number>3.</span> <span class=nav-text>协同感知中的LLM</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#V2V-LLM-Vehicle-to-Vehicle-Cooperative-Autonomous-Driving-with-Multi-Modal-Large-Language-Models><span class=nav-number>3.1.</span> <span class=nav-text>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#AGENTSCODRIVER-Large-Language-Model-Empowered-Collaborative-Driving-with-Lifelong-Learning><span class=nav-number>3.2.</span> <span class=nav-text>AGENTSCODRIVER: Large Language Model Empowered Collaborative Driving with Lifelong Learning</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Towards-Interactive-and-Learnable-Cooperative-Driving-Automation-a-Large-Language-Model-Driven-Decision-Making-Framework><span class=nav-number>3.3.</span> <span class=nav-text>Towards Interactive and Learnable Cooperative Driving Automation: a Large Language Model-Driven Decision-Making Framework</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Is-Intermediate-Fusion-All-You-Need-for-UAV-based-Collaborative-Perception><span class=nav-number>3.4.</span> <span class=nav-text>Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>3.4.1.</span> <span class=nav-text>摘要</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE-COOPERATIVEPERCEPTION><span class=nav-number>3.5.</span> <span class=nav-text>TASK-ORIENTEDCOMMUNICATIONFORVEHICLE-TO-INFRASTRUCTURE COOPERATIVEPERCEPTION</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E6%97%B6%E5%BB%B6%E9%97%AE%E9%A2%98><span class=nav-number>4.</span> <span class=nav-text>时延问题</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Latency-Aware-Collaborative-Perception><span class=nav-number>4.1.</span> <span class=nav-text>Latency-Aware Collaborative Perception</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Co-MTP-A-Cooperative-Trajectory-Prediction-Framework-with-Multi-Temporal-Fusion-for-Autonomous-Driving><span class=nav-number>4.2.</span> <span class=nav-text>Co-MTP: A Cooperative Trajectory Prediction Framework with Multi-Temporal Fusion for Autonomous Driving</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Asynchrony-Robust-Collaborative-Perception-via-Bird%E2%80%99s-Eye-View-Flow><span class=nav-number>4.3.</span> <span class=nav-text>Asynchrony-Robust Collaborative Perception via Bird’s Eye View Flow</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Flow-Based-Feature-Fusion-for-Vehicle-Infrastructure-Cooperative-3D-Object-Detection><span class=nav-number>4.4.</span> <span class=nav-text>Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Vehicle-Infrastructure-Cooperative-3D-Object-Detection-via-Feature-Flow-Prediction><span class=nav-number>4.5.</span> <span class=nav-text>Vehicle-Infrastructure Cooperative 3D Object Detection via Feature Flow Prediction</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2XPnP-Vehicle-to-Everything-Spatio-Temporal-Fusion-for-Multi-Agent-Perception-and-Prediction><span class=nav-number>4.6.</span> <span class=nav-text>V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#CoDynTrust-Robust-Asynchronous-Collaborative-Perception-via-Dynamic-Feature-Trust-Modulus><span class=nav-number>4.7.</span> <span class=nav-text>CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Leveraging-Temporal-Contexts-to-Enhance-Vehicle-Infrastructure-Cooperative-Perception><span class=nav-number>4.8.</span> <span class=nav-text>Leveraging Temporal Contexts to Enhance Vehicle-Infrastructure Cooperative Perception</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Multi-Agent-Collaborative-Perception-via-Motion-Aware-Robust-Communication-Network><span class=nav-number>4.9.</span> <span class=nav-text>Multi-Agent Collaborative Perception via Motion-Aware Robust Communication Network</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%88%A9%E7%94%A8query%E4%B8%8E%E6%99%9A%E6%9C%9F%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C%E3%80%81%E7%BD%AE%E4%BF%A1%E5%BA%A6%E9%99%8D%E4%BD%8E%E5%B8%A6%E5%AE%BD><span class=nav-number>5.</span> <span class=nav-text>利用query与晚期检测结果、置信度降低带宽</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#CoSDH-Communication-Efficient-Collaborative-Perception-via-Supply-Demand-Awareness-and-Intermediate-Late-Hybridization><span class=nav-number>5.1.</span> <span class=nav-text>CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#CoopDETR-A-Unified-Cooperative-Perception-Framework-for-3D-Detection-via-Object-Query><span class=nav-number>5.2.</span> <span class=nav-text>CoopDETR: A Unified Cooperative Perception Framework for 3D Detection via Object Query</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#How2Com><span class=nav-number>5.3.</span> <span class=nav-text>How2Com</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Inspiration><span class=nav-number>5.3.1.</span> <span class=nav-text>Inspiration</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>6.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>256</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>219</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/collaborative-perception/ rel=tag>collaborative perception</a><span class=tag-list-count>5</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2026</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>3.7m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>55:26</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>document.addEventListener("DOMContentLoaded", function() {
    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {
        var pvContainer = document.getElementById("busuanzi_container_site_pv");
        if (pvContainer && pvContainer.style.display !== "none") {
            var pvElement = document.getElementById("busuanzi_value_site_pv");
            if (pvElement) {
                pvElement.innerHTML = parseInt(pvElement.innerHTML) + countOffset;
                clearInterval(int);
            }
        }
        
        var uvContainer = document.getElementById("busuanzi_container_site_uv");
        if (uvContainer && window.getComputedStyle(uvContainer).display !== "none")
        {
            var uvElement = document.getElementById("busuanzi_value_site_uv");
            if (uvElement) {
                uvElement.innerHTML = parseInt(uvElement.innerHTML) + countOffset; // 加上初始数据 
                clearInterval(int); // 停止检测
            }
        }
    }
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	 '.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
  
  // Reinitialize TagCanvas for tag cloud
  if (typeof TagCanvas !== 'undefined' && document.getElementById('resCanvas')) {
    try {
      TagCanvas.textFont = 'Trebuchet MS, Helvetica';
      TagCanvas.textColour = '#333';
      TagCanvas.textHeight = 20;
      TagCanvas.outlineColour = '#E2E1D1';
      TagCanvas.maxSpeed = 0.3;
      TagCanvas.freezeActive = true;
      TagCanvas.outlineMethod = 'block';
      TagCanvas.minBrightness = 0.2;
      TagCanvas.depth = 0.92;
      TagCanvas.pulsateTo = 0.6;
      TagCanvas.initial = [0.1,-0.1];
      TagCanvas.decel = 0.98;
      TagCanvas.reverse = true;
      TagCanvas.hideTags = false;
      TagCanvas.shadow = '#ccf';
      TagCanvas.shadowBlur = 3;
      TagCanvas.weight = false;
      TagCanvas.imageScale = null;
      TagCanvas.fadeIn = 1000;
      TagCanvas.clickToFront = 600;
      TagCanvas.lock = false;
      TagCanvas.Start('resCanvas');
      TagCanvas.tc['resCanvas'].Wheel(true);
    } catch(e) {
      console.log('TagCanvas initialization failed:', e);
    }
  }
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>