<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=目前构建AI智能体应用的框架已经发展到了一定程度并基本趋于成熟.这篇文章尝试总结其中使用较多的框架并总结这些应用中常遇到的问题和对已经解决方案. name=description><meta content=article property=og:type><meta content=构建AI应用程序的框架以及解决方案 property=og:title><meta content=https://www.sekyoro.top/2025/10/04/%E6%9E%84%E5%BB%BAAI%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=目前构建AI智能体应用的框架已经发展到了一定程度并基本趋于成熟.这篇文章尝试总结其中使用较多的框架并总结这些应用中常遇到的问题和对已经解决方案. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2025/10/05/ASR2DkBgUfsZuTm.png property=og:image><meta content=https://s2.loli.net/2025/10/05/x7hMbdOGHUqozjX.png property=og:image><meta content=https://s2.loli.net/2025/10/05/4w1m6GntXS3zxIf.png property=og:image><meta content=2025-10-04T06:57:05.000Z property=article:published_time><meta content=2025-10-06T05:19:55.675Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="个人博客 技术学习 计算机 互联网 人工智能" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2025/10/05/ASR2DkBgUfsZuTm.png name=twitter:image><link href=https://www.sekyoro.top/2025/10/04/%E6%9E%84%E5%BB%BAAI%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>构建AI应用程序的框架以及解决方案 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2025/10/04/%E6%9E%84%E5%BB%BAAI%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>构建AI应用程序的框架以及解决方案</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2025-10-04 14:57:05" datetime=2025-10-04T14:57:05+08:00>2025-10-04</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2025-10-06 13:19:55" datetime=2025-10-06T13:19:55+08:00 itemprop=dateModified>2025-10-06</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>22k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>20 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>目前构建AI智能体应用的框架已经发展到了一定程度并基本趋于成熟.这篇文章尝试总结其中使用较多的框架并总结这些应用中常遇到的问题和对已经解决方案.<br><span id=more></span><p>对于开发者来说,目前的常用大模型应用开源框架有langchain,autogen,llama-index等等.对应语言的框架有Spring的Langchain4j,Spring AI等.<p>可以从RAG,状态记忆与上下文管理,工具调用,智能体协作多个维度和技术难点分析这类应用的问题.<h2 id=聊天记忆-上下文><a class=headerlink href=#聊天记忆-上下文 title=聊天记忆/上下文></a>聊天记忆/上下文</h2><p>隔离聊天记忆<p>持久化 MongoDB 无结构 方便存储<h2 id=外部工具-方法调用><a class=headerlink href=#外部工具-方法调用 title=外部工具/方法调用></a>外部工具/方法调用</h2><p>进行自然语言的订单,添加Tools<h2 id=RAG><a class=headerlink href=#RAG title=RAG></a>RAG</h2><h3 id=RAG方式><a class=headerlink href=#RAG方式 title=RAG方式></a>RAG方式</h3><p><img alt=image-20251005212809662 data-src=https://s2.loli.net/2025/10/05/ASR2DkBgUfsZuTm.png style=zoom:50%;><h4 id=Naive-RAG><a title="Naive RAG" class=headerlink href=#Naive-RAG></a>Naive RAG</h4><p>遵循传统的索引、检索和生成过程。简而言之，用户输入被用来查询相关文档，这些文档随后与提示结合并传递给模型以生成最终响应。如果应用涉及多轮对话交互，可以将对话历史集成到提示中。<p>普通RAG存在一些限制，例如<strong>低精度（检索到的块不匹配）</strong>和<strong>低召回率（未能检索到所有相关块）</strong>。还有可能将过时的信息传递给LLM，这是RAG系统最初应该试图解决的问题之一。这导致幻觉问题和差劲且不准确的响应。<p>当应用增强时，也可能出现冗余和重复的问题。在使用多个检索到的段落时，排名和协调风格/语气也很关键。另一个挑战是确保生成任务不过度依赖增强信息，这可能导致模型只是重复检索到的内容。<p>缺点:<p><strong>缺乏语义理解</strong>: 全文匹配依赖词汇匹配,无法捕获到query与文档之间的语义关联.<p><strong>输出效果差</strong>: 缺乏对query、文档的高级预处理、后处理以及召回的文档容易包含过多或过少信息,导致生成的回答过于宽泛.<p><strong>效果优化困难</strong>:过于依赖单一检索技术,未对query、文档进行增强,导致优化局限于检索技术.<h3 id=Advanced-RAG><a title="Advanced RAG" class=headerlink href=#Advanced-RAG></a><strong>Advanced</strong> RAG</h3><p>高级RAG有助于解决普通RAG中存在的问题，例如<strong>提高检索质量，这可能涉及优化检索前、检索和检索后的过程。</strong><br>检索前过程包括优化数据索引，旨在通过五个阶段提高索引数据的质量：<strong>增强数据粒度</strong>、<strong>优化索引结构</strong>、<strong>添加元数据</strong>、<strong>对齐优化和混合检索</strong>。<br>检索阶段可以通过<strong>优化嵌入模型本身来进一步改进</strong>，这直接影响构成上下文的块的质量。这可以通过<strong>微调嵌入以优化检索相关性或采用能够更好地捕捉上下文理解的动态嵌入</strong>（例如，OpenAI的embeddings-ada-02模型）来实现。<br>优化检索后过程主要关注<strong>避免上下文窗口限制</strong>和<strong>处理嘈杂或可能分散注意力的信息</strong>。解决这些问题的常见方法是<strong>通过重新排序</strong>，这可能涉及<strong>将相关上下文移至提示边缘或重新计算查询与相关文本块之间的语义相似度</strong>。<strong>提示压缩也可能有助于处理这些问题</strong>。<p>在检索前,<strong>增强文档质量,比如优化章节结构,增强标题,</strong>过滤低质量信息. <strong>优化索引结构,优化chunk size,使得上下文粒度符合应用场景需求</strong>.对chunk进行提取增强. 对<strong>用户的query进行重写</strong><p>在检索阶段,使用域内知识对embedding进行finetune,或者使用llm based-embedding模型,生成对上下文理解更准确的语义向量.<p>检索后阶段,增加<strong>rerank提交检索文档的相关性</strong>,<strong>增加context-compression使提供给模型的信息更加集中.</strong><h4 id=Modular-RAG><a title="Modular  RAG" class=headerlink href=#Modular-RAG></a>Modular RAG</h4><p>模块化RAG增强了功能模块，例如<strong>整合搜索模块以进行相似性检索</strong>以及<strong>在检索器中应用微调</strong>。普通RAG和高级RAG都是模块化RAG的特殊情况，由固定模块组成。<p><strong>扩展RAG模块包括搜索、记忆、融合、路由、预测和任务适配器，解决不同的问题</strong>。这些模块可以根据具体问题情境重新排列。因此，模块化RAG在多样性方面具有更大的优势，并且具有灵活性，可以根据任务需求添加或替换模块，或调整模块之间的流程。<p>鉴于构建RAG系统时的灵活性增加，已经提出了其他重要的优化技术来优化RAG流程，包括：<p><strong>混合搜索探索</strong>：这种方法利用了<strong>基于关键词的搜索和语义搜索等搜索技术的组合</strong>，以检索相关且信息丰富的内容；这在处理不同查询类型和信息需求时非常有用。<br><strong>递归检索和查询引擎</strong>：涉及一个递归检索过程，可能从小的语义块开始，随后检索更大的块以丰富上下文；这有助于平衡效率和上下文丰富的信息。<br><strong>StepBack-prompt</strong>：一种提示技术，它使LLM能够执行抽象，产生指导和推理的概念和原则；当应用于RAG框架时，这会导致更好的基于事实的响应，因为LLM远离了特定实例，如果需要，可以更广泛地进行推理。<br><strong>子查询</strong>：有不同查询策略，如树查询或块顺序查询，可用于不同场景。LlamaIndex提供了一个子问题查询引擎，它允许将查询分解为几个使用不同相关数据源的查询。<br><strong>假设文档嵌入</strong>：<strong>HyDE</strong>为查询生成一个假设答案，将其嵌入，并使用它来检索与假设答案相似的文档，而不是直接使用查询。<p><img alt=image-20251005225339665 data-src=https://s2.loli.net/2025/10/05/x7hMbdOGHUqozjX.png><figure class="highlight stata"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line><span class=keyword>Query</span></span><br><span class=line>  ↓</span><br><span class=line>Routing → 决定检索策略 / 模型 /知识源</span><br><span class=line>  ↓</span><br><span class=line><span class=keyword>Search</span> → 初步检索候选文档</span><br><span class=line>  ↓</span><br><span class=line><span class=keyword>Memory</span> → 补充历史信息 / 相关上下文</span><br><span class=line>  ↓</span><br><span class=line>Fusion → 多源信息融合</span><br><span class=line>  ↓</span><br><span class=line><span class=keyword>Predict</span> → LLM生成答案 / 预测结果</span><br><span class=line>  ↓</span><br><span class=line>Task Adapter → 调整答案格式 / 输出符合任务要求</span><br></pre></table></figure><p>从 Naive RAG 到 Agentic RAG，技术演进的核心是 “从静态检索到动态决策”“从单一功能到多元适配”。选型时需结合业务需求、资源投入与技术储备综合判断：<p>若需快速验证概念，且知识规模小：优先选择 Naive RAG；<p>若需平衡精度与成本，处理中等复杂度问题：Advanced RAG 是性价比之选；<p>若需对接多源数据或频繁迭代模块：Modular RAG 的灵活性更适配；<p>若需处理高复杂度、多步推理任务，且资源充足：Agentic RAG 是长期方向。<p>未来，RAG 的发展将进一步向 “多模态融合”、“实时知识更新”、“轻量化部署” 方向演进，而模块化与 Agent 化的结合（如 Modular Agentic RAG）可能成为主流形态。掌握四大模式的核心差异，将为技术落地提供清晰的路径图<p><img alt=image-20251005214930617 data-src=https://s2.loli.net/2025/10/05/4w1m6GntXS3zxIf.png><h4 id=Graph-RAG><a title="Graph RAG" class=headerlink href=#Graph-RAG></a>Graph RAG</h4><p>使用图结构来扩展传统RAG系统,利用图关系和层级结构,<strong>增强multi-hop推理</strong>和<strong>context丰富度</strong>. Graph RAG生成的结果更丰富和更准确,特别是对于<strong>需要关系理解的任务</strong><p>Graph RAG具备局限性:<p><strong>高质量图数据依赖</strong>: 高质量的图数据对于Graph RAG非常关键,对于无结构的纯文本或者标注质量不高的数据<p><strong>应用的复杂性</strong>:对于一个RAG系统,需要同时支持非结构化数据和图数据的混合检索,增加检索系统实现复杂性.<h5 id=Multi-hop><a class=headerlink href=#Multi-hop title=Multi-hop></a>Multi-hop</h5><blockquote><p><strong>Multi-hop</strong> 指的是在回答一个问题或生成答案时，模型需要<strong>跨多个信息片段、文档或知识源</strong>逐步推理，才能得到最终结果。</blockquote><p>简单说：<ul><li><strong>Single-hop（单跳）</strong>：答案可以从单个文档或单条检索结果直接得到。<br> 例：问 “东京的平均气温是多少？” → 可以直接从单篇气象报告得到答案。<li><strong>Multi-hop（多跳）</strong>：答案需要从<strong>多个文档/信息点</strong>组合推理。<br> 例：问 “A市到B市最快路线经过哪些城市？” → 需要先查 A 到 C，再查 C 到 B，再整合路线。</ul><ol><li><strong>跨文档依赖</strong>：答案不能从单个文档直接得到，需要聚合多个文档信息。<li><strong>逻辑推理链</strong>：需要模型跟踪中间步骤，类似 Chain-of-Thought。<li><strong>复杂 query</strong>：通常涉及条件、约束、因果关系等。<li><strong>更高的检索要求</strong>：Retriever 必须提供多条相关文档，而不是只靠 top-1。</ol><div class=table-container><table><thead><tr><th>技术点<th>作用<tbody><tr><td><strong>多轮检索（Iterative Retrieval）</strong><td>每一步检索为下一步提供上下文<tr><td><strong>中间答案缓存 / Memory</strong><td>保存前一步信息，支持跨步推理<tr><td><strong>路径评分 / Reranker</strong><td>对多跳检索结果按可信度排序<tr><td><strong>Fusion 模块</strong><td>将多文档多跳信息融合为 LLM 上下文<tr><td><strong>Router / Task Adapter</strong><td>决定何时继续跳，何时生成答案</table></div><p>Multi-hop = <strong>多步跨文档推理</strong><br> 核心挑战：需要追踪中间步骤，保证信息融合和逻辑一致性。<br> 在 RAG 系统中，多跳常通过 <strong>迭代检索、推理链、图结构</strong> 或 <strong>HyDE假设文档</strong> 来实现。<h4 id=Agentic-RAG><a title="Agentic  RAG" class=headerlink href=#Agentic-RAG></a>Agentic RAG</h4><p>Agentic RAG使用能够动态决策和工具调用的LLM-based的agent,来解决更复杂、实时和<strong>多域</strong>的查询.<p>Agentic RAG<strong>使用更多更复杂的工具来辅助检索</strong>,比如搜索引擎,计算器等各类以API形式能够直接访问的工具,另外可以<strong>根据时机的检索场景动态决策</strong>,比如决定是否进行检索,决定使用什么工具检索,评估检索到的上下文决定是否需要继续检索.<h5 id=如何实现动态决策><a class=headerlink href=#如何实现动态决策 title=如何实现动态决策></a>如何实现动态决策</h5><h2 id=实现动态决策的机制><a class=headerlink href=#实现动态决策的机制 title=实现动态决策的机制></a>实现动态决策的机制</h2><ol><li>基于规则（Rule-based Agent）</ol><ul><li>最早的方法，按 query 特征或关键词选择策略<li><strong>缺点</strong>：难适应多领域、多任务</ul><figure class="highlight nix"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=keyword>if</span> <span class=string>"法律"</span> <span class=keyword>in</span> query:</span><br><span class=line>    <span class=attr>knowledge_source</span> = legal_index</span><br><span class=line><span class=keyword>else</span>:</span><br><span class=line>    <span class=attr>knowledge_source</span> = general_index</span><br></pre></table></figure><ol><li>基于分类模型（Classifier Agent）</ol><ul><li>训练一个轻量模型预测 query 类型或策略<li>输出可能是：<ul><li>选择的知识源<li>是否多跳<li>检索策略类型</ul><li>优点：可扩展、可学习<li>缺点：需要标注数据</ul><ol><li>基于 LLM 的智能决策（LLM Agent / Planner）</ol><ul><li>使用大模型本身来做决策，输入 query + context，让模型输出行动计划<li><strong>形式示例</strong>：</ul><figure class="highlight markdown"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>Query: 2025年东京的平均气温是多少？</span><br><span class=line>Plan:</span><br><span class=line><span class=bullet>1.</span> 检索最近的气象报告</span><br><span class=line><span class=bullet>2.</span> 判断是否需要多跳合并其他文档</span><br><span class=line><span class=bullet>3.</span> 根据检索结果生成答案</span><br></pre></table></figure><ul><li>在 LangChain 中，可以用 <strong>LLMRouterChain / MultiRetrievalQAChain / AgentExecutor</strong> 实现<li>优点：策略可动态生成、可解释<li>缺点：成本高，需要 LLM 推理</ul><ol><li>基于强化学习 / Policy Learning（RL Agent）</ol><ul><li>将决策看作策略优化问题：<ul><li><strong>状态</strong> = 当前 query + 已检索文档 + 历史上下文<li><strong>动作</strong> = 选择知识源、检索策略、生成策略<li><strong>奖励</strong> = 生成答案的准确率或用户反馈</ul><li>可通过 RL 或 RLHF 训练策略网络，实现端到端的动态决策</ul><figure class="highlight angelscript"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line>Query</span><br><span class=line>  ↓</span><br><span class=line><span class=string>[Dynamic Decision / Router]</span></span><br><span class=line>  ├── Retrieve <span class=keyword>or</span> Skip</span><br><span class=line>  ├── Choose Knowledge Source</span><br><span class=line>  ├── Decide Multi-hop / Single-hop</span><br><span class=line>  ↓</span><br><span class=line><span class=string>[Search / Memory / Fusion]</span></span><br><span class=line>  ↓</span><br><span class=line><span class=string>[Predict / LLM Generation]</span></span><br><span class=line>  ↓</span><br><span class=line><span class=string>[Task Adapter / Output]</span></span><br></pre></table></figure><p>Router/Planner 可以是 规则、分类器或 LLM<p>Multi-hop 和 Fusion 模块执行动态决策后的操作<p>Task Adapter 根据任务要求调整输出格式或策略<h3 id=分块策略><a class=headerlink href=#分块策略 title=分块策略></a>分块策略</h3><p>按行文档分割器<p>按句子、按单词、按字符文档分割器<p><strong>Chunking</strong> 的目标：在保证语义完整性的前提下，将长文档切分成若干可检索的文本单元（chunks）。<br> 🔹 <strong>关键矛盾</strong>：<ul><li>太短 → 语义残缺，召回不准<li>太长 → 嵌入模糊、检索成本高、上下文浪费</ul><h4 id=常见的分块策略分类><a class=headerlink href=#常见的分块策略分类 title=常见的分块策略分类></a>常见的分块策略分类</h4><p>固定长度分块（Fixed-size Chunking）<p>最基础、最常见的做法。<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>按字数/Token数固定长度切分，如每500 token 一块<tr><td><strong>优点</strong><td>实现简单、速度快<tr><td><strong>缺点</strong><td>可能破坏语义边界（句子或段落被截断）<tr><td><strong>常用参数</strong><td>chunk_size=500~1000 tokens, overlap=100~200 tokens</table></div><p>👉 常用于：新闻、论文等较规则文本。<p>滑动窗口分块（Sliding Window Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>每次移动窗口产生重叠块，例如窗口大小1000，滑动步长500<tr><td><strong>优点</strong><td>保留上下文连续性，减少语义断裂<tr><td><strong>缺点</strong><td>重复嵌入，存储量大约增加1.5~2倍</table></div><p>👉 常用于：长文本、问答型知识文档。<p>语义分块（Semantic / Sentence-aware Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>利用自然语言分句或语义相似度聚合，如句向量相似度阈值分组<tr><td><strong>优点</strong><td>按语义边界切分，减少跨句割裂<tr><td><strong>缺点</strong><td>需要额外计算（NLP或Embedding模型）<tr><td><strong>常用方法</strong><td>spaCy句法切分、BERT句嵌入聚类、SimCSE相似度聚合</table></div><p>👉 适合：学术文档、法律条款、技术文档。<p>结构感知分块（Structure-aware Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>利用文档结构（如Markdown层级、HTML标签、PDF目录）分块<tr><td><strong>优点</strong><td>保留逻辑层级（标题+段落）信息，检索更精准<tr><td><strong>缺点</strong><td>不同文档结构需不同解析器<tr><td><strong>常用方法</strong><td>按标题层级聚合段落（H1→H2→p）；PDF 解析后按章节组织</table></div><p>👉 适合：报告、网页、企业知识文档。<p>动态语义合并（Adaptive Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>基于Embedding相似度或模型评分动态决定chunk大小<tr><td><strong>优点</strong><td>自适应内容密度（密集部分小块，稀疏部分大块）<tr><td><strong>缺点</strong><td>实现复杂，计算成本高<tr><td><strong>典型方法</strong><td>Cohere / OpenAI Adaptive Chunking 实验功能；Sentence-BERT聚类分块</table></div><p>👉 适合：多主题长文档或知识图谱型内容。<p>层次化分块（Hierarchical Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>先按大章节分，再在章节内小块切分<tr><td><strong>优点</strong><td>检索时可先 coarse-grained 检索章节，再 fine-grained 检索段落<tr><td><strong>缺点</strong><td>检索逻辑复杂<tr><td><strong>常见方案</strong><td>multi-stage RAG / hierarchical RAG（如 LlamaIndex TreeIndex）</table></div><p>👉 适合：书籍、长报告、代码库。<p>基于语篇或主题边界（Discourse / Topic Chunking）<div class=table-container><table><thead><tr><th>特征<th>说明<tbody><tr><td><strong>策略</strong><td>利用主题模型（LDA / BERTopic）或语篇分析确定边界<tr><td><strong>优点</strong><td>语义连贯性强、召回更准<tr><td><strong>缺点</strong><td>模型依赖强，主题分布可能不稳定</table></div><p>👉 用于：科研论文集合、政策文件、知识图谱构建。<p>三、业界成熟方案 / 工具支持<div class=table-container><table><thead><tr><th>工具 / 框架<th>分块策略支持<th>特点<tbody><tr><td><strong>LangChain</strong> (<code>RecursiveCharacterTextSplitter</code>, <code>MarkdownHeaderTextSplitter</code>, <code>SpacyTextSplitter</code>)<td>固定/递归/结构化分块<td>易用、生态成熟<tr><td><strong>LlamaIndex</strong> (<code>SentenceSplitter</code>, <code>SemanticSplitterNodeParser</code>, <code>HierarchicalNodeParser</code>)<td>语义+层次化<td>可组合策略，支持自适应chunking<tr><td><strong>Haystack</strong> (<code>PreProcessor</code>)<td>固定+重叠分块<td>可配置参数：<code>split_length</code>, <code>split_overlap</code>, <code>split_by</code><tr><td><strong>Unstructured.io</strong><td>结构化分块<td>支持PDF、Docx、HTML等解析<tr><td><strong>Cohere Embed API</strong><td>自适应语义分块（Adaptive Chunking）<td>自动检测语义边界<tr><td><strong>OpenAI text-embedding-utils</strong><td>Token-aware分块<td>提供按模型上下文限制动态切割</table></div><p>四、不同策略的性能影响（对召回与生成）<div class=table-container><table><thead><tr><th>策略类型<th>召回率<th>精确度<th>上下文完整性<th>成本<tbody><tr><td>固定长度<td>中<td>中<td>低<td>低<tr><td>滑动窗口<td>高<td>中<td>中<td>中<tr><td>语义分块<td>高<td>高<td>高<td>中高<tr><td>结构感知<td>高<td>高<td>高<td>中<tr><td>动态分块<td>高<td>高<td>高<td>高<tr><td>层次化分块<td>高<td>高<td>高<td>高</table></div><p>五、实践中的调优建议<div class=table-container><table><thead><tr><th>场景<th>推荐分块策略<th>参数建议<tbody><tr><td>FAQ / 短文档<td>固定长度<td>chunk=300~500, overlap=100<tr><td>技术文档 / 法律条款<td>语义分块 + 结构感知<td>句级语义分组；按标题聚合<tr><td>长论文 / 书籍<td>层次化分块<td>Chapter→Section→Paragraph<tr><td>企业知识库（多主题）<td>动态分块 / 语义聚类<td>相似度阈值0.75~0.85<tr><td>高精检索任务<td>滑动窗口 + Reranker<td>chunk=800, overlap=200</table></div><p>六、额外技巧：Chunking + Reranking + Context Filtering<p>现代 RAG 系统常采用多阶段策略：<figure class="highlight coq"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>Chunking → Embedding → 初筛检索(<span class=built_in>top</span><span class=number>-20</span>)</span><br><span class=line>    → Cross-encoder reranking → 精筛(<span class=built_in>top</span><span class=number>-5</span>)</span><br><span class=line>    → 生成</span><br></pre></table></figure><p>或引入 <strong>Context Filtering / Contextual Compression</strong>：<ul><li>对检索结果再压缩（如LlamaIndex的 Context Compressor）<li>仅保留与query最相关句子，提高上下文利用率</ul><p><strong>总结</strong><div class=table-container><table><thead><tr><th>方向<th>典型策略<th>代表框架<tbody><tr><td><strong>按长度</strong><td>固定、滑动<td>LangChain, Haystack<tr><td><strong>按语义</strong><td>语义/动态分块<td>LlamaIndex, Cohere<tr><td><strong>按结构</strong><td>Markdown/PDF层级<td>Unstructured.io<tr><td><strong>按层次</strong><td>Hierarchical RAG<td>LlamaIndex TreeIndex<tr><td><strong>自适应智能化</strong><td>Adaptive/Topic-based<td>Cohere Adaptive, BERTopic</table></div><p>RAG 分块的核心目的是<strong>让每个 chunk 信息完整又不过长</strong>，同时<strong>避免语义断裂</strong>。<br> 目前常见的几类分块策略如下：<div class=table-container><table><thead><tr><th>策略类型<th>特点<th>适用场景<th>优点<th>缺点<tbody><tr><td>🧱 <strong>固定长度分块 (Fixed-size Chunk)</strong><td>固定 token 数（如512）分割<td>通用文本、快速实验<td>实现简单<td>容易切断语义<tr><td>🪜 <strong>滑动窗口 (Sliding Window)</strong><td>每块之间有重叠<td>技术文档、代码<td>语义连续性强<td>存储开销大<tr><td>🧠 <strong>语义分块 (Semantic Chunking)</strong><td>通过语义相似度或句法边界切分<td>非结构化长文<td>高质量语义单元<td>成本较高（需Embedding + 聚类）<tr><td>📚 <strong>主题/章节分块 (Topic-aware)</strong><td>按段落、标题或主题聚类<td>报告、论文、书籍<td>上下文一致<td>对格式依赖强<tr><td>🗣 <strong>语篇/对话边界分块 (Discourse-aware)</strong><td>根据对话轮次或语义流切分<td>对话、会议摘要<td>保留语境<td>实现复杂<tr><td>🧩 <strong>动态分块 (Dynamic Chunking)</strong><td>根据查询动态裁剪chunk<td>Query-sensitive RAG<td>精准检索<td>实时计算成本高</table></div><p><strong>第一阶段（MVP）</strong>：固定长度 + 100 token overlap<p><strong>第二阶段（优化）</strong>：基于 <code>semantic similarity</code> 或 <code>topic boundary</code> 分块<p><strong>高级阶段</strong>：引入动态分块，如 <em>Dynamic Context Pruning</em> 或 <em>Query-aware Chunking</em>（LangChain、LlamaIndex 都支持）<h3 id=Embedding><a class=headerlink href=#Embedding title=Embedding></a>Embedding</h3><h4 id=分词器><a class=headerlink href=#分词器 title=分词器></a>分词器</h4><p>分词器的任务是：把自然语言（文本）转成模型可处理的 <strong>token 序列</strong>（整数 ID）<div class=table-container><table><thead><tr><th>算法类型<th>原理<th>优点<th>缺点<th>代表实现<tbody><tr><td><strong>Word-level</strong><td>按单词分割<td>语义直观<td>词表太大（百万级）<td>早期模型，如 word2vec<tr><td><strong>Subword-level</strong><td>按词根+子词分割<td>兼顾词表与语义<td>仍需人工训练词表<td>BPE、SentencePiece<tr><td><strong>Character-level</strong><td>按字符分割<td>通用性强<td>序列太长<td>中文 NLP 常见<tr><td><strong>Byte-level / Unigram</strong><td>按字节或概率最优子词<td>兼容多语言、表情符号<td>编码复杂<td>GPT 系列、LLaMA 系列使用</table></div><div class=table-container><table><thead><tr><th>分词器<th>主要算法<th>典型模型<th>特点<tbody><tr><td><strong>GPT Tokenizer</strong> (<code>tiktoken</code>)<td>Byte Pair Encoding (BPE)<td>GPT-3, GPT-4, GPT-4o<td>高效、多语言支持、可数token<tr><td><strong>SentencePiece</strong><td>Unigram / BPE<td>T5, Flan-T5, LLaMA, Mistral<td>谷歌开发，跨语言标准方案<tr><td><strong>BERT Tokenizer</strong><td>WordPiece<td>BERT, RoBERTa<td>英文任务最常见<tr><td><strong>Claude Tokenizer</strong><td>Custom BPE<td>Anthropic Claude 系列<td>类似GPT，优化多语言分词<tr><td><strong>ChatGLM Tokenizer</strong><td>SentencePiece (中英混合优化)<td>ChatGLM, ChatGLM2, GLM4<td>专为中英混合场景设计<tr><td><strong>Baichuan Tokenizer</strong><td>SentencePiece + 特殊token表<td>Baichuan, Qwen<td>兼容中英文及符号<tr><td><strong>Jieba / HanLP</strong><td>中文词级分词<td>中文传统NLP<td>不用于LLM，但适合前处理</table></div><h4 id=大模型><a class=headerlink href=#大模型 title=大模型></a>大模型</h4><p><strong>开源大模型</strong><div class=table-container><table><thead><tr><th>模型<th>开发机构<th>参数规模<th>特点<th>常见用途<tbody><tr><td><strong>LLaMA / LLaMA2 / LLaMA3</strong><td>Meta<td>7B / 13B / 70B<td>多语言强、生态成熟<td>通用对话、RAG<tr><td><strong>Mistral / Mixtral</strong><td>Mistral AI<td>7B / MoE (12x7B)<td>高效、可商用<td>本地问答、RAG<tr><td><strong>Falcon</strong><td>TII<td>7B / 40B<td>性能强，社区稳定<td>研究用<tr><td><strong>Qwen / Qwen2</strong><td>阿里<td>1.8B - 72B<td>中文理解强<td>中文知识问答<tr><td><strong>Baichuan2</strong><td>百川智能<td>7B / 13B<td>中文/中英混合<td>RAG、知识库<tr><td><strong>ChatGLM / GLM4</strong><td>清华&智谱<td>6B / 9B<td>中文最强之一<td>中文助手、RAG<tr><td><strong>Yi-1.5 / Yi-Large</strong><td>零一万物<td>6B / 34B<td>中文生成优<td>企业知识问答<tr><td><strong>Phi-3</strong><td>Microsoft<td>3.8B / 14B<td>小模型强性能<td>边缘端RAG<tr><td><strong>MPT / Dolly</strong><td>Databricks<td>7B / 12B<td>可商业部署<td>企业内部问答</table></div><blockquote><p>这些模型大多使用 <strong>SentencePiece</strong> 分词。</blockquote><p>闭源商用模型<div class=table-container><table><thead><tr><th>模型<th>公司<th>特点<th>是否RAG适配<tbody><tr><td><strong>GPT-4 / GPT-4o / GPT-4-turbo</strong><td>OpenAI<td>最强通用能力、多模态<td>✅<tr><td><strong>Claude 3 系列</strong><td>Anthropic<td>强推理与对话一致性<td>✅<tr><td><strong>Gemini 1.5 系列</strong><td>Google<td>多模态、超长上下文（百万token）<td>✅<tr><td><strong>Command R+ / R+ v2</strong><td>Cohere<td>专为RAG优化的模型<td>✅<tr><td><strong>Mistral Large</strong><td>Mistral<td>开源+商用混合策略<td>✅<tr><td><strong>Moonshot / Kimi / 通义千问-API版</strong><td>中国厂商<td>中文优化、企业集成强<td>✅</table></div><h3 id=检索算法><a class=headerlink href=#检索算法 title=检索算法></a>检索算法</h3><p>检索算法有哪些<p>常见类别：<div class=table-container><table><thead><tr><th>类型<th>原理<th>优点<th>缺点<th>框架支持<tbody><tr><td>🎯 <strong>Sparse（稀疏）检索</strong><td>TF-IDF / BM25<td>传统、快<td>不捕捉语义<td>Haystack, ElasticSearch<tr><td>🌌 <strong>Dense（稠密）检索</strong><td>向量相似度（Embedding）<td>语义检索强<td>需训练/Embedding计算<td>FAISS, Milvus, Pinecone<tr><td>🧠 <strong>Hybrid（混合）检索</strong><td>BM25 + Dense融合<td>折中方案<td>需融合策略<td>LangChain, Weaviate<tr><td>🔁 <strong>Cross-encoder Rerank</strong><td>二阶段 rerank 模型<td>高精度<td>延迟高<td>Cohere Rerank, HuggingFace<tr><td>🌐 <strong>Multi-hop Retrieval</strong><td>连续多轮检索<td>支持推理链<td>复杂实现<td>Self-RAG, GraphRAG<tr><td>🕸 <strong>Graph-based Retrieval</strong><td>知识图谱关联节点<td>强逻辑语义<td>构建成本高<td>GraphRAG, Neo4j<tr><td>🧭 <strong>Router-based Retrieval</strong><td>根据查询类型选择检索器<td>灵活、智能<td>路由模型复杂<td>Agentic RAG</table></div><div class=table-container><table><thead><tr><th>内容类型<th>推荐分块<th>推荐检索算法<th>说明<tbody><tr><td>FAQ / 短问答<td>固定长度 + overlap<td>Sparse + Dense Hybrid<td>高速高召回<tr><td>技术文档 / API<td>语义分块 + 滑动窗口<td>Dense + Rerank<td>精准匹配上下文<tr><td>学术论文 / 报告<td>主题分块<td>Dense + GraphRAG<td>结构化层次检索<tr><td>聊天记录 / 对话<td>语篇分块<td>Dense + Memory-based<td>语境一致<tr><td>复杂推理任务<td>动态分块<td>Multi-hop Dense<td>支持链式推理<tr><td>企业知识库<td>主题 + 动态分块<td>Hybrid + Router<td>性能与成本平衡</table></div><h4 id=评估指标-如何评价><a title="评估指标 如何评价" class=headerlink href=#评估指标-如何评价></a>评估指标 如何评价</h4><p>通过 <strong>RAG 评估框架（如 RAGAS、DeepEval）</strong> 验证效果。<p>关键指标包括：<div class=table-container><table><thead><tr><th>指标<th>说明<tbody><tr><td><strong>Context Precision / Recall</strong><td>检索文档是否相关<tr><td><strong>Faithfulness</strong><td>回答是否忠实于检索内容<tr><td><strong>Answer Relevance</strong><td>回答与问题匹配度<tr><td><strong>Latency / Cost</strong><td>实时性能<tr><td><strong>Context Utilization</strong><td>模型实际引用多少检索信息</table></div><blockquote><p>建议：先固定检索算法，调分块；再固定分块，调检索。</blockquote><h4 id=改进思路（实践经验）><a class=headerlink href=#改进思路（实践经验） title=改进思路（实践经验）></a>改进思路（实践经验）</h4><ol><li><strong>检索前增强（Query Rewriting / HyDE）</strong>：<br> 优化查询，提高检索相关性。<br> 👉 用 “hypothetical answer” 或 “semantic expansion” 技术。<li><strong>检索后过滤（Rerank / Context Pruning）</strong>：<br> 对召回结果二次排序，保留最高质量的段落。<li><strong>动态决策（Router / Agentic RAG）</strong>：<br> 不同问题用不同分块/检索器（例如技术问答 vs 概念定义）。<li><strong>融合多模态检索</strong>：<br> 对图片、表格、代码等特殊内容单独建索引。</ol><div class=table-container><table><thead><tr><th>阶段<th>推荐策略<tbody><tr><td>MVP 阶段<td>固定分块 + Dense 检索（FAISS）<tr><td>提升精度<td>语义分块 + Hybrid 检索 + Rerank<tr><td>智能化<td>动态分块 + Multi-hop 检索 + Router Agent<tr><td>企业级<td>GraphRAG + Memory + Self-RAG（反思机制）</table></div><h3 id=Reranker><a class=headerlink href=#Reranker title=Reranker></a>Reranker</h3><p><strong>reranker（重排序模型）</strong> 是提升检索阶段质量的关键组件之一，它负责对“初步检索得到的候选文档”进行更精细的语义匹配评分，从而筛选出真正与查询最相关的内容。<p>根据输入形式与架构，常见 reranker 模型分为三类：<div class=table-container><table><thead><tr><th>类型<th>典型架构<th>特点<tbody><tr><td><strong>1️⃣ 交叉编码器（Cross-Encoder）</strong><td><code>[CLS] query [SEP] document</code> 输入，整体编码<td>精度最高，直接学习语义匹配；但推理慢，计算量大（O(N)）<tr><td><strong>2️⃣ 双塔模型（Bi-Encoder / Dual Encoder）</strong><td>分别编码 query 和 doc，再计算相似度<td>速度快，可提前向量化 doc，但匹配精度低于交叉模型<tr><td><strong>3️⃣ 混合模型（Hybrid / ColBERT类）</strong><td>token 级交互，例如 ColBERT 用 token 向量最大相似度聚合<td>精度与效率折中，可高效索引且部分保留交互信息</table></div><p>一些在 RAG、搜索、问答场景中常用的 reranker 模型：<div class=table-container><table><thead><tr><th>模型<th>类型<th>核心思想 / 特点<th>开源地址<tbody><tr><td><strong>BM25 + Cross-Encoder</strong><td>经典组合<td>先用 BM25 粗检索，再用 Cross-Encoder 精排<td>传统 baseline<tr><td><strong>BERT-Reranker (MS MARCO)</strong><td>Cross-Encoder<td>在 MS MARCO 排序任务上 fine-tune 的 BERT 模型<td>microsoft/MSMARCO-BERT<tr><td><strong>MiniLM-L-6-v2 / -L-12-v2</strong><td>Cross-Encoder<td>Hugging Face 提供的轻量高效版本，效果-速度平衡好<td>sentence-transformers<tr><td><strong>MonoT5 / DuoT5</strong><td>Seq2Seq (T5系列)<td>将排序任务表述为 “哪个文档更相关” 的生成问题<td>castorini/monot5-base-msmarco<tr><td><strong>E5-Reranker / bge-reranker</strong><td>Cross-Encoder<td>针对中文/多语言优化的语义匹配 reranker<td>BAAI/bge-reranker-large<tr><td><strong>ColBERT / ColBERTv2</strong><td>Late Interaction<td>每个 token 有 embedding，匹配时计算最大相似度后聚合<td><a href=https://github.com/stanford-futuredata/ColBERT rel=noopener target=_blank>stanfordnlp/ColBERT</a><tr><td><strong>RankT5 / RankGPT</strong><td>LLM-based<td>利用大语言模型生成文档相关性排序得分，适合 Agentic RAG<td>RankGPT论文<tr><td><strong>Cohere Rerank API / OpenAI Rerank API</strong><td>商业API<td>已 fine-tune 的大型 cross-encoder 模型，可直接集成</table></div><p>常用的评价指标与信息检索（IR）一致：<div class=table-container><table><thead><tr><th>指标<th>含义<tbody><tr><td><strong>MRR (Mean Reciprocal Rank)</strong><td>关注首个正确答案的位置<tr><td><strong>NDCG@k (Normalized Discounted Cumulative Gain)</strong><td>按排名折减累积相关性，评估整体排序质量<tr><td><strong>Recall@k / Precision@k</strong><td>衡量前 k 个候选中是否包含正确文档<tr><td><strong>MAP (Mean Average Precision)</strong><td>平均精度，衡量多相关文档场景<tr><td><strong>Latency / Throughput</strong><td>工程指标，尤其在大规模检索中很关键</table></div><p>通常有三种典型接入方式：<div class=table-container><table><thead><tr><th>阶段<th>说明<th>代表实践<tbody><tr><td><strong>1️⃣ 检索后重排（Post-Retrieval Reranking）</strong><td>对初检索（BM25/向量）结果进行重排序<td>LangChain <code>rerank_documents</code> 或 LlamaIndex Reranker Node<tr><td><strong>2️⃣ 联合检索（Hybrid Retrieval + Rerank）</strong><td>融合语义检索和关键字检索后再 rerank<td><code>BM25 + dense + rerank</code><tr><td><strong>3️⃣ 动态上下文过滤（Adaptive Reranking）</strong><td>利用 reranker 判断“信息是否足够”来决定是否检索更多<td>在 Advanced RAG / Agentic RAG 中常见（如 HyDE + Rerank + LLM verify）</table></div><h3 id=上下文压缩><a class=headerlink href=#上下文压缩 title=上下文压缩></a>上下文压缩</h3><p>在 RAG 中，通常流程是：<figure class="highlight mathematica"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line><span class=built_in>Query</span> → 检索器（<span class=variable>Retriever</span>） → 得到若干候选文档 → 输入给<span class=variable>LLM</span></span><br></pre></table></figure><p>问题在于：<ul><li>检索出的文档往往很多；<li>每个文档可能很长；<li>直接传入LLM容易超出上下文窗口，或成本太高。</ul><p>于是，<strong>Context Compressor</strong> 就是在 “Retriever” 和 “LLM” 之间的一层“过滤/精简模块”，用于：<blockquote><p>“在不丢失重要信息的前提下，压缩传给模型的上下文内容。</blockquote><p>LlamaIndex 提供了多种压缩策略，核心思想是：<blockquote><p>让模型或算法根据 query 语义重要性，<strong>挑选/总结/截取</strong>文档内容。</blockquote><p>常见的几种方式：<div class=table-container><table><thead><tr><th>类型<th>说明<th>示例<tbody><tr><td><strong>LLM-based summarization compressor</strong><td>用语言模型生成摘要或提取与 query 相关的部分。<td><code>LLMContextCompressor</code><tr><td><strong>Re-ranking compressor</strong><td>用向量相似度或 cross-encoder 打分，仅保留最相关的若干段。<td><code>EmbeddingsFilter</code><tr><td><strong>Token-level truncation</strong><td>简单地按 token 数裁剪到一定长度。<td><code>TokenTextSplitter</code><tr><td><strong>Hybrid compressor</strong><td>先 re-rank，再 LLM 摘要。<td><code>HybridContextCompressor</code></table></div><p>以 LlamaIndex 的 <code>ContextualCompressionRetriever</code> 为例，调用链大致如下：<figure class="highlight reasonml"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line>User Query</span><br><span class=line>   ↓</span><br><span class=line><span class=module-access><span class=module><span class=identifier>BaseRetriever</span>.</span></span>retrieve(query)</span><br><span class=line>   ↓</span><br><span class=line>返回多个文档节点 NodeList</span><br><span class=line>   ↓</span><br><span class=line><span class=module-access><span class=module><span class=identifier>ContextCompressor</span>.</span></span>compress<span class=constructor>_nodes(<span class=params>nodes</span>, <span class=params>query</span>)</span></span><br><span class=line>   ↓</span><br><span class=line>（筛选、摘要、裁剪等操作）</span><br><span class=line>   ↓</span><br><span class=line>返回压缩后的 nodes</span><br><span class=line>   ↓</span><br><span class=line>传给 LLM 进行最终生成</span><br></pre></table></figure><div class=table-container><table><thead><tr><th>Compressor 类<th>功能<th>使用场景<tbody><tr><td><code>LLMTextCompressor</code><td>让模型总结文档，只保留与 query 相关部分<td>上下文长、内容冗余<tr><td><code>EmbeddingsFilter</code><td>用向量距离过滤低相关节点<td>文档块较多、结构化内容<tr><td><code>LLMRerank</code><td>用 LLM 对检索结果重新排序<td>对召回结果精度要求高<tr><td><code>ContextualCompressionRetriever</code><td>将上面几个组合在一起，形成可直接替换的 Retriever<td>实际部署推荐使用</table></div><div class=table-container><table><thead><tr><th>难题<th>Context Compressor 解决方式<tbody><tr><td>Token 超长，LLM输入溢出<td>过滤/摘要内容减少token数量<tr><td>检索召回过多，质量不均<td>重新排序或过滤低相关内容<tr><td>语义漂移（无关文档干扰）<td>让LLM只保留与query紧密相关的部分<tr><td>成本高、响应慢<td>通过压缩显著降低输入token成本</table></div><p><strong>Context Compressor 是 RAG 的“智能上下文裁剪器”</strong>，让系统既保持信息完整性，又降低 LLM 的上下文负担。<br> 它既可基于向量过滤，也可调用 LLM 自动摘要，是 LlamaIndex 构建高效 RAG 系统的核心模块之一<h3 id=RAG路由><a class=headerlink href=#RAG路由 title=RAG路由></a>RAG路由</h3><p>有了 <strong>Routing</strong> 之后，RAG 不再“一刀切”，而是：<ul><li>对不同类型的查询选择不同的 <strong>知识源（Knowledge Source）</strong>；<li>对不同任务选择不同的 <strong>检索算法或提示模板</strong>；<li>对简单问题甚至选择“跳过检索”，直接由LLM回答</ul><h3 id=Hyde-假设文档嵌入><a title="Hyde 假设文档嵌入" class=headerlink href=#Hyde-假设文档嵌入></a>Hyde 假设文档嵌入</h3><p>不直接用原始 query 去检索，而是先让 LLM 根据 query 生成一个“假设文档（hypothetical document）”，然后再用这个文档的 embedding 去检索<p>传统的 RAG 检索是这样的：<figure class="highlight erlang"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>用户 <span class=keyword>query</span> → 编码成 embedding → 检索向量库 → 返回相似文档</span><br></pre></table></figure><p>问题是：<ul><li>用户的 query 很短、模糊、不包含关键信息；<li>直接编码成向量，<strong>语义空间离相关文档很远</strong>；<li>导致检索召回的文档不够相关（Recall低）</ul><blockquote><p>让 LLM 先根据 query “想象”出一篇<strong>可能存在的理想答案文档（假设文档）</strong>，<br> 然后把这篇“假设文档”的 embedding 当成检索向量。</blockquote><p>这样就能更好地捕捉语义空间中与真实答案文档相近的区域。<div class=table-container><table><thead><tr><th>步骤<th>普通 RAG<th>HyDE<tbody><tr><td>输入<td>用户 query<td>用户 query<tr><td>编码<td>直接 embedding<td>先生成“假设文档”再 embedding<tr><td>检索<td>基于 query embedding<td>基于 hypothetical doc embedding<tr><td>结果<td>相关性有限<td>更接近语义空间中“答案文档”</table></div><p>HyDE 的逻辑是这样的：<ul><li>用户 query 通常是一个<strong>问题（Q-space）</strong>；<li>真实文档 embedding 是在<strong>答案空间（A-space）</strong>；<li>LLM 生成的假设文档能将 query 从 Q-space <strong>映射到 A-space</strong>；<li>从而弥合“问题语义”和“答案语义”的分布差异。</ul><p>简单来说：<blockquote><p>它把检索过程从 “query → 文档”<br> 变成了 “query → 假设答案 → 相似真实文档”。</blockquote><p><strong>改进与变体</strong><p>1️⃣ <strong>多样化 HyDE</strong><ul><li><strong>生成多个假设文档（multi-HyDE）</strong>，聚合embedding。<li>适合复杂/模糊 query。</ul><p>2️⃣ <strong>基于摘要的 HyDE</strong><ul><li><strong>生成简短摘要而非完整段落</strong>，减少噪声。</ul><p>3️⃣ <strong>自适应 HyDE (Adaptive HyDE)</strong><ul><li>当 query 较短或明确时直接用 query；<li>当 query 模糊时才生成 hypothetical 文档。</ul><p>4️⃣ <strong>Rerank + HyDE 结合</strong><ul><li>用 HyDE 提升召回，再用 Cross-Encoder 精排</ul><h2 id=任务分解><a class=headerlink href=#任务分解 title=任务分解></a>任务分解</h2><p><strong>Agent类RAG</strong> 和 <strong>LLM应用编排系统（如LangChain、LlamaIndex、Semantic Kernel）</strong> 的关键设计点之一：<strong>用户任务（query / request）的分解（decomposition）</strong>。<p>在Agent系统中，用户的问题往往不是“单步可解”的。例如：<blockquote><p>“帮我写一篇关于RAG系统评估方法的技术博客，并附上代码示例。”</blockquote><p>这个任务涉及：<ol><li>检索知识（RAG评估方法）<li>总结内容（技术博客结构）<li>生成示例代码（代码生成）<li>格式化输出（markdown</ol><p><strong>任务分解的常见方式</strong><ol><li>基于提示<li>基于结构化规划<li>基于思维链CoT<li>基于图结构任务分解<li>基于自反思</ol><h3 id=动态路由到Agent><a class=headerlink href=#动态路由到Agent title=动态路由到Agent></a>动态路由到Agent</h3><p>任务分解完后，如何<strong>动态路由（route）</strong>或<strong>分配（dispatch）</strong>子任务到合适的 Agent 执行。<figure class="highlight sqf"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>用户输入 → Planner（任务分解） → 多个子任务</span><br><span class=line>                ↓</span><br><span class=line>         Router / Coordinator</span><br><span class=line>                ↓</span><br><span class=line>   按语义/类型选择合适<span class=built_in>Agent</span>执行</span><br></pre></table></figure><ol><li>基于规则的路由</ol><p>根据 <strong>step 描述 / 工具类型 / 关键词匹配</strong> 来选 Agent。<ol><li><strong>基于LLM的路由</strong></ol><p><strong>语义选择（semantic routing）</strong>：用LLM本身作为路由控制器，根据语义决定执行者。<p>3.self-reflective Agent执行中实时判断是否需要切换任务或调用其他Agent。<h3 id=Agent选择工具><a class=headerlink href=#Agent选择工具 title=Agent选择工具></a>Agent选择工具</h3><figure class="highlight crmsh"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line>┌────────────────────────────┐</span><br><span class=line>│        <span class=keyword>User</span> <span class=title>Query</span>          │</span><br><span class=line>└────────────┬───────────────┘</span><br><span class=line>             │</span><br><span class=line>             ▼</span><br><span class=line>┌────────────────────────────┐</span><br><span class=line>│   Router / Coordinator     │ ← 判断该任务交给哪个Agent</span><br><span class=line>└────────────┬───────────────┘</span><br><span class=line>             │</span><br><span class=line>             ▼</span><br><span class=line>┌────────────────────────────┐</span><br><span class=line>│        Agent(n)            │ ← 每个Agent有自己的工具与策略</span><br><span class=line>│ ┌────────────────────────┐ │</span><br><span class=line>│ │  Planner / Controller  │ │ ← 决定使用哪个Tool</span><br><span class=line>│ └────────────────────────┘ │</span><br><span class=line>│  ┌──────────────┬────────┐│</span><br><span class=line>│  │  Tool A      │ Tool B ││ ← 例如检索、数据库、绘图、翻译等</span><br><span class=line>│  └──────────────┴────────┘│</span><br><span class=line>└────────────────────────────┘</span><br><span class=line></span><br></pre></table></figure><p>Router：负责任务<strong>分配给合适的Agent</strong>。<br> Agent：负责<strong>如何完成任务</strong>。<br> Tools：是Agent内部的<strong>执行手段</strong>（数据库查询、API调用、代码执行等）。<p>每个Agent内部通常包含三个核心组件：<div class=table-container><table><thead><tr><th>组件<th>作用<tbody><tr><td><strong>Planner（子任务规划）</strong><td>将当前任务拆解成一步步子操作<tr><td><strong>Executor / Controller</strong><td>决定使用哪个工具执行<tr><td><strong>Toolset（工具集）</strong><td>具体实现执行逻辑，比如SQL查询、API请求等</table></div><h2 id=重写query><a class=headerlink href=#重写query title=重写query></a>重写query</h2><p>重写query的目的<p><strong>桥接语义差距</strong>：用户 query 往往短、含糊或问法与知识库文本表述差异大，直接 embedding/检索召回差。<p><strong>提升召回（Recall）</strong>：把 query 扩展到更接近答案空间（answer space），能检出更多相关文档。<p><strong>减少误检/噪声</strong>：通过澄清或规范化避免检索到不相关语境。<p><strong>支持多跳/复杂检索</strong>：将复杂问题拆解或扩展成多个更易检索的子查询。<p><strong>控制检索方向</strong>：加入领域/时间/格式等约束（如“只检索2023以后”）。<p><strong>降低下游幻觉</strong>：更精确的上下文，减少 LLM “凭空想象”答案的概率<ol><li>规则与模板式重写（Rule-based）</ol><ul><li>思路：关键词替换、同义词扩展、实体规范化（“NYC”→“New York City”）、时间标准化等。<li>优点：可控、延迟低；缺点：覆盖面窄、维护成本高。</ul><ol><li>伪相关反馈（PRF, Pseudo-Relevance Feedback）</ol><ul><li>流程：初次检索 top-N → 从结果中抽取高频词/短语（或用 TF-IDF/Keyphrase） → 将这些词加回原 query → 再检索。<li>优点：简单高效，提升 recall；缺点：若初检结果噪声多，会放大错误。</ul><ol><li>HyDE（Hypothetical Document Embedding）</ol><ul><li>思路：先用 LLM 根据 query 生成一个“假设答案文档”（hypothetical doc），对该文档做 embedding 并检索。<li>优点：把 query 从问题空间映射到答案空间，零样本效果好；缺点：增加一次 LLM 调用成本。</ul><ol><li>LLM-based Query Reformulation（Prompt-driven）</ol><ul><li>思路：用 LLM 将短 query 扩写成更明确、检索友好的句子或多个候选 query（multi-query）。<li>可做：澄清问题、添加上下文（用户角色/意图）、生成多种问法。<li>优点：灵活，可控制风格；缺点：需设计好 prompt、成本较高。</ul><ol><li>Supervised Seq2Seq Rewriter（学习式）</ol><ul><li>思路：用训练数据（query → gold_rewrite）微调 seq2seq 模型（T5/BART/Flan）做自动重写。<li>优点：效果稳、可离线部署；缺点：需要标注数据或用弱监督生成标签。</ul><ol><li>Reinforcement / RLHF 优化（策略式）</ol><ul><li>把重写当作策略，用下游检索/生成的奖励信号训练策略模型（例如 RL/HF）。<li>更高级但复杂，适合对性能要求非常高的生产场景。</ul><ol><li>Multi-query / Decomposition（分解式）</ol><ul><li>对复杂 query 产生多个子 query（或 step-by-step 中间问题），分别检索并聚合。<li>用于 multi-hop 场景。</ul><ol><li>Hybrid（组合）</ol><ul><li>常见做法：先用 LLM 做 step-back（抽象），再做 PRF 或 HyDE，再用 reranker 精排</ul><p><strong>LangChain</strong>：<ul><li>用 <code>LLMChain</code> + 自定义 prompt 做重写（同步或异步）。<li>用 <code>RouterChain</code> 将 query 先路由到 “rewriter chain” 再到 <code>retriever</code>。<li>HyDE 实现：先用 LLMChain 生成 hypothetical doc，再 embed & search（见上面代码）。</ul><p><strong>LlamaIndex</strong>：<ul><li>提供 <code>QueryTransform</code> / <code>QueryEngine</code>，可以插入重写器（例如 <code>LLMQueryTransform</code>）。<li><code>ContextualCompressionRetriever</code> + <code>LLMTextCompressor</code> 可以结合 HyDE/summary。</ul><p><strong>工程注意</strong>：<ul><li>把重写器作为“检索前的中间链路（pre-retrieval hook）”，可配置采样率（例如只对 30% 的 query 做 LLM 重写以节省成本）。<li>对重写结果做日志（原 query, rewrite, retrieved docs）用于离线评估与模型改进。</ul><h3 id=评估重写query效果><a class=headerlink href=#评估重写query效果 title=评估重写query效果></a>评估重写query效果</h3><p><strong>直接检索层面指标</strong>（首选）：<ul><li>Recall@k（重要）<li>MRR, nDCG@k<li>Precision@k（当需要高精度时）</ul><p><strong>下游生成层面指标</strong>（更能反映最终效果）：<ul><li>Answer accuracy / F1（基于 gold answers）<li>Human evaluation（可读性、是否改变意图）<li>Faithfulness / Hallucination rate（是否降低幻觉）</ul><p><strong>效率与成本</strong>：<ul><li>Latency 增加（尤其 LLM 重写 / HyDE）<li>LLM 调用次数 / token 成本</ul><p>实验设计建议：<ul><li>A/B 比较：baseline（无改写） vs 各种改写策略<li>分析 fail cases：哪些 query 改写后反而降召回或改变意图？</ul><p>工程实践建议与调优要点<ol><li><strong>先做低成本 PRF + 规则，再逐步引入 HyDE/LLM</strong>（按成本阶梯上线）。<li><strong>针对 query 类型做分层策略</strong>：短查询/疑问句 → LLM 重写；明确事实查询 → 直接检索。<li><strong>多候选合并策略</strong>：对多个 rewrite 分别检索并合并，使用 reranker 精排。<li><strong>采样策略</strong>：生产环境可只对难题或抽样 query 使用昂贵重写方法。<li><strong>日志埋点</strong>：记录原 query、rewrite、检索结果、最终回答与用户反馈用于迭代。<li><strong>保留语义一致性</strong>：重写必须不改变用户意图；用 LLM 或规则做一致性校验（e.g., ask LLM 判断 rewrite 是否保留原意）。<li><strong>延迟/成本控制</strong>：HyDE + multi-rewrite 提升效果明显但成本高，需权衡。<li><strong>安全与过滤</strong>：重写可能把敏感文本扩写，注意对 PII、敏感内容做过滤。</ol><p>常见陷阱（注意避免）<ul><li><strong>初次检索全是噪声 → PRF 效果会更差</strong>（需先保证 base retriever 有一定质量）。<li><strong>重写改变原意</strong>（要有一致性检测或人类校验通道）。<li><strong>过度扩展导致精度下降</strong>（precision-recall 权衡）。<li><strong>LLM 生成的假设文档带入错误事实</strong>（HyDE 生成风格需控制，不要加入虚构细节）。<li><strong>成本不可控</strong>：LLM 重写 + 多次检索会增加延迟与费用，需限流/采样</ul><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=comment># 1. HyDE generate</span></span><br><span class=line>hypo = llm_chain.run(hypo_prompt.<span class=built_in>format</span>(query=query))</span><br><span class=line>hypo_emb = embed(hypo)</span><br><span class=line></span><br><span class=line><span class=comment># 2. initial dense search using hypo embedding</span></span><br><span class=line>candidates1 = vector_index.search_by_vector(hypo_emb, k=<span class=number>50</span>)</span><br><span class=line></span><br><span class=line><span class=comment># 3. PRF expansion from candidates1</span></span><br><span class=line>expansion_terms = extract_terms(candidates1, top_k=<span class=number>8</span>)</span><br><span class=line>expanded_query = query + <span class=string>" "</span> + <span class=string>" "</span>.join(expansion_terms)</span><br><span class=line></span><br><span class=line><span class=comment># 4. dense + bm25 hybrid search with expanded_query</span></span><br><span class=line>cand_dense = vector_index.search(embed(expanded_query), k=<span class=number>100</span>)</span><br><span class=line>cand_bm25 = bm25.search(expanded_query, k=<span class=number>100</span>)</span><br><span class=line>candidates = merge_and_dedup(cand_dense, cand_bm25)</span><br><span class=line></span><br><span class=line><span class=comment># 5. rerank top-20 with cross-encoder</span></span><br><span class=line>reranked = cross_encoder_reranker.rank(query, candidates, top_k=<span class=number>10</span>)</span><br><span class=line></span><br><span class=line><span class=comment># 6. pass top-k to LLM generator</span></span><br><span class=line>answer = llm.generate_with_context(query, reranked[:<span class=number>5</span>])</span><br><span class=line></span><br></pre></table></figure><h2 id=self-rag-反省><a title="self-rag 反省" class=headerlink href=#self-rag-反省></a>self-rag 反省</h2><p>普通RAG的问题,<ul><li><strong>检索到的文档可能无关或质量不高</strong>；<li>模型在生成时<strong>不能动态判断“是否需要检索更多”、“是否需要反思回答是否正确”</strong>；<li>一旦生成开始，<strong>流程单向、缺乏自我校正</strong>。</ul><p>Self-RAG（由 Meta AI 在 2024 年提出）通过让模型<strong>在生成前、生成中、生成后</strong>都进行自我反思，从而提升答案的准确性与可信度。<p>它让 LLM 在内部学会：<ul><li>何时需要检索（是否已有足够信息）；<li>检索什么（自适应地重写查询）；<li>生成后如何反思并改进回答。</ul><figure class="highlight mathematica"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line>    ┌────────────────┐</span><br><span class=line>    │   用户<span class=built_in>Query</span>     │</span><br><span class=line>    └──────┬─────────┘</span><br><span class=line>           │</span><br><span class=line>┌──────────▼──────────┐</span><br><span class=line>│   <span class=variable>Reflection</span> <span class=built_in>Module</span>  │ ←—— 模型判断：是否需要检索？</span><br><span class=line>└──────────┬──────────┘</span><br><span class=line>           │</span><br><span class=line>┌──────────▼──────────┐</span><br><span class=line>│   <span class=variable>Retriever</span> <span class=punctuation>(</span>动态<span class=punctuation>)</span>   │ ←—— 检索并生成候选文档</span><br><span class=line>└──────────┬──────────┘</span><br><span class=line>           │</span><br><span class=line>┌──────────▼──────────┐</span><br><span class=line>│   <span class=variable>Generator</span>          │ ←—— 基于上下文生成回答</span><br><span class=line>└──────────┬──────────┘</span><br><span class=line>           │</span><br><span class=line>┌──────────▼──────────┐</span><br><span class=line>│   <span class=variable>Critic</span> <span class=operator>/</span> <span class=variable>Reflector</span> │ ←—— 判断回答是否合理、是否需重试</span><br><span class=line>└──────────────────────┘</span><br><span class=line></span><br></pre></table></figure><p>Self-RAG 将反思嵌入在三个阶段：<p>1️⃣ <strong>Pre-generation Reflection</strong>（生成前反思）<ul><li>模型先判断是否需要检索外部知识。<li>如果内部知识足够，则直接回答；<li>否则构造一个检索意图（检索查询），去外部知识库找文档。</ul><p><strong>实现方式：</strong><br> 模型输出一个标签或控制信号，如：<figure class="highlight vim"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line><span class=symbol>&LTretrieve></span> climate <span class=keyword>change</span> impacts <span class=keyword>on</span> agriculture &LT/retrieve></span><br></pre></table></figure><p>再由检索器调用外部数据库。<p>In-generation Reflection（生成中反思）<ul><li>模型在回答中可以多次调用检索（multi-hop retrieval）；<li>比如生成到一半时发现信息不足，可插入“中途反思”：</ul><figure class="highlight xml"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>I realize I need more information about X...</span><br><span class=line><span class=tag><<span class=name>retrieve</span>></span> definition of X <span class=tag>&LT/<span class=name>retrieve</span>></span></span><br></pre></table></figure><p>Retriever 再次介入，模型更新上下文继续生成。<p><strong>Post-generation Reflection</strong>（生成后反思）<ul><li>模型生成后再评估自己的回答是否：<ul><li>覆盖了问题？<li>是否有幻觉？<li>是否逻辑一致？</ul></ul><p><strong>实现方式：</strong><br> 模型在输出末尾生成一段 meta-analysis，如：<figure class="highlight apache"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=section>&LTcritique></span></span><br><span class=line><span class=attribute>The</span> answer might miss recent research after <span class=number>2023</span>.</span><br><span class=line><span class=attribute>Confidence</span>: <span class=number>0</span>.<span class=number>75</span></span><br><span class=line><span class=section>&LT/critique></span></span><br></pre></table></figure><p>若信心低，则可触发再检索、再生成的循环。<p>Self-RAG 可以通过以下组件实现：<ol><li><strong>Retriever</strong>：支持动态调用（如FAISS、ElasticSearch、LlamaIndex Retriever等）；<li><strong>Reflection Controller</strong>：负责监控生成过程、解析 <code>&LTretrieve></code>、<code>&LTcritique></code> 等控制标志；<li><strong>Critic Model（可选）</strong>：可以是独立LLM，用来做“回答质量评估”；<li><strong>Memory机制</strong>：保存每次反思的结果（用于改进下轮生成或学习）。</ol><p>LangChain / LlamaIndex 等框架都能实现类似流程（例如用 <code>AgentExecutor</code> + <code>CriticChain</code>）<h2 id=评估应用><a class=headerlink href=#评估应用 title=评估应用></a>评估应用</h2><h3 id=评价指标><a class=headerlink href=#评价指标 title=评价指标></a>评价指标</h3><div class=table-container><table><thead><tr><th style=text-align:left>评估维度<th style=text-align:left>核心指标<th style=text-align:left>检测方法<tbody><tr><td style=text-align:left>会话质量<td style=text-align:left>相关性(Relevancy)<td style=text-align:left>LLM评分器（0-1分）<tr><td style=text-align:left><td style=text-align:left>完整性(Completeness)<td style=text-align:left>用户目标达成率分析<tr><td style=text-align:left>状态管理<td style=text-align:left>知识保留(Retention)<td style=text-align:left>关键信息回溯验证<tr><td style=text-align:left><td style=text-align:left>可靠性(Reliability)<td style=text-align:left>错误自我修正频次统计<tr><td style=text-align:left>安全合规<td style=text-align:left>幻觉率(Hallucination)<td style=text-align:left>声明拆解+事实核查<tr><td style=text-align:left><td style=text-align:left>毒性/偏见(Toxicity)<td style=text-align:left>专用分类模型检测</table></div><h3 id=评估数据集><a class=headerlink href=#评估数据集 title=评估数据集></a>评估数据集</h3><p><strong>MMLU</strong><p><strong>简介</strong>：MMLU是一个涵盖57个不同学科的多任务测试基准，包括数学、历史、法律、计算机科学等领域，旨在评估模型的多学科知识和推理能力。模型通常在零样本(zero-shot)和少样本(few-shot)设置下进行评估。<p><strong>数据来源</strong>：由Dan Hendrycks等人于2020年开发，数据来源于各种学术测试、专业资格考试和标准化测试<p><strong>GPQA</strong><p>GPQA是一个研究生物理水平的问答数据集，涵盖经典力学、量子力学、热力学、电磁学等领域的高级问题。<p><strong>数据来源</strong>：由物理学家和研究生物理教育专家创建，基于研究生课程和资格考试。<h3 id=评估库><a class=headerlink href=#评估库 title=评估库></a>评估库</h3><p><strong>Ragas</strong><p><strong>Ragas</strong>（Retrieval Augmented Generation Assessment）是一个专门评估 <strong>RAG pipeline 效果</strong> 的框架，目标是<strong>不依赖人工标注</strong>，而是自动衡量 RAG 系统中各个阶段（检索+生成）的性能。<div class=table-container><table><thead><tr><th>类别<th>指标<th>含义<th>计算方式简述<tbody><tr><td>🧭 检索质量<td><strong>Context Precision</strong><td>检索出的文档中，有多少是真正与答案相关的<td>计算每个检索段落与参考答案的语义相似度（通过 LLM 或 embedding）<tr><td><td><strong>Context Recall</strong><td>所需的关键信息中，有多少被检索出来<td>比较参考答案所需知识与检索内容的重叠度<tr><td><td><strong>Context Relevance</strong><td>检索内容是否真正回答了问题<td>用 LLM 判断 query 与 context 的相关性<tr><td>💬 生成质量<td><strong>Faithfulness</strong><td>生成内容是否忠实于检索内容（无幻觉）<td>用 LLM 比较生成回答与 context 是否一致<tr><td><td><strong>Answer Relevance</strong><td>生成回答是否与 query 对齐<td>用 LLM 判断生成回答与问题的语义相关度<tr><td><td><strong>Answer Correctness</strong><td>生成回答是否正确（基于参考答案）<td>比较生成回答与 ground truth 的语义一致性<tr><td>⚙️ 综合<td><strong>Context Utilization</strong><td>生成内容是否真正利用了检索信息<td>检查回答中是否引用了 context 中的信息</table></div><h2 id=Ragas-评估的实现思路（技术层面）><a title="Ragas 评估的实现思路（技术层面）" class=headerlink href=#Ragas-评估的实现思路（技术层面）></a>Ragas 评估的实现思路（技术层面）</h2><ol><li><p><strong>语义嵌入（Embedding）对齐</strong><br> 使用句向量模型（如 <code>sentence-transformers</code>）计算：</p> <ul><li>查询与文档相似度（评估检索）<li>答案与文档/参考答案相似度（评估生成）</ul><li><p><strong>LLM 语义打分（LLM-as-a-judge）</strong><br> 调用大模型，通过 Prompt 让其判断：</p> <blockquote><p>“是否回答了问题？是否忠实于上下文？”</blockquote><li><p><strong>多维聚合评分</strong><br> 将多个维度（Faithfulness, Relevance 等）进行加权，得到整体质量评分</p></ol><p><strong>DeepEval</strong><p>包括生产监控的功能.<figure class="highlight awk"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>用户请求 → RAG/Agent → 模型生成输出</span><br><span class=line>                ↓</span><br><span class=line>       DeepEval Collector（采集层）</span><br><span class=line>                ↓</span><br><span class=line>   Metrics Engine（指标计算：Faithfulness、Relevance 等）</span><br><span class=line>                ↓</span><br><span class=line>    Storage（结果存储：Postgres、BigQuery、Prometheus）</span><br><span class=line>                ↓</span><br><span class=line>  Dashboard & Alerts（Grafana <span class=regexp>/ Confident Cloud /</span> 自建可视化）</span><br></pre></table></figure><ol><li>在线采样 在生产环境中，你不需要评估每个请求（代价太高）<li>每次 LLM 响应后，DeepEval 自动计算核心指标：<li>DeepEval 提供非阻塞的日志机制：<li>可视化与趋势分析（Visualization & Drift Detection）<li>集成 CI/CD 与回归测试</ol><p>DeepEval 支持与 CI/CD 集成（如 GitHub Actions）：<ul><li>每次 Prompt 或模型更新时自动运行测试；<li>如果指标下降超过阈值则阻止部署</ul><ol><li><strong>异常报警机制（Alerting）</strong></ol><p>通过配置阈值和报警策略，当：<ul><li>Faithfulness < 0.7<li>或 Relevance 波动超过 20%<li>或 幻觉率 > 10%</ul><p>时，自动触发：<ul><li>邮件或 Slack 报警<li>HTTP webhook 推送到告警系统</ul><p><strong>MLFlow</strong><ul><li>初创验证阶段 → RAGAS（快速定位检索瓶颈）<li>生产环境部署 → DeepEval（定制指标+持续监控）<li>混合架构场景 → MLFlow（统一实验跟踪）</ul><h2 id=开源RAG框架><a class=headerlink href=#开源RAG框架 title=开源RAG框架></a>开源RAG框架</h2><h3 id=RAGFlow><a class=headerlink href=#RAGFlow title=RAGFlow></a>RAGFlow</h3><p>RAGFlow 是一款基于深度文档理解构建的开源 RAG（Retrieval-Augmented Generation）引擎。RAGFlow 可以为各种规模的企业及个人提供一套精简的 RAG 工作流程，结合大语言模型（LLM）针对用户各类不同的复杂格式数据提供可靠的问答以及有理有据的引用<h3 id=QAnything><a class=headerlink href=#QAnything title=QAnything></a>QAnything</h3><p>网易 致力于支持任意格式文件或数据库的本地知识库问答系统，可断网安装使用。<h3 id=MaxKB><a class=headerlink href=#MaxKB title=MaxKB></a>MaxKB</h3><p>MaxKB 是一款企业级知识库平台，其设计目标是为企业提供稳定、高效、易用的知识管理和 RAG 解决方案。MaxKB 强调开箱即用，提供了完善的功能和企业级特性，帮助企业快速构建和部署知识库应用，并提升知识管理的效率<h3 id=Dify><a class=headerlink href=#Dify title=Dify></a>Dify</h3><p>Dify 作为一款 LLM 应用开发平台，其 RAG 能力同样不容忽视。Dify 的优势在于其平台化的设计理念，将 RAG 引擎作为核心组件，并集成了模型管理、工作流编排、可观测性等企业级特性，使得开发者可以更专注于业务逻辑的实现。<h3 id=Haystack><a class=headerlink href=#Haystack title=Haystack></a>Haystack</h3><p>Haystack 是一个强大而灵活的框架，用于构建端到端问题解答和搜索系统。它采用模块化架构，允许开发人员轻松创建各种 NLP 任务的管道，包括文档检索、问题解答和摘要：<br>支持多种文档存储（Elasticsearch、FAISS、SQL 等）<br>与流行的语言模型（BERT、RoBERTa、DPR 等）集成<br>处理大量文件的可扩展架构<h1 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h1><ol><li><a href=https://microsoft.github.io/autogen/stable/ rel=noopener target=_blank>AutoGen — AutoGen</a><li><a href=https://docs.langchain.com/oss/python/langchain/overview rel=noopener target=_blank>Overview - Docs by LangChain</a><li><a href=https://medium.com/@rtamirasa/choosing-your-agent-toolkit-langchain-langgraph-llamaindex-autogen-explained-c3b2e144a015 rel=noopener target=_blank>Choosing Your Agent Toolkit: LangChain, LangGraph, LlamaIndex & AutoGen Explained | by Ridhi Tamirasa | Medium</a><li><a href=https://developers.llamaindex.ai/python/framework/#getting-started rel=noopener target=_blank>Welcome to LlamaIndex 🦙 ! | LlamaIndex Python Documentation</a><li><a href=https://huggingface.co/docs/smolagents/examples/rag rel=noopener target=_blank>Agentic RAG</a><li><a href=https://www.promptingguide.ai/ rel=noopener target=_blank>Prompt Engineering Guide | Prompt Engineering Guide</a><li><a href=https://zilliz.com/blog rel=noopener target=_blank>Vector Database Stories</a><li><a href=https://github.com/openai/openai-cookbook/tree/main rel=noopener target=_blank>openai/openai-cookbook: Examples and guides for using the OpenAI API</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2025/10/04/%E6%9E%84%E5%BB%BAAI%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/ title=构建AI应用程序的框架以及解决方案>https://www.sekyoro.top/2025/10/04/构建AI应用程序的框架以及解决方案/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-nav><div class=post-nav-item><a href=/2025/09/02/MySQL%E4%B8%8ERedis%E7%BB%8F%E5%85%B8%E9%9D%A2%E7%BB%8F/ rel=prev title=MySQL与Redis经典面经> <i class="fa fa-chevron-left"></i> MySQL与Redis经典面经 </a></div><div class=post-nav-item><a href=/2025/11/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8/ rel=next title=操作系统与计算机网络经典> 操作系统与计算机网络经典 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BF%86-%E4%B8%8A%E4%B8%8B%E6%96%87><span class=nav-number>1.</span> <span class=nav-text>聊天记忆/上下文</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7-%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8><span class=nav-number>2.</span> <span class=nav-text>外部工具/方法调用</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#RAG><span class=nav-number>3.</span> <span class=nav-text>RAG</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#RAG%E6%96%B9%E5%BC%8F><span class=nav-number>3.1.</span> <span class=nav-text>RAG方式</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Naive-RAG><span class=nav-number>3.1.1.</span> <span class=nav-text>Naive RAG</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Advanced-RAG><span class=nav-number>3.2.</span> <span class=nav-text>Advanced RAG</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Modular-RAG><span class=nav-number>3.2.1.</span> <span class=nav-text>Modular RAG</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Graph-RAG><span class=nav-number>3.2.2.</span> <span class=nav-text>Graph RAG</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Multi-hop><span class=nav-number>3.2.2.1.</span> <span class=nav-text>Multi-hop</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#Agentic-RAG><span class=nav-number>3.2.3.</span> <span class=nav-text>Agentic RAG</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%86%B3%E7%AD%96><span class=nav-number>3.2.3.1.</span> <span class=nav-text>如何实现动态决策</span></a></ol></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%86%B3%E7%AD%96%E7%9A%84%E6%9C%BA%E5%88%B6><span class=nav-number>4.</span> <span class=nav-text>实现动态决策的机制</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5><span class=nav-number>4.1.</span> <span class=nav-text>分块策略</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%B8%B8%E8%A7%81%E7%9A%84%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5%E5%88%86%E7%B1%BB><span class=nav-number>4.1.1.</span> <span class=nav-text>常见的分块策略分类</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Embedding><span class=nav-number>4.2.</span> <span class=nav-text>Embedding</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%88%86%E8%AF%8D%E5%99%A8><span class=nav-number>4.2.1.</span> <span class=nav-text>分词器</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%A4%A7%E6%A8%A1%E5%9E%8B><span class=nav-number>4.2.2.</span> <span class=nav-text>大模型</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%A3%80%E7%B4%A2%E7%AE%97%E6%B3%95><span class=nav-number>4.3.</span> <span class=nav-text>检索算法</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-%E5%A6%82%E4%BD%95%E8%AF%84%E4%BB%B7><span class=nav-number>4.3.1.</span> <span class=nav-text>评估指标 如何评价</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%94%B9%E8%BF%9B%E6%80%9D%E8%B7%AF%EF%BC%88%E5%AE%9E%E8%B7%B5%E7%BB%8F%E9%AA%8C%EF%BC%89><span class=nav-number>4.3.2.</span> <span class=nav-text>改进思路（实践经验）</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Reranker><span class=nav-number>4.4.</span> <span class=nav-text>Reranker</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8E%8B%E7%BC%A9><span class=nav-number>4.5.</span> <span class=nav-text>上下文压缩</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#RAG%E8%B7%AF%E7%94%B1><span class=nav-number>4.6.</span> <span class=nav-text>RAG路由</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Hyde-%E5%81%87%E8%AE%BE%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5><span class=nav-number>4.7.</span> <span class=nav-text>Hyde 假设文档嵌入</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3><span class=nav-number>5.</span> <span class=nav-text>任务分解</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%E5%88%B0Agent><span class=nav-number>5.1.</span> <span class=nav-text>动态路由到Agent</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Agent%E9%80%89%E6%8B%A9%E5%B7%A5%E5%85%B7><span class=nav-number>5.2.</span> <span class=nav-text>Agent选择工具</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E9%87%8D%E5%86%99query><span class=nav-number>6.</span> <span class=nav-text>重写query</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AF%84%E4%BC%B0%E9%87%8D%E5%86%99query%E6%95%88%E6%9E%9C><span class=nav-number>6.1.</span> <span class=nav-text>评估重写query效果</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#self-rag-%E5%8F%8D%E7%9C%81><span class=nav-number>7.</span> <span class=nav-text>self-rag 反省</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%AF%84%E4%BC%B0%E5%BA%94%E7%94%A8><span class=nav-number>8.</span> <span class=nav-text>评估应用</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87><span class=nav-number>8.1.</span> <span class=nav-text>评价指标</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>8.2.</span> <span class=nav-text>评估数据集</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AF%84%E4%BC%B0%E5%BA%93><span class=nav-number>8.3.</span> <span class=nav-text>评估库</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Ragas-%E8%AF%84%E4%BC%B0%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%EF%BC%88%E6%8A%80%E6%9C%AF%E5%B1%82%E9%9D%A2%EF%BC%89><span class=nav-number>9.</span> <span class=nav-text>Ragas 评估的实现思路（技术层面）</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%BC%80%E6%BA%90RAG%E6%A1%86%E6%9E%B6><span class=nav-number>10.</span> <span class=nav-text>开源RAG框架</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#RAGFlow><span class=nav-number>10.1.</span> <span class=nav-text>RAGFlow</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#QAnything><span class=nav-number>10.2.</span> <span class=nav-text>QAnything</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#MaxKB><span class=nav-number>10.3.</span> <span class=nav-text>MaxKB</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Dify><span class=nav-number>10.4.</span> <span class=nav-text>Dify</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Haystack><span class=nav-number>10.5.</span> <span class=nav-text>Haystack</span></a></ol></ol><li class="nav-item nav-level-1"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number></span> <span class=nav-text>参考资料</span></a></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>251</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>218</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>此文章目前无词云</h3></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>3.6m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>54:15</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>