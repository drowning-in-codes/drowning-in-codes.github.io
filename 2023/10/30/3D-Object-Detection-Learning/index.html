<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知. name=description><meta content=article property=og:type><meta content="3D Object Detection Learning" property=og:title><meta content=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知. property=og:description><meta content=zh_CN property=og:locale><meta content=https://i.imgur.com/f2UTqxK.png property=og:image><meta content=https://i.imgur.com/Q6l2kcO.png property=og:image><meta content=https://i.imgur.com/fFcXqBb.png property=og:image><meta content=https://i.imgur.com/JWpC9qY.png property=og:image><meta content=https://s2.loli.net/2023/12/01/OZL5Fz6Vi7mEcvK.png property=og:image><meta content=https://s2.loli.net/2023/12/01/t5BC7l1IqMNKDbf.png property=og:image><meta content=https://s2.loli.net/2023/12/01/q59OprywgN2Ehsk.png property=og:image><meta content=https://s2.loli.net/2023/12/01/GTviaIdYJ9gFA6Z.png property=og:image><meta content=https://i.imgur.com/GERZSZ7.png property=og:image><meta content=https://i.imgur.com/dhgI1JD.png property=og:image><meta content=https://i.imgur.com/wvQa3jb.png property=og:image><meta content=https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png property=og:image><meta content=https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png property=og:image><meta content=https://i.imgur.com/tS9TBGi.png property=og:image><meta content=https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png property=og:image><meta content=https://s2.loli.net/2024/01/01/McwI9AFzmXhLC6K.png property=og:image><meta content=https://s2.loli.net/2024/01/01/BotrdTR2xQ9UuVj.png property=og:image><meta content=https://s2.loli.net/2024/01/01/toNv8YDawXhpMqS.png property=og:image><meta content=https://s2.loli.net/2024/01/01/pgidY1Gzk3vDPAa.png property=og:image><meta content=https://s2.loli.net/2024/01/01/DuRbMClOPc6a9UL.png property=og:image><meta content=https://s2.loli.net/2024/01/01/sySo9kjwteduihm.png property=og:image><meta content=https://i.imgur.com/gPMcBei.png property=og:image><meta content=https://i.imgur.com/7ILcxnF.png property=og:image><meta content=2023-10-30T08:19:04.000Z property=article:published_time><meta content=2024-01-01T14:14:04.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="Deep Learning" property=article:tag><meta content="3D Object Detection" property=article:tag><meta content=summary name=twitter:card><meta content=https://i.imgur.com/f2UTqxK.png name=twitter:image><link href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>3D Object Detection Learning | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>3D Object Detection Learning</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-10-30 16:19:04" datetime=2023-10-30T16:19:04+08:00>2023-10-30</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-01-01 22:14:04" datetime=2024-01-01T22:14:04+08:00 itemprop=dateModified>2024-01-01</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>24k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>22 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><div class=post-tags><a href=/tags/Deep-Learning/ rel=tag># Deep Learning</a><a href=/tags/3D-Object-Detection/ rel=tag># 3D Object Detection</a></div><p>从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文,涉及到自动驾驶协同感知.</p><span id=more></span><p>先看几篇论文.<p>anchor-free detection,脱离了SSD,RetinaNet以及YOLO的anchor-based的工作。<h2 id=PointNet><a class=headerlink href=#PointNet title=PointNet></a>PointNet</h2><h3 id=abs><a class=headerlink href=#abs title=abs></a>abs</h3><p>点云是一种重要的几何数据结构。<strong>由于其不规则的格式，大多数研究人员将此类数据转换为规则的 3D 体素网格或图像集合</strong>。但是<strong>，这会使数据变得不必要地宽松(voluminous)并导致问题</strong>。在本文中，我们设计了一种新型的神经网络，它<strong>直接使用点云，它很好地尊重了输入中点的排列不变性</strong>。<h3 id=intro><a class=headerlink href=#intro title=intro></a>intro</h3><p>典型的卷积架构需要高度规则的输入数据格式，如图像网格或 3D 体素格式，以便执行权重共享和其他内核优化。由于点云或网格不是常规格式，因此大多数研究人员通常会将此类数据转换为常规的 3D 体素网格或图像集合（例如视图），然后再将它们馈送到深度网络架构。<p>然而，这种<strong>数据表示转换使生成的数据变得不必要地大量，同时还引入了量化伪影，这些伪影可能会掩盖数据的自然不变性</strong>(renders the resulting data unnecessarily voluminous — while also introducing quantization artifacts that can obscure natural invariances of the data.)。<p>点云是简单而统一的结构，避免了网格的组合不规则性和复杂性，因此更容易学习。<p>PointNet 是一个统一的架构，<strong>它直接将点云作为输入,并输出整个输入的类标签或输入的每个点段/部分标签</strong>。在基本设置中<strong>，每个点仅由其三个坐标（x、y、z）表示。可以通过计算法线和其他局部或全局特征来添加其他维度。</strong><p>我们方法的<strong>关键是使用单个对称函数，即最大池化</strong>。实际上，网络学习了一组优化函数/标准，这些函数/标准选择点云中信息丰富的点，并对其选择的原因进行编码。<p>网络的<strong>最终全连接层将这些学习到的最优值聚合到整个形状的全局描述符中</strong>，<p>输入格式很容易应用刚性或仿射变换，因为每个点都是独立变换的。因此，我们可以添加一个依赖于数据的<strong>空间转换器网络</strong>，<strong>该网络在PointNet处理数据之前尝试对数据进行规范化</strong>，从而进一步改善结果。<p>我们的网络学<strong>习通过一组稀疏的关键点来总结输入点云，这大致对应于根据可视化对象的骨架</strong>。<p>点云的大多数现有功能都是针对特定任务手工制作的。<strong>点特征通常对点的某些统计属性进行编码，并被设计为对某些变换不变，这些变换通常被归类为内在或外在</strong> 。它们还可以<strong>分为局部要素和全局要素</strong>。对于特定任务，找到最佳特征组合并非易事。<p>3D 数据具有多种流行的表示形式，从而产生了各种学习方法。体积 CNN是将 3D 卷积神经网络应用于体素化形状的先驱。然而，由于数据稀疏性和三维卷积的计算成本，体积表示受到其分辨率的限制。<p>我们设计了一个深度学习框架，直接使用无序点集作为输入。点云表示为一组 3D 点 {Pi| i = 1， …， n}，其中<strong>每个点 Pi 是其 （x， y， z） 坐标加上额外的特征通道（如颜色、法线等）的向量。为了简单明了起见，除非另有说明，否则我们仅使用 （x， y， z） 坐标作为点的通道</strong><p>对于对象分类任务，<strong>输入点云要么直接从形状中采样，要么从场景点云中预先分割</strong>。<p>对于语义分割，<strong>输入可以是用于部分区域分割的单个对象</strong>，也可以是用<strong>于对象区域分割的 3D 场景中的子体积</strong><p><img alt=image-20231225102852445 data-src=https://i.imgur.com/f2UTqxK.png><p>我们的网络有三个关键模块：<strong>最大池化层作为聚合所有点信息的对称函数</strong>,<strong>局部和全局信息组合结构</strong>,以及<strong>两个对齐输入点和点特征的联合对齐网络</strong>。<p><strong>Symmetry Function for Unordered Input</strong><p>为了使模型对输入排列不变，存在三种策略：1）将输入排序为规范顺序;<p>2）将输入视为训练RNN的序列，但通过各种排列来增强训练数据;<p>3）使用简单的对称函数来聚合每个点的信息。<p>对称函数将 n 个向量作为输入，并输出一个与输入顺序不变的新向量。<p>虽然排序听起来像是一个简单的解决方案，但在高维空间中，实际上并不存在一般意义<p>上的稳定的点扰动排序。<p>我们的想法是通过<strong>对集合中的变换元素应用对称函数来近似在点集上定义的一般函数</strong></p><script type="math/tex; mode=display">
\begin{equation}f(\{x_1,\ldots,x_n\})\approx g(h(x_1)\ldots,h(x_n)),\end{equation}</script><p>从经验上讲，我们的基本模块非常简单：<strong>通过多层感知器网络来近似 h</strong>,<strong>通过单个变量函数和最大池化函数的组合来近似 g</strong>.实验发现这效果很好。通过 h 的集合,我们可以学习多个 f 来捕获集合的不同性质.<p><strong>Local and Global Information Aggregation</strong><p>上一节的输出形成一个向量 [f1， . . . ， fK ]，它是输入集的全局签名。我们可以轻松地在形状全局特征上训练 SVM 或多层感知器分类器进行分类。但是，点<strong>分割需要结合本地和全局知识。</strong><p>在计算出全局点云特征向量后，我们通过将<strong>全局特征与每个点特征连接起来，将其反馈给每个点的特征。然后，我们根据组合的点特征提取新的每点特征</strong> - 这一次，每点特征同时识别局部和全局信息。<p><strong>Joint Alignment Network</strong><p>如果点云经历某些几何变换（例如刚性变换），则点云的语义标记必须是不变的。因此，我们期望点集的学习表示对于这些变换是不变的。<strong>一个自然的解决方案是在特征提取之前将所有输入集对齐到规范空间</strong>。Jaderberg等介绍了空间变换器的概念，通过采样和插值来对齐2D图像，这是通过在GPU上实现的专门定制层实现的。<blockquote><p>点云数据所代表的<strong>目标</strong>对某些空间转换应该具有不变性，如旋转和平移等刚体变换</blockquote><p>与相比,我们的点云输入形式使我们能够以更简单的方式实现这一目标。我们不需要发明任何新图层，也没有像图像案例那样引入别名。<p>通过一个微型网络<strong>预测一个仿射变换矩阵，并将该变换直接应用于输入点的坐标</strong>。小网络本身类似于大网络，由点无关特征提取、最大池化和全连接层等基础模块组成。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>STN3d</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, channel</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(STN3d, self).__init__()</span><br><span class=line>        self.conv1 = torch.nn.Conv1d(channel, <span class=number>64</span>, <span class=number>1</span>)</span><br><span class=line>        self.conv2 = torch.nn.Conv1d(<span class=number>64</span>, <span class=number>128</span>, <span class=number>1</span>)</span><br><span class=line>        self.conv3 = torch.nn.Conv1d(<span class=number>128</span>, <span class=number>1024</span>, <span class=number>1</span>)</span><br><span class=line>        self.fc1 = nn.Linear(<span class=number>1024</span>, <span class=number>512</span>)</span><br><span class=line>        self.fc2 = nn.Linear(<span class=number>512</span>, <span class=number>256</span>)</span><br><span class=line>        self.fc3 = nn.Linear(<span class=number>256</span>, <span class=number>9</span>)</span><br><span class=line>        self.relu = nn.ReLU()</span><br><span class=line></span><br><span class=line>        self.bn1 = nn.BatchNorm1d(<span class=number>64</span>)</span><br><span class=line>        self.bn2 = nn.BatchNorm1d(<span class=number>128</span>)</span><br><span class=line>        self.bn3 = nn.BatchNorm1d(<span class=number>1024</span>)</span><br><span class=line>        self.bn4 = nn.BatchNorm1d(<span class=number>512</span>)</span><br><span class=line>        self.bn5 = nn.BatchNorm1d(<span class=number>256</span>)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x</span>):</span></span><br><span class=line>        batchsize = x.size()[<span class=number>0</span>] <span class=comment># shape (batch_size,3,point_nums)</span></span><br><span class=line>        x = F.relu(self.bn1(self.conv1(x))) <span class=comment># shape (batch_size,64,point_nums)</span></span><br><span class=line>        x = F.relu(self.bn2(self.conv2(x))) <span class=comment># shape (batch_size,128,point_nums)</span></span><br><span class=line>        x = F.relu(self.bn3(self.conv3(x))) <span class=comment># shape (batch_size,1024,point_nums)</span></span><br><span class=line>        x = torch.<span class=built_in>max</span>(x, <span class=number>2</span>, keepdim=<span class=literal>True</span>)[<span class=number>0</span>] <span class=comment># shape (batch_size,1024,1)</span></span><br><span class=line>        x = x.view(-<span class=number>1</span>, <span class=number>1024</span>) <span class=comment># shape (batch_size,1024)</span></span><br><span class=line></span><br><span class=line>        x = F.relu(self.bn4(self.fc1(x))) <span class=comment># shape (batch_size,512)</span></span><br><span class=line>        x = F.relu(self.bn5(self.fc2(x))) <span class=comment># shape (batch_size,256)</span></span><br><span class=line>        x = self.fc3(x) <span class=comment># shape (batch_size,9)</span></span><br><span class=line></span><br><span class=line>        iden = Variable(torch.from_numpy(np.array([<span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>1</span>]).astype(np.float32))).view(<span class=number>1</span>, <span class=number>9</span>).repeat(</span><br><span class=line>            batchsize, <span class=number>1</span>) <span class=comment># # shape (batch_size,9)</span></span><br><span class=line>        <span class=keyword>if</span> x.is_cuda:</span><br><span class=line>            iden = iden.cuda()</span><br><span class=line>        <span class=comment># that's the same thing as adding a diagonal matrix(full 1)</span></span><br><span class=line>        x = x + iden <span class=comment># iden means that add the input-self</span></span><br><span class=line>        x = x.view(-<span class=number>1</span>, <span class=number>3</span>, <span class=number>3</span>) <span class=comment># shape (batch_size,3,3)</span></span><br><span class=line>        <span class=keyword>return</span> x</span><br></pre></table></figure><h2 id=PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space><a title="PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space" class=headerlink href=#PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space></a>PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</h2><p>之前很少有研究点集上的深度学习。PointNet 是这个方向的先驱。然而，根据设计，<strong>PointNet 不会捕获由度量空间点所在的局部结构引起的局部结构，这限制了其识别细粒度模式的能力和对复杂场景的泛化性。</strong><p>在这项工作中，我们引入了一个分层神经网络，该神经网络将PointNet递归应用于输入点集的嵌套分区。通过利用度量空间距离，我们的网络能够学习具有越来越大的上下文尺度的局部特征。随着进一步观察<strong>点集通常以不同的密度进行采样，这导致在均匀密度下训练的网络的性能大大降低</strong>，<strong>我们提出了新的集合学习层来自适应地组合来自多个尺度的特征</strong>。<p><img alt=image-20231225105720739 data-src=https://i.imgur.com/Q6l2kcO.png><p>虽然 PointNet 使用单个最大池化操作来聚合整个点集，但我们的新架构<strong>构建了点的分层分组，并沿着层次结构逐步抽象出越来越大的局部区域</strong>。<p>在每个级别上，都会对一组点进行处理和抽象，以生成具有较少元素的新集合。集合抽象层由三个关键层组成：<strong>采样层、分组层和 PointNet 层</strong>。<p><strong>采样层从输入点中选择一组点，用于定义局部区域的质心</strong>。然后<strong>，分组层通过查找质心周围的“相邻”点来构建局部区域集</strong>。PointNet 层使用微型 PointNet 将局部区域模式编码为特征向量。<p><strong>Sampling layer.</strong>给定输入点 {x1， x2， …， xn}，<strong>使用迭代最远点采样</strong> （FPS） 来选择点 {xi1 ， xi2 ， …， xim } 的子集，<strong>使得 xij 是相对于其余点与集合 {xi1 ， xi2 ， …， xij−1 } 最远的点</strong>）。<strong>与随机采样相比，在质心数量相同的情况下，它对整个点集的覆盖率更高</strong>。<strong>与扫描数据分布的向量空间无关的 CNN 相比，我们的采样策略以数据依赖的方式生成感受野。</strong><p><strong>Grouping layer.</strong>该层的输入是大小为 N × （d + C） 的点集和大小为 N ′ × d 的一组质心的坐标。<strong>输出是大小为 N ′ × K × （d + C） 的点集组</strong>，其中<strong>每组对应一个局部区域，K 是质心点邻域中的点数</strong>。请注意，K 因组而异，<strong>后续的 PointNet 层能够将灵活数量的点转换为固定长度的局部区域特征向量。</strong><p><strong>PointNet layer：</strong>在该层中,输入是数据大小为 N ′×K ×（d+C） 的点的 N ′ 局部区域。输出中的<strong>每个局部区域都由其质心和编码质心邻域的局部特征抽象</strong>。输出数据大小为 N ′ × （d + C′）。<h3 id=Robust-Feature-Learning-under-Non-Uniform-Sampling-Density><a title="Robust Feature Learning under Non-Uniform Sampling Density" class=headerlink href=#Robust-Feature-Learning-under-Non-Uniform-Sampling-Density></a>Robust Feature Learning under Non-Uniform Sampling Density</h3><p><strong>点集在不同区域具有不均匀的密度是很常见的。这种不均匀性给点集特征学习带来了重大挑战。在密集数据中学习的特征可能无法泛化到稀疏采样区域</strong>。因此，针对稀疏点云训练的模型可能无法识别细粒度的局部结构。<p>理想情况下，我们希望尽可能仔细地检查到一个点集，<strong>以捕获密集采样区域中最精细的细节</strong>。但是，在低密度区域禁止进行这种仔细检查，因为局部模式可能会因采样缺陷而损坏。在这种情况下，我们应该在更近的地方寻找更大尺度的模式。为了实现这一目标，我们提出了密度自适应PointNet层，<strong>当输入采样密度发生变化时，该层可以学习组合来自不同尺度区域的特征</strong>。<p>提出了MSG和MRG.<blockquote><p>对方法MSG而言，是对<strong>不同半径的子区域</strong>进行特征提取后进行<strong>特征堆叠</strong>，特征提取过程还是采用了PointNet<p>作者是考虑到上述的MSG方法<strong>计算量太大</strong>，提出来备选方案MRG。MRG用两个Pointnet对连续的两层分别做<strong>特征提取与聚合</strong>，然后再进行特征拼接</blockquote><p><img alt=image-20231225115753620 data-src=https://i.imgur.com/fFcXqBb.png><h2 id=Objects-as-Points-2019><a title="Objects as Points  2019" class=headerlink href=#Objects-as-Points-2019></a>Objects as Points 2019</h2><h3 id=Abs><a class=headerlink href=#Abs title=Abs></a>Abs</h3><p>检测是将图像中的物体识别为轴对齐的方框。大多数成功的物体检测器都会枚举几乎所有潜在的物体位置，并对每个位置进行分类。这不仅浪费资源、效率低下，还需要额外的后期处理。在本文中，我们采用了一种不同的方法。我们将物体建模为一个点—其边界框的中心点。<p>我们的检测器<strong>使用关键点估算来寻找中心点</strong>，并<strong>对所有其他物体属性进行回归，如大小、三维位置、方向甚至姿态</strong>。与相应的基于边界框的检测器相比，我们基于中心点的方法 CenterNet 是端到端可微分的，更简单、更快速、更准确。<h3 id=前置知识><a class=headerlink href=#前置知识 title=前置知识></a>前置知识</h3><p><img style="zoom: 67%;" alt=image-20231222110245995 data-src=https://i.imgur.com/JWpC9qY.png><p>使用中心点作为预测结果,输出是一个热力图.假设 I∈ R^W×H×3^ 是宽度为 W、高度为 H 的输入图像，我们的目标是生成一个关键点热图 ˆ Y∈ [0, 1] ^W/R×H/R×C^,R 是输出跨度，C 是关键点类型的数量(就是类别).使用R = 4 的默认输出跨度。输出步长对输出预测进行下采样.预测值 ˆ Y = 1 对应于检测到的关键点，而 ˆ Y= 0 则是背景。<blockquote><p>论文中使用几种不同的全卷积编码器-解码器网络来预测图像 I 中的ˆY：堆叠沙漏网络、上卷积残差网络（ResNet）和深层聚合（DLA）。</blockquote><p>对于gtbox中的每个中心(也就是keypoint)会计算出一个低分辨率等效点<img alt=image-20231201151345184 data-src=https://s2.loli.net/2023/12/01/OZL5Fz6Vi7mEcvK.png><p>因为预测的输出坐标是经过四倍下采样的,然后利用这个真值通过一个高斯核函数拼接到热图上.我们知道预测的输出是在0-1之间的,而且大小是W/R×H/R×C,利用这个核函数计算每个下采样后的关键点在热力图上的值.其中，σ 是与物体大小相适应的标准偏差，如果同一类别的两个高斯重叠，我们取元素最大值<p><img alt=image-20231201151511596 data-src=https://s2.loli.net/2023/12/01/t5BC7l1IqMNKDbf.png><p>损失使用RetinaNet提出的Focal损失变型.主要是得到预测的中心位置,ground truth没有直接使用,而是使用一个高斯核将不是中心的点的值设置为(0-1),相当于更好地优化了.从简单的0-1到离散值.<p><img alt=image-20231201151714656 data-src=https://s2.loli.net/2023/12/01/q59OprywgN2Ehsk.png><p>此外,因为需要恢复输出跨距造成的离散化误差,还添加了损失.<p>为每个中心点预测一个局部偏移量 ˆ O∈ R^W/R×H/RX2^。所有类别 c 共享相同的偏移预测,由于输入是一张图像,通过backbone(论文中的是ResNet和DLA)得到downsampling之后的feature map(原文叫heat map)<p><img alt=image-20231201152014024 data-src=https://s2.loli.net/2023/12/01/GTviaIdYJ9gFA6Z.png><p>由此得到了物体的中心点,接下来需要回归得到尺寸.<blockquote><p>我们使用关键点估计器 ˆ Y 来预测所有中心点。此外，我们对每个对象 k 的对象尺寸 sk = (x(k) 2 - x(k) 1 , y(k) 2 - y(k) 1 ) 进行回归。</blockquote><script type="math/tex; mode=display">
\begin{equation}L_{size}=\frac1N\sum_{k=1}^N\left|\hat{S}_{p_k}-s_k\right|.\end{equation}</script><script type="math/tex; mode=display">
L_{det}=L_k+\lambda_{size}L_{size}+\lambda_{off}L_{off}.</script><p>对于3D目标检测,还需要得到深度、三维空间和方向。会为每个输出添加一个单独的头部。<p>深度:深度 d 是每个中心点的<strong>单一标量</strong>。然而，深度很难直接回归。我们使用 Eigen 等人的输出变换和 d = 1/σ( ˆ d) - 1，其中 σ 是 sigmoid 函数。我们将深度作为关键点估计器的附加输出通道 ˆ D∈[0, 1] W R ×H R 来计算。<p>物体的三维尺寸是三个标量。使用单独的头</p><script type="math/tex; mode=display">
\begin{equation}\hat{\Gamma}\in\mathcal{R}^{\frac WR\times\frac HR\times3}\end{equation}</script><p>和 L1 损失直接回归到它们的绝对值（以米为单位）。<p>默认情况下，方向是一个单一标量。但是，很难对其进行回归。效仿 Mousavian 等人的研究，将方向表示为两个bins，并进行bins内回归。具体来说，bin使用 8 个标量编码，每个bin有 4 个标量。对于一个bins，两个标量用于softmax，其余两个标量在每个分区内回归到一个angle。<h2 id=Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation><a title="Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation" class=headerlink href=#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation></a>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</h2><p><a href=http://arxiv.org/abs/2111.09515 rel=noopener target=_blank>http://arxiv.org/abs/2111.09515</a><h3 id=Abs-1><a class=headerlink href=#Abs-1 title=Abs></a>Abs</h3><p>近年来，用于自动驾驶的激光雷达数据三维物体检测技术取得了长足进步,在最先进的方法中，将<strong>点云编码成鸟瞰图</strong>（BEV,bird’s eye view）已被证明是既有效又高效的方法。<strong>与透视图(perspective views)不同，鸟瞰图保留了物体之间丰富的空间和距离信息</strong>。然而,在 BEV 中,虽然<strong>同类型的远距离物体看起来并不更小</strong>,但它们<strong>包含的点云特征却更稀疏</strong>。这一事实<strong>削弱了使用共享权重卷积神经网络（CNN）提取 BEV 特征的能力</strong>.<p>为了应对这一挑战,我们提出了范围感知注意力网络 (RAANet),它能提取有效的 BEV 特征并生成出色的 3D object detection 输出.<p>范围感知注意力（RAA）卷积显著改善了<strong>对远近物体的特征提取</strong>。<p>此外，我们还提出了一<strong>种用于点密度估计</strong>(point density estimation)的新型辅助损失，以进一步<strong>提高 RAANet 对遮挡物体的检测精</strong>度。值得注意的是，我们提出的 RAA 卷积是轻量级的,可以集成到任何用于检测 BEV 的 CNN 架构中.<p>在 <strong>nuScenes 和 KITTI 数据集上</strong>进行的大量实验表明，在基于激光雷达(LiDAR-based 3D object detection)的三维物体检测方面，我们提出的方法优于最先进的方法，在 nuScenes 激光雷达帧上进行的测试中，完整版的实时推理速度为 16 Hz，精简版为 22 Hz。<h3 id=Intro><a class=headerlink href=#Intro title=Intro></a>Intro</h3><p>随着处理单元的快速改进，得益于深度神经网络的成功，自动驾驶的感知能力近年来得到了蓬勃发展。通过<strong>激光雷达传感器进行 3D 物体检测</strong>是自动驾驶的重要功能之一。<p>早期的研究采用了三维卷积神经网络（CNN），这种网络处理速度慢，内存需求大。<p>为了降低内存要求并提供实时处理，最近的方法利用了体素化(voxelization)和鸟瞰投影（BEV）。<p>体素化(Voxelization)作为三维点云(3D point clouds)的一种预处理方法得到了广泛应用，因为<strong>结构更合理的数据可提高计算效率和性能精度</strong>。<p>一般来说，体素化将点云划分为均匀分布的体素网格，然后将三维激光雷达点分配到各自的体素上。输出空间保留了物体之间的欧氏距离，并避免了边界框的重叠。<p>这些特点使得<strong>无论物体与激光雷达的距离如何，都能将物体的尺寸变化控制在一个相对较小的范围内</strong>，从而有<strong>利于在训练过程中进行形状回归</strong>。<p>在本文中，我们提出了距离感知注意力网络（RAANet），其中包含新型的范围感知注意力卷积层（RAAConv），设计<strong>用于LiDAR BEV的目标检测</strong>。RAAConv 由两个独立的卷积分支和注意力图组成,对输入特征图的位置信息敏感.<p>我们的方法受到BEV图像特性的启发，<strong>随着物体和自我车辆之间距离的增加，点变得越来越稀疏</strong>。<strong>理想情况下，对于BEV特征图，不同位置的元素应由不同的卷积核处理</strong>。但是，应用不同的内核会显着增加计算费用。<p>为了在BEV特征提取过程中<strong>利用位置信息，在避免繁重计算的同时，将BEV特征图视为稀疏特征和密集特征的组合</strong>。我们应用两个不同的卷积核来同时提取稀疏和密集特征。<p>每个提取的特征图的通道大小都是最终输出的一半。同时，根据输入形状生成范围和位置编码。然后，根<strong>据相应的特征图以及范围和位置编码计算每个范围感知注意力热图</strong>。最后，将<strong>注意力热图应用于特征图以增强特征表示</strong>。从两个分支生成的特征图按通道concat为 RAAConv 输出。<p>此外,遮挡的影响也不容忽视,因为同一物体在不同的遮挡量下可能具有不同的点分布。因此，我们提出了一个高效的辅助分支，称为<strong>辅助密度水平估计模块</strong>（ADLE），允许RAANet考虑遮挡。由于注释各种遮挡是一项耗时且昂贵的任务，因此我们<strong>设计了ADLE来估计每个对象的点密度水平。如果没有遮挡，则近处物体的点密度水平高于远处物体的点密度水平</strong>。<p>但是，<strong>如果附近的物体被遮挡，则其点密度水平会降低。因此，通过结合距离信息和密度水平信息，我们能够估计遮挡信息的存在</strong>。ADLE仅用于训练阶段，用于提供密度信息指导，在推理状态下可以删除，以提高计算效率。<p>主要贡献:<ol><li>我们提出了RAAConv层，它允许基于LiDAR的探测器提取更具代表性的BEV特征。此外，RAAConv 层可以集成到任何用于 LiDAR BEV 的 CNN 架构中。<li>我们提出了一种新的用于点密度估计的辅助损失，以帮助主网络学习与遮挡相关的特征。该密度水平估计器进一步提高了RAANet对被遮挡物体的检测精度。<li>我们提出了范围感知注意力网络（RAANet），它集成了前面提到的RAA和ADLE模块。RAANet通过基于ground truth生成各向异性(anistropic)高斯热图，进一步优化，</ol><h3 id=相关工作><a class=headerlink href=#相关工作 title=相关工作></a>相关工作</h3><p>大多数目标检测工作可以分为两大类：有锚点和无锚点的目标检测。此外，在早期阶段存在对点云数据进行编码的工作]，但它们超出了目标检测网络重构的范围。<h4 id=Object-detection-with-anchors><a title="Object detection with anchors" class=headerlink href=#Object-detection-with-anchors></a>Object detection with anchors</h4><p>固定形状的锚回归方法，以便可以提取中间特征<p>two-stage:RCNN家族<p>one-stage:YOLO,Retinanet,SSD<p>YOLO:将目标检测重新定义为单一回归问题，该问题采用端到端神经网络进行单次前向传播来检测目标<p>SSD:Liu等开发了一种多分辨率锚点技术，用于检测尺度混合物的物体，并在一定程度上学习偏移量，而不是学习锚点。<p>RetinaNet:Lin等提出了一种焦点损失，以解决密集和小目标检测问题，同时处理类不平衡和不一致。<p>Zhou和Tuzel(VoxelNet)以及Lang等(PointPillars)提出了用于点云的神经网络，这为3D检测任务开辟了新的可能性。<h4 id=Object-detection-without-anchors><a title="Object detection without anchors" class=headerlink href=#Object-detection-without-anchors></a>Object detection without anchors</h4><p>为了解决锚点回归带来的计算开销和超参数冗余问题，并有效地处理点云编码，无锚点目标检测已在许多工作中得到应用。无锚点目标检测可分为两大类，即<strong>基于中心的方法和基于关键点的方法</strong>。<p>基于中心的方法:在这种方法中，对象的中心点用于定义正样本和负样本，而不是IoU。该方法通过预测从正样本到物体边界的四个距离来生成边界框，从而大大降低了计算成本。<p>基于关键点的方法:通过几个预定义的方法或自学习模型定位关键点，然后生成边界框来对对象进行分类。<p>为了提取具有代表性的特征，我们重点关注两个主要组成部分：<strong>范围感知特征提取</strong>和<strong>遮挡监督</strong>。<p>我们提出的范围感知注意力网络（RAANet）的主要架构如图所示<p><img alt=image-20231119161935219 data-src=https://i.imgur.com/GERZSZ7.png><p>我们结合了CenterNet的思想来构建一个无锚探测器，并引入了两个新颖的模块：距离感知注意力卷积层（RAAConv）和辅助密度级估计模块（ADLE）。<p>区域建议网络 （RPN） 将该 BEV 特征图作为输入，并使用多个下采样和上采样模块来生成高维特征图。<p>除了主要任务中的检测头外，我们还提出了一个辅助任务，用于点密度水平估计，以实现更好的检测性能。<p>RAAConv 首先利用两组卷积核来提取每个分支的中间特征图。<p>然后，将热图 fa 和 fb 分别乘以可学习标量 γa 和 γb。γa 和 γb 初始化为 1.0，并在训练过程中逐渐学习它们的值<h2 id=VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017><a title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017" class=headerlink href=#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017></a>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017</h2><h3 id=abs-1><a class=headerlink href=#abs-1 title=abs></a>abs</h3><p>准确检测三维点云中的物体是自主导航、看家机器人和增强/虚拟现实等许多应用中的核心问题。点云数据 高度稀疏<p>为了将高度稀疏的激光雷达点云与区域建议网络（RPN）连接起来，现有的大部分工作都集中在手工制作的特征表示上，例如鸟瞰投影。<p>在这项工作中，我们<strong>不再需要对三维点云进行人工特征工程，而是提出了一种通用的三维检测网络—VoxelNet，它将特征提取和边界框预测统一为一个单一阶段、端到端可训练的深度网络</strong>。<h3 id=相关工作-1><a class=headerlink href=#相关工作-1 title=相关工作></a>相关工作</h3><p>3D传感器技术的快速发展促使研究人员开发有效的表示来<strong>检测和定位点云中的物体</strong>,当有丰富而详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。<p>然而，它们<strong>无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性</strong>，导致自主导航等不受控制的场景的成功有限。<p>鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框,然而，基于图像的三维检测方法的精度受深度估计精度的限制。<h3 id=网络结构><a class=headerlink href=#网络结构 title=网络结构></a>网络结构</h3><p>所提出的VoxelNet由三个功能块组成：（1）特征学习网络(Feature learning network)，（2）卷积中间层(Convolutional middle layers)，（3）区域建议网络(Region proposal network)。<h3 id=Feature-learning-network><a title="Feature learning network" class=headerlink href=#Feature-learning-network></a>Feature learning network</h3><p>Voxel Partition<p><img alt=image-20231119112120958 data-src=https://i.imgur.com/dhgI1JD.png><p>Stacked Voxel Feature Encoding<p>用V表示一个体素(Voxel),<h3 id=RPN><a class=headerlink href=#RPN title=RPN></a>RPN</h3><p><img alt=image-20231119112203016 data-src=https://i.imgur.com/wvQa3jb.png><p>RPN层有两个分支，一个用来输出类别的概率分布（通常叫做Score Map），一个用来输出Anchor到真实框的变化过程（通常叫做 Regression Map）<blockquote><p>注意这里论文是直接输出预测的anchor box的坐标而不是修正值.</blockquote><h4 id=高效实现><a class=headerlink href=#高效实现 title=高效实现></a>高效实现</h4><p>我们初始化一个 K × T × 7 维张量结构来<strong>存储体素输入特征缓冲区</strong>，其中 <strong>K 是非空体素的最大数量，T 是每个体素的最大点数，7 是每个点的输入编码维度</strong>。<p>这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。<h3 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h3><p><img alt=image-20231129221227758 data-src=https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png><p>da = √(la)2 + (wa)2 是anchor box的对角线。<p><img alt=image-20231129221345941 data-src=https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png><p>ui ∈ R^7^ 和 u∗ i ∈ R^7^ 分别是正锚点 a^pos^ ~i~ 的回归输出和地面实况。<h2 id=Center-based-3D-Object-Detection-and-Tracking><a title="Center-based 3D Object Detection and Tracking" class=headerlink href=#Center-based-3D-Object-Detection-and-Tracking></a>Center-based 3D Object Detection and Tracking</h2><h3 id=Introduction><a class=headerlink href=#Introduction title=Introduction></a>Introduction</h3><p>与研究透彻的二维检测问题相比，点云上的三维检测提出了一系列有趣的挑战.点云稀疏，<strong>三维空间的大部分区域都没有测量值</strong>,其次，<strong>输出结果是一个三维方框，通常无法与任何全局坐标框架很好地对齐</strong>。第三，<strong>三维物体有多种尺寸、形状和长宽比</strong>，例如，在交通领域，自行车接近平面，公共汽车和豪华轿车细长，行人高大。<p><strong>二维和三维检测之间的这些显著差异使得这两个领域之间的理念转换变得更加困难。问题的关键在于，轴对齐的二维方框 并不能代表自由形态的三维物体</strong><p>一种解决方案可能是为每个物体方向分类不同的模板（锚，<strong>但这不必要地增加了计算负担，并可能带来大量潜在的假阳性检测</strong>。我们认为，<strong>将二维和三维领域连接起来的主要挑战在于物体的这种表现形式</strong>。<p>然后，它将这一表示法<strong>扁平化为俯视地图视图，并使用标准的基于图像的关键点检测器来查找对象中心</strong>，对于每个检测到的中心点，它会<strong>根据中心点位置的点特征回归到所有其他物体属性，如三维尺寸、方向和速度</strong>。<p>​ 基于中心的表示法有几个主要优点：首先，<strong>与边界框不同，点没有固有方向。这大大缩小了物体检测器的搜索空间</strong>，同时<strong>允许骨干学习物体的旋转不变性和相对旋转的旋转等差性</strong>。其次，基于中心的表示法<strong>简化了追踪等下游任务</strong>。如果物体是点，小轨迹就是空间和时间中的路径。中<strong>心点可以预测连续帧之间物体的相对偏移（速度），然后将其贪婪地连接起来</strong>。第三，基于<strong>点的特征提取使我们能够设计一个有效的两阶段细化模块，其速度比以往的方法快得多</strong><p><img alt=image-20231222175607203 data-src=https://i.imgur.com/tS9TBGi.png><p>CenterPoint 的第一阶段预测特定类别的热图、物体大小、子象素位置细化、旋转和速度。所有输出均为密集预测。<h3 id=Center-heatmap-head><a title="Center heatmap head." class=headerlink href=#Center-heatmap-head></a>Center heatmap head.</h3><p>中心头的目标是在检测到的任何物体的中心位置生成一个热图峰值。该头会生成 K 个通道的热图 ˆ Y，K 个类别中的每个类别都有一个通道。<p>在训练过程中，它的目标是将注释边界框的三维中心投影到地图视图中产生的二维高斯。我们使用focal损耗<blockquote><p>自上而下地图视图中的物体比图像中的要稀疏。在地图视图中，距离是绝对的，而在图像视图中，距离会因透视而扭曲。以道路场景为例，在地图视图中，车辆所占的面积很小，但在图像视图中，几个大物体可能占据了屏幕的大部分区域</blockquote><p>采用 CenterNet的标准监督方式会导致监督信号非常稀疏，大多数位置都被视为背景。为了解决这个问题，我们通过<strong>扩大每个地面实况对象中心的高斯峰值，来增加目标热图 Y 的正向监督</strong>。<h3 id=Regression-heads><a title="Regression heads" class=headerlink href=#Regression-heads></a>Regression heads</h3><p>在物体的中心特征处存储了几个物体属性：<strong>子象素位置细化</strong>(sub-voxel) o∈R2、<strong>离地高度</strong> hg∈R、<strong>三维尺寸</strong>(3D dimension)s∈R3，<strong>以及偏航旋转角度</strong>（sin(α), cos(α)）∈R2。<p>子体素位置细化 o 可减少主干网络体素化和跨距造成的量化误差<p>地面高度 hg 可帮助定位三维物体，并补充地图视图投影中缺失的高程信息。<p>方位预测使用偏航角的正弦和余弦作为连续回归目标。<p>结合方框大小，这些回归头可提供三维边界框的全部状态信息。每个输出都使用自己的回归头。在训练时，只使用 L1 回归损失对地面实况中心进行监督。<h3 id=Two-Stage-CenterPoint><a title="Two-Stage CenterPoint" class=headerlink href=#Two-Stage-CenterPoint></a>Two-Stage CenterPoint</h3><p>第二阶段从骨干网的输出中提取额外的点特征。<p>我们从预测边界框的每个面的三维中心提取一个点特征。请注意，边界框中心、顶面和底面中心在地图视图中都投影到同一个点。<p>因此，我们只考虑四个朝外的方框面和预测的物体中心。对于每个点，我们使用双线性插值法从骨干地图视图输出 M 中提取特征。<p>第二阶段在单阶段 CenterPoint 预测结果的基础上，预测与类别无关的置信度得分和box refinement。<p>对于不区分类别的置信度得分预测，遵循的方法，使用得分目标 I，该目标由方框的 3D IoU 和相应的地面实况边界方框引导</p><script type="math/tex; mode=display">
\begin{equation}I=\min(1,\max(0,2\times IoU_t-0.5))\end{equation}</script><p>IoUt 是第 t 个建议框与gt bbox之间的 IoU</p><script type="math/tex; mode=display">
\begin{equation}L_{score}=-I_t\log(\hat{I}_t)-(1-I_t)\log(1-\hat{I}_t)\end{equation}</script><p>在推理过程中，<strong>直接使用单阶段中心点的类别预测，并以两个分数的几何平均值计算最终置信度分数</strong></p><script type="math/tex; mode=display">
\begin{equation}\hat{Q_t}=\sqrt{\hat{Y_t}*\hat{I_t}}\end{equation}</script><p>其中 ˆ Qt 是对象 t 的最终预测置信度，ˆ Yt = max0≤k≤K ˆ Yp,k 和 ˆ It 分别是对象 t 的第一阶段和第二阶段置信度。<p>对于<strong>bbox回归</strong>，模型<strong>在第一阶段建议的基础上预测细化，用 L1 损失来训练模</strong>型。<h2 id=SECOND-Sparsely-Embedded-Convolutional-Detection-2018><a title="SECOND: Sparsely Embedded Convolutional Detection 2018" class=headerlink href=#SECOND-Sparsely-Embedded-Convolutional-Detection-2018></a>SECOND: Sparsely Embedded Convolutional Detection 2018</h2><h1 id=PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018><a title="PointPillars: Fast Encoders for Object Detection from Point Clouds 2018" class=headerlink href=#PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018></a>PointPillars: Fast Encoders for Object Detection from Point Clouds 2018</h1><h1 id=PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019><a title="PIXOR: Real-time 3D Object Detection from Point Clouds 2019" class=headerlink href=#PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019></a>PIXOR: Real-time 3D Object Detection from Point Clouds 2019</h1><h2 id=Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021><a title="Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021" class=headerlink href=#Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021></a>Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021</h2><h2 id=CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021><a title="CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021" class=headerlink href=#CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021></a>CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021</h2><h2 id=协同感知-3D检测任务><a title="协同感知 3D检测任务" class=headerlink href=#协同感知-3D检测任务></a>协同感知 3D检测任务</h2><p>综述<h3 id=Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges><a title="Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges" class=headerlink href=#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges></a>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</h3><p>协作感知对于解决自动驾驶中的遮挡和传感器故障问题至关重要。<p>自动驾驶感知可分为<strong>个体感知和协作感知</strong>。虽然个体感知随着深度学习的发展取得了长足的进步，但一些问题也限制了其发展。首先，<strong>个体感知在感知相对全面的环境时经常会遇到遮挡</strong>。其次，<strong>车载传感器在感知远处物体时存在物理限制</strong>。此外，<strong>传感器噪音也会降低感知系统的性能</strong>。为了弥补个体感知的不足，协作或合作感知利用了多个代理之间的互动，受到了广泛关注。<p>协同感知是一种多agent系统，其中agent共享感知信息，以克服自我视听的视觉局限。在单个感知场景中，自我视听只能检测到附近物体的部分遮挡和远处稀疏的点云。在协作感知场景中，ego AV通过接收其他agent的信息来扩大视野。<strong>通过这种协作方式，ego AV不仅能检测到远处和被遮挡的物体，还能提高在密集区域的检测精度</strong>。<p><img alt=image-20231122203934187 data-src=https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png><p>长期以来，协作感知一直是人们关注的焦点。之前的工作专注于构建协作感知系统，以评估该技术的可行性。然而，由<strong>于缺乏大型公共数据集，它没有得到有效的推进</strong>。近年来，随着深度学习的发展和大规模协作感知数据集的公众关注和研究激增。<p><strong>考虑到通信中的带宽限制，大多数研究人员致力于设计新颖的协作模块，以实现精度和带宽之间的权衡</strong>。<p>在协作感知场景中，自我 AV 通过接收来自其他智能体的信息来扩展视野。通过这种协作方式，自我AV不仅可以检测远处和被遮挡的物体，还可以提高密集区域的检测精度。<p>为了总结这些技术和问题，我们回顾了自动驾驶中的协同感知方法，并从方法、数据集和挑战方面对近年来的进展进行了全面综述。我们还注意到近年来发表了一些关于协作感知的综述。<h3 id=Collaboration-scheme><a title="Collaboration scheme" class=headerlink href=#Collaboration-scheme></a>Collaboration scheme</h3><h4 id=早期融合><a class=headerlink href=#早期融合 title=早期融合></a>早期融合</h4><p>早期协作在网络输入端采用原始数据融合，也称为数据级或低级融合<p>因此，早期协作可以从根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效。<p>在自动驾驶场景中，自我车辆接收并转换来自其他智能体的原始传感器数据，然后聚合车载转换后的数据。原始数据包含最全面的信息和实质性的代理描述。因此，早期协作可以从<strong>根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效</strong>。<p>考虑到早期协作的高带宽，一些工作提出了中间协作感知方法来平衡性能-带宽的权衡。在中间协作中，其他智能体通常会将深层语义特征转移到自我载体。自我车辆融合特征以做出最终预测。中间协作已成为最流行的多智能体协作感知灵活性选择。然而，特征提取往往会造成信息丢失和不必要的信息冗余，这促使人们探索合适的特征选择和融合策略。<h4 id=中期><a class=headerlink href=#中期 title=中期></a>中期</h4><p>考虑到早期协作的高带宽，一些研究提出了中间协作感知方法，以平衡性能与带宽之间的权衡。在中间协作中，其他代理通常会将深层语义特征传输给自我车辆。<h3 id=晚期><a class=headerlink href=#晚期 title=晚期></a>晚期</h3><p>后期或对象级协作在网络输出端采用预测融合。每个代理单独训练网络并相互共享输出。自我车辆在空间上转换输出，并在后处理后合并所有输出。后期协作比早期和中期协作更节省带宽，也更简单。然而，后期的合作也有局限性。由于<strong>单个输出可能是嘈杂和不完整的，因此后期协作总是具有最差的感知性能</strong>。<h3 id=原始数据融合-Raw-Data-Fusion><a title="原始数据融合(Raw Data Fusion)" class=headerlink href=#原始数据融合-Raw-Data-Fusion></a>原始数据融合(Raw Data Fusion)</h3><p>早期协作在<strong>输入阶段采用原始数据融合。由于点云是不规则的，可以直接汇总</strong>，因此早期的协同工作通常采用<strong>点云融合策略</strong>。<p>第一个早期的协同感知系统 Cooper<strong>选择激光雷达数据</strong>作为融合目标。只需提取位置坐标和反射值，就能将点云压缩成较小的尺寸。在代理之间进行交互后，Cooper 利用变换矩阵重构接收到的点云，然后将自我点云集concat起来，进行最终预测。<p>受 Cooper 的启发，Coop3D 还探索了早期的协作，并引入了一种新的点云融合方法。具体来说，Coop3D 系统没有采用串联，而是利用空间变换来融合传感器数据。此外，与Cooper在车上共享车对车信息不同，Coop3D提出了一个中央系统来合并多个传感器数据，从而可以协同摊销传感器和处理成本。<h3 id=customized-communication-mechanism><a title="customized communication mechanism" class=headerlink href=#customized-communication-mechanism></a>customized communication mechanism</h3><p>早期协作中的原始数据融合拓宽了自我飞行器的视野，也<strong>造成了高带宽压力</strong>。为了缓解上述问题，<strong>越来越多的工作 发展了中间协作</strong>。<p>最初的中间协作方法<strong>遵循一种贪婪的通信机制，以获取尽可能多的信息。一般来说，它们会与通信范围内的所有代理共享信息，并将压缩后的完整特征图放入集体感知信息（CPM,collective perception message）中</strong>。然而，由于特征稀疏和代理冗余，贪婪通信可能会极大地浪费带宽。<p>Who2com 建立了首个带宽限制下的通信机制，通过三阶段握手实现。具体来说，<strong>Who2com 使用一般注意力函数计算代理之间的匹配分数，并选择最需要的代理，从而有效减少带宽</strong>。<p>在 Who2com 的基础上，When2com<strong>引入了缩放一般注意力来决定何时与他人交流</strong>。这样，自我代理只有在信息不足时才会与他人交流，从而有效地节省了协作资源。<p>除了选择合适的通信代理外，<strong>通信内容对于减少带宽压力也很重要</strong>。FPVRCNN 中提出了初始特征选择策略.具体来说，FPV-RCNN 采用检测头生成proposals，并只选择proposals中的特征点。<p><strong>关键点选择模块减少了共享深度特征的冗余，为初始proposals提供了有价值的补充信息。</strong><p>Where2comm 也提出了一种新颖的空间信心感知通信机制。其核心思想是<strong>利用空间置信度图来决定共享特征和通信目标</strong>。<strong>在特征选择阶段</strong>，<strong>Where2comm 选择并传输满足高置信度和其他agent请求的空间元素</strong>。在<strong>agent选择阶段，自我代理只与能提供所需特征的代理通信。通过发送和接收感知关键区域的特征，Where2comm 节省了大量带宽，并显著提高了协作效率</strong>。<h3 id=Feature-Fusion><a title="Feature Fusion" class=headerlink href=#Feature-Fusion></a>Feature Fusion</h3><blockquote><p>Feature fusion module is crucial in intermediate collaboration. After receiving CPMs from other agents, the ego vehicle can <strong>leverage different strategies to aggregate these features</strong>.</blockquote><p>可行的融合策略能够捕捉特征之间的潜在关系，提高感知网络的性能。根据基于特征融合的思想，我们将现有的特征融合方法分为传统融合、基于图的融合和基于注意力的融合。<h4 id=传统融合><a class=headerlink href=#传统融合 title=传统融合></a>传统融合</h4><p>在协同感知研究的早期阶段，研究人员倾向于使用传统的策略来融合特征，如concat、求和和线性加权。中级协作将这些不变的置换操作应用于深度特征，因其简单性而实现了快速推理。<p>第一个中间协同感知框架 FCooper<strong>提取了低级体素和深度空间特征</strong>。基于这两级特征，F-Cooper 提出了两种特征融合策略：<strong>体素特征融合</strong>（VFF）和<strong>空间特征融合</strong>（SFF）。<p>这两种方法都采用<strong>元素最大值（element-wise maxout）来融合重叠区域的特征</strong>。由于<strong>体素特征更接近原始数据，因此 VFF 与原始数据融合方法一样能够进行近距离物体检测</strong>。同时，SFF 也有其优势。<p>受 SENet的启发，SFF 选择选择部分信道来减少传输时间消耗，同时保持可比的检测精度<p>考虑到 F-Coope<strong>r忽略了低置信度特征的重要性</strong>，Guo 等人提出了 <strong>CoFF</strong> 来改进 F-Cooper。<strong>CoFF 通过测量重叠特征的相似度和重叠面积对其进行加权。相似度越小，距离越大，邻近特征提供的补充信息就越直观。</strong><p>此外，还添加了一个增强参数，以提高弱特征的值。<p>实验表明，简单而高效的设计使 CoFF 大大提高了 F-Cooper 的性能。<p>传统的融合方法虽然简单,但并没有被最近的方法所抛弃。Hu 等人提出了协作式纯相机三维检测（CoCa3D）,证明了协作在增强基于相机的三维检测方面的潜力。由于深度估计是基于相机的 3D 检测的瓶颈，因此 CoCa3D 包含协作深度估计 （Co-Depth）,但协作特征学习 （Co-FL） 除外。<h4 id=图融合><a class=headerlink href=#图融合 title=图融合></a>图融合</h4><p>基于图的融合：尽管传统的中间融合很简单，但它们忽略了多方agent之间的潜在关系，无法推理从发送方到接收方的信息。图神经网络（GNN）能够传播和聚合来自邻居的信息，最近的研究表明，图神经网络在感知和自动驾驶方面非常有效。<p>V2VNet 首先利用空间感知图神经网络（GNN）对代理之间的通信进行建模,在 GNN 信息传递阶段，V2VNet 利用变分图像压缩算法来压缩特征。在跨车辆聚合阶段，V2VNet 首先补偿时间延迟，为每个节点创建初始状态，然后对从邻近代理到自我车辆的压缩特征进行扭曲和空间变换，所有这些操作都在重叠视场中(overlapping fields of view)进行。在特征融合阶段，V2VNet 采用平均运算来聚合特征，并利用卷积门控递归单元（ConvGRU）更新节点状态。虽然 V2VNet与 GNN 相比性能有所提高，但标量值协作权重无法反映不同空间区域的重要性。受此启发，DiscoNet 提出使用矩阵值边缘权重来捕捉高分辨率的代理间注意力。在信息传递过程中，DiscoNet 将特征串联起来，并为特征图中的每个元素应用矩阵值边缘权重。此外，DiscoNet 还将早期融合和中期融合结合在一起，通过对特征图中的每个元素应用矩阵值边缘权重。zhou 等人提出了另一种基于 GNN 的广义感知框架 MP-Pose。在信息传递阶段，MP-Pose 利用空间编码网络编码相对空间关系，而不是直接扭曲特征。受图形注意网络（GAT）的启发，MP-Pose 进一步使用动态交叉注意编码网络来捕捉代理之间的关系，并像 GAT 一样聚合多个特征。<h4 id=Attention-based><a class=headerlink href=#Attention-based title=Attention-based></a>Attention-based</h4><p>除了图形学习，注意力机制也已成为探索特征关系的有力工具.注意机制可根据数据域分为<strong>通道注意、空间注意和通道与空间注意</strong><p>在过去的十年中，<strong>注意力机制在计算机视觉领域发挥了越来越重要的作用 ，并激发了协作感知研究</strong>。<p>为了捕捉特征图中特定区域之间的相互作用，Xu 等人提出了 AttFusion，并首先在准确的空间位置采用自注意操作。具体来说，<strong>AttFusion 引入了单头自注意融合模块，与传统方法 F-Cooper和基于图的方法 DiscoNet相比，实现了性能和推理速度之间的平衡</strong>。<p>除了传统的基于注意力的方法，基于transformer的方法也能激发协作感知。Cui 等人提出了基于点transformer的 COOPERNAUT，这是一种用于点云处理的自注意力网络。<p>接收到信息后，ego agent会使用下采样块和点transformer block来聚合点特征。这两种操作<strong>都保持了信息的排列不变性</strong>。更重要的是，COOPERNAUT <strong>将协同感知与控制决策相结合，这对自动驾驶的模块联动具有重要意义</strong><p>与 V2V 协作相比，<strong>V2I 可以利用大量基础设施提供更稳定的协作信息，但目前很少有研究关注这一场景</strong>。<p>Xu 等人提出了首个统一转换器架构（V2X-ViT），它同时涵盖了 V2V 和 V2I。为了在不同类型的agent之间建立互动模块，V2X-ViT 提出了一个新颖的异构多代理关注模块（HMSA）来学习 V2V 和 V2I 之间的不同关系。此外，还引入了多尺度窗口注意模块（MSwin），以捕捉高分辨率检测中的长距离空间交互。<p>定制损失函数：虽然 V2V 通信为自我车辆提供了相对丰富的感知视野，但共享信息的冗余性和不确定性带来了新的挑战。<p>以往的协作感知研究大多侧重于<strong>协作效率和感知性能</strong>，但所有这些方法都假设了完美的条件。在现实世界的自动驾驶场景中，通信系统可能存在以下问题<p>1) 定位错误；2) 通信延迟和中断；3) 模型或任务差异；4) 隐私和安全问题<h4 id=协同感知数据集><a class=headerlink href=#协同感知数据集 title=协同感知数据集></a>协同感知数据集</h4><ul><li><p>V2X-Sim是一个<strong>全面的模拟多代理感知数据集</strong>。它<strong>由交通模拟 SUMO 和 CARLA 模拟器生成</strong>，数据格式遵循 nuScenes 。V2X-Sim 配备了 RGB 摄像头、激光雷达、GPS 和 IMU，收集了 100 个场景共 10,000 个帧，每个场景包含 2-5 辆车。V2X-Sim 中的帧分为 8,000/1,000/1,000 帧，用于训练/验证/测试。V2X-Sim 的基准支持三个关键的感知任务：检测、跟踪和分割，需要注意的是，所有任务都采用鸟瞰（BEV）表示法，并以二维 BEV 生成结果。</p><li><p>OPV2V：O<strong>PV2V是另一个针对V2V通信的模拟协同感知数据集</strong>，它是<strong>通过协同模拟框架OpenCDA和CARLA模拟器收集的</strong>。整个数据集可通过提供的配置文件进行重现。OPV2V 包含 11,464 帧激光雷达点和 RGB 摄像机。OPV2V 的一个显著特点是提供了一个名为 “卡尔弗城 “的仿真测试集，可用于评估模型的泛化能力。其基准支持三维物体检测和 BEV 语义分割，目前只包含一种类型的物体（车辆）。</p><li><strong>V2XSet 是一个大规模的 V2X 感知开放模拟数据集</strong>。该数据集格式与 OPV2V类似，共有 11,447 个帧。与 V2X 协作数据集 V2X-Sim和 V2I 协作数据集 DAIR-V2X相比，V2XSet 包含更多场景，并且该基准考虑了不完美的真实世界条件。该基准支持 3D 物体检测和 BEV 分割，有两种测试设置（完美和嘈杂）供评估。<li>DAIR-V2X：作为<strong>第一个来自真实场景的大规模 V2I 协同感知数据集</strong>，DAIR-V2X [对自动驾驶的协同感知意义重大。DAIR-V2X-C 集可用于研究 V2I 协作，VIC3D 基准可用于探索 V2I 物体检测任务。与主要关注激光雷达点的 V2X-Sim和 V2XSet不同，VIC3D 物体检测基准同时提供了基于图像和基于激光雷达点的协作方法。<li>V2V4Real：V2V4Real 是<strong>首个大规模真实世界多模式 V2V 感知数据集</strong>，由俄亥俄州哥伦布市的一辆特斯拉汽车和一辆福特 Fusion 汽车收集而成，覆盖 410 公里的道路。该数据集包含 20,000 个 LiDAR 帧和超过 240,000 个三维边界框注释，涉及五个不同的车辆类别。此外，V2V4Real 还提供了三个合作感知任务的基准，包括三维物体检测、物体跟踪和域适应。</ul><h2 id=VoxelNet><a class=headerlink href=#VoxelNet title=VoxelNet></a>VoxelNet</h2><p>在这项工作中,我们消除了对 3D 点云进行手动特征工程的需要，并提出了 VoxelNet，这是一种通用的 3D 检测网络，将特征提取和边界框预测统一到一个单阶段、端到端可训练的深度网络中。具体来说，VoxelNet 将点云划分为等间距的 3D 体素，并<strong>通过新引入的体素特征编码 （VFE） 层将每个体素内的一组点转换为统一的特征表示</strong>。通过这种方式,<strong>点云被编码为描述性体积表示,然后将其连接到RPN以生成检测</strong>。<p><img alt=image-20240101215637846 data-src=https://s2.loli.net/2024/01/01/McwI9AFzmXhLC6K.png><p><img alt=image-20240101215613643 data-src=https://s2.loli.net/2024/01/01/BotrdTR2xQ9UuVj.png><p><img alt=image-20240101215656769 data-src=https://s2.loli.net/2024/01/01/toNv8YDawXhpMqS.png><h2 id=Point-Pillar><a title="Point Pillar" class=headerlink href=#Point-Pillar></a>Point Pillar</h2><p>点云中的物体检测是许多机器人应用（如自动驾驶）的一个重要方面。在本文中，我们考虑了将点云编码为适合下游检测管道的格式的问题。最近的文献提出了两种类型的编码器;固定编码器往往速度快，但会牺牲准确性，而从数据中学习的编码器更准确，但速度较慢。在这项工作中，我们<strong>提出了PointPillars，这是一种新颖的编码器，它利用PointNets来学习以垂直列（柱子）组织的点云的表示</strong>。虽然编码特征可以与任何标准的 2D 卷积检测架构一起使用，但我们进一步提出了一个精益的下游网络。广泛的实验表明，PointPillars 在速度和精度方面都远远优于以前的编码器。尽管只使用激光雷达，但我们的完整检测流程在3D和鸟瞰图KITTI基准测试方面都明显优于最先进的技术，即使在融合方法中也是如此。这种检测性能是在 62 Hz 下运行时实现的：运行时间提高了 2 - 4 倍。我们方法的更快版本与105 Hz的最新技术相匹配。这些基准测试表明，PointPillars 是用于点云中对象检测的合适编码。<p><img alt=image-20240101220949018 data-src=https://s2.loli.net/2024/01/01/pgidY1Gzk3vDPAa.png><h2 id=SECOND><a class=headerlink href=#SECOND title=SECOND></a>SECOND</h2><p>近年来，基于卷积神经网络（CNN）的目标检测[、实例分割[3]和关键点检测[4]取得了长足的进步。这种检测形式可用于基于单目或立体图像的自动驾驶。但是，用于处理图像的方法不能直接应用于LiDAR数据。这对于自动驾驶和机器人视觉等应用来说是一个重大限制。最先进的方法可以实现 90% 的 2D 汽车检测的平均精度 （AP），但对于基于 3D 图像的汽车检测，AP 仅为 15%。<p>为了克服仅靠图像提供的空间信息不足的问题，点云数据在 3D 应用中变得越来越重要。点云数据包含准确的深度信息，可以由LiDAR或RGB-D相机生成。<p>我们在基于LiDAR的目标检测中应用了稀疏卷积，从而大大提高了训练和推理的速度。• 我们提出了一种改进的稀疏卷积方法，使其运行得更快。<p>• 我们提出了一种新的角度损失回归方法，该方法显示出比其他方法更好的方向回归性能。<p>• 我们针对仅激光雷达的学习问题引入了一种新的数据增强方法，大大提高了收敛速度和性能。<p><img alt=image-20240101221307223 data-src=https://s2.loli.net/2024/01/01/DuRbMClOPc6a9UL.png><p><img alt=image-20240101221332774 data-src=https://s2.loli.net/2024/01/01/sySo9kjwteduihm.png><h2 id=PIXOR><a class=headerlink href=#PIXOR title=PIXOR></a>PIXOR</h2><h3 id=abs-2><a class=headerlink href=#abs-2 title=abs></a>abs</h3><p>由于点云的高维数，现有方法的计算成本很高。我们通过鸟瞰图（BEV）表示场景，更有效地利用3D数据，并提出了PIXOR，这是一种无需提案的单级检测器，可输出从像素级神经网络预测解码的定向3D对象估计值。<h3 id=intro-1><a class=headerlink href=#intro-1 title=intro></a>intro</h3><p>处理激光雷达数据的主要困难在于，传感器以点云的形式生成非结构化数据，通常每 360 度扫描包含大约 105 个 3D 点。<p>现有的表示主要分为两种类型：<strong>3D 体素网格</strong>和 <strong>2D 投影</strong>。<p>3D 体素网格<strong>将点云转换为规则间隔的 3D 网格</strong>，其中每个体素单元可以包含标量值（例如，占用率）或矢量数据（例如，根据该体素单元内的点计算出的手工统计数据）。然而，<strong>由于点云本质上是稀疏的，因此体素网格非常稀疏，因此很大一部分计算是冗余和不必要的</strong>。因此，使用此表示 的典型系统只能以 1-2 FPS 的速度运行<p>另一种方法是将点云投影到平面上，然后将其离散化为基于2D图像的表示，其中应用了2D卷积,在离散化过程中，手工制作的要素（或统计数据）将计算为 2D 图像的像素值.这些基于 2D 投影的表示更紧凑，但它们<strong>会在投影和离散化过程中带来信息丢失</strong>。例如，<strong>范围视图投影将具有扭曲的对象大小和形状</strong>.为了减轻信息损失，MV3D [3] 建议将 2D 投影与相机图像融合以带来更多信息。<p>我们选择 BEV 表示，因为<strong>它在计算上比 3D 体素网格更友好，并且还保留了度量空间</strong>，这使我们的模型能够探索有关对象类别的大小和形状的先验.<p>我们的探测器在鸟瞰图中以真实世界的尺寸输出精确定向的边界框。请注意，这些是 3D 估计值，因为我们假设物体在地面上。在自动驾驶场景中，这是一个合理的假设，因为车辆不会飞行。<p><img alt=image-20231225154926867 data-src=https://i.imgur.com/gPMcBei.png><p>我们的边界框估<strong>计不仅包含3D空间中的位置，还包含航向角</strong>，因为准确预测对于自动驾驶非常重要。我们利用了激光雷达点云的 2D 表示，因为与 3D 体素网格表示相比，它更紧凑，因此更适合实时推理。<p><strong>Input Representation</strong><p>标准卷积神经网络执行离散卷积，因此假设输入位于网格上。然而，3D点云是非结构化的，因此不能直接应用标准卷积。<p>一种选择是使用体素化形成 3D 体素格网，其中每个体素单元格包含该体素内点的某些统计数据。为了从此 3D 体素网格中提取特征表示，通常使用 3D 卷积。然而，这在计算中可能非常昂贵，因为我们必须沿着三维滑动 3D 卷积核。这也是不必要的，因为激光雷达点云非常稀疏，以至于大多数体素像元都是空的。<p>相反，<strong>我们可以仅从鸟瞰图 （BEV） 来表示场景</strong>。<p>通过将自由度从 3 减少到 2，我们不会丢失点云中的信息，因为我们仍然可以将高度信息保留为沿第三维的通道（如 2D 图像的 RGB 通道）。<p>在自动驾驶的背景下，<strong>这种降维是合理的，因为感兴趣的对象位于同一地面上</strong>。除了计算效率外，BEV表示还具有其他优势。<strong>它缓解了对象检测的问题，因为对象彼此不重叠（与前视图表示相比）。它还保留了度量空间，因此网络可以利用有关对象物理尺寸的先验。</strong><p>体素化激光雷达表示的<strong>常用特征是占用率、强度（反射率）、密度和高度特征</strong>。在PIXOR中，为了简单起见，我们只使用<strong>占用率和强度</strong>作为特征。在实践中，我们首先定义我们感兴趣的场景的 3D 物理尺寸 L × W × H。<p>然后，我们计算网格分辨率为 d~L~ × d~W~ × d~H~ 的占用特征图，并计算网格分辨率为 d~L~ × d~W~ × H 的强度特征图。请注意，<strong>在占用特征图中添加了两个额外的通道，以覆盖超出范围的点</strong>。<p>最终表示的形状为 L /d~L~ × W/d~W~ × （ H/d~H~ + 3）。<p><strong>Network Architecture</strong><p>PIXOR 使用<strong>全卷积神经网络</strong>，专为密集定向 3D 目标检测而设计。我们不采用常用的提案生成分支.取而代之的是，该网络<strong>在单个阶段输出像素级预测，每个预测对应于 3D 对象估计</strong>。<p>由于采用了全卷积架构，可以非常有效地计算出如此密集的预测。<p>在网络预测中 3D 对象的编码方面，我们使用直接编码，而不求助于预定义的对象锚点.<p>所有这些设计使PIXOR变得非常简单，并且由于网络架构中的零超参数，可以很好地泛化。具体来说，不需要设计对象锚点，也不需要调整从第一阶段传递到第二阶段的提案数量以及相应的非 Non-Maximum Suppression 阈值。<p>整个架构可以分为两个子网：backbone和header。<p><strong>backbone网络用于以卷积特征图的形式提取输入的一般表示。它具有很高的表示能力，可以学习鲁棒的特征表示。</strong><p><strong>header网络用于进行特定于任务的预测，在我们的示例中，它有多任务输出的单分支结构：表示对象类概率的分数图，以及编码定向 3D 对象的大小和形状的几何图。</strong><h4 id=Backbone-Network><a title="Backbone Network" class=headerlink href=#Backbone-Network></a>Backbone Network</h4><p>卷积神经网络通常由卷积层和池化层组成.<p>许多基于图像的物体检测器中的骨干网络通常具有 16 的下采样因子，并且通常设计为高分辨率层数较少，低分辨率层数较多。它适用于图像，因为对象的像素大小通常很大。但是，在我们的例子中，<strong>这将导致一个问题，因为对象可能非常小。使用 0.1m 的离散化分辨率时，典型车辆的尺寸为 18×40 。经过 16× 的下采样后，它仅覆盖大约 3 个像素。</strong>一个直接的<strong>解决方案是使用更少的池化层</strong>。然而，这将减小最终特征图中每个像素的感受野的大小，从而限制了表示能力。然而<strong>，这会导致高级特征图中出现棋盘伪影</strong>。我们的解决方案很简单，我们使用 16× 的下采样因子，但进行了两次修改。<p>首先，<strong>我们在较低的级别中添加更多具有较小通道数的层，以提取更精细的细节信息</strong>。其次，<strong>我们采用类似于FPN的自上而下的分支，将高分辨率特征图与低分辨率特征图相结合，从而对最终的特征表示进行上采样</strong><p><img alt=image-20231225165719123 data-src=https://i.imgur.com/7ILcxnF.png style=zoom:67%;><p>第二至第五块由残余层组成,每个残差块的第一个卷积的步幅为 2，以便对特征图进行下采样。总的来说，我们的下采样因子为 16。为了对特征图进行上采样，我们添加了一个自上而下的路径，每次对特征图进行上采样 2。<p>然后通过像素求和将其与相应分辨率的自下而上的特征图相结合。使用两个上采样层，从而得到最终的特征图，相对于输入，下采样因子为 4×。<h4 id=Header-Network><a title="Header Network" class=headerlink href=#Header-Network></a>Header Network</h4><p>Header 网络是一个多任务网络，可处理对象识别和定位。<p><strong>分类分支输出 1 通道特征图，后跟 sigmoid 激活功能</strong>。<strong>回归分支输出无非线性的 6 通道特征图。</strong><p>在两个分支之间共享权重的层数存在权衡。一方面，我们希望更有效地利用权重。另一方面，由于它们是不同的子任务，我们希望它们更加独立和专业化。<p>我们将<strong>每个对象参数化为定向边界框 b，即 {θ， x~c~， y~c~， w， l}，每个元素对应于航向角度（在 [−π， π] 范围内）、对象的中心位置和对象的大小。</strong><p>与基于长方体的 3D 目标检测相比，我们省略了沿 Z 轴的位置和大小，因为在自动驾驶等应用中，感兴趣的对象被限制在同一接地平面上，因此我们只关心如何在该平面上定位它（此设置在一些文献中也称为 3D 定位 ）。<p>请注意，航向角度被分解为两个相关值，以强制执行角度范围约束。学习目标是 {cos(θ),sin(θ),dx, dy,log(w), log(l)}，在训练集上预先归一化为零均值和单位方差。<h3 id=数据集><a class=headerlink href=#数据集 title=数据集></a>数据集</h3><h4 id=KITTI><a class=headerlink href=#KITTI title=KITTI></a>KITTI</h4><p>我们利用我们的自动驾驶平台Anniway开发了具有挑战性的新型现实世界计算机视觉基准。我们感兴趣的任务是：立体、光流、视觉里程计、三维物体检测和三维跟踪。为此，我们为一辆标准旅行车配备了两台高分辨率彩色和灰度摄像机。Velodyne激光扫描仪和GPS定位系统提供了准确的地面实况。我们的数据集是通过在中型城市卡尔斯鲁厄、农村地区和高速公路上行驶来获取的<strong>。每张图片最多可看到15辆汽车和30名行人</strong>。除了以原始格式提供所有数据外，我们还为每个任务提取基准。对于我们的每个基准，我们还提供了一个评估指标和这个评估网站。初步实验表明，在Middlebury等既定基准上排名靠前的方法在从实验室转移到现实世界时，表现低于平均水平。我们的目标是减少这种偏见，并通过向社区提供具有新困难的现实世界基准来补充现有基准。<h4 id=nuScenes><a class=headerlink href=#nuScenes title=nuScenes></a>nuScenes</h4><p>nuScenes数据集（发音为/nuõsiõnz/）是Motional（前身为nuTonomy）团队开发的一个用于<strong>自动驾驶的公共大规模数据集</strong>。Motional正在使无人驾驶汽车成为一种安全、可靠和可访问的现实。通过向公众发布我们的一部分数据，Motional旨在支持公众对计算机视觉和自动驾驶的研究。<p>为此，收集了波士顿和新加坡的1000个驾驶场景，这两个城市以交通密集和极具挑战性的驾驶环境而闻名。20秒长的场景是手动选择的，以展示一组多样而有趣的驾驶动作、交通状况和意外行为。nuScenes的丰富复杂性将鼓励开发能够在每个场景有几十个物体的城市地区安全驾驶的方法。收集不同大陆的数据进一步使我们能够研究计算机视觉算法在不同地点、天气条件、车辆类型、植被、道路标记以及左右交通中的通用性。<p>所有检测结果均按照平均精度 (mAP)、平均平移误差 (mATE)、平均比例误差 (mASE)、平均方向误差 (mAOE)、平均速度误差 (AVE)、平均属性误差 (AAE) 和 nuScenes 检测得分 (NDS) 进行评估。<h4 id=The-Waymo-opendataset><a title="The Waymo opendataset" class=headerlink href=#The-Waymo-opendataset></a>The Waymo opendataset</h4><p>机器学习领域正在迅速变化。Waymo通过创建和共享一些最大、最多样化的自动驾驶数据集，在为研究界做出贡献方面处于独特的地位。查看我们最新发布的感知对象资产数据集，其中包括31k个具有传感器数据的独特感知对象实例，用于生成建模！<p>从多传感器数据中提取感知对象：所有5个摄像头和顶级激光雷达 激光雷达特征包括支持三维物体形状重建的三维点云序列。<p>我们还通过点云形状注册为车辆类别中的所有对象提供了精细的长方体姿态 相机功能包括most_visible_Camera的相机补丁序列、相应相机上投影的激光雷达返回、每个像素的相机光线信息，以及支持对象NeRF重建的自动标记2D全景分割，本文对此进行了详细介绍 来自31K个独特对象实例的120万个对象帧，涵盖2类（车辆和行人）<p>2030个片段，每个片段20秒，在不同的地理位置和条件下以10Hz（390000帧）采集,1个中距离激光雷达4个短程激光雷达5个摄像头（正面和侧面）同步激光雷达和相机数据激光雷达到摄像机的投影传感器校准和车辆姿态<h3 id=学习资料><a class=headerlink href=#学习资料 title=学习资料></a>学习资料</h3><ol><li><a href=https://paperswithcode.com/task/3d-object-detection rel=noopener target=_blank>3D Object Detection | Papers With Code</a><li><a href=https://github.com/patrick-llgc/Learning-Deep-Learning rel=noopener target=_blank>patrick-llgc/Learning-Deep-Learning: Paper reading notes on Deep Learning and Machine Learning (github.com)</a><li><a href=https://www.stereolabs.com/docs/object-detection/ rel=noopener target=_blank>3D Object Detection Overview | Stereolabs</a><li><a href=https://zhuanlan.zhihu.com/p/591349104 rel=noopener target=_blank>系列二：3D Detection目标检测系列论文总结（2023年更） - 知乎 (zhihu.com)</a><li>3D点云<a href=https://github.com/HuangCongQing/3D-Point-Clouds rel=noopener target=_blank>HuangCongQing/3D-Point-Clouds: 🔥3D点云目标检测&语义分割(深度学习)-SOTA方法,代码,论文,数据集等 (github.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\10\23\DDNLP-深入NLP\ rel=bookmark>DDNLP:深入NLP</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="3D Object Detection Learning" href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/>https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/Deep-Learning/ rel=tag><i class="fa fa-tag"></i> Deep Learning</a><a href=/tags/3D-Object-Detection/ rel=tag><i class="fa fa-tag"></i> 3D Object Detection</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/ rel=prev title=后台执行:从nohup到tmux> <i class="fa fa-chevron-left"></i> 后台执行:从nohup到tmux </a></div><div class=post-nav-item><a href=/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ rel=next title=目标检测学习_P3> 目标检测学习_P3 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#PointNet><span class=nav-number>1.</span> <span class=nav-text>PointNet</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs><span class=nav-number>1.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#intro><span class=nav-number>1.2.</span> <span class=nav-text>intro</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#PointNet-Deep-Hierarchical-Feature-Learning-on-Point-Sets-in-a-Metric-Space><span class=nav-number>2.</span> <span class=nav-text>PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Robust-Feature-Learning-under-Non-Uniform-Sampling-Density><span class=nav-number>2.1.</span> <span class=nav-text>Robust Feature Learning under Non-Uniform Sampling Density</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Objects-as-Points-2019><span class=nav-number>3.</span> <span class=nav-text>Objects as Points 2019</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Abs><span class=nav-number>3.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86><span class=nav-number>3.2.</span> <span class=nav-text>前置知识</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation><span class=nav-number>4.</span> <span class=nav-text>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Abs-1><span class=nav-number>4.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Intro><span class=nav-number>4.2.</span> <span class=nav-text>Intro</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C><span class=nav-number>4.3.</span> <span class=nav-text>相关工作</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Object-detection-with-anchors><span class=nav-number>4.3.1.</span> <span class=nav-text>Object detection with anchors</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Object-detection-without-anchors><span class=nav-number>4.3.2.</span> <span class=nav-text>Object detection without anchors</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-2017><span class=nav-number>5.</span> <span class=nav-text>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 2017</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs-1><span class=nav-number>5.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-1><span class=nav-number>5.2.</span> <span class=nav-text>相关工作</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84><span class=nav-number>5.3.</span> <span class=nav-text>网络结构</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Feature-learning-network><span class=nav-number>5.4.</span> <span class=nav-text>Feature learning network</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#RPN><span class=nav-number>5.5.</span> <span class=nav-text>RPN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E9%AB%98%E6%95%88%E5%AE%9E%E7%8E%B0><span class=nav-number>5.5.1.</span> <span class=nav-text>高效实现</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>5.6.</span> <span class=nav-text>损失函数</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Center-based-3D-Object-Detection-and-Tracking><span class=nav-number>6.</span> <span class=nav-text>Center-based 3D Object Detection and Tracking</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Introduction><span class=nav-number>6.1.</span> <span class=nav-text>Introduction</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Center-heatmap-head><span class=nav-number>6.2.</span> <span class=nav-text>Center heatmap head.</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Regression-heads><span class=nav-number>6.3.</span> <span class=nav-text>Regression heads</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Two-Stage-CenterPoint><span class=nav-number>6.4.</span> <span class=nav-text>Two-Stage CenterPoint</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#SECOND-Sparsely-Embedded-Convolutional-Detection-2018><span class=nav-number>7.</span> <span class=nav-text>SECOND: Sparsely Embedded Convolutional Detection 2018</span></a></ol><li class="nav-item nav-level-1"><a class=nav-link href=#PointPillars-Fast-Encoders-for-Object-Detection-from-Point-Clouds-2018><span class=nav-number></span> <span class=nav-text>PointPillars: Fast Encoders for Object Detection from Point Clouds 2018</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#PIXOR-Real-time-3D-Object-Detection-from-Point-Clouds-2019><span class=nav-number></span> <span class=nav-text>PIXOR: Real-time 3D Object Detection from Point Clouds 2019</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#Keypoints-Based-Deep-Feature-Fusion-for-Cooperative-Vehicle-Detection-of-Autonomous-Driving-2021><span class=nav-number>1.</span> <span class=nav-text>Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving 2021</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#CIA-SSD-Confident-IoU-Aware-Single-Stage-Object-Detector-From-Point-Cloud-2021><span class=nav-number>2.</span> <span class=nav-text>CIA-SSD: Confident IoU-Aware Single Stage Object Detector From Point Cloud 2021</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5-3D%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1><span class=nav-number>3.</span> <span class=nav-text>协同感知 3D检测任务</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges><span class=nav-number>3.1.</span> <span class=nav-text>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Collaboration-scheme><span class=nav-number>3.2.</span> <span class=nav-text>Collaboration scheme</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%97%A9%E6%9C%9F%E8%9E%8D%E5%90%88><span class=nav-number>3.2.1.</span> <span class=nav-text>早期融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%B8%AD%E6%9C%9F><span class=nav-number>3.2.2.</span> <span class=nav-text>中期</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%99%9A%E6%9C%9F><span class=nav-number>3.3.</span> <span class=nav-text>晚期</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88-Raw-Data-Fusion><span class=nav-number>3.4.</span> <span class=nav-text>原始数据融合(Raw Data Fusion)</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#customized-communication-mechanism><span class=nav-number>3.5.</span> <span class=nav-text>customized communication mechanism</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Feature-Fusion><span class=nav-number>3.6.</span> <span class=nav-text>Feature Fusion</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BC%A0%E7%BB%9F%E8%9E%8D%E5%90%88><span class=nav-number>3.6.1.</span> <span class=nav-text>传统融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%9B%BE%E8%9E%8D%E5%90%88><span class=nav-number>3.6.2.</span> <span class=nav-text>图融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Attention-based><span class=nav-number>3.6.3.</span> <span class=nav-text>Attention-based</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.6.4.</span> <span class=nav-text>协同感知数据集</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#VoxelNet><span class=nav-number>4.</span> <span class=nav-text>VoxelNet</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Point-Pillar><span class=nav-number>5.</span> <span class=nav-text>Point Pillar</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#SECOND><span class=nav-number>6.</span> <span class=nav-text>SECOND</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#PIXOR><span class=nav-number>7.</span> <span class=nav-text>PIXOR</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs-2><span class=nav-number>7.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#intro-1><span class=nav-number>7.2.</span> <span class=nav-text>intro</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Backbone-Network><span class=nav-number>7.2.1.</span> <span class=nav-text>Backbone Network</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Header-Network><span class=nav-number>7.2.2.</span> <span class=nav-text>Header Network</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>7.3.</span> <span class=nav-text>数据集</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#KITTI><span class=nav-number>7.3.1.</span> <span class=nav-text>KITTI</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#nuScenes><span class=nav-number>7.3.2.</span> <span class=nav-text>nuScenes</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#The-Waymo-opendataset><span class=nav-number>7.3.3.</span> <span class=nav-text>The Waymo opendataset</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99><span class=nav-number>7.4.</span> <span class=nav-text>学习资料</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>263</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>224</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/3D-Object-Detection/ rel=tag>3D Object Detection</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/Deep-Learning/ rel=tag>Deep Learning</a><span class=tag-list-count>2</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2026</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>4.4m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>66:09</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>document.addEventListener("DOMContentLoaded", function() {
    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {
        var pvContainer = document.getElementById("busuanzi_container_site_pv");
        if (pvContainer && pvContainer.style.display !== "none") {
            var pvElement = document.getElementById("busuanzi_value_site_pv");
            if (pvElement) {
                pvElement.innerHTML = parseInt(pvElement.innerHTML) + countOffset;
                clearInterval(int);
            }
        }
        
        var uvContainer = document.getElementById("busuanzi_container_site_uv");
        if (uvContainer && window.getComputedStyle(uvContainer).display !== "none")
        {
            var uvElement = document.getElementById("busuanzi_value_site_uv");
            if (uvElement) {
                uvElement.innerHTML = parseInt(uvElement.innerHTML) + countOffset; // 加上初始数据 
                clearInterval(int); // 停止检测
            }
        }
    }
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	 '.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
  
  // Reinitialize TagCanvas for tag cloud
  if (typeof TagCanvas !== 'undefined' && document.getElementById('resCanvas')) {
    try {
      TagCanvas.textFont = 'Trebuchet MS, Helvetica';
      TagCanvas.textColour = '#333';
      TagCanvas.textHeight = 20;
      TagCanvas.outlineColour = '#E2E1D1';
      TagCanvas.maxSpeed = 0.3;
      TagCanvas.freezeActive = true;
      TagCanvas.outlineMethod = 'block';
      TagCanvas.minBrightness = 0.2;
      TagCanvas.depth = 0.92;
      TagCanvas.pulsateTo = 0.6;
      TagCanvas.initial = [0.1,-0.1];
      TagCanvas.decel = 0.98;
      TagCanvas.reverse = true;
      TagCanvas.hideTags = false;
      TagCanvas.shadow = '#ccf';
      TagCanvas.shadowBlur = 3;
      TagCanvas.weight = false;
      TagCanvas.imageScale = null;
      TagCanvas.fadeIn = 1000;
      TagCanvas.clickToFront = 600;
      TagCanvas.lock = false;
      TagCanvas.Start('resCanvas');
      TagCanvas.tc['resCanvas'].Wheel(true);
    } catch(e) {
      console.log('TagCanvas initialization failed:', e);
    }
  }
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>