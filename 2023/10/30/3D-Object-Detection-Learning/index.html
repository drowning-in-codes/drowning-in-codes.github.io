<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文. name=description><meta content=article property=og:type><meta content="3D Object Detection Learning" property=og:title><meta content=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文. property=og:description><meta content=zh_CN property=og:locale><meta content=https://i.imgur.com/GERZSZ7.png property=og:image><meta content=https://i.imgur.com/dhgI1JD.png property=og:image><meta content=https://i.imgur.com/wvQa3jb.png property=og:image><meta content=https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png property=og:image><meta content=https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png property=og:image><meta content=https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png property=og:image><meta content=c:/Users/proanimer/AppData/Roaming/Typora/typora-user-images/image-20231129091921213.png property=og:image><meta content=https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png property=og:image><meta content=https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png property=og:image><meta content=https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70 property=og:image><meta content=https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png property=og:image><meta content=https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png property=og:image><meta content=https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png property=og:image><meta content=2023-10-30T08:19:04.000Z property=article:published_time><meta content=2023-11-30T01:54:29.411Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="Deep Learning" property=article:tag><meta content="3D Object Detection" property=article:tag><meta content=summary name=twitter:card><meta content=https://i.imgur.com/GERZSZ7.png name=twitter:image><link href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>3D Object Detection Learning | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>3D Object Detection Learning</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-10-30 16:19:04" datetime=2023-10-30T16:19:04+08:00>2023-10-30</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2023-11-30 09:54:29" datetime=2023-11-30T09:54:29+08:00 itemprop=dateModified>2023-11-30</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>15k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>14 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>从一般的2D目标检测到3D目标检测.3D检测方面主要涉及到自动驾驶领域,这里主要看看论文.</p><span id=more></span><p>先看几篇论文.<h2 id=Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation><a title="Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation" class=headerlink href=#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation></a>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</h2><p><a href=http://arxiv.org/abs/2111.09515 rel=noopener target=_blank>http://arxiv.org/abs/2111.09515</a><h3 id=Abs><a class=headerlink href=#Abs title=Abs></a>Abs</h3><p>近年来，用于自动驾驶的激光雷达数据三维物体检测技术取得了长足进步,在最先进的方法中，将<strong>点云编码成鸟瞰图</strong>（BEV,bird’s eye view）已被证明是既有效又高效的方法。<strong>与透视图(perspective views)不同，鸟瞰图保留了物体之间丰富的空间和距离信息</strong>。然而,在 BEV 中,虽然<strong>同类型的远距离物体看起来并不更小</strong>,但它们<strong>包含的点云特征却更稀疏</strong>。这一事实<strong>削弱了使用共享权重卷积神经网络（CNN）提取 BEV 特征的能力</strong>.<p>为了应对这一挑战,我们提出了范围感知注意力网络 (RAANet),它能提取有效的 BEV 特征并生成出色的 3D object detection 输出.<p>范围感知注意力（RAA）卷积显著改善了<strong>对远近物体的特征提取</strong>。<p>此外，我们还提出了一<strong>种用于点密度估计</strong>(point density estimation)的新型辅助损失，以进一步<strong>提高 RAANet 对遮挡物体的检测精</strong>度。值得注意的是，我们提出的 RAA 卷积是轻量级的,可以集成到任何用于检测 BEV 的 CNN 架构中.<p>在 <strong>nuScenes 和 KITTI 数据集上</strong>进行的大量实验表明，在基于激光雷达(LiDAR-based 3D object detection)的三维物体检测方面，我们提出的方法优于最先进的方法，在 nuScenes 激光雷达帧上进行的测试中，完整版的实时推理速度为 16 Hz，精简版为 22 Hz。<h3 id=Intro><a class=headerlink href=#Intro title=Intro></a>Intro</h3><p>随着处理单元的快速改进，得益于深度神经网络的成功，自动驾驶的感知能力近年来得到了蓬勃发展。通过<strong>激光雷达传感器进行 3D 物体检测</strong>是自动驾驶的重要功能之一。<p>早期的研究采用了三维卷积神经网络（CNN），这种网络处理速度慢，内存需求大。<p>为了降低内存要求并提供实时处理，最近的方法利用了体素化(voxelization)和鸟瞰投影（BEV）。<p>体素化(Voxelization)作为三维点云(3D point clouds)的一种预处理方法得到了广泛应用，因为<strong>结构更合理的数据可提高计算效率和性能精度</strong>。<p>一般来说，体素化将点云划分为均匀分布的体素网格，然后将三维激光雷达点分配到各自的体素上。输出空间保留了物体之间的欧氏距离，并避免了边界框的重叠。<p>这些特点使得<strong>无论物体与激光雷达的距离如何，都能将物体的尺寸变化控制在一个相对较小的范围内</strong>，从而有<strong>利于在训练过程中进行形状回归</strong>。<p>在本文中，我们提出了距离感知注意力网络（RAANet），其中包含新型的范围感知注意力卷积层（RAAConv），设计<strong>用于LiDAR BEV的目标检测</strong>。RAAConv 由两个独立的卷积分支和注意力图组成,对输入特征图的位置信息敏感.<p>我们的方法受到BEV图像特性的启发，<strong>随着物体和自我车辆之间距离的增加，点变得越来越稀疏</strong>。<strong>理想情况下，对于BEV特征图，不同位置的元素应由不同的卷积核处理</strong>。但是，应用不同的内核会显着增加计算费用。<p>为了在BEV特征提取过程中<strong>利用位置信息，在避免繁重计算的同时，将BEV特征图视为稀疏特征和密集特征的组合</strong>。我们应用两个不同的卷积核来同时提取稀疏和密集特征。<p>每个提取的特征图的通道大小都是最终输出的一半。同时，根据输入形状生成范围和位置编码。然后，根<strong>据相应的特征图以及范围和位置编码计算每个范围感知注意力热图</strong>。最后，将<strong>注意力热图应用于特征图以增强特征表示</strong>。从两个分支生成的特征图按通道concat为 RAAConv 输出。<p>此外,遮挡的影响也不容忽视,因为同一物体在不同的遮挡量下可能具有不同的点分布。因此，我们提出了一个高效的辅助分支，称为<strong>辅助密度水平估计模块</strong>（ADLE），允许RAANet考虑遮挡。由于注释各种遮挡是一项耗时且昂贵的任务，因此我们<strong>设计了ADLE来估计每个对象的点密度水平。如果没有遮挡，则近处物体的点密度水平高于远处物体的点密度水平</strong>。<p>但是，<strong>如果附近的物体被遮挡，则其点密度水平会降低。因此，通过结合距离信息和密度水平信息，我们能够估计遮挡信息的存在</strong>。ADLE仅用于训练阶段，用于提供密度信息指导，在推理状态下可以删除，以提高计算效率。<p>主要贡献:<ol><li>我们提出了RAAConv层，它允许基于LiDAR的探测器提取更具代表性的BEV特征。此外，RAAConv 层可以集成到任何用于 LiDAR BEV 的 CNN 架构中。<li>我们提出了一种新的用于点密度估计的辅助损失，以帮助主网络学习与遮挡相关的特征。该密度水平估计器进一步提高了RAANet对被遮挡物体的检测精度。<li>我们提出了范围感知注意力网络（RAANet），它集成了前面提到的RAA和ADLE模块。RAANet通过基于ground truth生成各向异性(anistropic)高斯热图，进一步优化，</ol><h3 id=相关工作><a class=headerlink href=#相关工作 title=相关工作></a>相关工作</h3><p>大多数目标检测工作可以分为两大类：有锚点和无锚点的目标检测。此外，在早期阶段存在对点云数据进行编码的工作]，但它们超出了目标检测网络重构的范围。<h4 id=Object-detection-with-anchors><a title="Object detection with anchors" class=headerlink href=#Object-detection-with-anchors></a>Object detection with anchors</h4><p>固定形状的锚回归方法，以便可以提取中间特征<p>two-stage:RCNN家族<p>one-stage:YOLO,Retinanet,SSD<p>YOLO:将目标检测重新定义为单一回归问题，该问题采用端到端神经网络进行单次前向传播来检测目标<p>SSD:Liu等开发了一种多分辨率锚点技术，用于检测尺度混合物的物体，并在一定程度上学习偏移量，而不是学习锚点。<p>RetinaNet:Lin等提出了一种焦点损失，以解决密集和小目标检测问题，同时处理类不平衡和不一致。<p>Zhou和Tuzel(VoxelNet)以及Lang等(PointPillars)提出了用于点云的神经网络，这为3D检测任务开辟了新的可能性。<h4 id=Object-detection-without-anchors><a title="Object detection without anchors" class=headerlink href=#Object-detection-without-anchors></a>Object detection without anchors</h4><p>为了解决锚点回归带来的计算开销和超参数冗余问题，并有效地处理点云编码，无锚点目标检测已在许多工作中得到应用。无锚点目标检测可分为两大类，即<strong>基于中心的方法和基于关键点的方法</strong>。<p>基于中心的方法:基于中心的方法：在这种方法中，对象的中心点用于定义正样本和负样本，而不是IoU。该方法通过预测从正样本到物体边界的四个距离来生成边界框，从而大大降低了计算成本。<p>基于关键点的方法:通过几个预定义的方法或自学习模型定位关键点，然后生成边界框来对对象进行分类。<p>为了提取具有代表性的特征，我们重点关注两个主要组成部分：<strong>范围感知特征提取</strong>和<strong>遮挡监督</strong>。<p>我们提出的范围感知注意力网络（RAANet）的主要架构如图所示<p><img alt=image-20231119161935219 data-src=https://i.imgur.com/GERZSZ7.png><p>我们结合了CenterNet的思想来构建一个无锚探测器，并引入了两个新颖的模块：距离感知注意力卷积层（RAAConv）和辅助密度级估计模块（ADLE）。<p>区域建议网络 （RPN） 将该 BEV 特征图作为输入，并使用多个下采样和上采样模块来生成高维特征图。<p>除了主要任务中的检测头外，我们还提出了一个辅助任务，用于点密度水平估计，以实现更好的检测性能。<p>RAAConv 首先利用两组卷积核来提取每个分支的中间特征图。<p>然后，将热图 fa 和 fb 分别乘以可学习标量 γa 和 γb。γa 和 γb 初始化为 1.0，并在训练过程中逐渐学习它们的值<h2 id=VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection><a title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection" class=headerlink href=#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection></a>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</h2><h3 id=abs><a class=headerlink href=#abs title=abs></a>abs</h3><p>准确检测三维点云中的物体是自主导航、看家机器人和增强/虚拟现实等许多应用中的核心问题。点云数据 高度稀疏<p>为了将高度稀疏的激光雷达点云与区域建议网络（RPN）连接起来，现有的大部分工作都集中在手工制作的特征表示上，例如鸟瞰投影。<p>在这项工作中，我们<strong>不再需要对三维点云进行人工特征工程，而是提出了一种通用的三维检测网络—VoxelNet，它将特征提取和边界框预测统一为一个单一阶段、端到端可训练的深度网络</strong>。<h3 id=相关工作-1><a class=headerlink href=#相关工作-1 title=相关工作></a>相关工作</h3><p>3D传感器技术的快速发展促使研究人员开发有效的表示来<strong>检测和定位点云中的物体</strong>,当有丰富而详细的 3D 形状信息可用时，这些手工制作的特征会产生令人满意的结果。<p>然而，它们<strong>无法适应更复杂的形状和场景，也无法从数据中学习所需的不变性</strong>，导致自主导航等不受控制的场景的成功有限。<p>鉴于图像提供了详细的纹理信息，许多算法从 2D 图像推断出 3D 边界框,然而，基于图像的三维检测方法的精度受深度估计精度的限制。<h3 id=网络结构><a class=headerlink href=#网络结构 title=网络结构></a>网络结构</h3><p>所提出的VoxelNet由三个功能块组成：（1）特征学习网络(Feature learning network)，（2）卷积中间层(Convolutional middle layers)，（3）区域建议网络(Region proposal network)。<h3 id=Feature-learning-network><a title="Feature learning network" class=headerlink href=#Feature-learning-network></a>Feature learning network</h3><p>Voxel Partition<p><img alt=image-20231119112120958 data-src=https://i.imgur.com/dhgI1JD.png><p>Stacked Voxel Feature Encoding<p>用V表示一个体素(Voxel),<h3 id=RPN><a class=headerlink href=#RPN title=RPN></a>RPN</h3><p><img alt=image-20231119112203016 data-src=https://i.imgur.com/wvQa3jb.png><p>RPN层有两个分支，一个用来输出类别的概率分布（通常叫做Score Map），一个用来输出Anchor到真实框的变化过程（通常叫做 Regression Map）<blockquote><p>注意这里论文是直接输出预测的anchor box的坐标而不是修正值.</blockquote><h4 id=高效实现><a class=headerlink href=#高效实现 title=高效实现></a>高效实现</h4><p>我们初始化一个 K × T × 7 维张量结构来<strong>存储体素输入特征缓冲区</strong>，其中 <strong>K 是非空体素的最大数量，T 是每个体素的最大点数，7 是每个点的输入编码维度</strong>。<p>这些点在处理之前是随机的。对于点云中的每个点，我们检查相应的体素是否已经存在。<h3 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h3><p><img alt=image-20231129221227758 data-src=https://s2.loli.net/2023/11/29/gbiAQJM6nYqlfWP.png><p>da = √(la)2 + (wa)2 是anchor box的对角线。<p><img alt=image-20231129221345941 data-src=https://s2.loli.net/2023/11/29/LhUsRJ7QlSEnejq.png><p>ui ∈ R^7^ 和 u∗ i ∈ R^7^ 分别是正锚点 a^pos^ ~i~ 的回归输出和地面实况。<h2 id=协同感知-3D检测任务><a title="协同感知 3D检测任务" class=headerlink href=#协同感知-3D检测任务></a>协同感知 3D检测任务</h2><p>综述<h3 id=Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges><a title="Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges" class=headerlink href=#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges></a>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</h3><p>协作感知对于解决自动驾驶中的遮挡和传感器故障问题至关重要。<p>自动驾驶感知可分为<strong>个体感知和协作感知</strong>。虽然个体感知随着深度学习的发展取得了长足的进步，但一些问题也限制了其发展。首先，<strong>个体感知在感知相对全面的环境时经常会遇到遮挡</strong>。其次，<strong>车载传感器在感知远处物体时存在物理限制</strong>。此外，<strong>传感器噪音也会降低感知系统的性能</strong>。为了弥补个体感知的不足，协作或合作感知利用了多个代理之间的互动，受到了广泛关注。<p>协同感知是一种多agent系统，其中agent共享感知信息，以克服自我视听的视觉局限。在单个感知场景中，自我视听只能检测到附近物体的部分遮挡和远处稀疏的点云。在协作感知场景中，ego AV通过接收其他agent的信息来扩大视野。<strong>通过这种协作方式，ego AV不仅能检测到远处和被遮挡的物体，还能提高在密集区域的检测精度</strong>。<p><img alt=image-20231122203934187 data-src=https://s2.loli.net/2023/11/22/XItN7mTobxc6QAs.png><p>长期以来，协作感知一直是人们关注的焦点。之前的工作专注于构建协作感知系统，以评估该技术的可行性。然而，由<strong>于缺乏大型公共数据集，它没有得到有效的推进</strong>。近年来，随着深度学习的发展和大规模协作感知数据集的公众关注和研究激增。<p><strong>考虑到通信中的带宽限制，大多数研究人员致力于设计新颖的协作模块，以实现精度和带宽之间的权衡</strong>。<p>在协作感知场景中，自我 AV 通过接收来自其他智能体的信息来扩展视野。通过这种协作方式，自我AV不仅可以检测远处和被遮挡的物体，还可以提高密集区域的检测精度。<p>为了总结这些技术和问题，我们回顾了自动驾驶中的协同感知方法，并从方法、数据集和挑战方面对近年来的进展进行了全面综述。我们还注意到近年来发表了一些关于协作感知的综述。<h3 id=Collaboration-scheme><a title="Collaboration scheme" class=headerlink href=#Collaboration-scheme></a>Collaboration scheme</h3><h4 id=早期融合><a class=headerlink href=#早期融合 title=早期融合></a>早期融合</h4><p>早期协作在网络输入端采用原始数据融合，也称为数据级或低级融合<p>因此，早期协作可以从根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效。<p>在自动驾驶场景中，自我车辆接收并转换来自其他智能体的原始传感器数据，然后聚合车载转换后的数据。原始数据包含最全面的信息和实质性的代理描述。因此，早期协作可以从<strong>根本上克服个体感知中的遮挡和长距离问题，并最大程度地促进绩效</strong>。<p>考虑到早期协作的高带宽，一些工作提出了中间协作感知方法来平衡性能-带宽的权衡。在中间协作中，其他智能体通常会将深层语义特征转移到自我载体。自我车辆融合特征以做出最终预测。中间协作已成为最流行的多智能体协作感知灵活性选择。然而，特征提取往往会造成信息丢失和不必要的信息冗余，这促使人们探索合适的特征选择和融合策略。<h4 id=中期><a class=headerlink href=#中期 title=中期></a>中期</h4><p>考虑到早期协作的高带宽，一些研究提出了中间协作感知方法，以平衡性能与带宽之间的权衡。在中间协作中，其他代理通常会将深层语义特征传输给自我车辆。<h3 id=晚期><a class=headerlink href=#晚期 title=晚期></a>晚期</h3><p>后期或对象级协作在网络输出端采用预测融合。每个代理单独训练网络并相互共享输出。自我车辆在空间上转换输出，并在后处理后合并所有输出。后期协作比早期和中期协作更节省带宽，也更简单。然而，后期的合作也有局限性。由于<strong>单个输出可能是嘈杂和不完整的，因此后期协作总是具有最差的感知性能</strong>。<h3 id=原始数据融合-Raw-Data-Fusion><a title="原始数据融合(Raw Data Fusion)" class=headerlink href=#原始数据融合-Raw-Data-Fusion></a>原始数据融合(Raw Data Fusion)</h3><p>早期协作在<strong>输入阶段采用原始数据融合。由于点云是不规则的，可以直接汇总</strong>，因此早期的协同工作通常采用<strong>点云融合策略</strong>。<p>第一个早期的协同感知系统 Cooper<strong>选择激光雷达数据</strong>作为融合目标。只需提取位置坐标和反射值，就能将点云压缩成较小的尺寸。在代理之间进行交互后，Cooper 利用变换矩阵重构接收到的点云，然后将自我点云集concat起来，进行最终预测。<h3 id=customized-communication-mechanism><a title="customized communication mechanism" class=headerlink href=#customized-communication-mechanism></a>customized communication mechanism</h3><p>早期协作中的原始数据融合拓宽了自我飞行器的视野，也<strong>造成了高带宽压力</strong>。为了缓解上述问题，<strong>越来越多的工作 发展了中间协作</strong>。<p>最初的中间协作方法<strong>遵循一种贪婪的通信机制，以获取尽可能多的信息。一般来说，它们会与通信范围内的所有代理共享信息，并将压缩后的完整特征图放入集体感知信息（CPM,collective perception message）中</strong>。然而，由于特征稀疏和代理冗余，贪婪通信可能会极大地浪费带宽。<p>Who2com 建立了首个带宽限制下的通信机制，通过三阶段握手实现。具体来说，<strong>Who2com 使用一般注意力函数计算代理之间的匹配分数，并选择最需要的代理，从而有效减少带宽</strong>。<p>在 Who2com 的基础上，When2com<strong>引入了缩放一般注意力来决定何时与他人交流</strong>。这样，自我代理只有在信息不足时才会与他人交流，从而有效地节省了协作资源。<p>除了选择合适的通信代理外，<strong>通信内容对于减少带宽压力也很重要</strong>。FPVRCNN 中提出了初始特征选择策略.具体来说，FPV-RCNN 采用检测头生成proposals，并只选择proposals中的特征点。<p><strong>关键点选择模块减少了共享深度特征的冗余，为初始proposals提供了有价值的补充信息。</strong><p>Where2comm 也提出了一种新颖的空间信心感知通信机制。其核心思想是<strong>利用空间置信度图来决定共享特征和通信目标</strong>。<strong>在特征选择阶段</strong>，<strong>Where2comm 选择并传输满足高置信度和其他agent请求的空间元素</strong>。在<strong>agent选择阶段，自我代理只与能提供所需特征的代理通信。通过发送和接收感知关键区域的特征，Where2comm 节省了大量带宽，并显著提高了协作效率</strong>。<h3 id=Feature-Fusion><a title="Feature Fusion" class=headerlink href=#Feature-Fusion></a>Feature Fusion</h3><blockquote><p>Feature fusion module is crucial in intermediate collaboration. After receiving CPMs from other agents, the ego vehicle can <strong>leverage different strategies to aggregate these features</strong>.</blockquote><p>可行的融合策略能够捕捉特征之间的潜在关系，提高感知网络的性能。根据基于特征融合的思想，我们将现有的特征融合方法分为传统融合、基于图的融合和基于注意力的融合。<h4 id=传统融合><a class=headerlink href=#传统融合 title=传统融合></a>传统融合</h4><p>在协同感知研究的早期阶段，研究人员倾向于使用传统的策略来融合特征，如concat、求和和线性加权。中级协作将这些不变的置换操作应用于深度特征，因其简单性而实现了快速推理。<p>第一个中间协同感知框架 FCooper<strong>提取了低级体素和深度空间特征</strong>。基于这两级特征，F-Cooper 提出了两种特征融合策略：<strong>体素特征融合</strong>（VFF）和<strong>空间特征融合</strong>（SFF）。<p>这两种方法都采用<strong>元素最大值（element-wise maxout）来融合重叠区域的特征</strong>。由于<strong>体素特征更接近原始数据，因此 VFF 与原始数据融合方法一样能够进行近距离物体检测</strong>。同时，SFF 也有其优势。<p>受 SENet的启发，SFF 选择选择部分信道来减少传输时间消耗，同时保持可比的检测精度<p>考虑到 F-Coope<strong>r忽略了低置信度特征的重要性</strong>，Guo 等人提出了 CoFF 来改进 F-Cooper。<strong>CoFF 通过测量重叠特征的相似度和重叠面积对其进行加权。相似度越小，距离越大，邻近特征提供的补充信息就越直观。</strong><p>此外，还添加了一个增强参数，以提高弱特征的值。<p>实验表明，简单而高效的设计使 CoFF 大大提高了 F-Cooper 的性能。<p>传统的融合方法虽然简单，但并没有被最近的方法所抛弃。Hu 等人提出了协作式纯相机三维检测（CoCa3D），证明了协作在增强基于相机的三维检测方面的潜力。<h4 id=图融合><a class=headerlink href=#图融合 title=图融合></a>图融合</h4><p>基于图的融合：尽管传统的中间融合很简单，但它们忽略了多方agent之间的潜在关系，无法推理从发送方到接收方的信息。图神经网络（GNN）能够传播和聚合来自邻居的信息，最近的研究表明，图神经网络在感知和自动驾驶方面非常有效。<p>V2VNet 首先利用空间感知图神经网络（GNN）对代理之间的通信进行建模,在 GNN 信息传递阶段，V2VNet 利用变分图像压缩算法来压缩特征。在跨车辆聚合阶段，V2VNet 首先补偿时间延迟，为每个节点创建初始状态，然后对从邻近代理到自我车辆的压缩特征进行扭曲和空间变换，所有这些操作都在重叠视场中(overlapping fields of view)进行。在特征融合阶段，V2VNet 采用平均运算来聚合特征，并利用卷积门控递归单元（ConvGRU）更新节点状态。虽然 V2VNet与 GNN 相比性能有所提高，但标量值协作权重无法反映不同空间区域的重要性。受此启发，DiscoNet 提出使用矩阵值边缘权重来捕捉高分辨率的代理间注意力。在信息传递过程中，DiscoNet 将特征串联起来，并为特征图中的每个元素应用矩阵值边缘权重。此外，DiscoNet 还将早期融合和中期融合结合在一起，通过对特征图中的每个元素应用矩阵值边缘权重。zhou 等人提出了另一种基于 GNN 的广义感知框架 MP-Pose。在信息传递阶段，MP-Pose 利用空间编码网络编码相对空间关系，而不是直接扭曲特征。受图形注意网络（GAT）的启发，MP-Pose 进一步使用动态交叉注意编码网络来捕捉代理之间的关系，并像 GAT 一样聚合多个特征。<h4 id=Attention-based><a class=headerlink href=#Attention-based title=Attention-based></a>Attention-based</h4><p>除了图形学习，注意力机制也已成为探索特征关系的有力工具.注意机制可根据数据域分为<strong>通道注意、空间注意和通道与空间注意</strong><p>在过去的十年中，<strong>注意力机制在计算机视觉领域发挥了越来越重要的作用 ，并激发了协作感知研究</strong>。<p>为了捕捉特征图中特定区域之间的相互作用，Xu 等人提出了 AttFusion，并首先在准确的空间位置采用自注意操作。具体来说，<strong>AttFusion 引入了单头自注意融合模块，与传统方法 F-Cooper和基于图的方法 DiscoNet相比，实现了性能和推理速度之间的平衡</strong>。<p>除了传统的基于注意力的方法，基于transformer的方法也能激发协作感知。Cui 等人提出了基于点transformer的 COOPERNAUT，这是一种用于点云处理的自注意力网络。<p>接收到信息后，ego agent会使用下采样块和点transformer block来聚合点特征。这两种操作<strong>都保持了信息的排列不变性</strong>。更重要的是，COOPERNAUT <strong>将协同感知与控制决策相结合，这对自动驾驶的模块联动具有重要意义</strong><p>与 V2V 协作相比，<strong>V2I 可以利用大量基础设施提供更稳定的协作信息，但目前很少有研究关注这一场景</strong>。<p>Xu 等人提出了首个统一转换器架构（V2X-ViT），它同时涵盖了 V2V 和 V2I。为了在不同类型的agent之间建立互动模块，V2X-ViT 提出了一个新颖的异构多代理关注模块（HMSA）来学习 V2V 和 V2I 之间的不同关系。此外，还引入了多尺度窗口注意模块（MSwin），以捕捉高分辨率检测中的长距离空间交互。<p>定制损失函数：虽然 V2V 通信为自我车辆提供了相对丰富的感知视野，但共享信息的冗余性和不确定性带来了新的挑战。<p>以往的协作感知研究大多侧重于<strong>协作效率和感知性能</strong>，但所有这些方法都假设了完美的条件。在现实世界的自动驾驶场景中，通信系统可能存在以下问题<p>1) 定位错误；2) 通信延迟和中断；3) 模型或任务差异；4) 隐私和安全问题<h4 id=协同感知数据集><a class=headerlink href=#协同感知数据集 title=协同感知数据集></a>协同感知数据集</h4><ul><li><p>V2X-Sim [37, 38]是一个全面的模拟多代理感知数据集。它由交通模拟 SUMO [29] 和 CARLA 模拟器 [18] 生成，数据格式遵循 nuScenes [6]。V2X-Sim 配备了 RGB 摄像头、激光雷达、GPS 和 IMU，收集了 100 个场景共 10,000 个帧，每个场景包含 2-5 辆车。V2X-Sim 中的帧分为 8,000/1,000/1,000 帧，用于训练/验证/测试。V2X-Sim 的基准支持三个关键的感知任务：检测、跟踪和分割，需要注意的是，所有任务都采用鸟瞰（BEV）表示法，并以二维 BEV 生成结果。</p><li><p>OPV2V：OPV2V[79]是另一个针对V2V通信的模拟协同感知数据集，它是通过协同模拟框架OpenCDA[74]和CARLA模拟器[18]收集的。整个数据集可通过提供的配置文件进行重现。OPV2V 包含 11,464 帧激光雷达点和 RGB 摄像机。OPV2V 的一个显著特点是提供了一个名为 “卡尔弗城 “的仿真测试集，可用于评估模型的泛化能力。其基准支持三维物体检测和 BEV 语义分割，目前只包含一种类型的物体（车辆）。</p><li>V2XSet [78] 是一个大规模的 V2X 感知开放模拟数据集。该数据集格式与 OPV2V [79]类似，共有 11,447 个帧。与 V2X 协作数据集 V2X-Sim [37] 和 V2I 协作数据集 DAIR-V2X [82]相比，V2XSet 包含更多场景，并且该基准考虑了不完美的真实世界条件。该基准支持 3D 物体检测和 BEV 分割，有两种测试设置（完美和嘈杂）供评估。<li>DAIR-V2X：作为第一个来自真实场景的大规模 V2I 协同感知数据集，DAIR-V2X [82] 对自动驾驶的协同感知意义重大。DAIR-V2X-C 集可用于研究 V2I 协作，VIC3D 基准可用于探索 V2I 物体检测任务。与主要关注激光雷达点的 V2X-Sim [37] 和 V2XSet [78]不同，VIC3D 物体检测基准同时提供了基于图像和基于激光雷达点的协作方法。<li>V2V4Real：V2V4Real [77] 是首个大规模真实世界多模式 V2V 感知数据集，由俄亥俄州哥伦布市的一辆特斯拉汽车和一辆福特 Fusion 汽车收集而成，覆盖 410 公里的道路。该数据集包含 20,000 个 LiDAR 帧和超过 240,000 个三维边界框注释，涉及五个不同的车辆类别。此外，V2V4Real 还提供了三个合作感知任务的基准，包括三维物体检测、物体跟踪和域适应。</ul><h3 id=Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds><a title="Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds" class=headerlink href=#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds></a>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</h3><h4 id=Abs-1><a class=headerlink href=#Abs-1 title=Abs></a>Abs</h4><h3 id=F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds><a title="F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds" class=headerlink href=#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds></a>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</h3><h4 id=Abs-2><a class=headerlink href=#Abs-2 title=Abs></a>Abs</h4><p>自动驾驶汽车在很大程度上依赖于传感器来完善对周围环境的感知，然而，就目前的技术水平而言，汽车所使用的数据仅限于来自自身传感器的数据.车辆和/或边缘服务器之间的数据共享受到可用网络带宽和自动驾驶应用严格的实时性限制。<p>为了解决这些问题，我们为联网自动驾驶汽车提出了<strong>基于点云特征的合作感知框架</strong>（F-Cooper），以实现更高的目标检测精度。<p>基于特征的数据不仅足以满足训练过程的需要，我们还利用特征的固有小尺寸来实现实时边缘计算，而不会面临网络拥塞的风险。<p>我们的实验结果表明，通过融合特征，我们能够获得更好的物体检测结果，20 米内的检测结果提高了约 10%，更远距离的检测结果提高了 30%，同时还能以较低的通信延迟实现更快的边缘计算，在某些特征选择中只需 71 毫秒。<h3 id=Intro-1><a class=headerlink href=#Intro-1 title=Intro></a>Intro</h3><p>互联自动驾驶汽车（CAV）为改善道路安全提供了一个前景广阔的解决方案。这有赖于车辆能够实时感知路况并精确探测物体。<p>然而，准确和实时的感知在现场具有挑战性。它需要处理来自各种传感器的大量连续数据流，并有严格的时间要求。此外，车辆的感知精度往往会受到传感器有限视角和范围的影响。<h3 id=Proposed-Solution><a title="Proposed Solution" class=headerlink href=#Proposed-Solution></a>Proposed Solution</h3><p>我们提出的方法可以提高自动驾驶车辆的检测精度，同时不会带来太多的计算开销。一个有益的启示是，现代自动驾驶车辆的物体检测技术，<strong>无论是基于图像的还是基于三维激光雷达数据的，通常都采用卷积神经网络（CNN）来处理原始数据，并利用区域建议网络（RPN）来检测物体</strong>。我们认为，特征图的能力尚未得到充分挖掘，特别是对于自动驾驶车辆上生成的 3D LiDAR 数据，因为特征图仅用于单个车辆的物体检测。<p>为此，我们引入了基于特征的协同感知（FCooper）框架，利用特征级融合实现端到端的三维物体检测，从而提高检测精度。我们的 F-Cooper 框架支持两种不同的融合方案：<strong>体素特征融合和空间特征融合</strong>。<p>与原始数据级融合解决方案[3]相比，<strong>前者实现了几乎相同的检测精度提升，而后者则提供了动态调整待传输特征图大小的能力</strong>。F-Cooper 的独特之处在于它可以在车载和路边边缘系统上部署和执行。<p>除了能够<strong>提高检测精度</strong>外，<strong>特征融合所需的数据大小仅为原始数据的百分之一</strong>。对于一个典型的激光雷达传感器来说，每个激光雷达帧包含约 100,000 个点，约为 4 MB。对于任何现有的无线网络基础设施来说，如此庞大的数据量都将成为沉重的负担。<p>要确认特征对融合的有用性，我们必须回答以下三个基本问题。<p>1) 特征是否具备融合的必要手段？<br>2) 我们能否通过特征在自动驾驶车辆之间有效地交流数据？<br>3) 如果特征满足前面两个要求，那么我们从自动驾驶车辆中获取特征图的难度有多大？<h4 id=Fusion-Characteristics><a title="Fusion Characteristics" class=headerlink href=#Fusion-Characteristics></a>Fusion Characteristics</h4><p>受致力于融合不同层生成的特征图的研究成果（如特征金字塔网络（FPN） 和级联 R-CNN [2]）的启发，我们发现<strong>在不同的特征图中检测物体是可能的。例如，FPN 采用自上而下的金字塔结构特征图进行检测。这些网络非常善于复合特征融合的效率</strong>。<p>从这些著作中汲取灵感，我们假设兼容融合的汽车<strong>将使用相同的检测模型</strong>。这一点非常重要，因为我们看到只有最可靠的检测模型才会被用于自动驾驶。有了这个假设，我们现在来看看融合的特点<h4 id=Compression-and-Transmission><a title="Compression and Transmission" class=headerlink href=#Compression-and-Transmission></a>Compression and Transmission</h4><p>与原始数据相比，特征地图的另一个优势在于车辆之间的传输过程。原始数据可能有多种不同的格式，但它们都能达到一个目的，那就是保留所捕获数据的原始状态。例如，从驾驶过程中获取的激光雷达数据将存储驾驶过程中沿途的所有点云。不过，这种存储格式<strong>会将不必要的数据与基本数据一起记录下来</strong>；而特征地图则避免了这一问题.<p>在 CNN 网络处理原始数据的过程中，所有无关数据都会被网络过滤掉，只留下可能被网络用于物体检测的信息。<strong>这些特征图存储在稀疏矩阵中，只存储被认为有用的数据，任何被过滤掉的数据在矩阵中都存储为 0。</strong><p>通过<strong>无损压缩（如 gzip 压缩方法），数据大小的优势会进一步扩大，如文献[14]所示。再加上稀疏矩阵的特性，我们就能将二者结合起来，实现压缩后的特征数据不超过 1 MB</strong>，使特征数据成为部署 On-Edge 融合的最佳选择。<h4 id=Generic-and-Inherent-Properties><a title="Generic and Inherent Properties" class=headerlink href=#Generic-and-Inherent-Properties></a>Generic and Inherent Properties</h4><p>所有自动驾驶车辆都必须根据传感器生成的数据做出决策。<strong>原始数据由车辆上的物理传感器生成，然后传送到车载计算设备。在那里，原始数据通过基于 CNN 的深度学习网络进行处理，最终做出驾驶决策。</strong>在此过程中，<strong>我们可以提取提取的特征进行共享。这样，我们就能有效地获得原始数据的特征图，而无需额外的计算时间或车载计算设备的功率</strong>。迄今为止，几乎所有已知的自动驾驶车辆都使用了基于 CNN 的网络，因此特征提取是通用的，在融合之前无需进一步处理。<p>得益于自动驾驶车辆处理数据的方式，我们能够直接从原始激光雷达点云数据中提取处理后的特征图进行融合，因为这本身就提供了位置数据。<strong>只要激光雷达传感器已经按照自动驾驶所需的标准进行了校准，那么我们就能获得能够保留所有物体与车辆相对位置的特征地图</strong>。<p>为了融合两辆汽车的三维特征，设计了两种融合范式：体素特征融合和空间特征融合。在范式 I 中，首先融合两组体素特征，然后生成空间特征图。<h3 id=Voxel-Features-Fusion><a title="Voxel Features Fusion" class=headerlink href=#Voxel-Features-Fusion></a>Voxel Features Fusion</h3><p>与位图中的像素一样，体素代表三维空间中规则立方体上的一个数值。在一个体素内，可能有零个或多个由激光雷达传感器生成的点云。对于至少包含一个点的任何体素，VoxelNet 的 VFE 层可以生成一个体素特征<p>假设原始激光雷达检测区域被划分为一个体素网格。<p>在这些体素中，我们将获得绝大多数空体素，而剩余的体素则包含关键信息。所有非空的体素都会通过一系列全连接层进行转换，并转化为长度为 128 的固定大小的矢量。固定大小的向量通常被称为特征图。<blockquote><p>为了提高内存/计算效率，我们将非空体素的特征保存到哈希表中，并将体素坐标作为哈希键。由于我们的重点主要是自动驾驶，因此我们只将非空体素存储到哈希表中。</blockquote><p>在 VFF 中，我们明确地将来自两个输入的所有体素的特征结合起来。具体来说，来自汽车 1 的体素 3 和来自汽车 2 的体素 5 共享相同的校准位置。<blockquote><p>虽然两辆车的物理位置不同，但它们共享同一个配准的三维空间，不同的偏移量表示每辆车在所述三维标定空间中的相对物理位置。为此，我们采用了element-wise maxout来融合体素 3 和体素 5。</blockquote><p><img alt=image-20231129091921213 data-src=C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20231129091921213.png><p>受卷积神经网络的启发，使用 maxout 进行潜在规模选择，提取明显的特征，同时抑制对三维空间检测无益的特征，从而实现更小的数据量。在我们的实验中，我们使用 maxout 来决定在比较车辆间的数据时哪个特征最突出。<h3 id=Spatial-Feature-Fusion><a title="Spatial Feature Fusion" class=headerlink href=#Spatial-Feature-Fusion></a>Spatial Feature Fusion</h3><p>VFF 需要考虑两辆车所有体素的特征，这涉及车辆之间的大量数据交换。<strong>为了进一步减少网络流量，同时保持基于特征融合的优势，我们设计了一种空间特征融合（SFF）方案</strong>。与 VFF 相比，SFF 融合的是空间特征图，与体素特征图相比，空间特征图更为稀疏，因此更容易压缩以进行通信。<p>与 VFF 不同，我们对每辆车上的体素特征进行预处理，以获得空间特征。接下来，将两个源空间特征融合在一起，并将融合后的空间特征转发给 RPN，以进行区域建议和目标检测。<p><img alt=image-20231129210539499 data-src=https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png><p>特征学习网络的输出是一个稀疏张量，其形状为 128 × 10 × 400 × 352。为了整合所有体素特征，我们采用了三个三维卷积层，依次获得语义信息更多的较小特征图，大小为 64 × 2 × 400 × 352。然而，生成的特征无法满足传统区域建议网络的形状要求。为此，必须将输出重新塑造为 128 × 400 × 352 大小的三维特征图，然后才能将其转发给 RPN。<p>对于 SFF，我们生成一个更大的检测范围，大小为 W × H，其中 W > W1，H > H1。接下来，对重叠区域进行融合，同时保留非重叠区域的原始特征。假设 GPS 将汽车 1 的实际位置记录为 (x1，y1)，汽车 2 的实际位置记录为 (x2，y2)，如果 x2 + H1, y2 - W1 2 属于 2 号车的特征图，而左上角代表 1 号车的特征图，那么我们就可以得到左上角的位置。那么我们就很容易确定重叠区域。与 VFF 采用 maxout 策略类似，我们在 SFF 中也采用了 maxout 来融合重叠的空间特征。<p><img alt=image-20231129215351930 data-src=https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png style=zoom:50%;><p>最后，我们采用区域建议网络在融合特征图上提出潜在区域。<blockquote><p>SENet 等最新研究表明，不同的通道具有不同的权重。也就是说，特征图中的某些通道对分类/检测的贡献更大，而其他通道则是多余或不需要的。</blockquote><p>选择从全部 128 个通道中选择部分通道进行传输。我们假定自动驾驶汽车装配了与实际应用中相同的训练有素的检测模型。<h3 id=使用融合特征进行目标检测><a class=headerlink href=#使用融合特征进行目标检测 title=使用融合特征进行目标检测></a>使用融合特征进行目标检测</h3><p>为了检测车辆，我们将合成特征图输入区域建议网络（RPN）进行对象建议。然后应用损失函数进行网络训练。<h4 id=区域建议网络><a class=headerlink href=#区域建议网络 title=区域建议网络></a>区域建议网络</h4><p>RPN:区域建议网络。不管是采用体素融合范式还是空间融合范式，当我们得到空间特征图后，都会将其送入区域提议网络（RPN）。通过 RPN 网络后，我们将得到两个损失函数的输出结果：<p>(1) 提议感兴趣区域的概率分数 p∈ [0, 1]；<p>(2) 提议区域的位置 P = (Px , Pw , Pz , Pl , Pw , Ph, Pθ ) ，其中 Px , Py , Pz 表示提议区域的中心，(Pl , Pw , Ph, Pθ ) 分别表示长度、宽度、高度和旋转角度。<h4 id=损失函数-1><a class=headerlink href=#损失函数-1 title=损失函数></a>损失函数</h4><p>损失函数由两部分组成：分类损失 Lcls 和回归损失 Lreg。<p><img alt=img data-src=https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70><p>ground-truth bounding box,即gt-box表示为G = Gx , Gy, Gz, Gl , Gw , Gh, Gθ 其中，Gx , Gy , Gz 表示方框的中心点，（Gl , Gw , Gh, Gθ ）分别表示长度、宽度、高度和偏航旋转角<p>输出的值包括<p><img alt=image-20231129221138541 data-src=https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png><p><img alt=image-20231129225123172 data-src=https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png><p>损失可以表示为<p><img alt=image-20231129225143741 data-src=https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png><h4 id=数据集><a class=headerlink href=#数据集 title=数据集></a>数据集</h4><p><strong>KITTI</strong><p>由于我们的重点是三维物体检测，因此我们使用了 KITTI 数据集提供的三维 Velodyne 点云数据。<p>云点数据<strong>每帧提供 100K 个点</strong>，并存储在二进制浮点矩阵中。<strong>数据包括每个点的三维位置和相关的反射率信息</strong>。<strong>但是，由于 KITTI 数据是由单个车辆记录的，我们必须利用同一记录中的不同时间段来模拟由两辆车生成的数据</strong>。因此，KITTI 数据只适用于某些测试场景。<p>为了解决这个问题，我们在两辆名为汤姆和杰瑞（T&J）的车辆上安装了必要的传感器，如激光雷达（Velodyne VLP-16）、摄像头（Allied Vision Mako G-319C）、雷达（Delphi ESR 2.5）、IMU&GPS（Xsens MTi-G-710 套件）和边缘计算设备（Nvidia Drive PX2），以便在我们学校的校园内收集所需的数据。我们的车辆配有 16 波束 Velodyn 激光雷达传感器，以二进制原始以太网数据包的形式存储数据。由于我们的车辆可以相互独立移动，因此我们能够用两辆车在真实环境中测试各种场景。<h4 id=训练细节><a class=headerlink href=#训练细节 title=训练细节></a>训练细节</h4><p>在停车场环境中，我们将距离车辆 20 米以内的物体视为高优先级物体，20 米以外的物体视为低优先级物体。<p>由于我们的激光雷达传感器只有 16 个光束，因此与更高端的激光雷达传感器相比，得到的点云数据相对稀疏。为了减轻稀疏数据带来的负面影响，我们将探测范围限制在沿 <strong>X、Y 和 Z 轴[0,70.4]X[-40,40] X [-3,1]</strong> 。我们不使用超出探测范围的数据。除了车辆的检测范围外，我们还<strong>将体素大小设置为 vD = 0.4 米、vH = 0.2 米、vW = 0.2 米，因此 D1 = 10、H1 = 400、W1 = 352</strong>。在我们的实验中，F-Cooper 框架在配备 GeForce GTX 1080 Ti GPU 的计算机上运行。<p>为了评估 F-Cooper，我们在实验中收集并测试了 200 多组数据。根据处理激光雷达数据的方法，我们将测试分为四类，方法（1）到（3）均来自相同的检测模型：（1）作为基线的非融合，（2）带有 VFF 的 F-Cooper，（3）带有 SFF 的 F-Cooper，以及（4）原始点云融合方法 - Cooper。特征融合在上述四种情况中随机进行，重点放在繁忙的校园停车场，因为由于遮挡物较多，这是最困难的情况。<h3 id=数据集-1><a class=headerlink href=#数据集-1 title=数据集></a>数据集</h3><h4 id=KITTI><a class=headerlink href=#KITTI title=KITTI></a>KITTI</h4><h4 id=nuScenes><a class=headerlink href=#nuScenes title=nuScenes></a>nuScenes</h4><p>nuScenes数据集（发音为/nuõsiõnz/）是Motional（前身为nuTonomy）团队开发的一个用于<strong>自动驾驶的公共大规模数据集</strong>。Motional正在使无人驾驶汽车成为一种安全、可靠和可访问的现实。通过向公众发布我们的一部分数据，Motional旨在支持公众对计算机视觉和自动驾驶的研究。<p>为此，我们收集了波士顿和新加坡的1000个驾驶场景，这两个城市以交通密集和极具挑战性的驾驶环境而闻名。20秒长的场景是手动选择的，以展示一组多样而有趣的驾驶动作、交通状况和意外行为。nuScenes的丰富复杂性将鼓励开发能够在每个场景有几十个物体的城市地区安全驾驶的方法。收集不同大陆的数据进一步使我们能够研究计算机视觉算法在不同地点、天气条件、车辆类型、植被、道路标记以及左右交通中的通用性。<p>所有检测结果均按照平均精度 (mAP)、平均平移误差 (mATE)、平均比例误差 (mASE)、平均方向误差 (mAOE)、平均速度误差 (AVE)、平均属性误差 (AAE) 和 nuScenes 检测得分 (NDS) 进行评估。<h4 id=The-Waymo-opendataset><a title="The Waymo opendataset" class=headerlink href=#The-Waymo-opendataset></a>The Waymo opendataset</h4><h3 id=学习资料><a class=headerlink href=#学习资料 title=学习资料></a>学习资料</h3><ol><li><a href=https://paperswithcode.com/task/3d-object-detection rel=noopener target=_blank>3D Object Detection | Papers With Code</a><li><a href=https://github.com/patrick-llgc/Learning-Deep-Learning rel=noopener target=_blank>patrick-llgc/Learning-Deep-Learning: Paper reading notes on Deep Learning and Machine Learning (github.com)</a><li><a href=https://www.stereolabs.com/docs/object-detection/ rel=noopener target=_blank>3D Object Detection Overview | Stereolabs</a><li><a href=https://zhuanlan.zhihu.com/p/591349104 rel=noopener target=_blank>系列二：3D Detection目标检测系列论文总结（2023年更） - 知乎 (zhihu.com)</a><li>3D点云<a href=https://github.com/HuangCongQing/3D-Point-Clouds rel=noopener target=_blank>HuangCongQing/3D-Point-Clouds: 🔥3D点云目标检测&语义分割(深度学习)-SOTA方法,代码,论文,数据集等 (github.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\10\23\DDNLP-深入NLP\ rel=bookmark>DDNLP:深入NLP</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\02\17\DLHLP学习\ rel=bookmark>DLHLP学习</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="3D Object Detection Learning" href=https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/>https://www.sekyoro.top/2023/10/30/3D-Object-Detection-Learning/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/Deep-Learning/ rel=tag><i class="fa fa-tag"></i> Deep Learning</a><a href=/tags/3D-Object-Detection/ rel=tag><i class="fa fa-tag"></i> 3D Object Detection</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/10/26/%E5%90%8E%E5%8F%B0%E6%89%A7%E8%A1%8C-%E4%BB%8Enohup%E5%88%B0tmux/ rel=prev title=后台执行:从nohup到tmux> <i class="fa fa-chevron-left"></i> 后台执行:从nohup到tmux </a></div><div class=post-nav-item><a href=/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ rel=next title=目标检测学习_P3> 目标检测学习_P3 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#Range-Aware-Attention-Network-for-LiDAR-based-3D-Object-Detection-with-Auxiliary-Point-Density-Level-Estimation><span class=nav-number>1.</span> <span class=nav-text>Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Abs><span class=nav-number>1.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Intro><span class=nav-number>1.2.</span> <span class=nav-text>Intro</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C><span class=nav-number>1.3.</span> <span class=nav-text>相关工作</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Object-detection-with-anchors><span class=nav-number>1.3.1.</span> <span class=nav-text>Object detection with anchors</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Object-detection-without-anchors><span class=nav-number>1.3.2.</span> <span class=nav-text>Object detection without anchors</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection><span class=nav-number>2.</span> <span class=nav-text>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs><span class=nav-number>2.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C-1><span class=nav-number>2.2.</span> <span class=nav-text>相关工作</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84><span class=nav-number>2.3.</span> <span class=nav-text>网络结构</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Feature-learning-network><span class=nav-number>2.4.</span> <span class=nav-text>Feature learning network</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#RPN><span class=nav-number>2.5.</span> <span class=nav-text>RPN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E9%AB%98%E6%95%88%E5%AE%9E%E7%8E%B0><span class=nav-number>2.5.1.</span> <span class=nav-text>高效实现</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>2.6.</span> <span class=nav-text>损失函数</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5-3D%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1><span class=nav-number>3.</span> <span class=nav-text>协同感知 3D检测任务</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Collaborative-Perception-in-Autonomous-Driving-Methods-Datasets-and-Challenges><span class=nav-number>3.1.</span> <span class=nav-text>Collaborative Perception in Autonomous Driving:Methods,Datasets and Challenges</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Collaboration-scheme><span class=nav-number>3.2.</span> <span class=nav-text>Collaboration scheme</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%97%A9%E6%9C%9F%E8%9E%8D%E5%90%88><span class=nav-number>3.2.1.</span> <span class=nav-text>早期融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%B8%AD%E6%9C%9F><span class=nav-number>3.2.2.</span> <span class=nav-text>中期</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%99%9A%E6%9C%9F><span class=nav-number>3.3.</span> <span class=nav-text>晚期</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%90%88-Raw-Data-Fusion><span class=nav-number>3.4.</span> <span class=nav-text>原始数据融合(Raw Data Fusion)</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#customized-communication-mechanism><span class=nav-number>3.5.</span> <span class=nav-text>customized communication mechanism</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Feature-Fusion><span class=nav-number>3.6.</span> <span class=nav-text>Feature Fusion</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BC%A0%E7%BB%9F%E8%9E%8D%E5%90%88><span class=nav-number>3.6.1.</span> <span class=nav-text>传统融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%9B%BE%E8%9E%8D%E5%90%88><span class=nav-number>3.6.2.</span> <span class=nav-text>图融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Attention-based><span class=nav-number>3.6.3.</span> <span class=nav-text>Attention-based</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.6.4.</span> <span class=nav-text>协同感知数据集</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds><span class=nav-number>3.7.</span> <span class=nav-text>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Abs-1><span class=nav-number>3.7.1.</span> <span class=nav-text>Abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds><span class=nav-number>3.8.</span> <span class=nav-text>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Abs-2><span class=nav-number>3.8.1.</span> <span class=nav-text>Abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Intro-1><span class=nav-number>3.9.</span> <span class=nav-text>Intro</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Proposed-Solution><span class=nav-number>3.10.</span> <span class=nav-text>Proposed Solution</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Fusion-Characteristics><span class=nav-number>3.10.1.</span> <span class=nav-text>Fusion Characteristics</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Compression-and-Transmission><span class=nav-number>3.10.2.</span> <span class=nav-text>Compression and Transmission</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Generic-and-Inherent-Properties><span class=nav-number>3.10.3.</span> <span class=nav-text>Generic and Inherent Properties</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Voxel-Features-Fusion><span class=nav-number>3.11.</span> <span class=nav-text>Voxel Features Fusion</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Spatial-Feature-Fusion><span class=nav-number>3.12.</span> <span class=nav-text>Spatial Feature Fusion</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BD%BF%E7%94%A8%E8%9E%8D%E5%90%88%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B><span class=nav-number>3.13.</span> <span class=nav-text>使用融合特征进行目标检测</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8C%BA%E5%9F%9F%E5%BB%BA%E8%AE%AE%E7%BD%91%E7%BB%9C><span class=nav-number>3.13.1.</span> <span class=nav-text>区域建议网络</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1><span class=nav-number>3.13.2.</span> <span class=nav-text>损失函数</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>3.13.3.</span> <span class=nav-text>数据集</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82><span class=nav-number>3.13.4.</span> <span class=nav-text>训练细节</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86-1><span class=nav-number>3.14.</span> <span class=nav-text>数据集</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#KITTI><span class=nav-number>3.14.1.</span> <span class=nav-text>KITTI</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#nuScenes><span class=nav-number>3.14.2.</span> <span class=nav-text>nuScenes</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#The-Waymo-opendataset><span class=nav-number>3.14.3.</span> <span class=nav-text>The Waymo opendataset</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99><span class=nav-number>3.15.</span> <span class=nav-text>学习资料</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>151</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>178</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a></ul></div><div class="motion-element announcement"><div class=title>注意</div><p class=content>由于最近图床更新,可能有些图片显示不了.如果发现了有些图片无法显示影响阅读的,还烦请联系我,我有空补上.<p class=date>2023-10-6</div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/3D-Object-Detection/ rel=tag>3D Object Detection</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/Deep-Learning/ rel=tag>Deep Learning</a><span class=tag-list-count>3</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2023</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>944k</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>14:18</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>