<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。 name=description><meta content=article property=og:type><meta content=目标检测学习_P2 property=og:title><meta content=https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。 property=og:description><meta content=zh_CN property=og:locale><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/selective-search-algorithm.png property=og:image><meta content=https://i.imgur.com/8NYb1o7.png property=og:image><meta content=https://i.imgur.com/j8O2rCN.png property=og:image><meta content=https://i.imgur.com/elheTvF.png property=og:image><meta content=https://i.imgur.com/YTCcKjA.png property=og:image><meta content=https://i.imgur.com/2xOfOpy.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/fast-RCNN.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-pooling.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/l1-smooth.png property=og:image><meta content=https://i.imgur.com/xsrzh3i.png property=og:image><meta content=https://miro.medium.com/v2/resize:fit:700/1*U95lm-Jkwkpy3p8X6IOKlQ.png property=og:image><meta content=https://i.imgur.com/2leLfbr.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/faster-RCNN.png property=og:image><meta content=http://zh.d2l.ai/_images/faster-rcnn.svg property=og:image><meta content=https://i.imgur.com/BL8dkf1.png property=og:image><meta content=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/mask-rcnn.png property=og:image><meta content=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-align.png property=og:image><meta content=https://i.imgur.com/Md5zxS2.png property=og:image><meta content=https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png property=og:image><meta content=https://i.imgur.com/BaAJNpE.png property=og:image><meta content=https://pic1.zhimg.com/80/v2-0c3fd2c903db6887128ec390b8981ef0_720w.webp property=og:image><meta content=https://i.imgur.com/fwsdL0f.png property=og:image><meta content=https://i.imgur.com/9R3Qfia.png property=og:image><meta content=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png property=og:image><meta content=2023-10-17T02:21:51.000Z property=article:published_time><meta content=2024-07-01T08:22:27.470Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content=SSD property=article:tag><meta content=YOLO property=article:tag><meta content=summary name=twitter:card><meta content=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png name=twitter:image><link href=https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>目标检测学习_P2 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>目标检测学习_P2</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-10-17 10:21:51" datetime=2023-10-17T10:21:51+08:00>2023-10-17</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-07-01 16:22:27" datetime=2024-07-01T16:22:27+08:00 itemprop=dateModified>2024-07-01</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>25k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>23 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>R-CNN家族。它们都是基于区域的目标检测算法。它们可以实现高精度，但对于自动驾驶等特定应用来说可能太慢。</p><span id=more></span><p>R-CNN家族中的模型都是基于regions的。检测分为两个阶段：<p>（1）首先，该模型通过<strong>选择搜索</strong>或<strong>区域建议网络</strong>来提出一组感兴趣的区域。所提出的区域是稀疏的，因为潜在的边界框候选者可以是无限的。<p>（2） 然后分类器只处理候选区域。<p>这里深入细节实现R-CNN系列的检测网络.<h3 id=R-CNN><a class=headerlink href=#R-CNN title=R-CNN></a>R-CNN</h3><blockquote><p>R-CNN (<a href=https://arxiv.org/abs/1311.2524 rel=noopener target=_blank>Girshick et al., 2014</a>) is short for “Region-based Convolutional Neural Networks”. The main idea is composed of two steps. First, using <a href=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/#selective-search rel=noopener target=_blank>selective search</a>, it identifies a manageable number of bounding-box object region candidates (“region of interest” or “RoI”). And then it extracts CNN features from each region independently for classification.</blockquote><p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/RCNN.png><h4 id=选择算法-selective-search><a title="选择算法(selective search)" class=headerlink href=#选择算法-selective-search></a>选择算法(selective search)</h4><p>主要涉及到选择算法,用于提供可能包含对象的区域建议。它建立在图像分割输出的基础上，并使用基于区域的特征（注意：不仅仅是单个像素的属性）来进行自下而上的分层分组。<ol><li>在初始化阶段，首先应用Felzenszwalb和Huttenlocher的基于图的图像分割算法来创建区域。<li>使用贪婪算法迭代地将区域分组在一起：<ul><li>首先计算所有相邻区域之间的相似性。<li>将两个最相似的区域分组在一起，并计算得到的区域与其相邻区域之间的新相似性。</ul><li>重复对最相似区域进行分组的过程（步骤2），直到整个图像变成单个区域。</ol><p>可以使用颜色,材质,大小和形状作为相似度量.<p><img alt=img data-src=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/selective-search-algorithm.png style=zoom:67%;><h4 id=流程><a class=headerlink href=#流程 title=流程></a>流程</h4><p>R-CNN流程:<ol><li><p>使用一个预训练CNN网络,假设网络输出是K类.</p><li><p>通过选择性搜索提出与类别无关的感兴趣区域（每个图像约2k个候选）。这些区域可能包含目标对象，并且它们具有不同的大小。</p><li><p>区域被扭曲成一个固定大小.</p><li><p>对于第K+1类,在一个扭曲的候选区域上微调CNN(附加的一个类指的是背景（没有感兴趣的对象)）。在微调阶段，我们应该使用更小的学习率，并且小批量对阳性病例进行过采样，因为大多数提出的区域只是背景。</p><li><p>给定每个图像区域，通过CNN的一次正向传播生成一个特征向量。然后，该特征向量输入针对每个类独立训练的二进制SVM。</p> <p>正样本是IoU>=0.3的区域，而负样本是不相关的其他区域。</p><li><p>为了减少定位误差，训练回归模型来使用CNN特征校正边界框校正偏移上的预测检测窗口。</p></ol><p><img alt=image-20231022172302856 data-src=https://i.imgur.com/8NYb1o7.png><h4 id=常用技巧><a class=headerlink href=#常用技巧 title=常用技巧></a>常用技巧</h4><ol><li>Non-Maximum Suppression</ol><p>模型可能能够为同一对象找到多个边界框。非极大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。然后跳过与这个边界框具有高IoU（即大于0.5）的剩余框,重复这个过程直到挑选出需要数量的bbox</strong><blockquote><p>非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于人脸的概率 分别为A、B、C、D、E、F。<ol><li>从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<li>假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<li>从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<li>就这样一直重复，找到所有被保留下来的矩形框。</ol></blockquote><ol><li>Hard Negative Mining</ol><p>我们将没有对象的边界框视为Negative示例。<p>并非所有的Negative例子都同样难以识别。例如，如果它包含纯空背景，那么它很可能是一个“容易否定的”；但是，<strong>如果盒子中包含奇怪的嘈杂纹理或部分对象，可能很难被识别为背景，这些都是“硬阴性”</strong>。严厉的反面例子很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。<blockquote><p>也就说增加容易被FP的数据</blockquote><p>通过查看R-CNN的学习步骤，您可以很容易地发现训练R-CNN模型既昂贵又缓慢，因为以下步骤需要大量工作：<ol><li>运行选择性搜索，为每个图像提出2000个区域候选<li>为每个图像区域生成CNN特征向量（N个图像*2000）<li>整个过程分别涉及三个模型，没有太多的共享计算：用于图像分类和特征提取的卷积神经网络；用于识别目标对象的顶部SVM分类器；以及用于收紧区域边界框的回归模型。</ol><h3 id=实战><a class=headerlink href=#实战 title=实战></a>实战</h3><p>opencv实现了选择性算法.<p>可以参考<a href=https://github.com/Hulkido/RCNN/tree/master rel=noopener target=_blank>Hulkido/RCNN: FULL Implementation of RCNN from scratch (github.com)</a><h4 id=生成Region-proposals><a title="生成Region proposals" class=headerlink href=#生成Region-proposals></a>生成Region proposals</h4><blockquote><p>区域建议(Region proposals)只是图像的较小区域，可能包含我们在输入图像中搜索的对象。为了减少R-CNN中的区域建议，使用了一种称为选择性搜索的贪婪算法。</blockquote><p>首先需要定义IoU计算<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=comment># calculating dimension of common area between these two boxes.</span></span><br><span class=line>x_left = <span class=built_in>max</span>(bb1[<span class=string>'x1'</span>], bb2[<span class=string>'x1'</span>])</span><br><span class=line>y_bottom = <span class=built_in>max</span>(bb1[<span class=string>'y1'</span>], bb2[<span class=string>'y1'</span>])</span><br><span class=line>x_right = <span class=built_in>min</span>(bb1[<span class=string>'x2'</span>], bb2[<span class=string>'x2'</span>])</span><br><span class=line>y_top = <span class=built_in>min</span>(bb1[<span class=string>'y2'</span>], bb2[<span class=string>'y2'</span>])</span><br><span class=line><span class=comment># if there is no overlap output 0 as intersection area is zero.</span></span><br><span class=line><span class=keyword>if</span> x_right < x_left <span class=keyword>or</span> y_bottom < y_top:</span><br><span class=line>    <span class=keyword>return</span> <span class=number>0.0</span></span><br><span class=line><span class=comment># calculating intersection area.</span></span><br><span class=line><span class=comment># 计算交集</span></span><br><span class=line>intersection_area = (x_right - x_left) * (y_top - y_bottom)</span><br><span class=line><span class=comment># individual areas of both these bounding boxes.</span></span><br><span class=line><span class=comment># 计算各自区域面积</span></span><br><span class=line>bb1_area = (bb1[<span class=string>'x2'</span>] - bb1[<span class=string>'x1'</span>]) * (bb1[<span class=string>'y2'</span>] - bb1[<span class=string>'y1'</span>])</span><br><span class=line>bb2_area = (bb2[<span class=string>'x2'</span>] - bb2[<span class=string>'x1'</span>]) * (bb2[<span class=string>'y2'</span>] - bb2[<span class=string>'y1'</span>])</span><br><span class=line><span class=comment># union area = area of bb1_+ area of bb2 - intersection of bb1 and bb2.</span></span><br><span class=line><span class=comment># 并集就是各自之和减去交集</span></span><br><span class=line>iou = intersection_area / <span class=built_in>float</span>(bb1_area + bb2_area - intersection_area)</span><br></pre></table></figure><p>遍历选择性搜索得到的区域,计算每个区域与对应bbox(bounding box)的IoU.<p>然后将iou大于阈值(这里设置为0.7)的定为region proposal,同时resize这个区域建议.<blockquote><p>本身应该是warp,但我觉得差别不大.另外region proposal和ROI差别其实也不大,有说法是<strong>region proposal是对图像而言的，roi是针对feature map上的</strong>.</blockquote><p><img alt=image-20231022171507221 data-src=https://i.imgur.com/j8O2rCN.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2</span><br><span class=line>cv2.setUseOptimized(<span class=literal>True</span>);</span><br><span class=line>ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span><br><span class=line>ss.setBaseImage(image)   <span class=comment># setting given image as base image</span></span><br><span class=line>            ss.switchToSelectiveSearchFast()     <span class=comment># running selective search on bae image</span></span><br><span class=line>            ssresults = ss.process()     <span class=comment># processing to get the outputs</span></span><br><span class=line>            imout = image.copy()</span><br><span class=line>            counter = <span class=number>0</span></span><br><span class=line>            falsecounter = <span class=number>0</span></span><br><span class=line>            flag = <span class=number>0</span></span><br><span class=line>            fflag = <span class=number>0</span></span><br><span class=line>            bflag = <span class=number>0</span></span><br><span class=line>            <span class=keyword>for</span> e,result <span class=keyword>in</span> <span class=built_in>enumerate</span>(ssresults):</span><br><span class=line>                <span class=keyword>if</span> e < <span class=number>2000</span> <span class=keyword>and</span> flag == <span class=number>0</span>:     <span class=comment># till 2000 to get top 2000 regions only</span></span><br><span class=line>                    <span class=keyword>for</span> gtval <span class=keyword>in</span> gtvalues:</span><br><span class=line>                        x,y,w,h = result</span><br><span class=line>                        iou = get_iou(gtval,{<span class=string>"x1"</span>:x,<span class=string>"x2"</span>:x+w,<span class=string>"y1"</span>:y,<span class=string>"y2"</span>:y+h})  <span class=comment># calculating IoU for each of the proposed regions</span></span><br><span class=line>                        <span class=keyword>if</span> counter < <span class=number>30</span>:       <span class=comment># getting only 30 psoitive examples</span></span><br><span class=line>                            <span class=keyword>if</span> iou > <span class=number>0.70</span>:     <span class=comment># IoU or being positive is 0.7</span></span><br><span class=line>                                timage = imout[x:x+w,y:y+h]</span><br><span class=line>                                resized = cv2.resize(timage, (<span class=number>224</span>,<span class=number>224</span>), interpolation = cv2.INTER_AREA)</span><br><span class=line>                                train_images.append(resized)</span><br><span class=line>                                train_labels.append(<span class=number>1</span>)</span><br><span class=line>                                counter += <span class=number>1</span></span><br><span class=line>                        <span class=keyword>else</span> :</span><br><span class=line>                            fflag =<span class=number>1</span>              <span class=comment># to insure we have collected all psotive examples</span></span><br><span class=line>                        <span class=keyword>if</span> falsecounter <<span class=number>30</span>:      <span class=comment># 30 negatve examples are allowed only</span></span><br><span class=line>                            <span class=keyword>if</span> iou < <span class=number>0.3</span>:         <span class=comment># IoU or being negative is 0.3</span></span><br><span class=line>                                timage = imout[x:x+w,y:y+h]</span><br><span class=line>                                resized = cv2.resize(timage, (<span class=number>224</span>,<span class=number>224</span>), interpolation = cv2.INTER_AREA)</span><br><span class=line>                                train_images.append(resized)</span><br><span class=line>                                train_labels.append(<span class=number>0</span>)</span><br><span class=line>                                falsecounter += <span class=number>1</span></span><br><span class=line>                        <span class=keyword>else</span> :</span><br><span class=line>                            bflag = <span class=number>1</span>             <span class=comment>#to ensure we have collected all negative examples</span></span><br><span class=line>                    <span class=keyword>if</span> fflag == <span class=number>1</span> <span class=keyword>and</span> bflag == <span class=number>1</span>:</span><br><span class=line>                        <span class=built_in>print</span>(<span class=string>"inside"</span>)</span><br><span class=line>                        flag = <span class=number>1</span>        <span class=comment># to signal the complition of data extaction from a particular image</span></span><br></pre></table></figure><p>上面一共最多遍历生成的2000个ROI,选取其中的正例(超过阈值)的30,负例30.<p>下面是一张图像与其得到的正例和负例<p><img alt=image-20231022174714613 data-src=https://i.imgur.com/elheTvF.png><p><img alt=image-20231022174748063 data-src=https://i.imgur.com/YTCcKjA.png><h4 id=使用CNN模型二分类><a class=headerlink href=#使用CNN模型二分类 title=使用CNN模型二分类></a>使用CNN模型二分类</h4><p>一般直接使用预训练模型,这里使用keras,加载VGG16模型.这里只做目标是否在而不做具体分类,所以输出单个值作为二分类.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br></pre><td class=code><pre><span class=line>vgg = tf.keras.applications.vgg16.VGG16(include_top=<span class=literal>True</span>, weights=<span class=string>'imagenet'</span>, input_tensor=<span class=literal>None</span>, input_shape=<span class=literal>None</span>, pooling=<span class=literal>None</span>, classes=<span class=number>1000</span>)</span><br><span class=line><span class=keyword>for</span> layer <span class=keyword>in</span> vgg.layers[:-<span class=number>2</span>]:</span><br><span class=line>  layer.trainable = <span class=literal>False</span></span><br><span class=line>x = vgg.get_layer(<span class=string>'fc2'</span>)</span><br><span class=line>last_output =  x.output</span><br><span class=line>x = tf.keras.layers.Dense(<span class=number>1</span>,activation = <span class=string>'sigmoid'</span>)(last_output)</span><br><span class=line>model = tf.keras.Model(vgg.<span class=built_in>input</span>,x)</span><br><span class=line>model.<span class=built_in>compile</span>(optimizer = <span class=string>"adam"</span>,</span><br><span class=line>              loss = <span class=string>'binary_crossentropy'</span>,</span><br><span class=line>              metrics = [<span class=string>'acc'</span>])</span><br><span class=line></span><br><span class=line>model.summary()</span><br><span class=line>model.fit(X_new,Y_new,batch_size = <span class=number>64</span>,epochs = <span class=number>3</span>, verbose = <span class=number>1</span>,validation_split=<span class=number>.05</span>,shuffle = <span class=literal>True</span>)</span><br></pre></table></figure><h4 id=预训练模型提取特征再使用SVM二分类><a class=headerlink href=#预训练模型提取特征再使用SVM二分类 title=预训练模型提取特征再使用SVM二分类></a>预训练模型提取特征再使用SVM二分类</h4><p>原文中为每一类使用了一个二分类的SVM(毕竟一般的一个SVM只能二分类)<p>可以再上面的预训练模型只使用特征提取层,然后加个SVM<h4 id=bbox-regression><a title="bbox regression" class=headerlink href=#bbox-regression></a>bbox regression</h4><p>最后需要求的bbox的回归用于修正误差.<blockquote><p>回归后得到四个参数，即x，y中心点偏移量和高、宽缩放因子，利用这四个参数对剩余的高质量目标建议框进行调整，取得分最高的称为Bounding Box，完成定位任务。</blockquote><p><img alt=image-20231031112707018 data-src=https://i.imgur.com/2xOfOpy.png><p>如图,x,y坐标的修正由p~w~d~x~(p)+p~x~,而w,h由p~w~exp(d~w~(p))修正,已知t~i~函数,需要求d~i~(p)的回归值.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line>curr_iou = iou([gta[bbox_num, <span class=number>0</span>], gta[bbox_num, <span class=number>2</span>], gta[bbox_num, <span class=number>1</span>], gta[bbox_num, <span class=number>3</span>]], [x1_anc, y1_anc, x2_anc, y2_anc])</span><br><span class=line>						<span class=comment># calculate the regression targets if they will be needed</span></span><br><span class=line>						<span class=keyword>if</span> curr_iou > best_iou_for_bbox[bbox_num] <span class=keyword>or</span> curr_iou > C.rpn_max_overlap:</span><br><span class=line>							cx = (gta[bbox_num, <span class=number>0</span>] + gta[bbox_num, <span class=number>1</span>]) / <span class=number>2.0</span> <span class=comment># center point of bbox</span></span><br><span class=line>							cy = (gta[bbox_num, <span class=number>2</span>] + gta[bbox_num, <span class=number>3</span>]) / <span class=number>2.0</span></span><br><span class=line>							cxa = (x1_anc + x2_anc)/<span class=number>2.0</span> <span class=comment># center point of anchor box which scales to resized image</span></span><br><span class=line>							cya = (y1_anc + y2_anc)/<span class=number>2.0</span></span><br><span class=line></span><br><span class=line>							tx = (cx - cxa) / (x2_anc - x1_anc) <span class=comment># regression targets</span></span><br><span class=line>							ty = (cy - cya) / (y2_anc - y1_anc)</span><br><span class=line>							tw = np.log((gta[bbox_num, <span class=number>1</span>] - gta[bbox_num, <span class=number>0</span>]) / (x2_anc - x1_anc))</span><br><span class=line>							th = np.log((gta[bbox_num, <span class=number>3</span>] - gta[bbox_num, <span class=number>2</span>]) / (y2_anc - y1_anc))</span><br><span class=line>                            						<span class=keyword>if</span> img_data[<span class=string>'bboxes'</span>][bbox_num][<span class=string>'class'</span>] != <span class=string>'bg'</span>:</span><br><span class=line></span><br><span class=line>							<span class=comment># all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best</span></span><br><span class=line>							<span class=keyword>if</span> curr_iou > best_iou_for_bbox[bbox_num]:</span><br><span class=line>								best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]</span><br><span class=line>								best_iou_for_bbox[bbox_num] = curr_iou</span><br><span class=line>								best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]</span><br><span class=line>								best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]</span><br></pre></table></figure><h3 id=Fast-R-CNN><a title="Fast R-CNN" class=headerlink href=#Fast-R-CNN></a>Fast R-CNN</h3><p>为了使R-CNN更快，Girshick（2015）通过将三个独立的模型统一到一个联合训练的框架中并增加共享计算结果（称为Fast R-CNN）来改进训练过程。<p>不同于R-CNN对于每个region proposals提取特征,，而是将它们聚合到整个图像上的一个 CNN 前向传递中，并且region proposals共享此特征矩阵。然后，将相同的特征矩阵分支出来，用于学习对象分类器和边界框回归器。总之，计算共享加速了R-CNN。<p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/fast-RCNN.png><h4 id=ROI-Pooling><a title="ROI Pooling" class=headerlink href=#ROI-Pooling></a>ROI Pooling</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>x = torch.arange(<span class=number>16</span>).reshape(<span class=number>1</span>,<span class=number>1</span>,<span class=number>4</span>,<span class=number>4</span>)</span><br><span class=line>rois = torch.tensor([[<span class=number>0</span>, <span class=number>0</span>, <span class=number>0</span>, <span class=number>20</span>, <span class=number>20</span>], [<span class=number>0</span>, <span class=number>0</span>, <span class=number>10</span>, <span class=number>30</span>, <span class=number>30</span>]])</span><br><span class=line>torchvision.ops.roi_pool(X,rois, output_size=(<span class=number>2</span>, <span class=number>2</span>), spatial_scale=<span class=number>1.0</span>)</span><br></pre></table></figure><p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-pooling.png style=zoom:67%;><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>roi_pooling</span>(<span class=params>features, rois, output_size</span>):</span></span><br><span class=line>    <span class=comment># features: 输入特征图 (N, C, H, W)</span></span><br><span class=line>    <span class=comment># rois: 区域候选框 (N, 4) 其中每行表示一个候选框的坐标 (x1, y1, x2, y2)</span></span><br><span class=line>    <span class=comment># output_size: ROI Pooling的输出尺寸 (H', W')</span></span><br><span class=line></span><br><span class=line>    num_rois = rois.size(<span class=number>0</span>)</span><br><span class=line>    output = torch.zeros(num_rois, features.size(<span class=number>1</span>), output_size, output_size)</span><br><span class=line></span><br><span class=line>    <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(num_rois):</span><br><span class=line>        roi = rois[i]</span><br><span class=line>        x1, y1, x2, y2 = roi</span><br><span class=line>        roi_features = features[:, :, y1:y2 + <span class=number>1</span>, x1:x2 + <span class=number>1</span>]</span><br><span class=line>        roi_features = F.adaptive_max_pool2d(roi_features, output_size)</span><br><span class=line>        output[i] = roi_features</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> output</span><br></pre></table></figure><blockquote><p>上面的算法其实使用的是pytorch的adaptive_max_pool2d,这两个东西并不一样,但很多简易实现就是利用了这个函数</blockquote><p>由于要进行分类和回归(都使用一个FC),需要一个固定大小的输入.由于此时输入已经成了可训练的feature map而不是直接的图像,需要一个可微分的操作.<p>RoI 池化类似于 max-pooling。说白了就是将原本的region proposals分成hxw的grid,这里的h,w就是后面fc需要的输入,在每个grid中做max pooling<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>FastRCNN</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, num_classes</span>):</span></span><br><span class=line>        <span class=built_in>super</span>().__init__()</span><br><span class=line>        self.num_classes = num_classes</span><br><span class=line>        vgg = torchvision.models.vgg19_bn(pretrained=<span class=literal>True</span>)</span><br><span class=line>        self.features = nn.Sequential(*<span class=built_in>list</span>(vgg.features.children())[:-<span class=number>1</span>])</span><br><span class=line>        self.roipool = ROIPooling(output_size=(<span class=number>7</span>, <span class=number>7</span>))</span><br><span class=line>        <span class=comment>#roipooling之后得到B,C,7,7直接</span></span><br><span class=line>        self.output = nn.Sequential(*<span class=built_in>list</span>(vgg.classifier.children())[:-<span class=number>1</span>])</span><br><span class=line>        self.prob = nn.Linear(<span class=number>4096</span>, num_classes+<span class=number>1</span>)</span><br><span class=line>        self.loc = nn.Linear(<span class=number>4096</span>, <span class=number>4</span> * (num_classes + <span class=number>1</span>))</span><br><span class=line></span><br><span class=line>        self.cat_loss = nn.CrossEntropyLoss()</span><br><span class=line>        self.loc_loss = nn.SmoothL1Loss()</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, img, rois, roi_idx</span>):</span></span><br><span class=line>        res = self.features(img)</span><br><span class=line>        res = self.roipool(res, rois, roi_idx)</span><br><span class=line>        res = res.view(res.shape[<span class=number>0</span>], -<span class=number>1</span>)</span><br><span class=line>        features = self.output(res)</span><br><span class=line>        prob = self.prob(features)</span><br><span class=line>        loc = self.loc(features).view(-<span class=number>1</span>, self.num_classes+<span class=number>1</span>, <span class=number>4</span>)</span><br><span class=line>        <span class=keyword>return</span> prob, loc</span><br><span class=line>    </span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>loss</span>(<span class=params>self, prob, bbox, label, gt_bbox, lmb=<span class=number>1.0</span></span>):</span></span><br><span class=line>        loss_cat = self.cat_loss(prob, label)</span><br><span class=line>        lbl = label.view(-<span class=number>1</span>, <span class=number>1</span>, <span class=number>1</span>).expand(label.size(<span class=number>0</span>), <span class=number>1</span>, <span class=number>4</span>)</span><br><span class=line>        mask = (label != <span class=number>0</span>).<span class=built_in>float</span>().view(-<span class=number>1</span>, <span class=number>1</span>, <span class=number>1</span>).expand(label.shape[<span class=number>0</span>], <span class=number>1</span>, <span class=number>4</span>)</span><br><span class=line>        loss_loc = self.loc_loss(gt_bbox * mask, bbox.gather(<span class=number>1</span>, lbl).squeeze(<span class=number>1</span>) * mask)</span><br><span class=line>        loss = loss_cat + lmb * loss_loc</span><br><span class=line>        <span class=keyword>return</span> loss, loss_cat, loss_loc</span><br></pre></table></figure><ol><li>首先，在图像分类任务上预训练卷积神经网络。<li>通过选择性搜索提出区域建议（每张图像~2k个候选者）。<li>更改预训练的 CNN：<li><ul><li>将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。<li>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。</ul><li>最后，模型分为两个输出:K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。</ol><p>损失函数有分类损失和回归损失,回归损失用于计算bouding box的回归损失,可使用L1损失。</p><script type="math/tex; mode=display">
L_1^\text{smooth}(x) = \begin{cases}
    0.5 x^2             & \text{if } \vert x \vert < 1\\
    \vert x \vert - 0.5 & \text{otherwise}
\end{cases}</script><p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/l1-smooth.png><blockquote><p>在Fast RCNN中,改进并不显著，因为区域提案是由另一个模型单独生成的，而且非常耗时。</blockquote><h4 id=流程-1><a class=headerlink href=#流程-1 title=流程></a>流程</h4><p>首先，在图像分类任务上预训练卷积神经网络。<p>通过选择性搜索提出区域建议（每张图像~2k个候选者）。<p>更改预训练的 CNN：将预训练 CNN 的最后一个最大池化层替换为 RoI 池化层。<p>RoI 池化层输出区域建议的固定长度特征向量。共享 CNN 计算很有意义，因为相同图像的许多区域建议是高度重叠的。<p>将最后一个全连接层和最后一个 softmax 层（K 类）替换为全连接层和 K + 1 类上的 softmax。最后，模型分支为两个输出层：K + 1 类的 softmax 估计器（与 R-CNN 相同，+1 是“背景”类），输出每个 RoI 的离散概率分布。一个边界框回归模型，用于预测每个 K 类相对于原始 RoI 的偏移量。<p>其中关键是如何利用选择搜索得到的region proposals映射到通过CNN得到的feature map上,这样就只用在整个图像上进行一次CNN而不是单独在每个region proposal上滤波.如果只进行一次滤波,如何<code>准确地将输入图像的一个区域投影到卷积特征图的一个区域上</code>,此外还有ROI Pooling得到固定的ROI区域也是关键.<p>SPPNet介绍了 ROI Projection,看起来貌似就是算回去,不过在设计CNN时,padding = kernel_size/2这样避免计算复杂<p><img alt=image-20231029125650323 data-src=https://i.imgur.com/xsrzh3i.png><p>​<p><img alt=img data-src=https://miro.medium.com/v2/resize:fit:700/1*U95lm-Jkwkpy3p8X6IOKlQ.png><p>Loss包括分类损失和bouding box回归损失<p><img alt=image-20231029204052670 data-src=https://i.imgur.com/2leLfbr.png><h3 id=Faster-RNN><a title="Faster RNN" class=headerlink href=#Faster-RNN></a>Faster RNN</h3><p>重点说一下faster rcnn<p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/faster-RCNN.png><p>将选择性搜索算法(也就是用于生成region proposals的算法)融合到深度学习模型中<p><img alt=../_images/faster-rcnn.svg data-src=http://zh.d2l.ai/_images/faster-rcnn.svg><p>比如下面将一张图像通过一个模型<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br></pre><td class=code><pre><span class=line>ef nn_base(input_tensor=<span class=literal>None</span>, trainable=<span class=literal>False</span>):</span><br><span class=line></span><br><span class=line>    <span class=comment># Determine proper input shape</span></span><br><span class=line>    <span class=keyword>if</span> K.image_data_format() == <span class=string>'channels_first'</span>:</span><br><span class=line>        input_shape = (<span class=number>3</span>, <span class=literal>None</span>, <span class=literal>None</span>)</span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        input_shape = (<span class=literal>None</span>, <span class=literal>None</span>, <span class=number>3</span>)</span><br><span class=line></span><br><span class=line>    <span class=keyword>if</span> input_tensor <span class=keyword>is</span> <span class=literal>None</span>:</span><br><span class=line>        img_input = Input(shape=input_shape)</span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        <span class=keyword>if</span> <span class=keyword>not</span> K.is_keras_tensor(input_tensor):</span><br><span class=line>            img_input = Input(tensor=input_tensor, shape=input_shape)</span><br><span class=line>        <span class=keyword>else</span>:</span><br><span class=line>            img_input = input_tensor</span><br><span class=line></span><br><span class=line>    <span class=keyword>if</span> K.image_data_format() == <span class=string>'channels_last'</span>:</span><br><span class=line>        bn_axis = <span class=number>3</span></span><br><span class=line>    <span class=keyword>else</span>:</span><br><span class=line>        bn_axis = <span class=number>1</span></span><br><span class=line></span><br><span class=line>    x = ZeroPadding2D((<span class=number>3</span>, <span class=number>3</span>))(img_input)</span><br><span class=line></span><br><span class=line>    x = Convolution2D(<span class=number>64</span>, (<span class=number>7</span>, <span class=number>7</span>), strides=(<span class=number>2</span>, <span class=number>2</span>), name=<span class=string>'conv1'</span>, trainable = trainable)(x)</span><br><span class=line>    x = FixedBatchNormalization(axis=bn_axis, name=<span class=string>'bn_conv1'</span>)(x)</span><br><span class=line>    x = Activation(<span class=string>'relu'</span>)(x)</span><br><span class=line>    x = MaxPooling2D((<span class=number>3</span>, <span class=number>3</span>), strides=(<span class=number>2</span>, <span class=number>2</span>))(x)</span><br><span class=line></span><br><span class=line>    x = conv_block(x, <span class=number>3</span>, [<span class=number>64</span>, <span class=number>64</span>, <span class=number>256</span>], stage=<span class=number>2</span>, block=<span class=string>'a'</span>, strides=(<span class=number>1</span>, <span class=number>1</span>), trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>64</span>, <span class=number>64</span>, <span class=number>256</span>], stage=<span class=number>2</span>, block=<span class=string>'b'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>64</span>, <span class=number>64</span>, <span class=number>256</span>], stage=<span class=number>2</span>, block=<span class=string>'c'</span>, trainable = trainable)</span><br><span class=line></span><br><span class=line>    x = conv_block(x, <span class=number>3</span>, [<span class=number>128</span>, <span class=number>128</span>, <span class=number>512</span>], stage=<span class=number>3</span>, block=<span class=string>'a'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>128</span>, <span class=number>128</span>, <span class=number>512</span>], stage=<span class=number>3</span>, block=<span class=string>'b'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>128</span>, <span class=number>128</span>, <span class=number>512</span>], stage=<span class=number>3</span>, block=<span class=string>'c'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>128</span>, <span class=number>128</span>, <span class=number>512</span>], stage=<span class=number>3</span>, block=<span class=string>'d'</span>, trainable = trainable)</span><br><span class=line></span><br><span class=line>    x = conv_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'a'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'b'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'c'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'d'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'e'</span>, trainable = trainable)</span><br><span class=line>    x = identity_block(x, <span class=number>3</span>, [<span class=number>256</span>, <span class=number>256</span>, <span class=number>1024</span>], stage=<span class=number>4</span>, block=<span class=string>'f'</span>, trainable = trainable)</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> x</span><br></pre></table></figure><p>提出了<strong>Region Proposal Networks</strong><h4 id=Region-Proposal-Networks><a title="Region Proposal Networks" class=headerlink href=#Region-Proposal-Networks></a>Region Proposal Networks</h4><p>主要更改就是对于roi projection,之前还是在原图上使用ss得到region proposals然后映射到feature map上,现在用区域提案网络 (RPN)替代,其他部分不变.<p>RPN,简单地说，它是一个小型的全卷积网络，它接受feature map，并输出一组区域和每个区域的“objectiveness”分数（该区域包含对象的可能性）。<p><img alt=image-20231029172807621 data-src=https://i.imgur.com/BL8dkf1.png><p>首先使用一个cnn模型提取特征得到feature map,而RPN就会利用这个feature map,并且提出了anchor boxes概念,在feature map上使用一个大小为3x3的sliding window,在sliding window移动到一个位置的时候生成多个不同大小的anchor boxes.<p>在生成相关数据的时候,对于一个feature map,由于它已经经过多次卷积包含许多信息,在这个feature map上生成anchor box.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=comment># initialise empty output objectives</span></span><br><span class=line>	y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))</span><br><span class=line>	y_is_box_valid = np.zeros((output_height, output_width, num_anchors))</span><br><span class=line>	y_rpn_regr = np.zeros((output_height, output_width, num_anchors * <span class=number>4</span>))</span><br></pre></table></figure><p>其中num_anchors设定为9,包含三种不同大小以及比例的的anchor box.<p><code>y_rpn_overlap</code>表示这个anchor box是否包含物体,在生成数据时会将与bbox的iou大于一个阈值(比如0.7)的作为pos,小于一个阈值(0.3)作为neg表示不包含物体,也就是背景.在代码中,如果一个大于0.7的anchor box都没有那就直接取与bounding box的iou最大的作为包含物体的anchor box正例.<p><code>y_is_box_valid</code>表示所有正负例anchor box,去掉iou在0.3~0.7的,认为这些anchor box不明显.一般这个值会设置一个固定值,注意,生成的anchor box如果是pos,只表示其与其中一个或多个bbox的iou大于阈值,并不能知道其与哪个物体位置更相近.<p><code>y_rpn_regr</code>表示<code>anchor box</code>的具体位置<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>y_rpn_cls, y_rpn_regr = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class=number>1</span>)</span><br><span class=line>y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class=number>4</span>, axis=<span class=number>1</span>), y_rpn_regr], axis=<span class=number>1</span>)</span><br></pre></table></figure><p>计算rpn输出时为什么要concat这些数据?这是在代码实现层面的事,calc_rpn计算得到固定个数的正负例作为训练数据<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line><span class=comment># one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative</span></span><br><span class=line>	<span class=comment># regions. We also limit it to 256 regions.</span></span><br><span class=line>	num_regions = <span class=number>256</span></span><br><span class=line>	<span class=comment># 对于一张图 只选取256个anchor box</span></span><br><span class=line>	<span class=comment># change from len(pos_locs[0]) to len(pos_locs)</span></span><br><span class=line>	<span class=keyword>if</span> <span class=built_in>len</span>(pos_locs) > num_regions/<span class=number>2</span>:</span><br><span class=line>		val_locs = random.sample(<span class=built_in>range</span>(<span class=built_in>len</span>(pos_locs[<span class=number>0</span>])), <span class=built_in>len</span>(pos_locs[<span class=number>0</span>]) - num_regions/<span class=number>2</span>)</span><br><span class=line>		y_is_box_valid[<span class=number>0</span>, pos_locs[<span class=number>0</span>][val_locs], pos_locs[<span class=number>1</span>][val_locs], pos_locs[<span class=number>2</span>][val_locs]] = <span class=number>0</span></span><br><span class=line>		num_pos = num_regions/<span class=number>2</span></span><br><span class=line></span><br><span class=line>	<span class=keyword>if</span> <span class=built_in>len</span>(neg_locs) + num_pos > num_regions:</span><br><span class=line>		val_locs = random.sample(<span class=built_in>range</span>(<span class=built_in>len</span>(neg_locs[<span class=number>0</span>])), <span class=built_in>len</span>(neg_locs[<span class=number>0</span>]) - num_pos)</span><br><span class=line>		y_is_box_valid[<span class=number>0</span>, neg_locs[<span class=number>0</span>][val_locs], neg_locs[<span class=number>1</span>][val_locs], neg_locs[<span class=number>2</span>][val_locs]] = <span class=number>0</span></span><br><span class=line></span><br><span class=line>	<span class=comment>#  这里 y_rpn_regr 为什么要concatenate四个正例?</span></span><br><span class=line>	y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=<span class=number>1</span>)</span><br><span class=line>	y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, <span class=number>4</span>, axis=<span class=number>1</span>), y_rpn_regr], axis=<span class=number>1</span>)</span><br><span class=line>	<span class=comment>#  y_rpn_cls  (1, 18, 26, 35)</span></span><br><span class=line>	<span class=comment>#  y_rpn_regr (1, 72, 26, 35)</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>rpn_accuracy_rpn_monitor = []</span><br><span class=line>rpn_accuracy_for_epoch = []</span><br></pre></table></figure><p>训练代码中设计了<code>rpn_accuracy_rpn_monitor</code>和<code>rpn_accuracy_rpn_monitor</code>,前者会在处理过的图像达到一定数量后计算rpn的准确率,也就是生成的roi是否总是正例.<p>后者会在每次epoch完后计算mean loss和acc,因为在这个目标检测任务中,每次epoch的batch_size并不是像其他任务取多张图像,而是每次epoch中又取epoch_length,每次取一张图片.<h5 id=问题1><a class=headerlink href=#问题1 title=问题1></a>问题1</h5><blockquote><p>RPN如何设计的,3x3的sliding window有什么用,如果是为了生成anchor box,有必要设计一个sliding window吗?</blockquote><p>在通过一个预训练模型的feature层后(比如vgg,resnet,inception等等),再使用一个3x3的sliding window,通道数256(可以理解为再聚合一下信息),然后在原本的feature map上针对每个pixel生成多个anchor box.<p>遍历一个图像上的所有bbox,看它与生成的anchor box的iou,取一个大于0.7的最大的anchor box的pos,其余的小于0.3的是neg.<p>首先将feature map做一次3x3卷积,在进行全卷积的输出分别得到针对一个anchor box的二分类以及修正坐标(坐标回归)<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>rpn</span>(<span class=params>base_layers,num_anchors</span>):</span></span><br><span class=line></span><br><span class=line>    <span class=comment># important! same width,hight</span></span><br><span class=line>    x = Convolution2D(<span class=number>512</span>, (<span class=number>3</span>, <span class=number>3</span>), padding=<span class=string>'same'</span>, activation=<span class=string>'relu'</span>, kernel_initializer=<span class=string>'normal'</span>, name=<span class=string>'rpn_conv1'</span>)(base_layers)</span><br><span class=line></span><br><span class=line>    <span class=comment># get class and regr output</span></span><br><span class=line>    <span class=comment># fully convolution</span></span><br><span class=line>    x_class = Convolution2D(num_anchors, (<span class=number>1</span>, <span class=number>1</span>), activation=<span class=string>'sigmoid'</span>, kernel_initializer=<span class=string>'uniform'</span>, name=<span class=string>'rpn_out_class'</span>)(x)</span><br><span class=line>    x_regr = Convolution2D(num_anchors * <span class=number>4</span>, (<span class=number>1</span>, <span class=number>1</span>), activation=<span class=string>'linear'</span>, kernel_initializer=<span class=string>'zero'</span>, name=<span class=string>'rpn_out_regress'</span>)(x)</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> [x_class, x_regr, base_layers]</span><br></pre></table></figure><p>然后利用roi pooling以及classifier输出针对于21类的概率和20类回归坐标<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>classifier</span>(<span class=params>base_layers, input_rois, num_rois, nb_classes = <span class=number>21</span>, trainable=<span class=literal>False</span></span>):</span></span><br><span class=line></span><br><span class=line>    <span class=comment># compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround</span></span><br><span class=line></span><br><span class=line>    <span class=keyword>if</span> K.backend() == <span class=string>'tensorflow'</span>:</span><br><span class=line>        pooling_regions = <span class=number>14</span></span><br><span class=line>        input_shape = (num_rois,<span class=number>14</span>,<span class=number>14</span>,<span class=number>1024</span>)</span><br><span class=line>    <span class=keyword>elif</span> K.backend() == <span class=string>'theano'</span>:</span><br><span class=line>        pooling_regions = <span class=number>7</span></span><br><span class=line>        input_shape = (num_rois,<span class=number>1024</span>,<span class=number>7</span>,<span class=number>7</span>)</span><br><span class=line></span><br><span class=line>    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])</span><br><span class=line>    out = classifier_layers(out_roi_pool, input_shape=input_shape, trainable=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line>    out = TimeDistributed(Flatten())(out)</span><br><span class=line></span><br><span class=line>    out_class = TimeDistributed(Dense(nb_classes, activation=<span class=string>'softmax'</span>, kernel_initializer=<span class=string>'zero'</span>), name=<span class=string>'dense_class_{}'</span>.<span class=built_in>format</span>(nb_classes))(out)</span><br><span class=line>    <span class=comment># note: no regression target for bg class</span></span><br><span class=line>    out_regr = TimeDistributed(Dense(<span class=number>4</span> * (nb_classes-<span class=number>1</span>), activation=<span class=string>'linear'</span>, kernel_initializer=<span class=string>'zero'</span>), name=<span class=string>'dense_regress_{}'</span>.<span class=built_in>format</span>(nb_classes))(out)</span><br><span class=line>    <span class=keyword>return</span> [out_class, out_regr]</span><br></pre></table></figure><p>这样就训练三个模型<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>model_rpn = Model(img_input, rpn[:<span class=number>2</span>])</span><br><span class=line>model_classifier = Model([img_input, roi_input], classifier)</span><br><span class=line></span><br><span class=line><span class=comment># this is a model that holds both the RPN and the classifier, used to load/save weights for the models</span></span><br><span class=line>model_all = Model([img_input, roi_input], rpn[:<span class=number>2</span>] + classifier)</span><br></pre></table></figure><p>rpn损失计算,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>rpn_loss_regr</span>(<span class=params>num_anchors</span>):</span></span><br><span class=line>	<span class=function><span class=keyword>def</span> <span class=title>rpn_loss_regr_fixed_num</span>(<span class=params>y_true, y_pred</span>):</span></span><br><span class=line>		<span class=keyword>if</span> K.image_data_format() == <span class=string>'channels_first'</span>:</span><br><span class=line>			x = y_true[:, <span class=number>4</span> * num_anchors:, :, :] - y_pred</span><br><span class=line>			x_abs = K.<span class=built_in>abs</span>(x)</span><br><span class=line>			x_bool = K.less_equal(x_abs, <span class=number>1.0</span>)</span><br><span class=line>			<span class=keyword>return</span> lambda_rpn_regr * K.<span class=built_in>sum</span>(</span><br><span class=line>				y_true[:, :<span class=number>4</span> * num_anchors, :, :] * (x_bool * (<span class=number>0.5</span> * x * x) + (<span class=number>1</span> - x_bool) * (x_abs - <span class=number>0.5</span>))) / K.<span class=built_in>sum</span>(epsilon + y_true[:, :<span class=number>4</span> * num_anchors, :, :])</span><br><span class=line>		<span class=keyword>else</span>:</span><br><span class=line>			x = y_true[:, :, :, <span class=number>4</span> * num_anchors:] - y_pred</span><br><span class=line>			x_abs = K.<span class=built_in>abs</span>(x)</span><br><span class=line>			x_bool = K.cast(K.less_equal(x_abs, <span class=number>1.0</span>), tf.float32)</span><br><span class=line></span><br><span class=line>			<span class=keyword>return</span> lambda_rpn_regr * K.<span class=built_in>sum</span>(</span><br><span class=line>				y_true[:, :, :, :<span class=number>4</span> * num_anchors] * (x_bool * (<span class=number>0.5</span> * x * x) + (<span class=number>1</span> - x_bool) * (x_abs - <span class=number>0.5</span>))) / K.<span class=built_in>sum</span>(epsilon + y_true[:, :, :, :<span class=number>4</span> * num_anchors])</span><br><span class=line></span><br><span class=line>	<span class=keyword>return</span> rpn_loss_regr_fixed_num</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>rpn_loss_cls</span>(<span class=params>num_anchors</span>):</span></span><br><span class=line>	<span class=function><span class=keyword>def</span> <span class=title>rpn_loss_cls_fixed_num</span>(<span class=params>y_true, y_pred</span>):</span></span><br><span class=line>		<span class=keyword>if</span> K.image_data_format() == <span class=string>'channels_last'</span>:</span><br><span class=line>			<span class=keyword>return</span> lambda_rpn_class * K.<span class=built_in>sum</span>(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.<span class=built_in>sum</span>(epsilon + y_true[:, :, :, :num_anchors])</span><br><span class=line>		<span class=keyword>else</span>:</span><br><span class=line>			<span class=keyword>return</span> lambda_rpn_class * K.<span class=built_in>sum</span>(y_true[:, :num_anchors, :, :] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, num_anchors:, :, :])) / K.<span class=built_in>sum</span>(epsilon + y_true[:, :num_anchors, :, :])</span><br><span class=line></span><br><span class=line>	<span class=keyword>return</span> rpn_loss_cls_fixed_num</span><br></pre></table></figure><p>计算得到P_rpn也就是在feature maps上通过RPN网络得到的300个anchor boxes坐标以及sigmoid分数(表示是否包含物体)<p>然后调整anchor box使其区域在feature map内,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre><span class=line>anchor_x = (anchor_size * anchor_ratio[<span class=number>0</span>])/C.rpn_stride</span><br><span class=line>			anchor_y = (anchor_size * anchor_ratio[<span class=number>1</span>])/C.rpn_stride</span><br><span class=line>X, Y = np.meshgrid(np.arange(cols),np. arange(rows))</span><br><span class=line></span><br><span class=line>			A[<span class=number>0</span>, :, :, curr_layer] = X - anchor_x/<span class=number>2</span></span><br><span class=line>			A[<span class=number>1</span>, :, :, curr_layer] = Y - anchor_y/<span class=number>2</span></span><br><span class=line>			A[<span class=number>2</span>, :, :, curr_layer] = anchor_x</span><br><span class=line>			A[<span class=number>3</span>, :, :, curr_layer] = anchor_y</span><br><span class=line></span><br><span class=line>A[<span class=number>2</span>, :, :, curr_layer] = np.maximum(<span class=number>1</span>, A[<span class=number>2</span>, :, :, curr_layer])</span><br><span class=line>			A[<span class=number>3</span>, :, :, curr_layer] = np.maximum(<span class=number>1</span>, A[<span class=number>3</span>, :, :, curr_layer])</span><br><span class=line>			A[<span class=number>2</span>, :, :, curr_layer] += A[<span class=number>0</span>, :, :, curr_layer]</span><br><span class=line>			A[<span class=number>3</span>, :, :, curr_layer] += A[<span class=number>1</span>, :, :, curr_layer]</span><br><span class=line></span><br><span class=line>			A[<span class=number>0</span>, :, :, curr_layer] = np.maximum(<span class=number>0</span>, A[<span class=number>0</span>, :, :, curr_layer])</span><br><span class=line>			A[<span class=number>1</span>, :, :, curr_layer] = np.maximum(<span class=number>0</span>, A[<span class=number>1</span>, :, :, curr_layer])</span><br><span class=line>			A[<span class=number>2</span>, :, :, curr_layer] = np.minimum(cols-<span class=number>1</span>, A[<span class=number>2</span>, :, :, curr_layer])</span><br><span class=line>			A[<span class=number>3</span>, :, :, curr_layer] = np.minimum(rows-<span class=number>1</span>, A[<span class=number>3</span>, :, :, curr_layer])</span><br><span class=line>			</span><br></pre></table></figure><p>此外就是NMS了,输入是anchor box shape是(h*w*9,4)以及对应的一个概率对应着(h*w*9)值在0-1之间表示对应anchor box包含物体的概率.将从一个图像中得到的所有anchor box经过NMS,这样一张图像中的anchor box最多也就一定数量(比如300)了.<p>然后对于这些anchor box,计算与其iou最大的bbox,要求最大的iou大于一定值(比如0.7),那么这个就是roi,其类别就是对应bbox类别,并且设置其位置(修正位置).<p>然后设置一定数量需要的rois,尽量均匀分正负例<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>non_max_suppression_fast</span>(<span class=params>boxes, probs, overlap_thresh=<span class=number>0.9</span>, max_boxes=<span class=number>300</span></span>):</span></span><br><span class=line>    <span class=comment># code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</span></span><br><span class=line>    <span class=comment># if there are no boxes, return an empty list</span></span><br><span class=line>    <span class=keyword>if</span> <span class=built_in>len</span>(boxes) == <span class=number>0</span>:</span><br><span class=line>        <span class=keyword>return</span> []</span><br><span class=line></span><br><span class=line>    <span class=comment># grab the coordinates of the bounding boxes</span></span><br><span class=line>    x1 = boxes[:, <span class=number>0</span>]</span><br><span class=line>    y1 = boxes[:, <span class=number>1</span>]</span><br><span class=line>    x2 = boxes[:, <span class=number>2</span>]</span><br><span class=line>    y2 = boxes[:, <span class=number>3</span>]</span><br><span class=line></span><br><span class=line>    np.testing.assert_array_less(x1, x2)</span><br><span class=line>    np.testing.assert_array_less(y1, y2)</span><br><span class=line></span><br><span class=line>    <span class=comment># if the bounding boxes integers, convert them to floats --</span></span><br><span class=line>    <span class=comment># this is important since we'll be doing a bunch of divisions</span></span><br><span class=line>    <span class=keyword>if</span> boxes.dtype.kind == <span class=string>"i"</span>:</span><br><span class=line>        boxes = boxes.astype(<span class=string>"float"</span>)</span><br><span class=line></span><br><span class=line>    <span class=comment># initialize the list of picked indexes</span></span><br><span class=line>    pick = []</span><br><span class=line></span><br><span class=line>    <span class=comment># calculate the areas</span></span><br><span class=line>    area = (x2 - x1) * (y2 - y1)</span><br><span class=line></span><br><span class=line>    <span class=comment># sort the bounding boxes</span></span><br><span class=line>    idxs = np.argsort(probs)</span><br><span class=line></span><br><span class=line>    <span class=comment># keep looping while some indexes still remain in the indexes</span></span><br><span class=line>    <span class=comment># list</span></span><br><span class=line>    <span class=keyword>while</span> <span class=built_in>len</span>(idxs) > <span class=number>0</span>:</span><br><span class=line>        <span class=comment># grab the last index in the indexes list and add the</span></span><br><span class=line>        <span class=comment># index value to the list of picked indexes</span></span><br><span class=line>        last = <span class=built_in>len</span>(idxs) - <span class=number>1</span></span><br><span class=line>        i = idxs[last]</span><br><span class=line>        pick.append(i)</span><br><span class=line></span><br><span class=line>        <span class=comment># find the intersection</span></span><br><span class=line></span><br><span class=line>        xx1_int = np.maximum(x1[i], x1[idxs[:last]])</span><br><span class=line>        yy1_int = np.maximum(y1[i], y1[idxs[:last]])</span><br><span class=line>        xx2_int = np.minimum(x2[i], x2[idxs[:last]])</span><br><span class=line>        yy2_int = np.minimum(y2[i], y2[idxs[:last]])</span><br><span class=line></span><br><span class=line>        ww_int = np.maximum(<span class=number>0</span>, xx2_int - xx1_int)</span><br><span class=line>        hh_int = np.maximum(<span class=number>0</span>, yy2_int - yy1_int)</span><br><span class=line></span><br><span class=line>        area_int = ww_int * hh_int</span><br><span class=line></span><br><span class=line>        <span class=comment># find the union</span></span><br><span class=line>        area_union = area[i] + area[idxs[:last]] - area_int</span><br><span class=line></span><br><span class=line>        <span class=comment># compute the ratio of overlap</span></span><br><span class=line>        overlap = area_int / (area_union + <span class=number>1e-6</span>)</span><br><span class=line></span><br><span class=line>        <span class=comment># delete all indexes from the index list that have</span></span><br><span class=line>        idxs = np.delete(idxs, np.concatenate(([last],</span><br><span class=line>                                               np.where(overlap > overlap_thresh)[<span class=number>0</span>])))</span><br><span class=line></span><br><span class=line>        <span class=keyword>if</span> <span class=built_in>len</span>(pick) >= max_boxes:</span><br><span class=line>            <span class=keyword>break</span></span><br><span class=line></span><br><span class=line>    <span class=comment># return only the bounding boxes that were picked using the integer data type</span></span><br><span class=line>    boxes = boxes[pick].astype(<span class=string>"int"</span>)</span><br><span class=line>    probs = probs[pick]</span><br><span class=line>    <span class=keyword>return</span> boxes, probs</span><br><span class=line></span><br></pre></table></figure><p>大致逻辑是选取probs最大的(也就是通过RPN算出来包含物体概率最大的anchor box),计算其与剩余的anchor box的iou,去掉大于某个阈值(比如0.9)的,然后再重复,选择剩下来的probs最大的anchor box.一共选择固定数量的anchor box.返回相应的boxes以及probs.<p>可以看看这个<a href=https://zhuanlan.zhihu.com/p/43812909 rel=noopener target=_blank>Faster R-CNN 论文阅读记录（一）：概览 - 知乎 (zhihu.com)</a><p><a href=https://zhuanlan.zhihu.com/p/31426458 rel=noopener target=_blank>一文读懂Faster RCNN - 知乎 (zhihu.com)</a><h3 id=Mask-R-CNN><a title="Mask R-CNN" class=headerlink href=#Mask-R-CNN></a>Mask R-CNN</h3><p><img alt=img data-src=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/mask-rcnn.png style=zoom:67%;><p>主要是提出了ROIAlign将模型用于实例分割中,可以与原先的任务并行,<p><img alt=img data-src=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/roi-align.png style=zoom:50%;><p>RoIAlign 层旨在修复 RoI 池化中由量化(quantization)引起的位置错位。RoIAlign 删除哈希量化，例如，使用 x/16 而不是 [x/16]，以便提取的特征可以与输入像素正确对齐。<strong>双线性插值</strong>用于计算输入中的浮点位置值。<blockquote><p>作者认为Faster RCNN中ROI pooling的取整(quantization )操作会使得定位不准确<p>这种quantization还体现在proposals(在原图上)映射到特征层上的操作,因为这会导致不对准,对于detection任务有较大影响,</blockquote><p><img alt=image-20231102202646398 data-src=https://i.imgur.com/Md5zxS2.png><p>上图蓝框就是proposals到feature map上的框,没有取整.然后在每个roi中选择采样点计算这些采样的均值即为每个roi的值<p>可以看看这篇文章<a href=https://blog.csdn.net/qq_37541097/article/details/123754766 rel=noopener target=_blank>【精选】Mask R-CNN网络详解_mask rcnn详解-CSDN博客</a><p><a href=https://blog.csdn.net/qq_37541097/article/details/112564822 rel=noopener target=_blank>【精选】双线性插值-CSDN博客</a><h3 id=Region-based-Fully-Convolutional-Network-R-FCN><a title="Region-based Fully Convolutional Network (R-FCN)" class=headerlink href=#Region-based-Fully-Convolutional-Network-R-FCN></a>Region-based Fully Convolutional Network (R-FCN)</h3><p><img alt="r-fcn image" data-src=https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/images/r-fcn.png><p>可以了解一下全卷积网络(<a href=http://zh.d2l.ai/chapter_computer-vision/fcn.html rel=noopener target=_blank>13.11. 全卷积网络 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>).1×1卷积层通常用于调整网络层的通道数量和控制模型复杂性<blockquote><p>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数,最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测</blockquote><p>R-FCN核心操作是将ROI pooling改为了position-sensitive score maps.而且原本在ROI pooling之后的卷积层和全连接层(被认为是位置不敏感的操作,这种操作会影响目标检测的精度而且浪费神经网络的分类能力),所以文章将全部操作改为卷积<p><img alt=image-20231102205517036 data-src=https://i.imgur.com/BaAJNpE.png><p>大致流程如下:<ul><li>首先选择一张需要处理的图片，并对这张图片进行相应的预处理操作；<li>接着，我们将预处理后的图片送入一个预训练好的分类网络中（这里使用了ResNet-101网络的Conv4之前的网络），固定其对应的网络参数；<li>接着，在预训练网络的最后一个卷积层获得的feature map上存在3个分支，第1个分支就是在该feature map上面进行RPN操作，获得相应的ROI；第2个分支就是在该feature map上获得一个K*K*（C+1）维的位置敏感得分映射（position-sensitive score map），用来进行分类；第3个分支就是在该feature map上获得一个4*K*K维的位置敏感得分映射，用来进行回归；<li>最后，在K*K*（C+1）维的位置敏感得分映射和4<em>K</em>K维的位置敏感得分映射上面分别执行位置敏感的ROI池化操作（Position-Sensitive Rol Pooling，这里使用的是平均池化操作），获得对应的类别和位置信息。</ul><p>具体来说,通过一个CNN网络后得到feature map,一方面使用全卷积网络将通道数调整为k*k*(C+1)(对于分类任务),此外也需要一个RPN网络,用于提取roi区域,得到roi之后,将每个roi分为k*k个bins,这个时候每个bin就对应一个类别中k*k个通道之一,池化操作也就在这个通道上进行操作.i,j表示每个roi中的某个bin,c表示通道数,其中n表示bin中的像素数,相当于对于某个类别c中的某个bin,计算得到的scores(其实就是一个通道的和)除以bin中的像素数量.论文中的所谓vote就是简单地使用均值得到k*k个position-sensitive scores.<p><img alt=img data-src=https://pic1.zhimg.com/80/v2-0c3fd2c903db6887128ec390b8981ef0_720w.webp><p>然后按此计算某个类别的池化值,也就是k*k个bin的值的和,每个bin是映射在某个score map上的avg pooling.<p><img alt=image-20231102214505751 data-src=https://i.imgur.com/fwsdL0f.png><p>得到C+1个值然后做softmax作为评价交叉熵巡视以及对roi的排名.<p><img alt=image-20231102214651913 data-src=https://i.imgur.com/9R3Qfia.png><p><a href=https://zhuanlan.zhihu.com/p/30867916 rel=noopener target=_blank>详解R-FCN - 知乎 (zhihu.com)</a><h3 id=Summary-of-R-CNN-family><a title="Summary of R-CNN family" class=headerlink href=#Summary-of-R-CNN-family></a>Summary of R-CNN family</h3><p><img alt=img data-src=http://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png><p>以上都是two-stage detector,另一种不同的方法跳过区域建议阶段，直接在可能位置的密集采样上运行检测。这就是单阶段目标检测算法的工作原理。这更快、更简单，但可能会降低performance。<p>在One-stage中对象检测是一个简单的回归问题，需要输入并学习概率类和边界框坐标。YOLO、YOLO v2、SSD、RetinaNet等属于一个相位检测器。对象检测是图像分类的一种高级形式，其中神经网络预测图像中的对象，并以边界框的形式引起人们的注意。<h2 id=论文相关><a class=headerlink href=#论文相关 title=论文相关></a>论文相关</h2><h3 id=Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation><a title="Rich feature hierarchies for accurate object detection and semantic segmentation" class=headerlink href=#Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation></a>Rich feature hierarchies for accurate object detection and semantic segmentation</h3><p>2014年的论文,为后面RCNN系列目标检测方法奠定基础<h4 id=abs><a class=headerlink href=#abs title=abs></a>abs</h4><blockquote><p>Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.The best-performing methods are <strong>complex ensemble systems that typically combine multiple low-level image features with high-level context.</strong>In this paper, we propose a simple and <strong>scalable detection algorithm that improves mean average precision</strong> (mAP) by more than 30% relative to the previous best result on VOC 2012—achieving a mAP of 53.3%.Our approach combines two key insights: (1) <strong>one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects</strong> and (2) when labeled training data is scarce, <strong>supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning,</strong> yields a significant performance boost.</blockquote><h3 id=Mask-RCNN><a class=headerlink href=#Mask-RCNN title=Mask-RCNN></a>Mask-RCNN</h3><h4 id=abs-1><a class=headerlink href=#abs-1 title=abs></a>abs</h4><blockquote><p>We present a conceptually simple, flexible, and general framework for <strong>object instance segmentation</strong>. Our approach efficiently <strong>detects objects</strong> in an image while <strong>simultaneously generating a high-quality segmentation mask for each instance</strong>.<p>The method, called Mask R-CNN, <strong>extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition</strong>. Mask R-CNN is <strong>simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps</strong>.<p>Moreover, Mask R-CNN is <strong>easy to generalize to other tasks</strong>, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection.</blockquote><h3 id=R-FCN><a class=headerlink href=#R-FCN title=R-FCN></a>R-FCN</h3><h4 id=abs-2><a class=headerlink href=#abs-2 title=abs></a>abs</h4><blockquote><p>We present <strong>region-based</strong>, <strong>fully convolutional networks</strong> for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply <strong>a costly per-region subnetwork hundreds of times</strong>, our region-based detector is fully convolutional with almost all computation shared on the entire image.<p>To achieve this goal, we propose <strong>position-sensitive score maps</strong> to <strong>address a dilemma between translation-invariance in image classification and translation-variance in object detection</strong>.<p>Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet</blockquote><h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/ rel=noopener target=_blank>Object Detection Part 4: Fast Detection Models | Lil’Log (lilianweng.github.io)</a><li><a href=https://www.analyticsvidhya.com/blog/2022/09/object-detection-using-yolo-and-mobilenet-ssd/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/ rel=noopener target=_blank>Object Detection Using YOLO And Mobilenet SSD Computer Vision - (analyticsvidhya.com)</a><li><a href=https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/ rel=noopener target=_blank>Object Detection for Dummies Part 3: R-CNN Family | Lil’Log (lilianweng.github.io)</a><li><a href=https://tjmachinelearning.com/lectures/1718/obj/ rel=noopener target=_blank>Object Detection | TJHSST Machine Learning Club (tjmachinelearning.com)</a><li><a href=https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4 rel=noopener target=_blank>A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN | by Dhruv Parthasarathy | Athelas</a></ol><h3 id=代码实现><a class=headerlink href=#代码实现 title=代码实现></a>代码实现</h3><ol><li>PyTorch: <a href=https://github.com/longcw/faster_rcnn_pytorch rel=noopener target=_blank>https://github.com/longcw/faster_rcnn_pytorch</a><li>Keras:<a href=https://github.com/drowning-in-codes/Keras-frcnn rel=noopener target=_blank>drowning-in-codes/Keras-frcnn: Keras Implementation of Faster R-CNN (github.com)</a> 目前许多keras项目版本有点老,keras版本目前到了2.14,tensorflow也是. 我之前学过keras,也是新版本的了,大概5年前的老代码差异还是有的.<li>PyTorch: <a href=https://github.com/felixgwu/mask_rcnn_pytorch rel=noopener target=_blank>https://github.com/felixgwu/mask_rcnn_pytorch</a><li><a href=https://blog.csdn.net/yx123919804/article/details/114800885 rel=noopener target=_blank>【精选】保姆级 Keras 实现 Faster R-CNN 一_keras实现rcnn-CSDN博客</a> l</ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/10/17/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P2/ title=目标检测学习_P2>https://www.sekyoro.top/2023/10/17/目标检测学习-P2/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/SSD/ rel=tag><i class="fa fa-tag"></i> SSD</a><a href=/tags/YOLO/ rel=tag><i class="fa fa-tag"></i> YOLO</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/10/14/DDS%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AE%9E%E6%88%98/ rel=prev title=DDS学习与实战> <i class="fa fa-chevron-left"></i> DDS学习与实战 </a></div><div class=post-nav-item><a href=/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/ rel=next title=Python并行计算> Python并行计算 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-3"><a class=nav-link href=#R-CNN><span class=nav-number>1.</span> <span class=nav-text>R-CNN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95-selective-search><span class=nav-number>1.1.</span> <span class=nav-text>选择算法(selective search)</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B5%81%E7%A8%8B><span class=nav-number>1.2.</span> <span class=nav-text>流程</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7><span class=nav-number>1.3.</span> <span class=nav-text>常用技巧</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%AE%9E%E6%88%98><span class=nav-number>2.</span> <span class=nav-text>实战</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90Region-proposals><span class=nav-number>2.1.</span> <span class=nav-text>生成Region proposals</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BD%BF%E7%94%A8CNN%E6%A8%A1%E5%9E%8B%E4%BA%8C%E5%88%86%E7%B1%BB><span class=nav-number>2.2.</span> <span class=nav-text>使用CNN模型二分类</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%E5%86%8D%E4%BD%BF%E7%94%A8SVM%E4%BA%8C%E5%88%86%E7%B1%BB><span class=nav-number>2.3.</span> <span class=nav-text>预训练模型提取特征再使用SVM二分类</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#bbox-regression><span class=nav-number>2.4.</span> <span class=nav-text>bbox regression</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Fast-R-CNN><span class=nav-number>3.</span> <span class=nav-text>Fast R-CNN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#ROI-Pooling><span class=nav-number>3.1.</span> <span class=nav-text>ROI Pooling</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%B5%81%E7%A8%8B-1><span class=nav-number>3.2.</span> <span class=nav-text>流程</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Faster-RNN><span class=nav-number>4.</span> <span class=nav-text>Faster RNN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Region-Proposal-Networks><span class=nav-number>4.1.</span> <span class=nav-text>Region Proposal Networks</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E9%97%AE%E9%A2%981><span class=nav-number>4.1.1.</span> <span class=nav-text>问题1</span></a></ol></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Mask-R-CNN><span class=nav-number>5.</span> <span class=nav-text>Mask R-CNN</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Region-based-Fully-Convolutional-Network-R-FCN><span class=nav-number>6.</span> <span class=nav-text>Region-based Fully Convolutional Network (R-FCN)</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Summary-of-R-CNN-family><span class=nav-number>7.</span> <span class=nav-text>Summary of R-CNN family</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3><span class=nav-number></span> <span class=nav-text>论文相关</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation><span class=nav-number>1.</span> <span class=nav-text>Rich feature hierarchies for accurate object detection and semantic segmentation</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abs><span class=nav-number>1.1.</span> <span class=nav-text>abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Mask-RCNN><span class=nav-number>2.</span> <span class=nav-text>Mask-RCNN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abs-1><span class=nav-number>2.1.</span> <span class=nav-text>abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#R-FCN><span class=nav-number>3.</span> <span class=nav-text>R-FCN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abs-2><span class=nav-number>3.1.</span> <span class=nav-text>abs</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number></span> <span class=nav-text>参考资料</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0><span class=nav-number>1.</span> <span class=nav-text>代码实现</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>215</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>202</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/SSD/ rel=tag>SSD</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/YOLO/ rel=tag>YOLO</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.8m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>27:38</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>