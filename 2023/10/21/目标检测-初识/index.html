<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=需要一些基本的cv知识 name=description><meta content=article property=og:type><meta content=目标检测_初识 property=og:title><meta content=https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=需要一些基本的cv知识 property=og:description><meta content=zh_CN property=og:locale><meta content=https://editor.analyticsvidhya.com/uploads/61607page%2015.gif property=og:image><meta content=https://s2.loli.net/2023/11/23/li2FYa5bBWDy6uv.png property=og:image><meta content=https://docs.opencv.org/3.4/Pyramids_Tutorial_Pyramid_Theory.png property=og:image><meta content=https://i.imgur.com/OzzSilQ.png property=og:image><meta content=https://i.imgur.com/gkNZfIa.png property=og:image><meta content=https://i.imgur.com/Agk6m0B.jpg property=og:image><meta content=https://img-blog.csdnimg.cn/ab91284739724a70a016342a38f84baa.png#pic_center property=og:image><meta content=https://i.imgur.com/Nsds6wk.png property=og:image><meta content=https://i.imgur.com/GY8rrIF.png property=og:image><meta content=https://i.imgur.com/Gb1mmoJ.png property=og:image><meta content=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/watershed.gif property=og:image><meta content=https://i.imgur.com/O85ljV8.png property=og:image><meta content=https://i.imgur.com/Tzrz5xe.png property=og:image><meta content=https://i.imgur.com/NTrwEOY.png property=og:image><meta content=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/Image-Filtering.png property=og:image><meta content=https://i.imgur.com/0EahLmk.png property=og:image><meta content=https://i.imgur.com/Wtr5Pab.png property=og:image><meta content=https://i.imgur.com/ttcJEOh.png property=og:image><meta content=https://i.imgur.com/BBs3dlN.jpg property=og:image><meta content=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/02/keypoint_matching.png property=og:image><meta content=https://editor.analyticsvidhya.com/uploads/81269Capture.PNG property=og:image><meta content=https://s2.loli.net/2023/11/23/8LM4JgTNA2ZDVpy.png property=og:image><meta content=https://i.imgur.com/XXcPkBM.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-24-18-27-46.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-12-48-03-300x205.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-14-18-26.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-16-50-01-300x207.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-26-20-10-52.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/index_71.png property=og:image><meta content=https://av-eks-blogoptimized.s3.amazonaws.com/index_61.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/image-gradient-vector-pixel-location.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/HOG-histogram-creation.png property=og:image><meta content=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-19-18-24-37-300x87.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/block_histogram.png property=og:image><meta content=https://editor.analyticsvidhya.com/uploads/887441.png property=og:image><meta content=2023-10-21T13:22:20.000Z property=article:published_time><meta content=2023-11-30T15:05:42.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="object detection" property=article:tag><meta content=cv property=article:tag><meta content=summary name=twitter:card><meta content=https://editor.analyticsvidhya.com/uploads/61607page%2015.gif name=twitter:image><link href=https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>目标检测_初识 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>目标检测_初识</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-10-21 21:22:20" datetime=2023-10-21T21:22:20+08:00>2023-10-21</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2023-11-30 23:05:42" datetime=2023-11-30T23:05:42+08:00 itemprop=dateModified>2023-11-30</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>15k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>14 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>需要一些基本的cv知识<br><span id=more></span><h3 id=图像分辨率><a class=headerlink href=#图像分辨率 title=图像分辨率></a>图像分辨率</h3><blockquote><p>图像分辨率可以定义为图像中存在的像素数。当像素数量增加时，图像的质量会增加。我们已经在前面看到了图像的形状，它给出了行和列的数量。这可以说是该图像的分辨率。几乎所有人都知道的一些标准分辨率是320 x 240像素（主要适用于小屏幕设备）、1024 x 768像素（适用于在标准计算机显示器上观看）、720 x 576像素（适合在宽高比为4:3的标准清晰度电视机上观看），1280 x 1024像素（适用于在宽高比为5:4的液晶显示器上全屏观看）、1920 x 1080像素（用于在高清电视上观看），现在我们甚至有4K、5K和8K分辨率，超高清显示器和电视分别支持3840 x 2160像素、5120 x 2880像素和7680 x 4320像素</blockquote><p>图像像素的高位包含的信息比低位更多,我们可以将图像划分为不同级别的位平面。例如，将图像划分为8位（0-7）平面，其中最后几个平面包含图像的大部分信息。<p><img alt="bit plans " data-src=https://editor.analyticsvidhya.com/uploads/61607page%2015.gif><p><img alt=image-20231123110414030 data-src=https://s2.loli.net/2023/11/23/li2FYa5bBWDy6uv.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line><span class=keyword>import</span> matplotlib.pylab <span class=keyword>as</span> plt</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line></span><br><span class=line>img  = cv2.imread(<span class=string>"../imgs/00000.png"</span>,cv2.IMREAD_GRAYSCALE)</span><br><span class=line></span><br><span class=line>c1 = np.mod(img,<span class=number>2</span>)</span><br><span class=line>c2 = np.mod(np.floor(img/<span class=number>2</span>),<span class=number>2</span>)</span><br><span class=line>c3 = np.mod(np.floor(img/<span class=number>4</span>),<span class=number>2</span>)</span><br><span class=line>c4 = np.mod(np.floor(img/<span class=number>8</span>),<span class=number>2</span>)</span><br><span class=line>c5 = np.mod(np.floor(img/<span class=number>16</span>),<span class=number>2</span>)</span><br><span class=line>c6 = np.mod(np.floor(img/<span class=number>32</span>),<span class=number>2</span>)</span><br><span class=line>c7 = np.mod(np.floor(img/<span class=number>64</span>),<span class=number>2</span>)</span><br><span class=line>c8 = np.mod(np.floor(img/<span class=number>128</span>),<span class=number>2</span>)</span><br><span class=line></span><br><span class=line>cc = <span class=number>2</span>*(<span class=number>2</span>*(<span class=number>2</span>*c8+c7)+c6)</span><br><span class=line>to_plot = [img,c1,c2,c3,c4,c5,c6,c7,c8,cc]</span><br><span class=line>fig,axes = plt.subplots(<span class=number>2</span>,<span class=number>5</span>, subplot_kw={<span class=string>'xticks'</span>: [], <span class=string>'yticks'</span>: []})</span><br><span class=line>fig.subplots_adjust(hspace=<span class=number>0.05</span>, wspace=<span class=number>0.05</span>)</span><br><span class=line><span class=keyword>for</span> ax,i <span class=keyword>in</span> <span class=built_in>zip</span>(axes.flat, to_plot):</span><br><span class=line>    ax.imshow(i, cmap=<span class=string>'gray'</span>)</span><br><span class=line></span><br><span class=line>plt.tight_layout()</span><br><span class=line>plt.show()</span><br><span class=line></span><br><span class=line>cv2.waitKey()</span><br></pre></table></figure><p>可以使用像素的高位重建图像<h4 id=图像金字塔><a class=headerlink href=#图像金字塔 title=图像金字塔></a>图像金字塔</h4><p><img alt=Pyramids_Tutorial_Pyramid_Theory.png data-src=https://docs.opencv.org/3.4/Pyramids_Tutorial_Pyramid_Theory.png><p><strong>图像金字塔是一组图像，所有图像都来自一张原始图像，这些图像被连续下采样，直到达到某个期望的停止点</strong>。<p>有两种常见的图像金字塔：<ul><li>高斯金字塔：用于对图像进行下采样<li>拉普拉斯金字塔：用于从金字塔中较低的图像重建上采样图像（分辨率较低）</ul><h3 id=图像直方图><a class=headerlink href=#图像直方图 title=图像直方图></a>图像直方图</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>his</span>(<span class=params>img_gray</span>):</span></span><br><span class=line>    hist = cv2.calcHist([img_gray], [<span class=number>0</span>], <span class=literal>None</span>, [<span class=number>256</span>], [<span class=number>0</span>, <span class=number>256</span>])</span><br><span class=line></span><br><span class=line>    plt.figure()</span><br><span class=line>    plt.title(<span class=string>"Grayscale Histogram"</span>)</span><br><span class=line>    plt.xlabel(<span class=string>"bins"</span>)</span><br><span class=line>    plt.ylabel(<span class=string>"pixels"</span>)</span><br><span class=line>    plt.plot(hist)</span><br><span class=line>    plt.xlim([<span class=number>0</span>, <span class=number>256</span>])</span><br><span class=line>    plt.show()</span><br></pre></table></figure><p><img alt=image-20231123134730139 data-src=https://i.imgur.com/OzzSilQ.png><h3 id=图像深度><a class=headerlink href=#图像深度 title=图像深度></a>图像深度</h3><p>数字化图像的每个像素是用一组二进制数进行描述，像素的色彩由RGB通道决定，其中包含表示图像颜色的位数称为图像深度。如灰度图像，每个像素颜色占用1个字节8位，则称图像深度为8位，而RGB的彩色图像占用3字节，图像深度为24位。<p>图像深度又称为色深（Color Depth），它确定了一幅图像中最多能使用的颜色数，即彩色图像的每个像素最大的颜色数，或者确定灰度图像的每个像素最大的灰度级数。<br>使用opencv的imread读取模式有<blockquote><p>IMREAD_UNCHANGED = -1, //返回原始图像。alpha通道不会被忽略，如果有的话。加载给定格式的图像，包括alpha通道。Alpha通道存储透明度信息——Alpha通道的值越高，像素就越不透明<br>IMREAD_GRAYSCALE = 0, //返回灰度图像<br>IMREAD_COLOR = 1, //返回通道顺序为BGR的彩色图像<br>IMREAD_ANYDEPTH = 2, //当输入具有相应的深度时返回16位/ 32位图像，否则将其转换为8位。.<br>IMREAD_ANYCOLOR = 4, //则以任何可能的颜色格式读取图像。</blockquote><h3 id=颜色空间><a class=headerlink href=#颜色空间 title=颜色空间></a>颜色空间</h3><p>颜色空间是一种协议(protocol)，用于以易于再现的方式表示颜色。我们知道，灰度图像具有单个像素值，彩色图像每个像素包含3个值——红色、绿色和蓝色通道的强度。<p>大多数计算机视觉用例处理RGB格式的图像。然而,<strong>视频压缩和设备独立存储等应用程序在很大程度上依赖于其他颜色空间，如色相(Hue即色相，就是我们平时所说的红、绿，如果你分的更细的话可能还会有洋红、草绿等等)、饱和度(色彩的深浅度(0-100%，对于一种颜色比如红色，我们可以用浅红——大红——深红——红得发紫等等语言来描述它)、色调(纯度，色彩的亮度(0-100%) ，这个在调整屏幕亮度的时候比较常见)即HSV颜色空间</strong>。<p><strong>RGB图像由不同颜色通道的颜色强度组成，即强度和颜色信息在RGB颜色空间中混合</strong>，但<strong>在HSV颜色空间中,颜色和强度信息彼此分离。这将使HSV颜色空间对光源更改更加稳健</strong>。<p><img alt=image-20231123115359582 data-src=https://i.imgur.com/gkNZfIa.png><h3 id=图像resize><a class=headerlink href=#图像resize title=图像resize></a>图像resize</h3><p><strong>机器学习模型使用固定大小的输入</strong>。同样的想法也适用于计算机视觉模型。<strong>我们用于训练模型的图像必须具有相同的大小</strong>。现在，<strong>如果我们通过从各种来源抓取图像来创建自己的数据集，这可能会成为问题。这就是调整图像大小的功能凸显出来的地方</strong>。<p><img alt data-src=https://i.imgur.com/Agk6m0B.jpg><blockquote><p>INTER_NEAREST:最近邻插值<p>INTER_LINEAR:双线性插值<p>INTER_AREA：使用像素面积关系重新采样<p>INTER_CUBIC:4×4像素邻域上的双三次插值<p>INTER_LANCZOS4:8邻域上的Lanczos插值</blockquote><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> npimg = cv2.imread(<span class=string>"../imgs/00000.png"</span>)</span><br><span class=line>cv2.imshow(<span class=string>"img"</span>,img)</span><br><span class=line>smaller_img = cv2.resize(img,(<span class=number>200</span>,<span class=number>200</span>),interpolation=cv2.INTER_LINEAR)</span><br><span class=line>cv2.imshow(<span class=string>"smaller_img"</span>,smaller_img)</span><br></pre></table></figure><h3 id=图像旋转以及平移><a class=headerlink href=#图像旋转以及平移 title=图像旋转以及平移></a>图像旋转以及平移</h3><p>可以用作图像增强的技术<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line>img = cv2.imread(<span class=string>"../imgs/00000.png"</span>)</span><br><span class=line>rows,cols = img.shape[:<span class=number>2</span>]</span><br><span class=line>M = cv2.getRotationMatrix2D((cols/<span class=number>2</span>,rows/<span class=number>2</span>),<span class=number>45</span>,<span class=number>1</span>)</span><br><span class=line>dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class=line>cv2.imshow(<span class=string>"dst"</span>,dst)</span><br><span class=line>cv2.waitKey()</span><br></pre></table></figure><p>会使用到cv2.getRotationMatrix2D与cv2.warpAffine.<p>cv2.getRotationMatrix2D参数分别是中心,旋转角度以及缩放系数.<p>cv2.warpAffine是做仿射变换,<p><img alt=在这里插入图片描述 data-src=https://img-blog.csdnimg.cn/ab91284739724a70a016342a38f84baa.png#pic_center><p>图像平移可以用于为模型添加平移不变性<strong>，因为通过平移，我们可以改变对象在图像中的位置，为模型提供更多的多样性，从而获得更好的可推广性</strong>，这在困难的条件下有效，即当对象没有完全对准图像中心时。这种增强技术还可以帮助模型正确地对具有部分可见对象的图像进行分类。以下图为例。即使图像中没有完整的鞋子，模型也应该能够将其分类为鞋子<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>M = np.float32([[<span class=number>1</span>,<span class=number>0</span>,-<span class=number>100</span>],[<span class=number>0</span>,<span class=number>1</span>,-<span class=number>100</span>]])</span><br><span class=line>dst = cv2.warpAffine(img,M,(cols,rows))</span><br><span class=line>plt.imshow(dst)</span><br><span class=line>cv2.imshow(<span class=string>"dst"</span>,dst)</span><br><span class=line></span><br></pre></table></figure><p><img alt=image-20231123123137312 data-src=https://i.imgur.com/Nsds6wk.png><h3 id=图像阈值><a class=headerlink href=#图像阈值 title=图像阈值></a>图像阈值</h3><p>阈值分割是一种图像分割方法。它将像素值与阈值进行比较，并相应地进行更新。图像阈值的一个简单应用可以将图像划分为前景和背景,阈值只能应用于灰度图像。<p><img alt=image-20231123123310090 data-src=https://i.imgur.com/GY8rrIF.png><p>上面是简单阈值,此外还有自适应阈值<p>简单阈值是全局的，对于在不同区域具有不同照明条件的图像可能不太适用，此时可以使用自适应阈值处理。<strong>算法计算图像的局部阈值，在同一图像的不同区域获得不同的阈值，并为具有不同照明的图像提供了更好的结果</strong>。在自适应阈值的情况下，对图像的不同部分使用不同的阈值。该函数可为具有不同照明条件的图像提供更好的结果，因此被称为“自适应”。<strong>Otsu的二值化方法为整个图像找到一个最佳阈值。它适用于双峰图像</strong>（直方图中有2个峰值的图像）。<p><code>cv2.ADAPTIVE_THRESH_MEAN_C</code>：阈值是邻域的<strong>平均值</strong>。<br><code>cv2.ADAPTIVE_THRESH_GAUSSIAN_C</code>：阈值是邻域值的<strong>加权和</strong>，其中权重是高斯窗口。<br><code>Block Size</code> 决定邻域的大小。<br><code>C</code> 从计算的平均值或加权平均值中减去常数。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line></span><br><span class=line><span class=comment>#import the libraries</span></span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line>%matplotlib inline</span><br><span class=line></span><br><span class=line><span class=comment>#ADAPTIVE THRESHOLDING</span></span><br><span class=line>gray_image = cv2.imread(<span class=string>'index.png'</span>,<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>ret,thresh_global = cv2.threshold(gray_image,<span class=number>127</span>,<span class=number>255</span>,cv2.THRESH_BINARY)</span><br><span class=line><span class=comment>#here 11 is the pixel neighbourhood that is used to calculate the threshold value</span></span><br><span class=line>thresh_mean = cv2.adaptiveThreshold(gray_image,<span class=number>255</span>,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,<span class=number>11</span>,<span class=number>2</span>)</span><br><span class=line></span><br><span class=line>thresh_gaussian = cv2.adaptiveThreshold(gray_image,<span class=number>255</span>,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,<span class=number>11</span>,<span class=number>2</span>)</span><br><span class=line></span><br><span class=line>names = [<span class=string>'Original Image'</span>,<span class=string>'Global Thresholding'</span>,<span class=string>'Adaptive Mean Threshold'</span>,<span class=string>'Adaptive Gaussian Thresholding'</span>]</span><br><span class=line>images = [gray_image,thresh_global,thresh_mean,thresh_gaussian]</span><br><span class=line></span><br><span class=line><span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>4</span>):</span><br><span class=line>    plt.subplot(<span class=number>2</span>,<span class=number>2</span>,i+<span class=number>1</span>),plt.imshow(images[i],<span class=string>'gray'</span>)</span><br><span class=line>    plt.title(names[i])</span><br><span class=line>    plt.xticks([]),plt.yticks([])</span><br><span class=line>    </span><br><span class=line>plt.show()</span><br></pre></table></figure><p><img alt=image-20231123135259991 data-src=https://i.imgur.com/Gb1mmoJ.png><h3 id=图像分割><a class=headerlink href=#图像分割 title=图像分割></a>图像分割</h3><p><strong>图像分割是将图像中的每个像素分类到某个类别的任务</strong>。例如，将每个像素分类为前景或背景。图像分割对于从图像中提取相关部分是重要的。<p>分水岭算法是一种经典的图像分割算法。它将图像中的像素值视为地形。为了找到对象边界，它将初始标记作为输入。然后，该算法开始从标记淹没盆地(flooding the basin from the markers)，直到标记在对象边界相遇。<p><img alt="watershed algorithm" data-src=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/watershed.gif><h3 id=位操作><a class=headerlink href=#位操作 title=位操作></a>位操作</h3><p>按位运算包括AND、OR、NOT和XOR。在计算机视觉中，当我们<strong>有一个遮罩图像并想将该遮罩应用于另一个图像以提取感兴趣的区域时</strong>，这些操作非常有用。<p><img alt=image-20231123161116735 data-src=https://i.imgur.com/O85ljV8.png><p><img alt=image-20231123161520256 data-src=https://i.imgur.com/Tzrz5xe.png><h3 id=边缘检测><a class=headerlink href=#边缘检测 title=边缘检测></a>边缘检测</h3><p><strong>边缘是图像中图像亮度急剧变化或具有不连续性的点</strong>。这种不连续性通常对应于：<ul><li>深度不连续<li>表面方向的不连续性<li>材料特性的变化<li>场景照明的变化</ul><p>边缘是图像的非常有用的特征，可以用于不同的应用，如图像中对象的分类和定位。甚至深度学习模型也会计算边缘特征，以提取图像中存在的对象的信息。<p><img alt=image-20231123161625316 data-src=https://i.imgur.com/NTrwEOY.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line></span><br><span class=line>img = cv2.imread(<span class=string>"../imgs/00000.png"</span>)</span><br><span class=line></span><br><span class=line>edges = cv2.Canny(img, <span class=number>100</span>, <span class=number>200</span>)</span><br><span class=line></span><br><span class=line>cv2.imshow(<span class=string>"edges"</span>, edges)</span><br><span class=line>cv2.waitKey()</span><br></pre></table></figure><h3 id=图像滤波><a class=headerlink href=#图像滤波 title=图像滤波></a>图像滤波</h3><p>在图像滤波中，使用像素值的相邻值来更新像素值。但是，这些值最初是如何更新的呢？，有多种更新像素值的方法，例如从邻居中选择最大值，使用邻居的平均值等。每种方法都有自己的用途。例如，对邻域中的像素值取平均值用于图像模糊。<p>高斯滤波也用于图像模糊，其根据相邻像素与所考虑像素的距离为相邻像素赋予不同的权重。对于图像过滤，我们使用内核。核是不同形状的数字矩阵，如3 x 3、5 x 5等。核用于计算图像一部分的点积。当计算像素的新值时，内核中心与像素重叠。相邻像素值与内核中的相应值相乘。计算出的值被分配给与内核中心重合的像素。<p><img alt=img data-src=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/Image-Filtering.png><p><img alt=image-20231123161852790 data-src=https://i.imgur.com/0EahLmk.png><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line></span><br><span class=line>image = cv2.imread(<span class=string>'../imgs/test.png'</span>)</span><br><span class=line>kernel = np.ones((<span class=number>5</span>,<span class=number>5</span>),np.float32)/<span class=number>25</span></span><br><span class=line><span class=comment>#using the averaging kernel for image smoothening</span></span><br><span class=line>averaging_kernel = np.ones((<span class=number>3</span>,<span class=number>3</span>),np.float32)/<span class=number>9</span></span><br><span class=line>filtered_image = cv2.filter2D(image,-<span class=number>1</span>,kernel)</span><br><span class=line>cv2.imshow(<span class=string>"avg_filtered_image"</span>,filtered_image)</span><br><span class=line><span class=comment>#get a one dimensional Gaussian Kernel</span></span><br><span class=line>gaussian_kernel_x = cv2.getGaussianKernel(<span class=number>5</span>,<span class=number>1</span>)</span><br><span class=line>gaussian_kernel_y = cv2.getGaussianKernel(<span class=number>5</span>,<span class=number>1</span>)</span><br><span class=line><span class=comment>#converting to two dimensional kernel using matrix multiplication</span></span><br><span class=line>gaussian_kernel = gaussian_kernel_x * gaussian_kernel_y.T</span><br><span class=line><span class=comment>#you can also use cv2.GaussianBLurring(image,(shape of kernel),standard deviation) instead of cv2.filter2D</span></span><br><span class=line>filtered_image = cv2.filter2D(image,-<span class=number>1</span>,gaussian_kernel)</span><br><span class=line>cv2.imshow(<span class=string>"filtered_image"</span>,filtered_image)</span><br><span class=line>cv2.waitKey()</span><br></pre></table></figure><p><img alt=image-20231123170248409 data-src=https://i.imgur.com/Wtr5Pab.png><p>上图的两个卷积核分别是</p><script type="math/tex; mode=display">
\left.\left[\begin{matrix}0&-1&0\\-1&5&-1\\0&-1&0\\\end{matrix}\right.\right]</script><script type="math/tex; mode=display">
\left.\left[\begin{matrix}0&-1&0\\-1&4&-1\\0&-1&0\\\end{matrix}\right.\right]</script><h3 id=图像轮廓-contours><a class=headerlink href=#图像轮廓-contours title=图像轮廓(contours)></a>图像轮廓(contours)</h3><p>轮廓是表示图像中对象边界的点或线段的闭合曲线。<strong>轮廓本质上是图像中对象的形状。与边缘不同，轮廓不是图像的一部分。相反，它们是与图像中对象的形状相对应的点和线段的抽象集合</strong>。我们可以使用轮廓来计算图像中对象的数量，根据对象的形状对其进行分类，或者从图像中选择特定形状的对象。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line>image = cv2.imread(<span class=string>'../imgs/test.png'</span>)</span><br><span class=line><span class=comment>#converting RGB image to Binary</span></span><br><span class=line>gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class=line>ret,thresh = cv2.threshold(gray_image,<span class=number>127</span>,<span class=number>255</span>,<span class=number>0</span>)</span><br><span class=line><span class=comment>#calculate the contours from binary image</span></span><br><span class=line>contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class=line>with_contours = cv2.drawContours(image,contours,-<span class=number>1</span>,(<span class=number>0</span>,<span class=number>255</span>,<span class=number>0</span>),<span class=number>3</span>)</span><br><span class=line>plt.imshow(with_contours[...,::-<span class=number>1</span>])</span><br><span class=line>plt.show()</span><br></pre></table></figure><p><img alt=image-20231123163932600 data-src=https://i.imgur.com/ttcJEOh.png><h3 id=特征匹配><a class=headerlink href=#特征匹配 title=特征匹配></a>特征匹配</h3><p>使用SIFT或SURF从不同图像中提取的特征可以进行匹配，以找到存在于不同图像中的相似对象/模式。OpenCV库支持多种特征匹配算法，如brute force 匹配、knn特征匹配等。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br></pre><td class=code><pre><span class=line></span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line></span><br><span class=line><span class=comment>#reading images in grayscale format</span></span><br><span class=line>image1 = cv2.imread(<span class=string>'../imgs/00000.png'</span>,<span class=number>0</span>)</span><br><span class=line></span><br><span class=line>image2 = cv2.warpAffine(image1, np.float32([[<span class=number>1</span>, <span class=number>0</span>, <span class=number>100</span>], [<span class=number>0</span>, <span class=number>1</span>, -<span class=number>100</span>]]), (image1.shape[<span class=number>1</span>], image1.shape[<span class=number>0</span>]))</span><br><span class=line></span><br><span class=line>sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class=line><span class=comment>#finding out the keypoints and their descriptors</span></span><br><span class=line>keypoints1,descriptors1 = sift.detectAndCompute(image1,<span class=literal>None</span>)</span><br><span class=line>keypoints2,descriptors2 = sift.detectAndCompute(image2,<span class=literal>None</span>)</span><br><span class=line></span><br><span class=line><span class=comment>#matching the descriptors from both the images</span></span><br><span class=line>bf = cv2.BFMatcher()</span><br><span class=line>matches = bf.knnMatch(descriptors1,descriptors2,k = <span class=number>2</span>)</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=comment>#selecting only the good features</span></span><br><span class=line>good_matches = []</span><br><span class=line><span class=keyword>for</span> m,n <span class=keyword>in</span> matches:</span><br><span class=line>    <span class=keyword>if</span> m.distance < <span class=number>0.75</span>*n.distance:</span><br><span class=line>        good_matches.append([m])</span><br><span class=line>image3 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good_matches,<span class=literal>None</span>,flags=<span class=number>2</span>)</span><br><span class=line>cv2.imshow(<span class=string>"image3"</span>,image3)</span><br><span class=line>cv2.waitKey()</span><br></pre></table></figure><p><img alt=image-20231123165033904 data-src=https://i.imgur.com/BBs3dlN.jpg><p><img alt=img data-src=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/02/keypoint_matching.png><p>在上面的图像中，我们可以看到从原始图像（左侧）中提取的关键点与其旋转版本的关键点相匹配。这是因为特征是使用SIFT提取的，SIFT对这种变换是不变的。<h3 id=人脸检测><a class=headerlink href=#人脸检测 title=人脸检测></a>人脸检测</h3><p>OpenCV支持基于haar级联的对象检测。<strong>Haar级联是基于机器学习的分类器，用于计算图像中的不同特征，如边缘、线条等</strong>。然后，这些分类器使用多个正样本和负样本进行训练。OpenCV Github仓库中提供了针对人脸、眼睛等不同对象的经过训练的分类器，也可以针对任何对象训练自己的haar级联。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br></pre><td class=code><pre><span class=line><span class=comment>#import required libraries</span></span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> cv2 </span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line></span><br><span class=line><span class=comment>#load the classifiers downloaded </span></span><br><span class=line>face_cascade = cv2.CascadeClassifier(<span class=string>'haarcascade_frontalface_default.xml'</span>)</span><br><span class=line>eye_cascade = cv2.CascadeClassifier(<span class=string>'haarcascade_eye.xml'</span>)</span><br><span class=line><span class=comment>#read the image and convert to grayscale format</span></span><br><span class=line>img = cv2.imread(<span class=string>'rotated_face.jpg'</span>)</span><br><span class=line>gray = cv2.cvtColor(img, cv.COLOR_BGR2GRAY)</span><br><span class=line><span class=comment>#calculate coordinates </span></span><br><span class=line>faces = face_cascade.detectMultiScale(gray, <span class=number>1.1</span>, <span class=number>4</span>)</span><br><span class=line><span class=keyword>for</span> (x,y,w,h) <span class=keyword>in</span> faces:</span><br><span class=line>    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class=number>255</span>,<span class=number>0</span>,<span class=number>0</span>),<span class=number>2</span>)</span><br><span class=line>    roi_gray = gray[y:y+h, x:x+w]</span><br><span class=line>    roi_color = img[y:y+h, x:x+w]</span><br><span class=line>    eyes = eye_cascade.detectMultiScale(roi_gray)</span><br><span class=line>    <span class=comment>#draw bounding boxes around detected features</span></span><br><span class=line>    <span class=keyword>for</span> (ex,ey,ew,eh) <span class=keyword>in</span> eyes:</span><br><span class=line>        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(<span class=number>0</span>,<span class=number>255</span>,<span class=number>0</span>),<span class=number>2</span>)</span><br><span class=line><span class=comment>#plot the image</span></span><br><span class=line>plt.imshow(img)</span><br><span class=line><span class=comment>#write image </span></span><br><span class=line>cv2.imwrite(<span class=string>'face_detection.jpg'</span>,img)</span><br></pre></table></figure><p>很多级联器的xml文件都是直接拿别人的,网上也有训练的教程.<h3 id=图像梯度向量><a class=headerlink href=#图像梯度向量 title=图像梯度向量></a>图像梯度向量</h3><p>image gradient vector<p>图像梯度矢量被定义为每个单独像素的度量,包含x轴和y轴上的像素颜色变化。该定义与连续多变量函数的梯度一致，该函数是所有变量的偏导数的向量。<h3 id=边缘检测算子><a class=headerlink href=#边缘检测算子 title=边缘检测算子></a>边缘检测算子</h3><p><img alt="Sharpening An Image  2" data-src=https://editor.analyticsvidhya.com/uploads/81269Capture.PNG><h4 id=prewitt><a class=headerlink href=#prewitt title=prewitt></a>prewitt</h4><p>Prewitt算子不是只依赖于四个直接相邻的邻居，而是利用八个周围的像素来获得更平滑的结果。<p><img alt=image-20231123095504365 data-src=https://s2.loli.net/2023/11/23/8LM4JgTNA2ZDVpy.png><h4 id=sobel><a class=headerlink href=#sobel title=sobel></a>sobel</h4><p>为了更加强调直接相邻像素的影响，它们被分配了更高的权重。<p><img alt=image-20231123172141630 data-src=https://i.imgur.com/XXcPkBM.png><h3 id=角点检测><a class=headerlink href=#角点检测 title=角点检测></a>角点检测</h3><blockquote><p>角点检测(Corner Detection)是计算机视觉系统中用来获得图像特征的一种方法，广泛应用于运动检测、图像匹配、视频跟踪、三维建模和目标识别等领域中。也称为特征点检测。 <strong>角点通常被定义为两条边的交点，更严格的说，角点的局部邻域应该具有两个不同区域的不同方向的边界。</strong>而实际应用中，大多数所谓的角点检测方法检测的是拥有特定特征的图像点，而不仅仅是“角点”。这些特征点在图像中有具体的坐标，并具有某些数学特征，如局部最大或最小灰度、某些梯度特征等</blockquote><h4 id=Harris><a class=headerlink href=#Harris title=Harris></a>Harris</h4><ul><li>计算窗口中各像素点在x和y方向的梯度；<li>计算两个方向梯度的乘积,即Ix ^ 2 , Iy ^ 2 , IxIy(可以用一些一阶梯度算子求得图像梯度)<li>使用滤波核对窗口中的每一像素进行加权，生成矩阵M和元素A，B，C<li>计算每个像素的Harris响应值R，并对小于某阈值T的R置0；<li>由于角点所在区域的一定邻域内都有可能被检测为角点，所以为了防止角点聚集，最后在3×3或5×5的邻域内进行非极大值抑制，局部最大值点即为图像中的角点。</ul><p><a href=https://blog.csdn.net/SESESssss/article/details/106774854 rel=noopener target=_blank>【理解】经典角点检测算法—Harris角点-CSDN博客</a><h3 id=常用特征><a class=headerlink href=#常用特征 title=常用特征></a>常用特征</h3><h4 id=SIFT><a class=headerlink href=#SIFT title=SIFT></a>SIFT</h4><p>关键点是处理图像时应该注意的一个概念。这些基本上是图像中的兴趣点。关键点类似于给定图像的特征。它们是定义图像中有趣内容的位置。关键点很重要，因为无论图像如何修改（旋转、收缩、扩展、失真），我们都会为图像找到相同的关键点。<p>尺度不变特征变换（SIFT）是一种非常流行的关键点检测算法。它包括以下步骤：<ul><li>Scale-space extrema detection<li>Keypoint localization<li>Orientation assignment<li>Keypoint descriptor<li>Keypoint matching</ul><p>从SIFT提取的特征可用于图像拼接、对象检测等应用。下面的代码和输出显示了使用SIFT计算的关键点及其方向。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=comment>#import required libraries</span></span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line>%matplotlib inline</span><br><span class=line><span class=comment>#show OpenCV version</span></span><br><span class=line><span class=built_in>print</span>(cv2.__version__)</span><br><span class=line><span class=comment>#read the iamge and convert to grayscale</span></span><br><span class=line>image = cv2.imread(<span class=string>'index.png'</span>)</span><br><span class=line>gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class=line><span class=comment>#create sift object</span></span><br><span class=line>sift  = cv2.xfeatures2d.SIFT_create()</span><br><span class=line><span class=comment>#calculate keypoints and their orientation</span></span><br><span class=line>keypoints,descriptors = sift.detectAndCompute(gray,<span class=literal>None</span>)</span><br><span class=line><span class=comment>#plot keypoints on the image</span></span><br><span class=line>with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class=line><span class=comment>#plot the image</span></span><br><span class=line>plt.imshow(with_keypoints)</span><br></pre></table></figure><blockquote><p>SIFT算法有助于定位图像中的局部特征，通常称为图像的“关键点”。这些关键点是尺度和旋转不变量，可用于各种计算机视觉应用，如图像匹配、对象检测、场景检测等。我们还可以在模型训练期间使用使用SIFT生成的关键点作为图像的特征。SIFT特征、边缘特征或弓形特征的主要优点是它们不受图像的大小或方向的影响。</blockquote><p>整个过程可以分为4个部分：<p><strong>Constructing a Scale Space</strong>：确保要素与scale无关<p><strong>Keypoint Localisation</strong>：识别合适的特征或关键点<p><strong>Orientation Assignment</strong>：确保关键点旋转不变<p><strong>Keypoint Descriptor:</strong>：为每个关键点分配一个唯一的id<h5 id=Constructing-the-Scale-Space><a title="Constructing the Scale Space" class=headerlink href=#Constructing-the-Scale-Space></a>Constructing the Scale Space</h5><p>我们需要识别给定输入图像中最明显的特征，同时忽略任何噪声。此外，我们需要确保这些功能不依赖于scale。<p>对于图像中的每个像素，高斯模糊会基于其具有特定σ值的相邻像素来计算一个值。<p><strong>纹理和次要细节将从图像中删除，只保留相关信息，如形状和边缘</strong><blockquote><p>比例空间是从单个图像生成的具有不同比例的图像的集合。</blockquote><p>因此，这些模糊图像是为多个比例创建的。为了创建一组不同比例的新图像，将拍摄原始图像并将比例缩小一半。对于每个新图像，我们将创建模糊版本。<p>理想的缩放次数是四次，对于每次缩放，模糊图像的数量应该是五个。<p><img alt="sift octave | SIFT algorithm" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-24-18-27-46.png><blockquote><p>高斯差分是一种特征增强算法，它涉及<strong>将原始图像的一个模糊版本与另一个模糊程度较低的原始图像版本相减</strong>。</blockquote><p>DoG为每个octave创建另一组图像，方法是从相同比例的前一个图像中减去每个图像。<p>到目前为止，我们已经创建了<strong>多个尺度的图像</strong>（通常用σ表示），并对<strong>每个尺度使用高斯模糊来减少图像中的噪声</strong>。接下来将尝试使用一种名为<strong>高斯差分</strong>（DoG）的技术来增强这些特征。<p><img alt="difference of gaussian" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-12-48-03-300x205.png><p>如下图,在左边，我们有5个图像，都来自第一个octave(我的理解就是不同scale的图像)。通过在前一图像上应用高斯模糊来创建每个后续图像。在右边，我们有四个通过减去连续的高斯而生成的图像。<p><img alt="difference of gaussian | SIFT algorithm" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-14-18-26.png><p>我们为这些图像中的每一个都增强了功能。现在我们有了一组新的图像，我们将使用它来找到重要的关键点<h5 id=Keypoint-Localization><a title="Keypoint Localization" class=headerlink href=#Keypoint-Localization></a>Keypoint Localization</h5><p>一旦创建了图像，下一步就是从图像中<strong>找到可用于特征匹配的重要关键点</strong>。其思想是<strong>找到图像的局部最大值和最小值</strong>。<p>本部分分为两个步骤：1)求局部最大值和最小值 2)删除低对比度关键点（关键点选择）<blockquote><p>为了定位局部最大值和最小值，我们遍历图像中的每个像素，并将其与相邻像素进行比较。</blockquote><p>当说“相邻”时，这不仅包括该图像的周围像素（像素所在），还包括octave中上一个和下一个图像的九个像素。<p><img alt="Scale invariant" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-25-16-50-01-300x207.png><p>这意味着将每个像素值与其他26个像素值进行比较，以确定它是否是称为极值的局部最大值/最小值。例如，有三个来自第一个octave的图像。标记为x的像素与相邻像素（绿色）进行比较，如果它是相邻像素中最高或最低的，则选择它作为关键点或兴趣点<h5 id=Keypoint-Selection><a title="Keypoint Selection" class=headerlink href=#Keypoint-Selection></a>Keypoint Selection</h5><p>已经成功地生成了<strong>尺度不变的关键点</strong>。但是<strong>这些关键点中的一些可能对噪声不具有鲁棒性</strong>。我们需要进行最终检查，以确保我们有最准确的关键点来表示图像特征<p>因此，我们<strong>将消除对比度低或非常靠近边缘的关键点</strong>。为了处理低对比度关键点，为每个关键点计算二阶泰勒展开。如果结果值小于0.03（以大小计），我们将拒绝关键点。<h5 id=Keypoint-Descriptor><a title="Keypoint Descriptor" class=headerlink href=#Keypoint-Descriptor></a>Keypoint Descriptor</h5><p>到目前为止已经有了尺度不变和旋转不变的稳定关键点。<p>最后将使用相邻的像素、它们的方向和大小来为这个关键点生成一个独特的特征，称为“描述符”。<p>首先在关键点周围取一个16×16的邻域。这个16×16的块被进一步划分为4×4个子块，对于这些子块中的每一个子块，我们使用幅度和方向来生成直方图。<p><img alt="sift feature" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/Screenshot-from-2019-09-26-20-10-52.png><p>在这个阶段，bin的尺寸增加了，我们只取了8个bins。这些箭头中的每一个表示8bins，箭头的长度定义了大小。因此，我们将为每个关键点总共有128个bin值。<h5 id=Feature-Matching><a title="Feature Matching" class=headerlink href=#Feature-Matching></a>Feature Matching</h5><p>现在将使用SIFT特征进行特征匹配。为<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> cv2 </span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line><span class=comment># read images</span></span><br><span class=line>img1 = cv2.imread(<span class=string>'eiffel_2.jpeg'</span>)  </span><br><span class=line>img2 = cv2.imread(<span class=string>'eiffel_1.jpg'</span>) </span><br><span class=line></span><br><span class=line>img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class=line>img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class=line></span><br><span class=line><span class=comment>#sift</span></span><br><span class=line>sift = cv2.xfeatures2d.SIFT_create()</span><br><span class=line></span><br><span class=line>keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class=literal>None</span>)</span><br><span class=line>keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class=literal>None</span>)</span><br><span class=line></span><br><span class=line><span class=comment># read images</span></span><br><span class=line>img1 = cv2.imread(<span class=string>'eiffel_2.jpeg'</span>)  </span><br><span class=line>img2 = cv2.imread(<span class=string>'eiffel_1.jpg'</span>) </span><br><span class=line></span><br><span class=line>img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)</span><br><span class=line>img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)</span><br><span class=line></span><br><span class=line><span class=comment>#sift</span></span><br><span class=line>sift = cv2.xfeatures2d.SIFT_create()</span><br><span class=line></span><br><span class=line>keypoints_1, descriptors_1 = sift.detectAndCompute(img1,<span class=literal>None</span>)</span><br><span class=line>keypoints_2, descriptors_2 = sift.detectAndCompute(img2,<span class=literal>None</span>)</span><br><span class=line></span><br><span class=line><span class=comment>#feature matching</span></span><br><span class=line>bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck=<span class=literal>True</span>)</span><br><span class=line></span><br><span class=line>matches = bf.match(descriptors_1,descriptors_2)</span><br><span class=line>matches = <span class=built_in>sorted</span>(matches, key = <span class=keyword>lambda</span> x:x.distance)</span><br><span class=line></span><br><span class=line>img3 = cv2.drawMatches(img1, keypoints_1, img2, keypoints_2, matches[:<span class=number>50</span>], img2, flags=<span class=number>2</span>)</span><br><span class=line>plt.imshow(img3),plt.show()</span><br></pre></table></figure><p><img alt="SIFT algorithm" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/index_71.png><p><img alt="feature matching | SIFT algorithm" data-src=https://av-eks-blogoptimized.s3.amazonaws.com/index_61.png><ul><li>SIFT（Scale Invariant Feature Transform，尺度不变特征变换）是一种强大的图像匹配技术，它可以识别和匹配图像中对<strong>缩放、旋转和仿射失真不变的特征</strong>。<li>它被广泛应用于计算机视觉应用，包括图像匹配、物体识别和三维重建。SIFT技术包括生成具有不同尺度的图像的尺度空间，然后使用高斯差分（DoG）方法来识别图像中的关键点。<li>它还涉及为每个关键点计算描述符，这些描述符可用于特征匹配和对象识别。<li>它可以使用Python和OpenCV库来实现，OpenCV库提供了一组用于检测关键点、计算描述符和匹配特征的函数。</ul><h4 id=SURF><a class=headerlink href=#SURF title=SURF></a>SURF</h4><p>Speeded Up Robust Features（SURF）是SIFT的增强版。它的工作速度要快得多，并且对图像转换更健壮。<p>在SIFT中，使用高斯拉普拉斯算子来近似尺度空间。拉普拉斯算子是用于计算图像边缘的核。拉普拉斯核通过近似图像的二阶导数来工作。因此，它对噪声非常敏感。<strong>我们通常将高斯核应用于拉普拉斯核之前的图像，因此将其命名为高斯拉普拉斯</strong>。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br></pre><td class=code><pre><span class=line><span class=comment>#import required libraries</span></span><br><span class=line><span class=keyword>import</span> cv2</span><br><span class=line><span class=keyword>import</span> numpy <span class=keyword>as</span> np</span><br><span class=line><span class=keyword>import</span> matplotlib.pyplot <span class=keyword>as</span> plt</span><br><span class=line><span class=comment>#show OpenCV version</span></span><br><span class=line><span class=built_in>print</span>(cv2.__version__)</span><br><span class=line><span class=comment>#read image and convert to grayscale</span></span><br><span class=line>image = cv2.imread(<span class=string>'index.png'</span>)</span><br><span class=line>gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)</span><br><span class=line><span class=comment>#instantiate surf object</span></span><br><span class=line>surf  = cv2.xfeatures2d.SURF_create(<span class=number>400</span>)</span><br><span class=line><span class=comment>#calculate keypoints and their orientation</span></span><br><span class=line>keypoints,descriptors = surf.detectAndCompute(gray,<span class=literal>None</span>)</span><br><span class=line></span><br><span class=line>with_keypoints = cv2.drawKeypoints(gray,keypoints)</span><br><span class=line></span><br><span class=line>plt.imshow(with_keypoints)</span><br></pre></table></figure><h4 id=HOG><a class=headerlink href=#HOG title=HOG></a>HOG</h4><p>面向梯度直方图（HOG）是一种从像素颜色中提取特征的有效方法，用于构建对象识别分类器。<ol><li>预处理图像，包括调整大小和颜色标准化。<li>计算每个像素的梯度矢量，以及其大小和方向。</ol><p><img style="zoom: 33%;" alt=img data-src=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/image-gradient-vector-pixel-location.png><ol><li>将图像划分为许多8x8像素的单元格。在每个单元中，这64个单元的幅度值被装箱，并累积添加到9个无符号方向的bucket中（没有符号，因此0-180度而不是0-360度；这是基于经验实验的实际选择）。</ol><p><img alt=img data-src=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/HOG-histogram-creation.png><p>4.然后，我们在图像上滑动一个2x2个单元格（因此是16x16像素）的块。在每个块区域中，<strong>4个单元的4个直方图被连接成36个值的一维向量，然后被归一化为具有单位权重。最终的HOG特征向量是所有块向量的级联</strong>。<p><img alt=img data-src=https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-19-18-24-37-300x87.png><p>它可以被输入到像SVM这样的分类器中，用于学习对象识别任务。<p><img alt=img data-src=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/block_histogram.png><h3 id=Camera-Calibration><a title="Camera Calibration" class=headerlink href=#Camera-Calibration></a>Camera Calibration</h3><p>相机是一种将3D世界转换为2D图像的设备。相机在捕捉三维图像并将其存储在二维图像中起着非常重要的作用。<p>相机校准是图像处理或计算机视觉领域中常用的词。<strong>相机校准方法旨在识别图像创建过程的几何特征</strong>。这是在许多计算机视觉应用中执行的重要步骤，尤其是当需要场景上的度量信息时。在这些应用中，<strong>相机通常根据一组固有参数进行分类，如轴的偏斜、焦距和主点，其方向由旋转和平移等外部参数表示。线性或非线性算法用于实时利用已知点及其在图像平面中的投影来估计内在和外在参数</strong>。<p><img alt="Overview of Camera Calibration" data-src=https://editor.analyticsvidhya.com/uploads/887441.png><p>1.内在或内部参数它允许在图像帧中的像素坐标和相机坐标之间进行映射。例如，透镜的光学中心、焦距和径向失真系数。<p>2.外部或外部参数它描述了相机的方向和位置。这是指相机相对于某个世界坐标系的旋转和平移。<h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><p>综述<ol><li><a href=https://viso.ai/deep-learning/object-detection/ rel=noopener target=_blank>Object Detection in 2023: The Definitive Guide - viso.ai</a><li><a href=https://arxiv.org/abs/1905.05055 rel=noopener target=_blank>[1905.05055] Object Detection in 20 Years: A Survey (arxiv.org)</a><li><a href=https://arxiv.org/abs/2104.11892 rel=noopener target=_blank>[2104.11892] A Survey of Modern Deep Learning based Object Detection Models (arxiv.org)</a><li><a href=https://www.sciencedirect.com/science/article/pii/S1051200422004298 rel=noopener target=_blank>A comprehensive review of object detection with deep learning - ScienceDirect</a></ol><p>博客<ol><li><a href=https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/ rel=noopener target=_blank>Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS | Lil’Log (lilianweng.github.io)</a><li><a href=https://www.analyticsvidhya.com/blog/2021/09/a-beginners-guide-to-image-processing-with-opencv-and-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/ rel=noopener target=_blank>A Beginner’s Guide to Image Processing With OpenCV and Python (analyticsvidhya.com)</a><li>也许还不错的学习网站<a href=https://pyimagesearch.com/ rel=noopener target=_blank>PyImageSearch - You can master Computer Vision, Deep Learning, and OpenCV.</a><li>维基百科 卷积核<a href=https://en.wikipedia.org/wiki/Kernel_(image_processing rel=noopener target=_blank>Kernel (image processing) - Wikipedia</a>)</ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\12\22\目标检测综述\ rel=bookmark>目标检测综述</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\11\01\目标检测学习-P3\ rel=bookmark>目标检测学习_P3</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/10/21/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%88%9D%E8%AF%86/ title=目标检测_初识>https://www.sekyoro.top/2023/10/21/目标检测-初识/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/object-detection/ rel=tag><i class="fa fa-tag"></i> object detection</a><a href=/tags/cv/ rel=tag><i class="fa fa-tag"></i> cv</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/10/20/Python%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/ rel=prev title=Python并行计算> <i class="fa fa-chevron-left"></i> Python并行计算 </a></div><div class=post-nav-item><a href=/2023/10/23/DDNLP-%E6%B7%B1%E5%85%A5NLP/ rel=next title=DDNLP:深入NLP> DDNLP:深入NLP <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E5%88%86%E8%BE%A8%E7%8E%87><span class=nav-number>1.</span> <span class=nav-text>图像分辨率</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94><span class=nav-number>1.1.</span> <span class=nav-text>图像金字塔</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE><span class=nav-number>2.</span> <span class=nav-text>图像直方图</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E6%B7%B1%E5%BA%A6><span class=nav-number>3.</span> <span class=nav-text>图像深度</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4><span class=nav-number>4.</span> <span class=nav-text>颜色空间</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8Fresize><span class=nav-number>5.</span> <span class=nav-text>图像resize</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E6%97%8B%E8%BD%AC%E4%BB%A5%E5%8F%8A%E5%B9%B3%E7%A7%BB><span class=nav-number>6.</span> <span class=nav-text>图像旋转以及平移</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E9%98%88%E5%80%BC><span class=nav-number>7.</span> <span class=nav-text>图像阈值</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2><span class=nav-number>8.</span> <span class=nav-text>图像分割</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BD%8D%E6%93%8D%E4%BD%9C><span class=nav-number>9.</span> <span class=nav-text>位操作</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B><span class=nav-number>10.</span> <span class=nav-text>边缘检测</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E6%BB%A4%E6%B3%A2><span class=nav-number>11.</span> <span class=nav-text>图像滤波</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93-contours><span class=nav-number>12.</span> <span class=nav-text>图像轮廓(contours)</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D><span class=nav-number>13.</span> <span class=nav-text>特征匹配</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B><span class=nav-number>14.</span> <span class=nav-text>人脸检测</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E6%A2%AF%E5%BA%A6%E5%90%91%E9%87%8F><span class=nav-number>15.</span> <span class=nav-text>图像梯度向量</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E7%AE%97%E5%AD%90><span class=nav-number>16.</span> <span class=nav-text>边缘检测算子</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#prewitt><span class=nav-number>16.1.</span> <span class=nav-text>prewitt</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#sobel><span class=nav-number>16.2.</span> <span class=nav-text>sobel</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B><span class=nav-number>17.</span> <span class=nav-text>角点检测</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Harris><span class=nav-number>17.1.</span> <span class=nav-text>Harris</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%B8%B8%E7%94%A8%E7%89%B9%E5%BE%81><span class=nav-number>18.</span> <span class=nav-text>常用特征</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#SIFT><span class=nav-number>18.1.</span> <span class=nav-text>SIFT</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Constructing-the-Scale-Space><span class=nav-number>18.1.1.</span> <span class=nav-text>Constructing the Scale Space</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Keypoint-Localization><span class=nav-number>18.1.2.</span> <span class=nav-text>Keypoint Localization</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Keypoint-Selection><span class=nav-number>18.1.3.</span> <span class=nav-text>Keypoint Selection</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Keypoint-Descriptor><span class=nav-number>18.1.4.</span> <span class=nav-text>Keypoint Descriptor</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Feature-Matching><span class=nav-number>18.1.5.</span> <span class=nav-text>Feature Matching</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#SURF><span class=nav-number>18.2.</span> <span class=nav-text>SURF</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#HOG><span class=nav-number>18.3.</span> <span class=nav-text>HOG</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Camera-Calibration><span class=nav-number>19.</span> <span class=nav-text>Camera Calibration</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number></span> <span class=nav-text>参考资料</span></a></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>236</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>211</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/cv/ rel=tag>cv</a><span class=tag-list-count>1</span><li class=tag-list-item><a class=tag-list-link href=/tags/object-detection/ rel=tag>object detection</a><span class=tag-list-count>3</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2025</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2.6m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>38:55</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>