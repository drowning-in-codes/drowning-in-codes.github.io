<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=课程作业 name=description><meta content=article property=og:type><meta content=图像融合论文阅读 property=og:title><meta content=https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=课程作业 property=og:description><meta content=zh_CN property=og:locale><meta content=https://i.imgur.com/czqa4Qz.png property=og:image><meta content=https://i.imgur.com/nP1CzPO.png property=og:image><meta content=https://pic4.zhimg.com/80/v2-13bb3c60b27920c3ec834e045ec8756f_720w.webp property=og:image><meta content=https://i.imgur.com/xSMcHqm.png property=og:image><meta content=https://i.imgur.com/5OAM3nA.png property=og:image><meta content=https://i.imgur.com/EnD6Jys.png property=og:image><meta content=https://i.imgur.com/FW1COsX.png property=og:image><meta content=https://i.imgur.com/qczz1P9.png property=og:image><meta content=https://i.imgur.com/Hqdxy5j.png property=og:image><meta content=https://i.imgur.com/DmKSSiy.png property=og:image><meta content=https://i.imgur.com/OP6inkz.png property=og:image><meta content=https://i.imgur.com/gcNlg22.png property=og:image><meta content=https://i.imgur.com/XZhUCg2.png property=og:image><meta content=https://i.imgur.com/Qwko06N.png property=og:image><meta content=https://i.imgur.com/lv9Mg1o.png property=og:image><meta content=https://i.imgur.com/vuIYIRj.png property=og:image><meta content=https://i.imgur.com/A3gJRD2.png property=og:image><meta content=https://i.imgur.com/Fgpy1VL.png property=og:image><meta content=https://i.imgur.com/oI8nYfO.png property=og:image><meta content=https://i.imgur.com/WPHLLvv.png property=og:image><meta content=https://i.imgur.com/l2ONRey.png property=og:image><meta content=https://i.imgur.com/veuN5Um.png property=og:image><meta content=https://i.imgur.com/TYR6p1y.png property=og:image><meta content=https://i.imgur.com/YrdNz1J.png property=og:image><meta content=https://i.imgur.com/8y3MF5I.png property=og:image><meta content=https://i.imgur.com/6H8J7CW.png property=og:image><meta content=2023-11-02T14:24:07.000Z property=article:published_time><meta content=2023-11-18T08:43:48.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="image fusion" property=article:tag><meta content=summary name=twitter:card><meta content=https://i.imgur.com/czqa4Qz.png name=twitter:image><link href=https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>图像融合论文阅读 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>图像融合论文阅读</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-11-02 22:24:07" datetime=2023-11-02T22:24:07+08:00>2023-11-02</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2023-11-18 16:43:48" datetime=2023-11-18T16:43:48+08:00 itemprop=dateModified>2023-11-18</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>13k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>12 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>课程作业<br><span id=more></span><h1 id=abs><a class=headerlink href=#abs title=abs></a>abs</h1><p>介绍图像融合概念，回顾sota模型，其中包括数字摄像图像融合，多模态图像融合，<p>接着评估一些代表方法<p>介绍一些常见应用，比如RGBT目标跟踪，医学图像检查，遥感监测<h1 id=Intro><a class=headerlink href=#Intro title=Intro></a>Intro</h1><p>动机：<p>由于硬件设备的理论和技术限制，单一传感器或单一拍摄设置所拍摄的图像无法有效、全面地描述成像场景<p>图像融合：图像融合能够将不同源图像中的有意义信息结合起来，生成单一图像，该图像包含更丰富的信息，更有利于后续应用<p>由于融合图像的优异特性，图像融合作为一种图像增强方法已被广泛应用于许多领域，例如摄影可视化<h2 id=传统融合方法：><a class=headerlink href=#传统融合方法： title=传统融合方法：></a>传统融合方法：</h2><p>在深度学习盛行之前，图像融合已经得到了深入的研究。早期实现图像融合的方法采用相关的数学变换，在<strong>空间域或变换域</strong>人工分析活动水平并设计融合规则，称为传统融合方法。<p>典型的传统融合方法包括基于<strong>多尺度变换</strong>的方法、基于<strong>稀疏表示</strong>的方法、<strong>基于子空间</strong>的方法、基于<strong>显著性</strong>的方法、基于total-variance的方法等。<p>传统图像融合方法的缺点：<ol><li>为了保证后续图像融合的可行性，传统方法会对不同源的图像采用相同变换来提取特征。这种方法没有考虑到源图像的特征差异，可能导致提取的特征表现力较差。<li>融合策略粗糙，表现较差。</ol><p>引入深度学习的优势：<ol><li><p>可以利用不同的网络实现差异化的特征提取</p><li><p>在良好设计的损失函数下，融合策略可以学到更合理的特征</p></ol><p>现有的深度学习方法致力于解决图像融合中的三个主要问题：“feature extraction, feature fusion and image reconstruction.” (Zhang 等, 2021, p. 323)<p>现有方法可以分为 AutoEncoder-based，CNN- based，GAN-based。<h3 id=1-AE-based><a class=headerlink href=#1-AE-based title=1.AE-based></a>1.AE-based</h3><p>AE 方法通常会预先训练一个自动编码器。然后利用训练好的自编码器实现特征提取和图像重建，中间的特征融合则根据传统的融合规则实现。<p>DenseFuse<h3 id=2-CNN-based><a class=headerlink href=#2-CNN-based title=2.CNN-based></a>2.CNN-based</h3><p>他们通常以两种不同的形式将卷积神经网络引入图像融合。一种是通过使用精心设计的损失函数和网络结构，实现端到端的特征提取、特征融合和图像重建<p>PMGI。它提出了梯度和强度的比例维护损失，引导网络直接生成融合图像。<p>另外还有使用CNN得到融合规则，而使用传统的特征提取和重建方法<h3 id=3-GAN-baesd><a class=headerlink href=#3-GAN-baesd title=3.GAN-baesd></a>3.GAN-baesd</h3><p>GAN 方法依靠生成器和判别器之间的对抗博弈来估计目标的概率分布，从而以隐含的方式共同完成特征提取、特征融合和图像重构<p>比如FusionGAN 是基于 GAN 的图像融合的先驱，它在融合图像和可见图像之间建立对抗博弈，以进一步丰富融合图像的纹理细节。由于各种图像融合任务存在显著差异，这些方法在不同融合场景中的实现方式也不尽相同。<h2 id=图像融合场景><a class=headerlink href=#图像融合场景 title=图像融合场景></a>图像融合场景</h2><p>digital photography image fusion<p>由于数字成像设备的性能限制，传感器无法在单一设置完全表征成像场景中的信息<p>例如，数码摄影产生的图像只能承受有限的光照变化，并具有预定的景深。<p>多曝光度图像融合和多聚焦图像融合<p>以产生高动态范围和完全清晰的效果。<p>人们使用摄像机拍摄时,希望可以获得同一场景中所有景物都清晰的图像。但是<strong>摄像机镜头受景深的限制,无法同时聚焦所有目标,因此拍摄的照片中部分区域清晰,部分区域模糊。多聚焦图像融合技术可以将多幅同一场景下聚焦区域不同的图像融合成一幅全清晰的图像</strong>,从而有效地解决这个问题,提高图像的信息利用率。<p>multi-modal image fusion<p>由于成像原理的限制，单个传感器只能捕捉到部分场景信息。多模态图像融合将多个传感器获取的图像中最重要的信息结合起来，从而实现对场景的有效描述。<p>典型的多模态图像融合包括红外和可见光图像融合<p>sharpening fusion<p>在保证信噪比的前提下，光谱/滤镜与瞬时视场（IFOV）之间存在一定的矛盾。<p>换句话说，没有任何传感器能同时捕捉高空间分辨率和高光谱分辨率的图像。<p>锐化融合专门用于融合不同空间/光谱分辨率的图像，以生成所需的结果，这些结果不仅具有高空间分辨率，还具有高光谱分辨率。<p>典型的锐化融合包括多光谱（MS）锐化和高光谱锐化。从源图像成像的角度来看，锐化融合也属于多模态图像融合。不过，就融合目标而言，锐化融合比上述多模态图像融合要求更高的光谱/空间保真度，能直接提高分辨率。因此，锐化融合将作为一个单独的类别进行讨论。<p>多光谱锐化是将低空间分辨率（LRMS）的多光谱图像与全色（PAN）图像融合，生成高空间分辨率的多光谱图像。<p>与多光谱图像相比，高光谱图像具有更高的光谱分辨率和更低的空间分辨率。<blockquote><p>多光谱: 谱段有多个,可以看做是高光谱的一种情况，即成像的波段数量比高光谱图像少，一般只有几个到十几个。由于光谱信息其实也就对应了色彩信息，所以多波段遥感图像可以得到地物的色彩信息，但是空间分辨率较低。更进一步，光谱通道越多，其分辨物体的能力就越强，即光谱分辨率越高。<p>高光谱:高光谱由更窄的波段（10-20 nm）组成，具有较高的光谱分辨率，可以检测物体的光谱特效，可提供更多无形的数据,图像可能有数百或数千个波段<p>全色图:全色图像是单通道的，其中全色是指全部可见光波段0.38~0.76um，全色图像为这一波段范围的混合图像。因为是单波段，所以在图上显示为灰度图片。全色遥感图像一般空间分辨率高，但无法显示地物色彩，也就是图像的光谱信息少。</blockquote><h3 id=digital-photography-image-fusion><a title="digital photography image fusion" class=headerlink href=#digital-photography-image-fusion></a>digital photography image fusion</h3><p>数字成像设备利用光学镜头捕捉反射的可见光然后采用CCD和CMOS等书子模块记录场景信息。另一方面，由于动态范围有限，这些数字模块无法承受过大的成像曝光差异。<p>一方面，由于光学镜头受景深限制，通常无法同时聚焦所有物体。<h3 id=Infrared-and-visible-image-fusion”><a title="Infrared and visible image fusion”" class=headerlink href=#Infrared-and-visible-image-fusion”></a>Infrared and visible image fusion”</h3><p>红外图像具有<strong>明显的对比度</strong>，即使在恶劣天气下也能从背景中有效地突出目标。可见光图像包含丰富的纹理细节，更符合人类的视觉感知。红外和可见光图像融合就是要将这两种特性结合起来，生成对比度高、纹理丰富的图像。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。<p><strong>高对比度，恶劣条件下也能有效突出目标</strong>。<p><strong>可见光图像包含丰富的纹理信息</strong>，更符合人类视觉感知。红外和可见光图像融合就是要<strong>将这两种特性结合起来，生成对比度高、纹理丰富的图像</strong>。为了实现这一目标，AE、CNN 和 GAN 方法都被引入到这项任务中。<p>AE方法<p>首先使用数据集训练一个autoencoder，训练好的自动编码器自然可以用来解决图像融合中的两个子问题：特征提取和图像重建<p>图像融合的关键在于<strong>特征融合策略</strong>的设计。目前，在红外和可见光图像融合中，特征融合的策略仍然是手工计算的，无法学习，如加法、l1-norm [19]、注意力加权等。这种手工计算的融合策略比较粗糙，限制了红外图像和可见光图像融合的进一步改进。<p>一种用于红外和可见光图像融合的 CNN 方法是端对端地实现三个子问题。这种CNN结构通常需要残差连接，全连接以及双端结构。<p>由于红外图像和可见光图像融合没有ground truth，因此损失函数的设计在于确定<strong>融合结果和源图像之间对比度和纹理的相似性</strong>。<p>参与红外和可见光图像融合的另一种 CNN 形式是使用预先训练好的网络（如 VGGNet）从源图像中提取特征，并根据这些特征生成融合权重图。<p>从这个角度看，卷积神经网络只实现了融合，而不考虑特征提取和图像重建，带来的融合性能非常有限。<h2 id=GAN><a class=headerlink href=#GAN title=GAN></a>GAN</h2><p>GAN 方法是目前红外和可见光图像融合领域最流行的方法，它能够以<strong>隐含的方式完成特征提取、特征融合和图像重建</strong>。<p>一般来说，GAN 方法依赖于两种损失函数，即内容损失和对抗损失。内容损失与 CNN 方法类似，用于初步融合源图像，而对抗损失则进一步限制信息融合的趋势。<p>早期GAN方法 fusionGAN，只是在融合后的图像和可见光图像之间建立对抗博弈，以进一步增强对可见光图像丰富细节的保留。<p>为了更好地平衡红外信息和可见光信息，随后的方法 [25,66-69] 开始使用具有<strong>多个分类约束条件的单一判别器或双判别器来同时估计源图像的两种概率分布。</strong><p>一般来说，GAN 方法可以产生很好的融合结果。然而，在训练过程中保持生成器和判别器之间的平衡并非易事。<h3 id=评估><a class=headerlink href=#评估 title=评估></a>评估</h3><p>评估指标包括:EN,SSIM,PSNR,SF,SD,CC,SF,VIF以及融合运行时间等等.<h4 id=EN><a class=headerlink href=#EN title=EN></a>EN</h4><p>熵值<p><img alt=image-20231103165404222 data-src=https://i.imgur.com/czqa4Qz.png><p>p~l~是融合图像中相应灰度级的归一化直方图<p>熵越大，融合图像包含的信息就越多，融合方法的性能就越好。<h4 id=SD><a class=headerlink href=#SD title=SD></a>SD</h4><p>标准差<p><img alt=image-20231103165557350 data-src=https://i.imgur.com/nP1CzPO.png><p>对比度高的区域总是能吸引人的注意力，而对比度高的融合图像往往能产生较大的标清值，这意味着融合图像能达到更好的视觉效果。<h4 id=SSIM><a class=headerlink href=#SSIM title=SSIM></a>SSIM</h4><p>结构相似性,取值[-1,1],<strong>数值越接近1表示结构相似性越高</strong><p>SSIM 用于建立图像损失和失真的模型，<strong>衡量源图像和融合图像之间的结构相似性</strong>。SSIM 主要由三部分组成：<strong>相关性损失、亮度失真和对比度失真</strong>。<p><img alt=img data-src=https://pic4.zhimg.com/80/v2-13bb3c60b27920c3ec834e045ec8756f_720w.webp><p>在融合任务中,计算两张源图与融合后图像的SSIM和<p><img alt=image-20231103171546346 data-src=https://i.imgur.com/xSMcHqm.png><h4 id=PSNR><a class=headerlink href=#PSNR title=PSNR></a>PSNR</h4><p>峰值信噪比,衡量图像有效信息与噪声之间的比率,能够反映图像是否失真.<strong>数值越大表示失真越小</strong><p><img alt=image-20231103164139843 data-src=https://i.imgur.com/5OAM3nA.png><p>Z表示理想参考图像灰度最大值与最小值之差，通常为255。PSNR的值越大，表示融合图像的质量越好。<blockquote><p>PSNR值的范围通常在<strong>0到100之间</strong>，单位为分贝（dB）。 通常情况下，PSNR值越高，表示原始图像与重建图像之间的差异越小，图像质量越接近原始图像。 一般来说，PSNR值在30到40dB之间被认为是可以接受的</blockquote><h4 id=CC><a class=headerlink href=#CC title=CC></a>CC</h4><p>CC 衡量融合图像与源图像的线性相关程度,CC 越大，融合后的图像与源图像越相似，融合效果越好<p><img alt=image-20231103172617252 data-src=https://i.imgur.com/EnD6Jys.png><p><img alt=image-20231103172623697 data-src=https://i.imgur.com/FW1COsX.png><h4 id=SF-空间频率><a title="SF 空间频率" class=headerlink href=#SF-空间频率></a>SF 空间频率</h4><p><img alt=image-20231103173521300 data-src=https://i.imgur.com/qczz1P9.png><p>测量图像的梯度分布<p><img alt=image-20231103173745980 data-src=https://i.imgur.com/Hqdxy5j.png><p><img alt=image-20231103173758017 data-src=https://i.imgur.com/DmKSSiy.png><p>SF 越大，融合图像的边缘和纹理就越丰富<p>​<h4 id=VIF-空间信息保真度><a title="VIF 空间信息保真度" class=headerlink href=#VIF-空间信息保真度></a>VIF 空间信息保真度</h4><p>VIF 衡量融合图像的信息保真度，其计算方法分为四个步骤：首先，将源图像和融合图像划分为不同的区块；然后，评估每个区块在失真和未失真情况下的视觉信息；接着，评估每个子波段的 VIF；最后，根据 VIF 计算总体指标。<h3 id=数据集><a class=headerlink href=#数据集 title=数据集></a>数据集</h3><p>TNO TNO影像融合数据集包含不同军事相关场景的单光谱（增强视觉、近红外和长波红外或热）夜间影像，在不同的多波段camnera系统中注册。<p>INO RoadScene MSRS LLVIP MFD<h2 id=实战><a class=headerlink href=#实战 title=实战></a>实战</h2><p>主要关注红外图像与可见光图像融合以及多焦点图像融合,从这些出发,最后到一个统一的图像融合框架.<h3 id=FusionGAN><a class=headerlink href=#FusionGAN title=FusionGAN></a>FusionGAN</h3><p>2019年较早的使用GAN作为图像融合算法融合红外和可见光<p><img alt=image-20231104161851282 data-src=https://i.imgur.com/OP6inkz.png><h4 id=Generator><a class=headerlink href=#Generator title=Generator></a>Generator</h4><p><img alt=image-20231104161928602 data-src=https://i.imgur.com/gcNlg22.png><p>注意损失函数设计</p><script type="math/tex; mode=display">
\mathcal{L}_G=V_\text{FusionGAN}(G)+\lambda\mathcal{L}_{\mathrm{content}},</script><p>使用了一个对于GAN对抗的融合损失以及一个内容损失,对抗损失,这种想法来源LSGAN,a 和 b 分别表示虚假数据和真实数据的标签，c表示生成器希望鉴别器相信的虚假数据值。</p><script type="math/tex; mode=display">
\begin{aligned}\min_DV_{\mathrm{LSGAN}}(D)&=~\frac12\mathbb{E}_{x\sim p_{data}(x)}[(D(x)-b)^2]+\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-a)^2],\\\min_GV_{\mathrm{LSGAN}}(G)&=~\frac12\mathbb{E}_{z\sim p_z(z)}[(D(G(z))-c)^2],\end{aligned}</script><p>有两种方法可以确定公式中的 a、b 和 c 值。第一种是设置 b - c = 1 和 b - a = 2，从而最小化公式 ，使 P~data~ +P~g~ 与 P~g~ 之间的 Pearson χ2 最小化<p>第二种是设置 c = b，使生成器生成的样本尽可能真实。上述两种方法通常能获得相似的性能。</p><script type="math/tex; mode=display">
V_{\text{FusionGAN}} ( G ) = \frac 1 N \sum _ { n = 1 }^{N}\left(D_{\theta_D}(I_f^n)-c\right)^2,</script><p>第二项 L~content~代表内容损失，λ 用于在 V~FusionGAN~(G) 和 L~content~之间取得平衡。由于红外图像的热辐射信息由其像素强度表征，而可见光图像的纹理细节信息可部分由其梯度表征. 当然可以有其他用于表征图片的某些特性的指标,比如上面介绍的SSIM等.</p><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{content}}=\frac1{HW}(\|I_f-I_r\|_F^2+\xi\|\nabla I_f-\nabla I_v\|_F^2),</script><p>实际上，如果没有 D~θ~，我们也可以得到融合图像，它可以保留红外图像中的热辐射信息和可见光图像中的梯度信息。<p>但这往往还不够，因为仅使用梯度信息无法完全表现可见图像中的纹理细节。因此，我们在生成器 G~θG~和判别器 D~θD~ 之间建立了一个对抗博弈，以调整基于可见光图像 IIv 的融合图像 If。<h4 id=Discriminator><a class=headerlink href=#Discriminator title=Discriminator></a>Discriminator</h4><p>从第一层到第四层，我们在卷积层中使用 3 × 3 滤波器，并将stride设为 2，不带填充。这与生成器网络不同。其根本原因在于，鉴别器是一个分类器，它首先从输入图像中提取特征图，然后进行分类。因此，它的工作方式与池化层相同，将stride设置为 2。<p><img alt=image-20231104163811787 data-src=https://i.imgur.com/XZhUCg2.png></p><script type="math/tex; mode=display">
\mathcal{L}_D=\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_v)-b\right)^2+\frac{1}{N}\sum_{n=1}^N\left(D_{\theta_D}(I_f)-a\right)^2,</script><p>我使用了这个模型<h3 id=TarDAL><a class=headerlink href=#TarDAL title=TarDAL></a>TarDAL</h3><p>面向检测的融合<p>我们采用双层优化公式同时进行图像融合和物体检测，不仅检测精度高，而且融合后的图像视觉效果更好。<p>我们设计了一种参数较少的目标感知双对抗学习网络（TarDAL），用于面向检测的融合。这种 “求同存异 “的单生成器双判别器网络可保留红外目标信息和可见光纹理细节。<p>我们从双层表述中推导出一种合作训练方案，为快速推理（融合和检测）提供最佳网络参数。<p>与以往追求高视觉质量的方法不同，我们认为，IVIF 必须生成既有利于视觉检测又有利于计算机感知的图像，即面向检测的融合。<h3 id=问题建模><a class=headerlink href=#问题建模 title=问题建模></a>问题建模</h3><p><img alt=image-20231104153245210 data-src=https://i.imgur.com/Qwko06N.png><p>假设红外图像、可见光图像和融合图像都是大小为 m×n 的灰度图像，分别用列向量 x、y 和 u∈R^mn×1^ 表示。<p>L~d~ 是目标检测相关的训练损失,Ψ是一个目标检测网络,f () 是一个基于能量的保真度项，包含融合图像 u 以及源图像 x 和 y，而 g~T~ () 和 g~D~ () 则是两个可行性约束条件,分别定义在红外图像和可见光图像上。<p><img alt=image-20231104154941474 data-src=https://i.imgur.com/lv9Mg1o.png><p>引入一个带有学习参数 ω~f~的融合网络 Φ，并将双级优化转换为单级优化.<p>因此，我们将优化分解为两个学习网络 Φ 和 Ψ。我们采用 YOLOv53 作为检测网络 Ψ 的主干，其中 L~d~也沿用其设置，并精心设计了融合网络 Φ 。<blockquote><p>典型的深度融合方法致力于学习两种不同成像模式的共同特征。相反，我们的融合网络在<strong>学习这两种成像方式互补特征的差异的同时，也在寻求共性</strong>。通常情况下<strong>，红外图像能突出显示目标的独特结构，而可见光图像则能提供背景的纹理细节</strong>。</blockquote><h4 id=Target-aware-dual-adversarial-network><a title="Target-aware dual adversarial network" class=headerlink href=#Target-aware-dual-adversarial-network></a>Target-aware dual adversarial network</h4><p><img alt=image-20231107114627598 data-src=https://i.imgur.com/vuIYIRj.png><p>生成器G生成一张逼真的融合图像,目标判别器D~T~使用强度一致性评估红外图像中的目标与G提供的融合图像中被mask的目标.细节判别器 D~D~判别的是可见光梯度分布与融合图像的梯度分布<h4 id=生成器><a class=headerlink href=#生成器 title=生成器></a>生成器</h4><p>生成器的作用是生成能保留整体结构并保持与源图像相似强度分布的融合图像。常用的结构相似性指数(SSIM)作为损失函数.<p>为了平衡源图像的像素强度分布，引入了基于突出度权重（SDW）的像素损失。<p>另外提出了一个基于显著性pixel loss</p><script type="math/tex; mode=display">
S_{\mathbf{x}(k)}=\sum_{i=0}^{2\text{55}} \boldsymbol { H _ { \mathbf{x}}}(i)|\mathbf{x}(k)-i|,</script><p>其中H~x~(i)表示直方图中i的值,x(k)表示第k个值,因为x为一个大小为mn的vector</p><script type="math/tex; mode=display">
\mathscr{L}_{\mathrm{pixe}1}=\|\mathrm{u}-\omega_1\mathrm{x}\|_1+\|\mathrm{u}-\omega_2\mathrm{y}\|_1,</script><p>pixel loss如上,其中</p><script type="math/tex; mode=display">
\boldsymbol{\omega}_1=S_\mathbf{x}(k)/[S_\mathbf{x}(k)-S_\mathbf{y}(k)],\boldsymbol{\omega}_\mathbf{2}=1-\boldsymbol{\omega}_\mathbf{1}.</script><p>使用 5 层密集块作为 G 来提取共同特征，然后使用包含三个卷积层的合并块进行特征聚合。每个卷积层由一个卷积运算、批处理归一化和 ReLU 激活函数组成。生成的融合图像 u 与源图像大小相同。<h4 id=目标鉴别器与细节鉴别器><a class=headerlink href=#目标鉴别器与细节鉴别器 title=目标鉴别器与细节鉴别器></a>目标鉴别器与细节鉴别器</h4><p>目标判别器 D~T~ 用于将融合结果的前景热目标与红外目标区分开来。而细节判别器 D~D~ 的作用是将融合结果的背景细节与可见光图像的细节区分开来。<p>采用了预训练的显著性检测网络从红外图像中计算出目标掩码 m，这样两个判别器就能在各自的区域（目标和背景）进行判别(也就是将图像中的目标与背景分割)<p>对抗损失如下,R(x)表示红外图像中的目标,R(u)表示融合后图像中的目标,R^^^则表示背景</p><script type="math/tex; mode=display">
\begin{gathered}\mathcal{L}_{D_T}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\mathcal{R}(\mathbf{x}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\mathcal{R}(\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{D_D}^\mathbf{f}=\mathbb{E}_{x\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{y}))}[D(x)]-\mathbb{E}_{\tilde{x}\sim\tilde{p}(\hat{\mathcal{R}}(\nabla\mathbf{u}))}[D(\tilde{x})],\\\mathcal{L}_{\mathbf{f}}^{\mathrm{adv}}=\mathcal{L}_{D_T}^\mathbf{f}+\mathcal{L}_{D_D}^\mathbf{f},\end{gathered}</script><p>R = x*m ,R^^^= 1 − R. m表示使用预训练模型得到mask.<p>对于鉴别器,损失分别是</p><script type="math/tex; mode=display">
\begin{gathered}
\mathcal{L}_{D_T}=\mathcal{L}_{D_T}^\mathbf{f}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\mathcal{R}(\mathbf{x}))}[(\|\nabla D_T(\tilde{x})\|)^p], \\
\mathcal{L}_{D_{D}}=\mathcal{L}_{D_{D}}^{\mathbf{f}}+k\mathbb{E}_{\tilde{x}\sim\tilde{r}(\hat{\mathcal{R}}(\nabla\mathbf{x}))}[(\|\nabla D_{D}(\tilde{x})\|)^{p}], 
\end{gathered}</script><p>两个鉴别器 D~T~ 和 D~D~ 具有相同的网络结构，即四个卷积层和一个全连接层。<p><img alt=image-20231107131708716 data-src=https://i.imgur.com/A3gJRD2.png><h4 id=合作训练策略><a class=headerlink href=#合作训练策略 title=合作训练策略></a>合作训练策略</h4><script type="math/tex; mode=display">
\begin{aligned}\min_{\boldsymbol{\omega}_{\mathbf{d}},\boldsymbol{\omega}_{\mathbf{f}}}\mathcal{L}^{\mathbf{d}}(\Psi(\mathbf{u}^*;\boldsymbol{\omega}_{\mathbf{d}}))+\lambda\mathcal{L}^{\mathbf{f}}\big(\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}})\big),\\s.t.\mathbf{~u}^*=\Phi(\mathbf{x},\mathbf{y};\boldsymbol{\omega}_{\mathbf{f}}),\end{aligned}</script><p>双层优化自然会衍生出一种合作训练策略，以获得最佳网络参数 ω = (ω~d~, ω~f~)<p>引入了一个融合正则因子 L^f^，将受融合约束的检测优化转换为相互优化<p>损失函数包含目标检测的损失函数以及融合的损失函数.<h3 id=红外与可见光图像的融合结果><a class=headerlink href=#红外与可见光图像的融合结果 title=红外与可见光图像的融合结果></a>红外与可见光图像的融合结果</h3><h4 id=定性比较><a class=headerlink href=#定性比较 title=定性比较></a>定性比较</h4><p>首先，可以很好地保留红外图像中的分辨目标。如图 6（第二组的绿色缠结）所示，我们的方法中的人表现出高对比度和独特的突出轮廓，因此有利于视觉观察.<p>其次，我们的结果可以保留可见光图像中丰富的纹理细节（第一组和第三组的绿色缠结），这更符合人类的视觉系统。<h4 id=定量比较><a class=headerlink href=#定量比较 title=定量比较></a>定量比较</h4><p>在 400 个图像对（20 个来自 TNO 的图像对、40 个来自 RoadScene 的图像对和 340 个来自 M3FD 的图像对）上对我们的 TarDAL 和上述竞争对手进行了定量比较。<p>使用了MI,EN和SD作为评估指标.<h3 id=红外与可见光目标检测结果><a class=headerlink href=#红外与可见光目标检测结果 title=红外与可见光目标检测结果></a>红外与可见光目标检测结果</h3><h4><a title=" " class=headerlink href=#></a></h4><h3 id=多聚焦图像融合><a class=headerlink href=#多聚焦图像融合 title=多聚焦图像融合></a>多聚焦图像融合</h3><h3 id=MFIF-GAN><a class=headerlink href=#MFIF-GAN title=MFIF-GAN></a>MFIF-GAN</h3><p>针对多焦点图像融合<p>在数码摄影领域，有限的景深（DOF）导致单一场景中可能存在多种图像焦点区域，并产生散焦效应（DSE）[1]。作为一种图像增强技术，多焦点图像融合（MFIF）被用来融合图 1(a) 和图 1(b) 所示的多焦点图像，使融合结果（如图 1(c) 所示）能够清晰地保留来源信息。这一操作是各类计算机视觉（CV）任务的前提条件，例如物体检测和定位、识别和分割<h4 id=网络结构><a class=headerlink href=#网络结构 title=网络结构></a>网络结构</h4><p>MFIF-GAN 中的生成器将源彩色图像 IA 和 IB 作为输入，旨在生成焦点图 ̂ F。判别器的输入是 IA、IB 和（真实或生成的）焦点图的连接。生成器的目的是尽可能精确地重建焦点图，而鉴别器的目的是将生成的焦点图与真实的焦点图区分开来。<p>G 包括一个编码器、一个张量连接模块和一个解码器。<strong>为了有效处理彩色图像，编码器被设计成六个并行子网络分支，共享源图像每个通道的参数</strong>。<h3 id=FuseGAN><a class=headerlink href=#FuseGAN title=FuseGAN></a>FuseGAN</h3><p>我们的目标是通过构建基于 cGAN 的网络 FuseGAN，学习从源图像到置信度图的映射，从而为融合任务提供重点信息。我们首先详细介绍了该架构，然后分析了其目标函数；最后阐述了融合方案.<p>生成器 G：如图所示，生成器 G 由三个部分组成：编码器、张量并合器和解码器。<strong>它将一对多焦点图像作为输入，并输出置信度图</strong>。具体来说，编码器有两个分支，每个分支包含 12 个块。为简单起见，我们将卷积层、批规范层和转置卷积层分别称为 Conv、BN 和 Decov。其中，第一块是 Conv-BN-ReLu，滤波器尺寸较大，为 7×7，步长为 1，目的是粗略提取特征。<p><img style="zoom: 80%;" alt=image-20231108184311430 data-src=https://i.imgur.com/Fgpy1VL.png><p>我们利用 中的 PatchGAN 作为判别器 D。从概念上讲，它试图辨别图像中每个大小为 K×K 的patch是真是假。鉴别器对图像中的所有响应进行卷积平均，最后生成输出。。<p><img alt=image-20231108190809183 data-src=https://i.imgur.com/oI8nYfO.png><p>因此，我们利用自适应权重块设计的特定内容损失函数可以自适应地引导融合图像在像素级逼近源图像中重点区域的强度分布和梯度分布<p>此外,由于我们的优化目标是<strong>基于每个像素</strong>,<strong>为了避免融合后的图像出现色差,保证整体的自然度,我们增加了 SSIM 损失项</strong>。根据统计学原理,计算每个源图像片段中较大分数的平均值,作为相应 SSIM 损失项的权重。<h3 id=MFFGAN><a class=headerlink href=#MFFGAN title=MFFGAN></a>MFFGAN</h3><p>图像融合的理念是从源图像中提取并组合最有意义的信息。<strong>对于多焦点图像融合来说，最有意义的信息是源图像中的锐利区域，这些区域反映在强度分布和纹理细节上</strong>。自然，在信息提取过程中，应保留锐利区域的这些信息，而摒弃模糊区域的这些信息。<p>当然，在信息提取过程中，尖锐区域的信息应该保留，模糊区域的信息应该舍弃。因此，有必要在优化过程中<strong>引入损失函数的调整机制</strong>，以约束网络有选择地提取和重构信息。<p>首先，我们设计了一个自适应决策块，它可以根据重复模糊原理评估每个像素的清晰度,也就是说，清晰度较高的图像，经过模糊处理后，像素值变化较大。根据这一观察结果，生成screening map来描述有效信息的位置。screening map作用于我们构建的特定内容损失函数，从而在像素尺度上调整优化目标。<p>判定块可以自适应地引导融合结果在像素尺度上逼近清晰源图像的强度分布和梯度分布<p>.我们的具体方法是选择分数较大的像素（放弃较小的分数）作为两个源图像相应像素位置的优化目标。在决策块和内容损失的共同作用下，生成器可以得到相对清晰自然的结果。<p>我们将联合梯度图定义为真实数据，将融合图像的梯度图定义为假数据。持续的对抗性学习可以引导生成器更专注于纹理的保留。因此，我们可以获得更高质量的融合结果，其中包含更丰富的纹理细节。<h3 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h3><p>损耗函数由生成器损耗L~G~和鉴别器损耗L~D~组成。<h4 id=生成器-1><a class=headerlink href=#生成器-1 title=生成器></a>生成器</h4><p>生成器的损失有两部分，即用于提取和重构信息的内容损失L~Gcon~，以及用于增强纹理细节的对抗性损失L~Gadv~。</p><script type="math/tex; mode=display">
\mathcal{L}_{G}=\mathcal{L}_{G_{\mathrm{adv}}}+\alpha L_{G_{\mathrm{con}}}</script><script type="math/tex; mode=display">
\mathcal{L}_{G_{\mathrm{adv}}}=\frac{1}{N}\sum_{n=1}^{N}(D(\nabla(I_{\mathrm{fused}}^{n}))-a)^2</script><p>其中 N 是训练期间批次中融合图像的数量，a 是生成器期望判别器确定融合图像的概率标签</p><script type="math/tex; mode=display">
L_{G_{\mathrm{con}}}=\beta_{1}\mathcal{L}_{\mathrm{int}}+\beta_{2}\mathcal{L}_{\mathrm{grad}}</script><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{int}}=\frac{1}{HW}\sum_{\cdot}\sum_{\cdot}S_{1_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{1_{i,j}})^{2}+S_{2_{i,j}}\cdot(I_{\mathrm{fused}_{i,j}}-I_{2_{i,j}})^{2}</script><script type="math/tex; mode=display">
\begin{aligned}S_{1_{i,j}}&=\operatorname{sign}(RB(I_{1_{i,j}})-\min(RB(I_{1_{i,j}}),RB(I_{2_{i,j}}))),\\S_{2_{i,j}}&=1-S_{1_{i,j}},\end{aligned}</script><p>重复模糊函数</p><script type="math/tex; mode=display">
RB(\cdot)~=~abs(I_{i,j}-LP(I_{i,j}))</script><p>LP （⋅） 表示低通滤波器函数。值得注意的是，S（⋅）的大小也是H × W。</p><script type="math/tex; mode=display">
\begin{aligned}\mathcal{L}_{\mathrm{grad}}&=\frac1{HW}\sum_i\sum_jS_{\mathbf{1}_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{\mathbf{1}_{i,j}})^2\\&+S_{2_{i,j}}\cdot(\nabla I_{\mathrm{fused}_{i,j}}-\nabla I_{2_{i,j}})^2.\end{aligned}</script><h3 id=判别器><a class=headerlink href=#判别器 title=判别器></a>判别器</h3><p>判别器的损失功能使判别器能够准确识别真假数据。在我们的方法中，假数据是融合图像的梯度图。真实数据是我们构建的联合梯度图。</p><script type="math/tex; mode=display">
Grad_{\mathrm{fused}}=\mathrm{abs}(\nabla I_{\mathrm{fused}}) \\
Grad_{\mathrm{joint}}=\max(\mathrm{abs}(\nabla I_1),\mathrm{abs}(\nabla I_2)),</script><script type="math/tex; mode=display">
\mathcal{L}_{D_{\mathrm{adv}}}=\frac1N\sum_{n=1}^{N}[D(Grad_{\mathrm{fused}}^{n})-b]^{2}+[D(Grad_{\mathrm{joint}}^{n})-c]^{2}</script><p>其中 b 是融合图像梯度图的标签，应设置为 0。c 是联合梯度图的标签，应设置为 1。<p>也就是说，判别器期望准确地将联合梯度图识别为真实数据，将融合图像的梯度图识别为假数据。在这种约束下，判别器可以引导生成器在信息维护方面的倾向，即有利于强纹理保存.<h4 id=总体架构><a class=headerlink href=#总体架构 title=总体架构></a>总体架构</h4><p><img alt=image-20231108222112489 data-src=https://i.imgur.com/WPHLLvv.png><h4 id=生成器架构><a class=headerlink href=#生成器架构 title=生成器架构></a>生成器架构</h4><p>我们将生成器拆分为两条路径来提取信息，对应于两个源图像。生成器网络的设计灵感来自pseudo-Siamese，它擅长处理两种相对不同的输入。由于多焦点图像对在相应的像素位置清晰或模糊，因此pseudo-Siamese网络适用于此类图像<p><img alt=image-20231108215808309 data-src=https://i.imgur.com/l2ONRey.png><p>在这两条路径中，都有四个卷积层来提取特征。第一个卷积层使用 5 × 5 卷积核，其余三个卷积层使用 3 × 3 卷积核。它们都使用 Leaky ReLU 作为激活函数。为了防止卷积过程中的信息丢失，我们根据 DenseNet 的思想重用了这些特征.<p>同时，为了提取更充分的信息，我们在两条路径之间交换信息。具体来说，交换的信息是通过连接和卷积的方法生成的。然后，交换的信息与所有前一个卷积层的输出连接在一起，作为下一个卷积层的输入。<p>最后，我们将两条路径中所有卷积层的输出串联起来，然后通过卷积层生成融合图像。卷积层的核大小为 1 × 1，激活函数为 tanh。值得注意的是，在所有卷积层中，填充模式设置为“SAME”，即特征图的大小在整个卷积过程中没有变化，这与源图像的大小相同。<h4 id=判别器架构><a class=headerlink href=#判别器架构 title=判别器架构></a>判别器架构</h4><p><img alt=image-20231108222051637 data-src=https://i.imgur.com/veuN5Um.png><p>判别器中的输入有两种类型，一种是<strong>基于源图像的联合梯度图和融合图像的梯度图</strong>。鉴别器由四个卷积层和一个线性层组成。四个卷积层的卷积核大小为 3 × 3，它们都使用了LeakyReLU 激活函数。这些卷积层的步幅设置为 2。最后一层是用于查找分类概率的线性层。<h3 id=训练细节><a class=headerlink href=#训练细节 title=训练细节></a>训练细节</h3><p>我们的实验是在两个数据集上进行的，比如Lytro数据集[34]和我们基于公共数据库构建的MFI-WHU数据集。<p>在 Lytro 数据集和 MFI-WHU 数据集上，用于测试的图像对数分别为 10 和 30。对于训练，为了获得更多的训练数据，我们采用了剪裁分解的扩展策略。具体来说，对于 Lytro 数据集，我们将其余图像裁剪为 22,090 个大小为 60 × 60 的图像图块对进行训练;对于 MFI-WHU 数据集，我们将其余图像裁剪为 202,246 个大小为 60 × 60 的图像patch对进行训练。<p>batch_size=32,epochs=20,训练一个epoch需要m步数,将一张图片分为多个patch,m设置为所有patch数除以b. 一般考虑训练更多的判别器,训练判别器次数是生成器的p倍.<p><img alt=image-20231109104901606 data-src=https://i.imgur.com/TYR6p1y.png><p>我们将图像从 RGB 转换为 YCbCr 色彩空间。由于 Y 通道（亮度通道）可以表示结构细节和亮度变化，因此我们只致力于融合 Y 通道值。对于 Cb 和 Cr 通道（色度通道），我们以传统方式融合它们。然后，将这些通道的融合分量转移到RGB以获得最终结果。<h3 id=一些结果><a class=headerlink href=#一些结果 title=一些结果></a>一些结果</h3><h4 id=多焦图像融合><a class=headerlink href=#多焦图像融合 title=多焦图像融合></a>多焦图像融合</h4><p><img alt=image-20231118163720106 data-src=https://i.imgur.com/YrdNz1J.png><p><img alt=image-20231118164233372 data-src=https://i.imgur.com/8y3MF5I.png><h4 id=红外可见光图像融合><a class=headerlink href=#红外可见光图像融合 title=红外可见光图像融合></a>红外可见光图像融合</h4><p><img alt=image-20231118164258767 data-src=https://i.imgur.com/6H8J7CW.png><h3 id=代码链接><a class=headerlink href=#代码链接 title=代码链接></a>代码链接</h3><p><a href=https://github.com/drowning-in-codes/UFGAN rel=noopener target=_blank>drowning-in-codes/UFGAN: GAN for Image Fusion which is inspired by FusionGAN and U-net (github.com)</a><p><a href=https://github.com/drowning-in-codes/MFF-GAN rel=noopener target=_blank>drowning-in-codes/MFF-GAN: Code of MFF-GAN: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion. (github.com)</a><p>colab 链接<a href=https://colab.research.google.com/drive/1wcb28gzgF62GphVdx42XkoZ68GxDaRpo#scrollTo=K8FjqBFQxmnR rel=noopener target=_blank>UFGAN.ipynb - Colaboratory (google.com)</a><h3 id=一些想法><a class=headerlink href=#一些想法 title=一些想法></a>一些想法</h3><p>利用预训练模型提供内容和风格 transfer learning?<p>利用cGAN思想? 此外损失函数的设计有必要换成神经网络而不是人工设计的一些值了.可以看看一篇CVPR的TARDAL<a href=http://arxiv.org/abs/2203.16220 rel=noopener target=_blank>http://arxiv.org/abs/2203.16220</a><h3 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h3><ol><li><p><a href=https://blog.csdn.net/Chaolei3/article/details/79404806 rel=noopener target=_blank>详细理解RGB图像、全色图像、多光谱图像、高光谱图像-CSDN博客</a></p><li><p><a href=https://github.com/Linfeng-Tang/Image-Fusion rel=noopener target=_blank>Linfeng-Tang/Image-Fusion: Deep Learning-based Image Fusion: A Survey (github.com)</a></p> <p><strong>综述</strong></p><li><p><a href=https://www.sciencedirect.com/science/article/abs/pii/S1566253521001342 rel=noopener target=_blank>Image fusion meets deep learning: A survey and perspective - ScienceDirect</a></p><li><p><a href=https://www.sciencedirect.com/science/article/abs/pii/S1566253522001518 rel=noopener target=_blank>Current advances and future perspectives of image fusion: A comprehensive review - ScienceDirect</a></p></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ title=图像融合论文阅读>https://www.sekyoro.top/2023/11/02/图像融合论文阅读/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/image-fusion/ rel=tag><i class="fa fa-tag"></i> image fusion</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ rel=prev title=目标检测学习_P3> <i class="fa fa-chevron-left"></i> 目标检测学习_P3 </a></div><div class=post-nav-item><a title="transformer family(一):from Bahdanau Attention to transformers" href=/2023/11/08/transformer-and-attention/ rel=next> transformer family(一):from Bahdanau Attention to transformers <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-1"><a class=nav-link href=#abs><span class=nav-number>1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-1"><a class=nav-link href=#Intro><span class=nav-number>2.</span> <span class=nav-text>Intro</span></a><ol class=nav-child><li class="nav-item nav-level-2"><a class=nav-link href=#%E4%BC%A0%E7%BB%9F%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%EF%BC%9A><span class=nav-number>2.1.</span> <span class=nav-text>传统融合方法：</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#1-AE-based><span class=nav-number>2.1.1.</span> <span class=nav-text>1.AE-based</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#2-CNN-based><span class=nav-number>2.1.2.</span> <span class=nav-text>2.CNN-based</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#3-GAN-baesd><span class=nav-number>2.1.3.</span> <span class=nav-text>3.GAN-baesd</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E5%9C%BA%E6%99%AF><span class=nav-number>2.2.</span> <span class=nav-text>图像融合场景</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#digital-photography-image-fusion><span class=nav-number>2.2.1.</span> <span class=nav-text>digital photography image fusion</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Infrared-and-visible-image-fusion%E2%80%9D><span class=nav-number>2.2.2.</span> <span class=nav-text>Infrared and visible image fusion”</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#GAN><span class=nav-number>2.3.</span> <span class=nav-text>GAN</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AF%84%E4%BC%B0><span class=nav-number>2.3.1.</span> <span class=nav-text>评估</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#EN><span class=nav-number>2.3.1.1.</span> <span class=nav-text>EN</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SD><span class=nav-number>2.3.1.2.</span> <span class=nav-text>SD</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SSIM><span class=nav-number>2.3.1.3.</span> <span class=nav-text>SSIM</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#PSNR><span class=nav-number>2.3.1.4.</span> <span class=nav-text>PSNR</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#CC><span class=nav-number>2.3.1.5.</span> <span class=nav-text>CC</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SF-%E7%A9%BA%E9%97%B4%E9%A2%91%E7%8E%87><span class=nav-number>2.3.1.6.</span> <span class=nav-text>SF 空间频率</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#VIF-%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E4%BF%9D%E7%9C%9F%E5%BA%A6><span class=nav-number>2.3.1.7.</span> <span class=nav-text>VIF 空间信息保真度</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>2.3.2.</span> <span class=nav-text>数据集</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%AE%9E%E6%88%98><span class=nav-number>2.4.</span> <span class=nav-text>实战</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#FusionGAN><span class=nav-number>2.4.1.</span> <span class=nav-text>FusionGAN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Generator><span class=nav-number>2.4.1.1.</span> <span class=nav-text>Generator</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Discriminator><span class=nav-number>2.4.1.2.</span> <span class=nav-text>Discriminator</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#TarDAL><span class=nav-number>2.4.2.</span> <span class=nav-text>TarDAL</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E9%97%AE%E9%A2%98%E5%BB%BA%E6%A8%A1><span class=nav-number>2.4.3.</span> <span class=nav-text>问题建模</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Target-aware-dual-adversarial-network><span class=nav-number>2.4.3.1.</span> <span class=nav-text>Target-aware dual adversarial network</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90%E5%99%A8><span class=nav-number>2.4.3.2.</span> <span class=nav-text>生成器</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%9B%AE%E6%A0%87%E9%89%B4%E5%88%AB%E5%99%A8%E4%B8%8E%E7%BB%86%E8%8A%82%E9%89%B4%E5%88%AB%E5%99%A8><span class=nav-number>2.4.3.3.</span> <span class=nav-text>目标鉴别器与细节鉴别器</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%90%88%E4%BD%9C%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5><span class=nav-number>2.4.3.4.</span> <span class=nav-text>合作训练策略</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BA%A2%E5%A4%96%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E7%9A%84%E8%9E%8D%E5%90%88%E7%BB%93%E6%9E%9C><span class=nav-number>2.4.4.</span> <span class=nav-text>红外与可见光图像的融合结果</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%AE%9A%E6%80%A7%E6%AF%94%E8%BE%83><span class=nav-number>2.4.4.1.</span> <span class=nav-text>定性比较</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%AE%9A%E9%87%8F%E6%AF%94%E8%BE%83><span class=nav-number>2.4.4.2.</span> <span class=nav-text>定量比较</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BA%A2%E5%A4%96%E4%B8%8E%E5%8F%AF%E8%A7%81%E5%85%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C><span class=nav-number>2.4.5.</span> <span class=nav-text>红外与可见光目标检测结果</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link><span class=nav-number>2.4.5.1.</span> <span class=nav-text> </span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%A4%9A%E8%81%9A%E7%84%A6%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88><span class=nav-number>2.4.6.</span> <span class=nav-text>多聚焦图像融合</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#MFIF-GAN><span class=nav-number>2.4.7.</span> <span class=nav-text>MFIF-GAN</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84><span class=nav-number>2.4.7.1.</span> <span class=nav-text>网络结构</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#FuseGAN><span class=nav-number>2.4.8.</span> <span class=nav-text>FuseGAN</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#MFFGAN><span class=nav-number>2.4.9.</span> <span class=nav-text>MFFGAN</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>2.4.10.</span> <span class=nav-text>损失函数</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90%E5%99%A8-1><span class=nav-number>2.4.10.1.</span> <span class=nav-text>生成器</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%88%A4%E5%88%AB%E5%99%A8><span class=nav-number>2.4.11.</span> <span class=nav-text>判别器</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84><span class=nav-number>2.4.11.1.</span> <span class=nav-text>总体架构</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%94%9F%E6%88%90%E5%99%A8%E6%9E%B6%E6%9E%84><span class=nav-number>2.4.11.2.</span> <span class=nav-text>生成器架构</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%88%A4%E5%88%AB%E5%99%A8%E6%9E%B6%E6%9E%84><span class=nav-number>2.4.11.3.</span> <span class=nav-text>判别器架构</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82><span class=nav-number>2.4.12.</span> <span class=nav-text>训练细节</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%B8%80%E4%BA%9B%E7%BB%93%E6%9E%9C><span class=nav-number>2.4.13.</span> <span class=nav-text>一些结果</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%A4%9A%E7%84%A6%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88><span class=nav-number>2.4.13.1.</span> <span class=nav-text>多焦图像融合</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E7%BA%A2%E5%A4%96%E5%8F%AF%E8%A7%81%E5%85%89%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88><span class=nav-number>2.4.13.2.</span> <span class=nav-text>红外可见光图像融合</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BB%A3%E7%A0%81%E9%93%BE%E6%8E%A5><span class=nav-number>2.4.14.</span> <span class=nav-text>代码链接</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95><span class=nav-number>2.4.15.</span> <span class=nav-text>一些想法</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>2.4.16.</span> <span class=nav-text>参考资料</span></a></ol></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>170</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>186</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a></ul></div><div class="motion-element announcement"><div class=title>注意</div><p class=content>由于最近图床更新,可能有些图片显示不了.如果发现了有些图片无法显示影响阅读的,还烦请联系我,我有空补上.<p class=date>2023-10-6</div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/image-fusion/ rel=tag>image fusion</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>18:23</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>