<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.主要介绍attention和transformer系列. name=description><meta content=article property=og:type><meta content="transformer family(一):from Bahdanau Attention to transformers" property=og:title><meta content=https://www.sekyoro.top/2023/11/08/transformer-and-attention/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.主要介绍attention和transformer系列. property=og:description><meta content=zh_CN property=og:locale><meta content=https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png property=og:image><meta content=https://i.imgur.com/Y7S2I1W.png property=og:image><meta content=https://i.imgur.com/DAhCPHu.png property=og:image><meta content=https://i.imgur.com/PcgRBVU.png property=og:image><meta content=https://i.imgur.com/bmErBND.png property=og:image><meta content=https://s2.loli.net/2024/01/10/SImfCJajrYsKyqO.png property=og:image><meta content=https://s2.loli.net/2024/01/10/Kxgkua1MC7PnrSR.png property=og:image><meta content=https://machinelearningmastery.com/wp-content/uploads/2021/09/bahdanau_1-780x1024.png property=og:image><meta content=https://s2.loli.net/2024/01/19/O6t3nIfWwr8LTgZ.png property=og:image><meta content=https://machinelearningmastery.com/wp-content/uploads/2021/10/luong_1-1024x426.png property=og:image><meta content=https://s2.loli.net/2024/01/19/OklsgwpPCe4NvJu.png property=og:image><meta content=2023-11-08T03:14:00.000Z property=article:published_time><meta content=2024-01-23T12:42:30.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="deep learning" property=article:tag><meta content=summary name=twitter:card><meta content=https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png name=twitter:image><link href=https://www.sekyoro.top/2023/11/08/transformer-and-attention/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>transformer family(一):from Bahdanau Attention to transformers | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/11/08/transformer-and-attention/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>transformer family(一):from Bahdanau Attention to transformers</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-11-08 11:14:00" datetime=2023-11-08T11:14:00+08:00>2023-11-08</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-01-23 20:42:30" datetime=2024-01-23T20:42:30+08:00 itemprop=dateModified>2024-01-23</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-folder"></i> </span> <span class=post-meta-item-text>分类于</span> <span itemprop=about itemscope itemtype=http://schema.org/Thing> <a href=/categories/deep-learning/ itemprop=url rel=index><span itemprop=name>deep learning</span></a> </span> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>6.9k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>6 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p><del>流行的深度学习模型,其中的思想以及模型在后面其他任务中也经常使用,所以这里介绍一些常用好用的模型.</del>主要介绍attention和transformer系列.</p><span id=more></span><h2 id=Attention-Is-All-You-Need><a title="Attention Is All You Need" class=headerlink href=#Attention-Is-All-You-Need></a>Attention Is All You Need</h2><h3 id=abs><a class=headerlink href=#abs title=abs></a>abs</h3><p>主流的序列转换模型基于<strong>复杂的递归或卷积神经网络</strong>，其中<strong>包括一个编码器和一个解码器</strong>。性能最好的模型还通过注意力机制将编码器和解码器连接起来。我们提出了一种新的简单网络架构—“transformer”，它<strong>完全基于注意力机制，无需递归和卷积</strong>。<h3 id=intro><a class=headerlink href=#intro title=intro></a>intro</h3><p>递归神经网络，特别是长短期记忆和门控递归神经网络，已被牢固确立为语言建模和机器翻译等序列建模和转译问题的最先进方法。自此以后，许多人继续努力推动递归语言模型和编码器-解码器架构的发展。<p>递归模型通常按照输入和输出序列的符号位置进行计算。将位置与计算时间的步长对齐，它们会生成隐藏状态 h~t~ 的序列，作为前一个隐藏状态 h~t-1~ 和位置 t 的输入的函数。<strong>这种固有的序列性质排除了训练实例内的并行化，而在序列长度较长时，这一点变得至关重要，因为内存约束限制了跨实例的批处理。</strong>最近的研究通过因式分解技巧和条件计算显著提高了计算效率，同时也改善了后者的模型性能。然而，顺序计算的基本限制仍然存在。<p>在各种任务中，注意力机制已成为引人注目的序列建模和转导模型的一个组成部分，它可以对依赖关系进行建模，而无需考虑其在输入或输出序列中的距离。然而，除了少数情况，这种注意机制都是与递归网络结合使用的。<blockquote><p>比如下图,利用一个双向RNN得到每个token的状态,利用一个简单的ffn聚合这些状态作为输出token的上一个状态</blockquote><p><img alt="Image showing an encoder/decoder model with an additive attention layer" data-src=https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/images/encoder-decoder-attention.png><p>在这项工作中，我们提出了 Transformer 模型架构，<strong>它摒弃了递归</strong>，<strong>而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系</strong>。Transformer <strong>可以大大提高并行化程度</strong>，在 8 个 P100 GPU 上只需训练 12 个小时，翻译质量就能达到新的水平。<h3 id=Background><a class=headerlink href=#Background title=Background></a>Background</h3><p>减少顺序计算的目标也是Extended Neural GPU、ByteNet和 ConvS2S的基础，它们都使用<strong>卷积神经网络</strong>作为基本构建模块<strong>，并行计算</strong>所有输入和输出位置的隐藏表示。在这些模型中，将两个任意输入或输出位置的信号联系起来所需的运算次数随位置间距离的增加而增加，<strong>ConvS2S 是线性增加，ByteNet 是对数增加。这就增加了学习远距离位置之间依赖关系的难度</strong>。在 Transformer 中，这将被减少到一个恒定的操作数(O(1))，尽管<strong>代价是由于平均注意力加权位置而降低了有效分辨率</strong>。<p>自我注意（有时也称为内部注意）是一种注意机制，它将单个序列的不同位置联系起来，以计算序列的表征。自我注意已成功应用于多种任务中，包括阅读理解、抽象概括、文本引申和学习与任务无关的句子表征。<h3 id=model-architecture><a title="model architecture" class=headerlink href=#model-architecture></a>model architecture</h3><p>大多数转导模型都具有编码器-解码器结构.在这里,编码器将输入的符号表示序列 (x1, …, xn) 映射为连续表示序列 z = (z1, …, zn)。 在给定 z 的情况下，解码器会逐个元素生成一个符号输出序列（y1, …, ym）。在每一步中，模型都是自动回归的，在生成下一步时，会消耗之前生成的符号作为额外输入。<p><img style="zoom: 67%;" alt=image-20231108115505228 data-src=https://i.imgur.com/Y7S2I1W.png><h4 id=Encoder><a class=headerlink href=#Encoder title=Encoder></a>Encoder</h4><p>编码器由 N = 6 层相同的层堆叠组成。每一层都有两个子层。第一个是多头自注意机制，第二个是简单的位置全连接前馈网络。我们在两个子层的每个周围都采用了残差连接，然后进行层归一化。也就是说，每个子层的输出都是 LayerNorm(x + Sublayer(x))，其中 Sublayer(x) 是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生维数为 dmodel = 512 的输出。<h4 id=Decoder><a class=headerlink href=#Decoder title=Decoder></a>Decoder</h4><p>解码器也由 N = 6 层相同的层堆叠组成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，对编码器堆栈的输出执行多头关注。<p>与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。我们还<strong>修改了解码器堆栈中的自我关注子层，以防止位置关注到后续位置</strong>。这种屏蔽，再加上输出嵌入偏移一个位置的事实，确保了对位置 i 的预测只能依赖于小于 i 的位置的已知输出。<h3 id=Attention><a class=headerlink href=#Attention title=Attention></a>Attention</h3><p>注意力函数可以描述为将一个查询和一组键值对映射到一个输出，其中查询、键、值和输出都是向量。<strong>输出结果以值的加权和的形式计算</strong>，<strong>其中分配给每个值的权重是通过查询与相应密钥的兼容性函数计算得出的</strong>。<p><img alt=image-20231108120527812 data-src=https://i.imgur.com/DAhCPHu.png><blockquote><p>上图就是一般用的q与k的计算方式,说白了就是矩阵相乘,而其中的mask是为了把其中用不上的token置为-∞,这样后面做softmax权重就是0了. 因为tensor维度都是相同的,q与k,</blockquote><h4 id=Scaled-Dot-Product-Attention><a title="Scaled Dot-Product Attention" class=headerlink href=#Scaled-Dot-Product-Attention></a>Scaled Dot-Product Attention</h4><p>输入包括维度为 d~k~的query和key,以及维度为 d~v~的value。我们计算query与所有keys的点积，将每个点积除以 √dk，然后应用软最大函数获得值的权重。</p><script type="math/tex; mode=display">
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V</script><p>最常用的两种注意力函数是<strong>加法注意力</strong>(additive attention)和<strong>点积</strong>(dot production)注意力。点积注意力与我们的算法相同，只是缩放因子为 1 √dk。<p>加法注意使用单隐层前馈网络计算相容函数(相当于用一个全连接网络得到一个输出)。虽然两者的理论复杂度相似，但点积注意力在实际应用中速度更快，空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。<p>虽然在 dk 值较小的情况下，这两种机制的表现类似，但在 d~k~ 值较大的情况下，加法注意比点积注意更胜一筹。我们猜测，对于较大的 d~k~ 值，点积的幅度会越来越大，从而将软最大函数推向梯度极小的区域<blockquote><p>也就是说除以d~k~原因是使得梯度更大,效果更好</blockquote><h4 id=Multi-Head-attention><a title="Multi-Head attention" class=headerlink href=#Multi-Head-attention></a>Multi-Head attention</h4><p>我们发现，将查询、键值和值分别线性投影到 d~k~、d~k~ 和 d~v~ 维度，而不是对 d~model~ 维度的键、值和查询执行单一的注意函数，这样做是有益的。<p>多头注意力允许模型<strong>在不同位置共同关注来自不同表征子空间的信息</strong>。而在单注意头的情况下，平均化会抑制这一点。</p><script type="math/tex; mode=display">
MultiHead( Q, K, V) = Concat( \mathrm{head}_1, ..., \mathrm{head}_\mathrm{h} ) W^O \\
 where \ head¡=Attention( QW_i^Q, KW_i^K, VW_i^V)</script><p>其中，投影是参数矩阵 W^Q^~i~∈R^dmodel×dk^ , W^K^ ~i~∈R^dmodel×dk^ , W^V^~i~∈R^dmodel×dv^ 和 W O∈R^hdv×dmodel^<h3 id=Transformer的应用><a class=headerlink href=#Transformer的应用 title=Transformer的应用></a>Transformer的应用</h3><p>在 “encoder-decoder 注意 “层中,query来自前一个decoder层，而memory keys和memory values则来自encoder的输出。这使得decoder中的每个位置都能关注输入序列中的所有位置。这模仿了序列到序列模型中典型的encoder-decoder注意机制。<p>encoder包含自注意层。在自注意层中，所有的键、值和查询都来自同一个地方，在这种情况下，就是encoder中上一层的输出。encoder中的每个位置都可以关注encoder上一层的所有位置。<p>同样，解码器中的自关注层允许解码器中的每个位置关注解码器中包括该位置在内的所有位置。<strong>我们需要防止decoder中的信息向左流动，以保持自动回归特性</strong>。<p>通过点乘注意力中的mask实现,也就是在输出序列中,把后面的token得到的value设置为-∞,<h3 id=Position-wise-Feed-Forward-Networks><a title="Position-wise Feed-Forward Networks" class=headerlink href=#Position-wise-Feed-Forward-Networks></a>Position-wise Feed-Forward Networks</h3><p>除了注意力子层外，我们的编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别对每个位置进行相同的处理。这包括两个线性变换，中间有一个 ReLU 激活。</p><script type="math/tex; mode=display">
\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2</script><p>虽然不同位置的线性变换相同，但各层使用的参数不同。<p>另一种描述方法是<strong>两个内核大小为 1 的卷积</strong>(全卷积)。输入和输出的维度为 d~model~ = 512，内层的维度为 d~ff~= 2048。<h3 id=Embedding-and-Softmax><a title="Embedding and Softmax" class=headerlink href=#Embedding-and-Softmax></a>Embedding and Softmax</h3><p>与其他序列转换模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度为 d~model~的向量。我们还使用通常学习到的线性变换和softmax，将解码器输出转换为预测的下一个标记词概率，在模型中，我们在两个嵌入层和pre-softmax linear transformation之间共享相同的权重矩阵。<p>在嵌入层中，我们将这些权重乘以 √dmodel。<h3 id=衍生><a class=headerlink href=#衍生 title=衍生></a>衍生</h3><h4 id=Bert><a class=headerlink href=#Bert title=Bert></a>Bert</h4><p><img alt=image-20231125145939351 data-src=https://i.imgur.com/PcgRBVU.png><p>BERT（来自 Transformers 的双向编码器表示）是一个非常大的多层 Transformer 网络,BERT-base 有 12 层，BERT-large 有 24 层,其旨在通过在所有层中联合调节左右上下文来预训练未标记文本的深度双向表示。因此，预训练的 BERT 模型只需一个额外的输出层即可进行微调，从而为各种任务（例如问答和语言推理）创建最先进的模型，而无需进行大量任务特定的架构修改。<h4 id=Vit><a class=headerlink href=#Vit title=Vit></a>Vit</h4><p><img alt=image-20231125145906882 data-src=https://i.imgur.com/bmErBND.png><p>在CV领域,注意力要么与卷积网络结合使用,要么用来替换卷积网络的某些组件,整体结构保持不变。本文证明了CV领域不一定依赖CNN,使用纯粹的Transformer用于图片块序列，也可以很好的完成图像分类任务<h2 id=Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows><a title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" class=headerlink href=#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h2><p><img alt=image-20240110092327560 data-src=https://s2.loli.net/2024/01/10/SImfCJajrYsKyqO.png></p><script type="math/tex; mode=display">
\begin{aligned}
&\hat{\mathbf{z}}^l=\text{W-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l-1}\right)\right)+\mathbf{z}^{l-1}, \\
&\mathbf{z}^{l}=\mathbf{MLP}\left(\mathbf{LN}\left(\hat{\mathbf{z}}^{l}\right)\right)+\hat{\mathbf{z}}^{l}, \\
&\hat{\mathbf{z}}^{l+1}=\text{SW-MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l}\right)\right)+\mathbf{z}^{l}, \\
&\mathbf{z}^{l+1}=\mathbf{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^{l+1}\right)\right)+\hat{\mathbf{z}}^{l+1},
\end{aligned}</script><p><img alt=image-20240110093609755 data-src=https://s2.loli.net/2024/01/10/Kxgkua1MC7PnrSR.png><h2 id=The-Bahdanau-Attention-Algorithm-2014><a title="The Bahdanau Attention Algorithm 2014" class=headerlink href=#The-Bahdanau-Attention-Algorithm-2014></a>The Bahdanau Attention Algorithm 2014</h2><p>之前的处理NLP任务的方法<a href=https://arxiv.org/abs/1406.1078 rel=noopener target=_blank>[1406.1078] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (arxiv.org)</a>和<a href=https://arxiv.org/abs/1409.3215 rel=noopener target=_blank>[1409.3215] Sequence to Sequence Learning with Neural Networks (arxiv.org)</a>将变长源句变为固定长度的向量,这种方式无疑会随着源句变长效果变差.<blockquote><p>Bahdanau等人（2014）认为，这种将可变长度输入编码到固定长度向量中的方法会挤压源句子的信息，而不管其长度如何，导致基本编码器-解码器模型的性能随着输入句子长度的增加而迅速恶化。他们提出的方法<strong>用可变长度的矢量代替固定长度的矢量</strong>，以提高基本编码器-解码器模型的翻译性能。</blockquote><p><strong>名词解释</strong>: s~t-1~是上一个time step decoder的隐状态,c~t~是time step t的上下文向量,它是在每个时间步生成用于得到需要的输出y~t~,h~i~是用于捕获包含在整个输入句子中的信息,α~t,i~是每个annotation的权重,e~t,i~是alignment model生成的注意力分数,表示s~t-1~和h~i~匹配程度<p><img style="zoom: 50%;" alt=img data-src=https://machinelearningmastery.com/wp-content/uploads/2021/09/bahdanau_1-780x1024.png><p>对于encoder目的是生成annotation,采用了双向RNN,</p><script type="math/tex; mode=display">
\mathrm{h_{i}=[\stackrel{\rightharpoonup}{h_{i}^{T}};\stackrel{\rightharpoonup}{h_{i}^{T}}]}^{T}</script><p>decoder解码器的作用是通过关注源句子中包含的最相关的信息来产生目标单词。为此，它利用了一种注意力机制。<br>利用注意力机制计算上一步的hidden state和这一步的annotation</p><script type="math/tex; mode=display">
e_{t,i}=a(s_{t-1},h_{i})e_{t,i}=a(s_{t-1},h_{i})</script><p>有两种实现方法,一种将输入的encode h与隐状态s concat起来,另一种使用两个W矩阵分别与它们相乘</p><script type="math/tex; mode=display">
\begin{aligned}\mathrm{a}(s_{t-1},h_i)&=v^\top tanh(W[h_i;s_{t-1}])\\\mathrm{a}(s_{t-1},h_i)&=v^\top tanh(W_1h_i+W_2s_{t-1})\end{aligned}</script><p>这里v是权重向量.然后做个softmax,这样就得到在某个time step下得到权重</p><script type="math/tex; mode=display">
\alpha_{t,i}=\mathrm{softmax(e_{t,i})}</script><p>上下文向量c为α与annotation的weighted sum</p><script type="math/tex; mode=display">
c_{t}=\sum_{i=1}^{\top}\alpha_{t,i}h_{i}</script><p>最后y~t~的输出由decoder上一个输出y~t-1~,隐状态s~t-1~和c~t~进行计算<p><img alt=image-20240119120931215 data-src=https://s2.loli.net/2024/01/19/O6t3nIfWwr8LTgZ.png><h2 id=The-Luong-Attention-Mechanism-2015><a title="The Luong Attention Mechanism 2015" class=headerlink href=#The-Luong-Attention-Mechanism-2015></a>The Luong Attention Mechanism 2015</h2><p>提出了两种attention机制,包括global attention和local attention.<p>全局注意力跟Bahdanau类似但是试图更简化,局部注意力受到软注意力和硬注意影响并且只关注少数位置. 这两种注意力主要差别就是关注的上下文大小不同.<p>大致流程如下:<ol><li>从输入序列中生成一系列annotations H = h~i~<li>decoder的隐状态由上一层的隐状态和上层的decoder的输出决定<li>alignment model用于计算一个score,使用隐状态和annotation<li>将aliment score做softmax方便作为权重<li>计算aliment score与annotation H的weighted sum<li>增加了一个隐状态,基于上下文向量和当前解码器隐藏状态的加权级联来计算注意力隐藏状态</ol><script type="math/tex; mode=display">
\tilde{s}_{t}=\tanh(W_{c}[c_{t};s_{t}])</script><ol><li>解码器通过向其提供加权的注意力隐藏状态来产生最终输出<script type="math/tex; mode=display">
y_t=\operatorname{softmax}(\mathcal{W}_y\hat{s}_t)</script></ol><h3 id=global-attention><a title="global attention" class=headerlink href=#global-attention></a>global attention</h3><p>全局注意力模型在生成对齐分数时，并最终在计算上下文向量时，考虑输入句子中的所有源词。</p><script type="math/tex; mode=display">
\begin{gathered}
a(s_{t},h_{i})=v_{s}^{\top}\tanh(\mathcal{W}_{a}[s_{t};h_{i})] \\
\mathbf{a}(\mathbf{s}_{t},\mathbf{h}_{i})=\mathbf{s}_{t}^{\top}\mathbf{h}_{i} \\
\mathrm{a(s_{t},h_{i})=s_{t}^{T}W_{a}h_{i}} 
\end{gathered}</script><p>第一种跟Bahdanau类似,第二种和第三种方法实现的乘法注意<h3 id=local-attentiAxialon><a title="local attentiAxialon" class=headerlink href=#local-attentiAxialon></a>local attentiAxialon</h3><p>在处理所有源词时，全局注意力模型在计算上是昂贵的，并且对于翻译较长的句子可能变得不切实际。<p>在计算局部注意力时使用了一个窗口,假设中心是p~t~.</p><script type="math/tex; mode=display">
[\mathbf{p}_{t}-\mathbf{D},\mathbf{p}_{t}+\mathbf{D}]</script><p>D是凭经验取,D有两种方法.</p><script type="math/tex; mode=display">
p_{t}=t \\
\mathbf{p}_{t}=S\cdot\mathrm{sigmoid}(\mathbf{v}_{p}^{\mathrm{T}}\tanh(\mathbf{W}_{p},s_{t}))</script><p>与<strong>Bahdanau Attention</strong>的差别<p>1, 计算y使用的隐状态不同<p><img alt=img data-src=https://machinelearningmastery.com/wp-content/uploads/2021/10/luong_1-1024x426.png><p>2.Luong等人放弃了Bahdanau模型使用的双向编码器，而是将LSTM顶层的隐藏状态用于编码器和解码器。<p>3.Luong等人的全局注意模型研究了乘法注意作为Bahdanau加法注意的替代方法的使用。<h3 id=General-Attention-Mechanism><a title="General Attention Mechanism" class=headerlink href=#General-Attention-Mechanism></a>General Attention Mechanism</h3><p>上面讲的是比较早的了,现在很多使用Q,K,V来计算.可以进行类比,上一层的隐状态s~t-1~就是Q,而K,V是H<p><img alt=image-20240119132119458 data-src=https://s2.loli.net/2024/01/19/OklsgwpPCe4NvJu.png><h2 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h2><ol><li><a href=https://machinelearningmastery.com/the-transformer-attention-mechanism/ rel=noopener target=_blank>The Transformer Attention Mechanism - MachineLearningMastery.com</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\09\24\回看深度学习-经典网络学习\ rel=bookmark>回看深度学习:经典网络学习</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\07\30\profile-a-deep-learning-model\ rel=bookmark>profile a deep learning model</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\18\从论文中看AI绘画-二\ rel=bookmark>从论文中看AI绘画(二)</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\12\myJourneyToAI-深度学习之旅\ rel=bookmark>myJourneyToAI:深度学习之旅</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\02\从论文中看AI绘画(一)\ rel=bookmark>从论文中看AI绘画(一)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a title="transformer family(一):from Bahdanau Attention to transformers" href=https://www.sekyoro.top/2023/11/08/transformer-and-attention/>https://www.sekyoro.top/2023/11/08/transformer-and-attention/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/deep-learning/ rel=tag><i class="fa fa-tag"></i> deep learning</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ rel=prev title=图像融合论文阅读> <i class="fa fa-chevron-left"></i> 图像融合论文阅读 </a></div><div class=post-nav-item><a href=/2023/11/16/%E5%9B%BE%E5%83%8F%E8%AF%BB%E5%8F%96%E4%B8%8E%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98/ rel=next title=图像读取与显示的问题> 图像读取与显示的问题 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#Attention-Is-All-You-Need><span class=nav-number>1.</span> <span class=nav-text>Attention Is All You Need</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs><span class=nav-number>1.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#intro><span class=nav-number>1.2.</span> <span class=nav-text>intro</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Background><span class=nav-number>1.3.</span> <span class=nav-text>Background</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#model-architecture><span class=nav-number>1.4.</span> <span class=nav-text>model architecture</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Encoder><span class=nav-number>1.4.1.</span> <span class=nav-text>Encoder</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Decoder><span class=nav-number>1.4.2.</span> <span class=nav-text>Decoder</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Attention><span class=nav-number>1.5.</span> <span class=nav-text>Attention</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Scaled-Dot-Product-Attention><span class=nav-number>1.5.1.</span> <span class=nav-text>Scaled Dot-Product Attention</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Multi-Head-attention><span class=nav-number>1.5.2.</span> <span class=nav-text>Multi-Head attention</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Transformer%E7%9A%84%E5%BA%94%E7%94%A8><span class=nav-number>1.6.</span> <span class=nav-text>Transformer的应用</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Position-wise-Feed-Forward-Networks><span class=nav-number>1.7.</span> <span class=nav-text>Position-wise Feed-Forward Networks</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Embedding-and-Softmax><span class=nav-number>1.8.</span> <span class=nav-text>Embedding and Softmax</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%A1%8D%E7%94%9F><span class=nav-number>1.9.</span> <span class=nav-text>衍生</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Bert><span class=nav-number>1.9.1.</span> <span class=nav-text>Bert</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Vit><span class=nav-number>1.9.2.</span> <span class=nav-text>Vit</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows><span class=nav-number>2.</span> <span class=nav-text>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#The-Bahdanau-Attention-Algorithm-2014><span class=nav-number>3.</span> <span class=nav-text>The Bahdanau Attention Algorithm 2014</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#The-Luong-Attention-Mechanism-2015><span class=nav-number>4.</span> <span class=nav-text>The Luong Attention Mechanism 2015</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#global-attention><span class=nav-number>4.1.</span> <span class=nav-text>global attention</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#local-attentiAxialon><span class=nav-number>4.2.</span> <span class=nav-text>local attentiAxialon</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#General-Attention-Mechanism><span class=nav-number>4.3.</span> <span class=nav-text>General Attention Mechanism</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>5.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>213</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>197</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/deep-learning/ rel=tag>deep learning</a><span class=tag-list-count>9</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>30:08</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>