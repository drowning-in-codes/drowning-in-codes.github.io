<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=除了3D目标检测算法外,基于协同感知的自动驾驶还需要将获取到的3D点云或者图像数据或者处理后的特征进行通信和融合,这里介绍相关论文. name=description><meta content=article property=og:type><meta content=协同感知学习(一) property=og:title><meta content=https://www.sekyoro.top/2023/11/30/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%80/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=除了3D目标检测算法外,基于协同感知的自动驾驶还需要将获取到的3D点云或者图像数据或者处理后的特征进行通信和融合,这里介绍相关论文. property=og:description><meta content=zh_CN property=og:locale><meta content=https://s2.loli.net/2024/01/07/MXuJrkjUeywFPf1.png property=og:image><meta content=https://s2.loli.net/2024/01/07/JMEKzxTutGrfW8h.png property=og:image><meta content=https://s2.loli.net/2024/01/10/kU47aJnKuABtjS3.png property=og:image><meta content=https://s2.loli.net/2024/01/10/CnGtDVSgHJWEjLd.png property=og:image><meta content=https://s2.loli.net/2024/01/02/I1xRkviLBmbnqfQ.png property=og:image><meta content=https://s2.loli.net/2024/01/03/w5AtSc4o2gsTzB9.png property=og:image><meta content=https://s2.loli.net/2024/01/05/Sel1ad8xyFHqLfJ.png property=og:image><meta content=https://s2.loli.net/2024/01/03/NVMak8mhlgxeKwo.png property=og:image><meta content=https://s2.loli.net/2024/01/07/Q8ywAZ2N7IcCBSG.png property=og:image><meta content=https://s2.loli.net/2024/01/07/dc81urwk6RBAfoX.png property=og:image><meta content=https://s2.loli.net/2024/01/07/zpJc21GkmUEadLO.png property=og:image><meta content=https://s2.loli.net/2024/01/08/R7C6MQ8PWzI13JK.png property=og:image><meta content=c:/Users/proanimer/AppData/Roaming/Typora/typora-user-images/image-20240108204441345.png property=og:image><meta content=https://s2.loli.net/2024/01/08/W5qG8EdRyZwz6lM.png property=og:image><meta content=https://s2.loli.net/2024/01/08/9eTsclkiWp3Z4fV.png property=og:image><meta content=https://s2.loli.net/2024/01/09/1fg5jPZS6lUhmxz.png property=og:image><meta content=https://s2.loli.net/2023/11/30/pKu3lxqZ489NEk2.png property=og:image><meta content=https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png property=og:image><meta content=https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png property=og:image><meta content=https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70 property=og:image><meta content=https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png property=og:image><meta content=https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png property=og:image><meta content=https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png property=og:image><meta content=https://s2.loli.net/2024/05/18/Jt7BYCRpnbIWvPL.png property=og:image><meta content=https://s2.loli.net/2024/05/18/hRt384OIHjLkFyw.png property=og:image><meta content=https://s2.loli.net/2024/05/08/CInhm62SfjXLK5w.png property=og:image><meta content=2023-11-30T02:58:03.000Z property=article:published_time><meta content=2024-05-23T03:08:55.545Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="collaborative perception" property=article:tag><meta content=summary name=twitter:card><meta content=https://s2.loli.net/2024/01/07/MXuJrkjUeywFPf1.png name=twitter:image><link href=https://www.sekyoro.top/2023/11/30/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%80/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>协同感知学习(一) | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/11/30/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%80/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>协同感知学习(一)</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-11-30 10:58:03" datetime=2023-11-30T10:58:03+08:00>2023-11-30</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-05-23 11:08:55" datetime=2024-05-23T11:08:55+08:00 itemprop=dateModified>2024-05-23</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>31k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>29 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag># collaborative perception</a></div><p>除了3D<strong>目标检测</strong>算法外,基于协同感知的自动驾驶还需要将获取到的3D点云或者图像数据或者处理后的特征进行<strong>通信</strong>和<strong>融合</strong>,这里介绍相关论文.<br><span id=more></span><br>重要的几个仓库<a href=https://github.com/coperception/coperception rel=noopener target=_blank>coperception/coperception: An SDK for multi-agent collaborative perception. (github.com)</a>,<a href=https://github.com/DerrickXuNu/OpenCOOD和[ucla-mobility/V2V4Real rel=noopener target=_blank>https://github.com/DerrickXuNu/OpenCOOD和[ucla-mobility/V2V4Real</a>: <a href=https://github.com/ucla-mobility/v2v4real rel=noopener target=_blank>CVPR2023 Highlight] The official codebase for paper “V2V4Real: A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception” (github.com)</a><p>此外<a href=https://github.com/Little-Podi/Collaborative_Perception rel=noopener target=_blank>Little-Podi/Collaborative_Perception: This repository is a paper digest of recent advances in collaborative / cooperative / multi-agent perception for V2I / V2V / V2X autonomous driving scenario. (github.com)</a>会汇总相关领域最新的论文.<h3 id=OPV2V-An-Open-Benchmark-Dataset-and-Fusion-Pipeline-for-Perception-with-Vehicle-to-Vehicle-Communication><a title="OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication" class=headerlink href=#OPV2V-An-Open-Benchmark-Dataset-and-Fusion-Pipeline-for-Perception-with-Vehicle-to-Vehicle-Communication></a>OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication</h3><h4 id=abs><a class=headerlink href=#abs title=abs></a>abs</h4><p>利用车对车通信提高自动驾驶技术的感知性能近年来引起了广泛关注;然而，由于缺乏合适的开放数据集来对算法进行基准测试，因此很难开发和评估协同感知技术。<h4 id=融合方法介绍><a class=headerlink href=#融合方法介绍 title=融合方法介绍></a>融合方法介绍</h4><p>论文中融合算法介绍了多种,这里按时间线写一下.<p>早期融合有最早的Cooper.<p>主要介绍中期融合<h3 id=F-cooper><a class=headerlink href=#F-cooper title=F-cooper></a>F-cooper</h3><p>2019论文中提出了VFF和SFF.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>SpatialFusion</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(SpatialFusion, self).__init__()</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>regroup</span>(<span class=params>self, x, record_len</span>):</span></span><br><span class=line>        cum_sum_len = torch.cumsum(record_len, dim=<span class=number>0</span>)</span><br><span class=line>        split_x = torch.tensor_split(x, cum_sum_len[:-<span class=number>1</span>].cpu())</span><br><span class=line>        <span class=keyword>return</span> split_x</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, record_len</span>):</span></span><br><span class=line>        <span class=comment># x: B, C, H, W, split x:[(B1, C, W, H), (B2, C, W, H)]</span></span><br><span class=line>        split_x = self.regroup(x, record_len)</span><br><span class=line>        out = []</span><br><span class=line></span><br><span class=line>        <span class=keyword>for</span> xx <span class=keyword>in</span> split_x:</span><br><span class=line>            xx = torch.<span class=built_in>max</span>(xx, dim=<span class=number>0</span>, keepdim=<span class=literal>True</span>)[<span class=number>0</span>]</span><br><span class=line>            out.append(xx)</span><br><span class=line>        <span class=keyword>return</span> torch.cat(out, dim=<span class=number>0</span>)</span><br></pre></table></figure><p>最后利用融合后的特征传给cls_head和reg_head<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>psm = self.cls_head(fused_feature)</span><br><span class=line>rm = self.reg_head(fused_feature)</span><br><span class=line>output_dict = {<span class=string>'psm'</span>: psm,</span><br><span class=line>               <span class=string>'rm'</span>: rm}</span><br></pre></table></figure><h3 id=V2VNet-Vehicle-to-Vehicle-Communication-for-Joint-Perception-and-Prediction-2020><a title="V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction 2020" class=headerlink href=#V2VNet-Vehicle-to-Vehicle-Communication-for-Joint-Perception-and-Prediction-2020></a>V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction 2020</h3><h5 id=Abs><a class=headerlink href=#Abs title=Abs></a>Abs</h5><p>探讨了使用<strong>车对车（V2V）通信来改善自动驾驶汽车的感知和运动预测性能</strong>。通过智能地聚合从附近多辆车接收到的信息，我们可以从不同的视角观察同一场景。这使我们能够<strong>穿透遮挡物并远距离检测参与者，其中观察结果非常稀疏或不存在</strong>。<p>SDV 需要对场景进行 3D 推理，识别其他智能体，并预测他们的未来可能如何发展。这些任务通常称为感知和运动预测。强大的感知和运动预测对于 SDV 规划和操纵交通以安全地从一个点到另一个点都至关重要.<p>尽管取得了这些进展，但挑战依然存在。例如，<strong>被严重遮挡或距离较远的物体会导致观测稀疏，并对现代计算机视觉系统构成挑战</strong>。<p>在本文中，考虑了车对车 （V2V） 通信设置，其中每辆车都可以向附近的车辆（半径 70m 以内）广播和接收信息。请注意，基于现有的通信协议，这个广播范围是现实的。为了在满足现有硬件传输带宽能力的同时，实现具有强大感知和运动预测性能的最佳折衷方案，我们应该发送P&P神经网络的压缩中间表示。因此，我们推导出了一种名为V2VNet的新颖的P&P模型，该模型<strong>利用空间感知图神经网络（GNN）来聚合从附近所有SDV接收到的信息</strong>，使我们能够智能地组合来自不同时间点和场景中视点的信息。<p><img alt=image-20240107145123567 data-src=https://s2.loli.net/2024/01/07/MXuJrkjUeywFPf1.png><p><img alt=image-20240107145440332 data-src=https://s2.loli.net/2024/01/07/JMEKzxTutGrfW8h.png><h5 id=总结><a class=headerlink href=#总结 title=总结></a>总结</h5><p>较早的一篇使用全面神经网络探讨V2V协同的目标感知与检测,贡献了V2V-sim仿真数据集.<h3 id=DiscoNet><a class=headerlink href=#DiscoNet title=DiscoNet></a>DiscoNet</h3><p><img alt=image-20240110151834154 data-src=https://s2.loli.net/2024/01/10/kU47aJnKuABtjS3.png><p>涉及到了图和蒸馏的方法,我了解不多.<p><img alt=image-20240110151903360 data-src=https://s2.loli.net/2024/01/10/CnGtDVSgHJWEjLd.png><h3 id=CoBEVT-2022><a title="CoBEVT 2022" class=headerlink href=#CoBEVT-2022></a>CoBEVT 2022</h3><p><strong>Abstract</strong><p>在本文中，提出了CoBEVT，这是第一个可以<strong>协同生成BEV地图预测的通用多智能体多相机感知框架</strong>。<p>为了在底层 Transformer 架构中有效地融合来自多视图和多智能体数据的相机特征，我们设计了一个融合轴向注意力模块 （FAX），该模块可以捕获跨视图和智能体的稀疏局部和全局空间交互.<p>将多摄像头视图投射到整体纯电动汽车空间中，在空间和时间上保持道路元素的位置和比例方面具有明显的优势，这对于各种自动驾驶任务（包括场景理解和规划）至关重要<p>地图视图（或 BEV）语义分割是一项基本任务,旨在根据单个或多个校准的摄像头输入预测路段。在基于摄像头的精确BEV语义分割方面已经做出了重大努力。最流行的技术之一是利用<strong>深度信息来推断相机视图和规范地图之间的对应关系</strong>。另一个系列使用<strong>基于注意力的模型直接学习摄像头到BEV的空间转换</strong>，无论是隐式还是显式<p>尽管取得了令人鼓舞的结果，<strong>但基于视觉的感知系统具有固有的局限性——众所周知，相机传感器对物体遮挡和景深有限很敏感，这可能导致在严重遮挡或远离相机镜头的区域性能较差</strong><p><img alt=image-20240102164321954 data-src=https://s2.loli.net/2024/01/02/I1xRkviLBmbnqfQ.png>`<h4 id=Related-Work><a title="Related Work" class=headerlink href=#Related-Work></a>Related Work</h4><p>最近，V2VNet提出将从3D骨干中提取的中间特征（即中间融合）循环，然后利用空间感知图神经网络进行多智能体特征聚合。遵循类似的传输范式，<strong>OPV2V 采用简单的智能体单头注意力来融合所有特征</strong>。<strong>F-Cooper使用简单的 maxout 操作来融合特征</strong>。<p><strong>DiscoNet通过约束中间特征图来匹配早期融合教师模型中的对应关系来探索知识蒸馏。</strong><p>与之前的多智能体算法相比,我 CoBEVT 率先<strong>采用稀疏transformer来高效</strong>、详尽地探索车辆之间的相关性。<p>此外，以往的方法主要侧重于与激光雷达的协同感知，而我们的目标是提出一种低成本的<strong>基于摄像头</strong>的、<strong>没有激光雷达设备的协同感知解决方案</strong>。<blockquote><p>Transformer 最初是为自然语言处理而提出的 。ViT 首次证明，一个纯粹的 Transformer 只是将图像块视为视觉词，通过大规模预训练就足以完成视觉任务。Swin Transformer通过限制局部（移位）窗口中的注意力场，进一步提高了纯 Transformer 的通用性和灵活性。对于高维数据，Video Swin Transformer 将 Swin 方法扩展到移动的 3D 时空窗口，从而实现高性能和低复杂性。<p>最近的工作主要集中在<strong>改进注意力模型的结构</strong>上，包括稀疏注意力，扩大的感受野，金字塔设计，有效的替代方案]等。</blockquote><h4 id=Fused-Axial-Attention><a title="Fused Axial Attention" class=headerlink href=#Fused-Axial-Attention></a>Fused Axial Attention</h4><p><strong>融合来自多个智能体的 BEV 特征需要跨所有智能体的空间位置进行局部和全局交互</strong>。一方面,相邻的自动驾驶汽车通常对同一物体具有不同的遮挡级别;因此,更关心细节的局部注意力可以帮助在该对象上构建像素到像素的对应关系。<p><strong>ego应汇总附近自动驾驶汽车每个位置的所有BEV特征</strong>，以获得可靠的估计。另一方面，<strong>长期的全局情境感知也有助于理解道路拓扑语义或交通状态</strong>——车辆前方的道路拓扑和交通密度通常与后方的道路拓扑和交通密度高度相关。这种全局推理也有利于多相机视图的理解。<p>同一辆车被分成多个视图,global attention能够将它们连接起来以进行语义推理<p><img alt=image-20240103202226517 data-src=https://s2.loli.net/2024/01/03/w5AtSc4o2gsTzB9.png><p>提出了一种称为融合轴向注意力的新颖 3D 注意力机制,作为 SinBEVT 和 FuseBEVT 的核心组件,可以有效地聚合本地和全局范围内跨代理或相机视图的特征。<p><img alt=image-20240105103533757 data-src=https://s2.loli.net/2024/01/05/Sel1ad8xyFHqLfJ.png><p><strong>设 X∈ R^N×H×W×C^ 是来自 N 智能体的空间维度为 H × W 的堆叠 BEV 特征</strong><p>在局部特征上<strong>,将特征图划分为 3D 非重叠窗口</strong>,每个窗口的大小为 N × P × P,然后将形状为(H/P× W/P,N × P^2^,C) 的分割张量 输入到自注意力模型中<p>在全局特征中,使用统一的 3D 网格 N ×G×G 将特征 X 划分为形状 (N × G^2^,H/G×W /G,C)<p>将注意力集中在该张量的第一个轴上,该张量表示关注稀疏采样的标记<p><img alt=image-20240103203511174 data-src=https://s2.loli.net/2024/01/03/NVMak8mhlgxeKwo.png><p>将这种 3D 局部和全局注意力与 Transformer 的典型设计相结合 ，包括层归一化 （LN）、MLP 和跳跃连接，形成了提出的 FAX 注意力块<h4 id=SinBEVT-for-Single-agent-BEV-Feature-Computation><a title="SinBEVT for Single-agent BEV Feature Computation" class=headerlink href=#SinBEVT-for-Single-agent-BEV-Feature-Computation></a>SinBEVT for Single-agent BEV Feature Computation</h4><p>CVT 使用低分辨率的 BEV 查询，该查询完全交叉关注图像特征，这会导致小物体的性能下降，尽管效率很高。因此，<strong>CoBEVT 学习高分辨率的 BEV 嵌入，然后使用分层结构来优化分辨率降低的 BEV 特征</strong>。为了在高分辨率下有效地查询来自相机编码器的特征，FAX-SA模块进一步扩展为构建FAX交叉关注（FAX-CA）模块，其中查询向量使用BEV嵌入获得，而键/值向量由多视图相机特征投影。<h5 id=FuseBEVT-for-Multi-agent-BEV-Feature-Fusion><a title="FuseBEVT for Multi-agent BEV Feature Fusion" class=headerlink href=#FuseBEVT-for-Multi-agent-BEV-Feature-Fusion></a>FuseBEVT for Multi-agent BEV Feature Fusion</h5><p>一旦接收到包含中间BEV表示的广播消息和发送者的姿态,自我车辆就会应用可微分的空间变换算子Γξ,将接收到的特征几何扭曲到ego的坐标系上:H~i~ = Γ~ξ~ (F~i~)∈ R^H×W×C^<p>3D FAX-SA 可以处理从多个代理抽取的同一估计区域（红框），以得出最终的聚合表示。此外，稀疏采样token（蓝框）可以全局交互，以获得对地图语义（如道路、交通等）的上下文理解。<h5 id=总结-1><a class=headerlink href=#总结-1 title=总结></a>总结</h5><p>使用swim transfomer类似架构进行多层级的特征融合.设计了agent_size和window_size.<h3 id=Robust-Collaborative-3D-Object-Detection-in-Presence-of-Pose-Errors-CoAlign-2022><a title="Robust Collaborative 3D Object Detection in Presence of Pose Errors (CoAlign) 2022" class=headerlink href=#Robust-Collaborative-3D-Object-Detection-in-Presence-of-Pose-Errors-CoAlign-2022></a>Robust Collaborative 3D Object Detection in Presence of Pose Errors (CoAlign) 2022</h3><h5 id=Abs-1><a class=headerlink href=#Abs-1 title=Abs></a>Abs</h5><p>协作式 3D 目标检测利用多个智能体之间的信息交换，在存在传感器损伤（如遮挡）的情况下提高目标检测的准确性,然而，在实践中，由于定位不完善导致的姿态估计误差会导致空间信息错位，并显著降低协作性能。<p>为了减轻姿势错误的不利影响，我们提出了CoAlign，这是一种新颖的混合协作框架，对<strong>未知的姿势错误具有鲁棒性</strong>。所提出的解决方案依赖于一种<strong>新颖的智能体-对象姿态图建模来增强协作智能体之间的姿态一致性</strong>。此外，<strong>采用多尺度数据融合策略，在多个空间分辨率下聚合中间特征</strong>。<p>尽管大规模数据集和强大模型的发展速度很快，<strong>但单个智能体的 3D 对象检测仍存在固有的局限性，例如遮挡和远处的物体。通过利用智能体之间的通信，例如驾驶场景中的车联网（V2X），多个智能体可以相互共享互补的感知信息，从而促进更全面的接受领域。</strong><p>为了实现这种协作式3D检测，最近的工作<strong>贡献了高质量的数据集</strong>和<strong>有效的协作方法</strong>。但在这个新兴领域仍然存在许多挑战，例如通<strong>信带宽限制</strong>、<strong>延迟</strong>和<strong>对抗性攻击</strong>。这项工作的重点是减轻姿势误差的负面影响。<p>由于我们的<strong>智能体-对象姿态图</strong>在优化过程中<strong>没有使用任何训练参数</strong>，因此该方法具有<strong>很强的泛化能力，可以适应任意级别的姿态误差</strong>。为了有效缓解姿态误差的影响，我们进一步考虑了一种<strong>多尺度中间融合策略，该策略在多个空间尺度上全面聚合协作信息。</strong><p>我们在仿真和真实数据集上对基于LiDAR的3D目标检测任务进行了广泛的实验，包括<strong>OPV2V 、V2X-Sim 2.0和DAIR-V2X</strong>.所提出的CoAlign在存在姿态误差的协同3D目标检测任务中实现了<strong>至少12%的性能提升</strong>。<p>考虑场景中的 N 个代理。每个智能体都具有感知、沟通和检测的能力。目标是通过分布式协作来达到每个智能体更好的 3D 检测能力。<p>在以前的文献中，有三种类型的协作：早期协作，传输原始观测数据，中间协作，，传输中间特征，后期协作，传输检测输出。<p>设 O~i~ 和 B~i~ 分别是第 i 个智能体的感知观察和检测输出。对于第 i 个代理，基于中间协作的标准 3D 对象检测的工作原理如下</p><script type="math/tex; mode=display">
\begin{aligned}
&\mathbf{F}_{i}=f_{\mathrm{encoder}}\left(\mathbf{O}_{i}\right), \\
&\mathbf{M}_{j\to i}=f_{\mathbf{transform}}\left(\xi_i,(\mathbf{F}_j,\xi_j)\right), \\
&\mathbf{F}_{i}^{\prime}=f_{\mathrm{fusion}}\left(\mathbf{F}_{i},\{\mathbf{M}_{j\to i}\}_{j=1,2,\cdots,N}\right), \\
&\mathbf{B}_{i}=f_{\mathrm{decoder}}\left(\mathbf{F}_{i}^{\prime}\right),
\end{aligned}</script><blockquote><p>在实践中，定位模块估计的每个 6DoF 姿态 ξi 通常是有噪声的。然后，在步骤2中进行姿态变换后，每个消息Mj→i将具有不同的固有坐标系，导致步骤3中的融合错位和步骤4中的检测输出不好。<p>这项工作的目标<strong>是通过在步骤2之前引入额外的姿势校正来最大限度地减少姿势错误的影响</strong></blockquote><p>CoAlign<strong>结合了中间和后期协作策略。与中间融合相比，这种混合协作可以利用智能体检测到的边界框作为场景地标，并校正智能体之间的相对姿态</strong>。</p><script type="math/tex; mode=display">
\begin{gathered}
\mathbf{F}_{i},\mathbf{B}_{i} =\quad f_\text{detection}\left(\mathbf{O}_i\right), \text{(2a)} \\
\{\xi_{j\rightarrow i}^{\prime}\}_j =\quad f_{\mathrm{correction}}\left(\{\mathbf{B}_{j},\xi_{j}\}_{j=1,2,\cdots,N}\right), (2\mathbf{b}) \\
\mathrm{M}_{j\rightarrow i} =\quad f_{\mathrm{transform}}\left(\mathbf{F}_j,\xi_{j\to i}^{\prime}\right), \text{(2c)} \\
\mathbf{F}_{i}^{\prime} =\quad f_{\mathrm{fusion}}\left(\{\mathrm{M}_{j\to i}\}_{j=1,2,\cdots,N}\right), \text{(2d)} \\
\mathbf{B}_{i}^{\prime} =\quad f_\text{decoder}\left(\mathbf{F}_i^{\prime}\right), \text{(2c)} 
\end{gathered}</script><p><img alt=image-20240107164141678 data-src=https://s2.loli.net/2024/01/07/Q8ywAZ2N7IcCBSG.png><p>为了实现单智能体 3D 目标检测器 fdetection(·),可以利用现成的设计，例如 PointPillars ,为第 i 个智能体生成中间特征 F~i~ 和估计边界框 B~i~。请注意，对<strong>于每个边界框，我们还估计其不确定性</strong>。<p>由于我们后来依靠这些框来纠正姿势错误，因此混乱的检测可能会导致更糟糕的相对姿势。每个框的估计不确定度可以提供有益的置信度信息，以排除不需要的检测。<p>每个框的估计不确定度可以提供有益的置信度信息，以排除不需要的检测。<h5 id=Agent-Object-Pose-Graph-Optimization><a title="Agent-Object Pose Graph Optimization" class=headerlink href=#Agent-Object-Pose-Graph-Optimization></a>Agent-Object Pose Graph Optimization</h5><p>单智能体检测后，第 i 个智能体共享三类消息，包括<p>i） 其姿态 ξ~i~ 由其自身定位模块估计;<p>ii） 第 i 个agent检测到的边界框;<p>iii） 其特征图 F~i~<p>为了可靠地融合来自其他智能体的特征图,每个智能体都需要校正相对姿态。<h5 id=Multiscale-Feature-Fusion><a title="Multiscale Feature Fusion" class=headerlink href=#Multiscale-Feature-Fusion></a>Multiscale Feature Fusion</h5><p>空间对齐后，每个智能体聚合其他智能体的协作信息，并获得信息量更大的特征。但是，即使在相对姿态校正之后，特征图之间的错位可能仍然存在。更精细尺度的特征可以提供更详细的几何和语义信息，而粗尺度的特征对位姿误差的敏感度较低。多尺度结构可以同时发挥优势，并产生信息丰富和强大的功能。</p><script type="math/tex; mode=display">
\begin{aligned}
\mathbf{F}_{j\to i}^{(1)}& =\quad\mathbf{M}_{j\to i},j=1,2,\cdots N,  \\
\mathbf{F}_{j\to i}^{(\ell+1)}& =\quad g_\ell(\mathbf{F}_{j\to i}^{(\ell)}),\ell=1,2,\cdots,L,  \\
\mathbf{F}_{i}^{(\ell)}& =\quad\text{Fuse}(\{\mathbf{F}_{j\to i}^{(\ell)}\}_{j=1,2,\cdots,N}),  \\
\mathbf{F}_{i}^{\prime}& =\quad\mathrm{Cat}([\mathbf{F}_i^{(1)},u_2(\mathbf{F}_i^{(2)}),\cdots,u_L(\mathbf{F}_i^{(L)})]), 
\end{aligned}</script><p><strong>总结</strong><p>结合了单车的late和intermediate的输出,提出了agent-pose map用于修正pose.使用多尺度融合.CoAlign在融合层面主要使用self-attention和multi-scale(backbone使用了多尺度的ResNet).在代码中使用resnet并下采样得到多尺度的特征,将这些多尺度特征通过融合模块.融合模块种通过warp_affine将特征转换到(H,W)整个感知范围内,再进行self-attention.然后将融合后的多尺度特征通过多级decoder最后concat在一起.<p>其中affine操作利用车之间转换矩阵以及affine_grid和affine_sample得到,也就是将特征仿射变换到lidar range.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>normalize_pairwise_tfm</span>(<span class=params>pairwise_t_matrix, H, W, discrete_ratio, downsample_rate=<span class=number>1</span></span>):</span></span><br><span class=line>    pairwise_t_matrix = pairwise_t_matrix[:,:,:,[<span class=number>0</span>, <span class=number>1</span>],:][:,:,:,:,[<span class=number>0</span>, <span class=number>1</span>, <span class=number>3</span>]] <span class=comment># [B, L, L, 2, 3]</span></span><br><span class=line>    pairwise_t_matrix[...,<span class=number>0</span>,<span class=number>1</span>] = pairwise_t_matrix[...,<span class=number>0</span>,<span class=number>1</span>] * H / W</span><br><span class=line>    pairwise_t_matrix[...,<span class=number>1</span>,<span class=number>0</span>] = pairwise_t_matrix[...,<span class=number>1</span>,<span class=number>0</span>] * W / H</span><br><span class=line>    pairwise_t_matrix[...,<span class=number>0</span>,<span class=number>2</span>] = pairwise_t_matrix[...,<span class=number>0</span>,<span class=number>2</span>] / (downsample_rate * discrete_ratio * W) * <span class=number>2</span></span><br><span class=line>    pairwise_t_matrix[...,<span class=number>1</span>,<span class=number>2</span>] = pairwise_t_matrix[...,<span class=number>1</span>,<span class=number>2</span>] / (downsample_rate * discrete_ratio * H) * <span class=number>2</span></span><br><span class=line></span><br><span class=line>    normalized_affine_matrix = pairwise_t_matrix</span><br><span class=line></span><br><span class=line>    <span class=keyword>return</span> normalized_affine_matrix</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>warp_affine_simple</span>(<span class=params></span></span></span><br><span class=line><span class=params><span class=function>        src, M, dsize,</span></span></span><br><span class=line><span class=params><span class=function>        mode=<span class=string>'bilinear'</span>,</span></span></span><br><span class=line><span class=params><span class=function>        padding_mode=<span class=string>'zeros'</span>,</span></span></span><br><span class=line><span class=params><span class=function>        align_corners=<span class=literal>False</span></span>):</span></span><br><span class=line>    B, C, H, W = src.size()</span><br><span class=line>    grid = F.affine_grid(M, [B, C, dsize[<span class=number>0</span>], dsize[<span class=number>1</span>]],</span><br><span class=line>                         align_corners=align_corners).to(src)</span><br><span class=line>    <span class=keyword>return</span> F.grid_sample(src, grid, align_corners=align_corners)</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>Att_w_Warp</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, feature_dims</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(Att_w_Warp, self).__init__()</span><br><span class=line>        self.att = ScaledDotProductAttention(feature_dims)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, xx, record_len, normalized_affine_matrix</span>):</span></span><br><span class=line>        _, C, H, W = xx.shape</span><br><span class=line>        B, L = normalized_affine_matrix.shape[:<span class=number>2</span>]</span><br><span class=line>        split_x = regroup(xx, record_len)</span><br><span class=line>        batch_node_features = split_x</span><br><span class=line>        out = []</span><br><span class=line>        <span class=comment># iterate each batch</span></span><br><span class=line>        <span class=keyword>for</span> b <span class=keyword>in</span> <span class=built_in>range</span>(B):</span><br><span class=line>            N = record_len[b]</span><br><span class=line>            t_matrix = normalized_affine_matrix[b][:N, :N, :, :]</span><br><span class=line>            <span class=comment># update each node i</span></span><br><span class=line>            i = <span class=number>0</span> <span class=comment># ego</span></span><br><span class=line>            x = warp_affine_simple(batch_node_features[b], t_matrix[i, :, :, :], (H, W))</span><br><span class=line>            cav_num = x.shape[<span class=number>0</span>]</span><br><span class=line>            x = x.view(cav_num, C, -<span class=number>1</span>).permute(<span class=number>2</span>, <span class=number>0</span>, <span class=number>1</span>) <span class=comment>#  (H*W, cav_num, C), perform self attention on each pixel.</span></span><br><span class=line>            h = self.att(x, x, x)</span><br><span class=line>            h = h.permute(<span class=number>1</span>, <span class=number>2</span>, <span class=number>0</span>).view(cav_num, C, H, W)[<span class=number>0</span>, ...]  <span class=comment># C, W, H before</span></span><br><span class=line>            out.append(h)</span><br><span class=line></span><br><span class=line>        out = torch.stack(out)</span><br><span class=line>        <span class=keyword>return</span> out</span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>ScaledDotProductAttention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=line>        self.sqrt_dim = np.sqrt(dim)</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, query, key, value</span>):</span></span><br><span class=line>        score = torch.bmm(query, key.transpose(<span class=number>1</span>, <span class=number>2</span>)) / self.sqrt_dim</span><br><span class=line>        attn = F.softmax(score, -<span class=number>1</span>)</span><br><span class=line>        context = torch.bmm(attn, value)</span><br><span class=line>        <span class=keyword>return</span> context</span><br></pre></table></figure><h3 id=Learning-for-Vehicle-to-Vehicle-Cooperative-Perception-under-Lossy-Communication-V2VAM-2023><a title="Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication (V2VAM) 2023" class=headerlink href=#Learning-for-Vehicle-to-Vehicle-Cooperative-Perception-under-Lossy-Communication-V2VAM-2023></a>Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication (V2VAM) 2023</h3><h5 id=Abs-2><a class=headerlink href=#Abs-2 title=Abs></a>Abs</h5><p>深度学习已广泛应用于智能车辆驾驶感知系统，如3D目标检测。一种很有前途的技术是协同感知，它利用车对车（V2V）通信在车辆之间共享基于深度学习的特征。然而，大<strong>多数协作感知算法都假设了理想的通信，而没有考虑现实世界中非常常见的有损通信（LC）对特征共享的影响</strong>。<p>在本文中探讨了LC对合作感知的影响，并提出了一种新的方法来减轻这些影响。我们的方法包括LC感知维修网络（LCRN）和V2V注意力模块（V2VAM），具有车内注意力和不确定性感知车辆间注意力。<p>V2V通信中的信息共享有三种方式：（1）将原始传感器数据共享为早期融合，（2）将基于深度学习的检测网络的中间特征共享为中间融合，（3）将检测结果共享为后期融合。<p><strong>最近的研究]表明，中间融合是检测精度和带宽要求之间的最佳权衡</strong>。<p>最近提出了许多用于V2V协同感知的中间融合方法;但是，它们都假定了理想的通信。<strong>唯一一项考虑非理想通信的V2V协同感知研究仅关注通信延迟</strong>。迄今为止，尚无研究探讨有损通信（LC）对复杂真实驾驶环境中V2V协同感知的影响<p>在城市交通场景中，V2V通信容易受到一系列可能导致有损通信的因素的影响，例如<strong>来自障碍物</strong>（例如建筑物和车辆）的多径效应(电磁波经不同路径传播后，各分量场到达接收端时间不同，按各自相位相互叠加而造成干扰，使得原来的信号失真，或者产生错误)、<strong>快速移动的车辆引入的多普勒频移</strong>、<strong>其他通信网络产生的干扰</strong>.<p>该文首先研究了<strong>有损通信在V2V协同感知中的负面影响</strong>，<strong>然后提出了一种新的中间LC感知特征融合方法</strong>。具体而言，所提方法<strong>包括LC感知修复网络</strong>（LCRN）和<strong>专门设计的V2V注意力模块</strong>（V2VAM），以增强自我车辆与其他车辆之间的交互.<p>V2VAM包括<strong>ego车辆的车内注意力</strong>和<strong>不确定性感知的车辆间注意力</strong>。在真实驾驶中收集具有有损通信的真实CAV感知数据具有挑战性.<blockquote><p>基于LiDAR的检测方法，这些方法通常将LiDAR点转换为体素或柱子，在基于体素的、或基于Pillar的目标检测方法中占主导地位。PointRCNN提出了一种基于原始点云的两阶段策略，即先学习粗略估计，然后用语义属性对其进行细化。一些方法[]建议将空间分割成体素，并为每个体素生成特征。但是，3D 体素的处理成本通常很高。为了解决这个问题，PointPillars建议将沿z轴的所有体素压缩为一个柱子，然后在鸟瞰空间中预测3D框。此外，最近的一些方法结合了基于体素和基于点的方法，以联合检测3D物体。</blockquote><p>3D 感知方法的性能很大程度上取决于 3D 点云的精度。然而，LiDAR 摄像头存在折射、遮挡和远距离等问题，因此单车系统在一些具有挑战性的情况下可能会变得不可靠 。近年来，车对车（V2V）/车对基础设施（V2X）协同系统被提出，以克服单车系统使用多辆车的弊端。不同车辆之间的协作使3D感知网络能够融合来自不同来源的信息。<p>为了在数据负载和准确性之间找到平衡，<strong>最近的方法侧重于通过共享中间表示来进行中间融合。F-cooper应用体素特征融合和空间特征融合来自两辆车。V2VNet 采用图神经网络来聚合 LiDAR 从每辆车中提取的特征。V2X-ViT 提出了一种视觉 Transformer 架构，用于融合车辆和基础设施的功能。Cui等提出了一种基于点的Transformer进行点云处理，该Transformer可以将协同感知与控制决策相结合</strong><p>Tu等提出了一种基于<strong>中间表示的多智能体深度学习系统中高效实用的在线攻击网络</strong>。Luo等利用<strong>注意力模块融合中间特征，增强特征互补性</strong>。Lei等提出了一种<strong>延迟补偿模块，以实现中间特征级同步</strong>。胡(where2comm)等提出了一种空间置信度感知沟通策略，通过关注感知关键领域来使用较少的communication来提高绩效.<p><strong>OPV2V利用自注意力模块来融合接收到的中间特征。CoBEVT 提出了局部全局稀疏注意力，可以捕获视图和智能体之间的复杂空间交互，以提高协作感知的性能</strong><p>然而，这些融合方法都是以理想通信为前提的，在现实世界中，<strong>有损通信会使性能急剧下降。为了解决这个问题，我们设计了一种特殊的V2V注意力模块（V2VAM），包括ego车辆的车内注意力和不确定性感知的车辆间注意力,以增强V2V交互</strong>。<p><img alt=image-20240107223439462 data-src=https://s2.loli.net/2024/01/07/dc81urwk6RBAfoX.png><p>本文主要关注数据传输过程中的有损通信挑战 所以<strong>假设V2V系统中不存在通信延迟或定位错误</strong>。<strong>共享数据也可能在到达目的地之前受到其他信号的干扰或被攻击者修改</strong>，从而导致有损数据。在这项工作中，旨在通过<strong>提出LC感知修复网络</strong>和<strong>提高V2V感知网络的鲁棒性来消除有损通信</strong>。<p>该文提出了一种新的中间LCaware特征融合框架。包括五个组件：1）V2V元数据共享，2）激光雷达特征提取，3）特征共享，4）LC感知修复网络和V2V注意力模块，5）分类和回归头。<p>LC感知修复网络和V2V注意力模块.从周围其他CAV聚合的中间特征被输入到我们框架的主要组件中,即<strong>LC-Aware修复网络,用于使用张量滤波在有损通信中恢复中间特征图,</strong>以及<strong>V2V注意力模块,用于利用注意力机制进行迭代的车辆间和车辆内特征融合</strong>.<p><strong>LC-aware Repair Network</strong><p>LC感知修复网络的框架如图3所示，该网络是一种具有跳跃连接的编码器-解码器架构。该网络生成一个特定的每个张量过滤器内核，以共同对齐和恢复输入损坏的特征，以生成输出特征的恢复版本。LC感知修复网络的输入特征为S ∈ R^c×h×w^，然后生成一个张量核K并应用于S，以产生恢复的输出特征ˆ S ∈ R^c×h×w^。<p><img alt=image-20240107231955750 data-src=https://s2.loli.net/2024/01/07/zpJc21GkmUEadLO.png><p>LC感知修复网络的输入特征为S ∈ R^c×h×w^，然后生成一个张量核K并应用于S，以产生恢复的输出特征ˆ S ∈ R^c×h×w^。<p>LC 感知修复损失函数 L~LC~（ ˆ S， ˆ S^g^） 是遭受有损通信之前的真值原始特征 ˆ S^g^ 与修复特征 ˆ S 之间的张量 L1 距离。<p><strong>V2V Attention Module</strong><p>自注意力的关键思想是将某个位置的响应计算为所有位置的特征的加权总和，特征之间的相互作用由特征本身决定，而不是像卷积那样由它们的相对位置决定。<p>通过考虑有损通信情况，设计了一种定制化的车内和车间注意力融合方法，以增强自我CAV与其他CAV之间的交互.此外,我们在提出的V2V注意力方法中采用了<strong>纵横交错的注意力模块</strong>,可以利用该模块更有效地从全功能依赖关系中捕获上下文信息。<p>仅对于ego车辆，车内注意力模块可以使任何位置的特征进行全局感知，从而享受全图像上下文信息，以更好地捕捉代表性特征。从形式上讲，让 H^e^ ∈ R^C×H×W^ 成为自我车辆的输入特征图，它是自我车辆生成的完美数据，不会遭受任何有损通信。</p><script type="math/tex; mode=display">
\mathbf{A}^{in\boldsymbol{l}ra}=\mathrm{softmax}\left(\frac{\mathbf{Q}^{e}\mathbf{K}^{e\mathbf{T}}}{\sqrt{d_{k}^{e}}}\right)\mathbf{V}^{e},</script><p>在车内注意力模块中，<strong>特征图H^e^将由三个1×1卷积层计算，分别产生三个特征向量Qe、Ke和Ve</strong>，其中所有特征向量都具有相同的大小，{Q^e^，K^e^，V^e^}∈R^C×H×W^。按照中的缩放点积注意力，我们计算了Q^e^和K^e^的点积，然后使用比例因子（即特征向量的维度）将它们除以，并应用softmax函数来获得V^e^上的权重。<p>在V2V协同感知中，从其他CAV聚合而来的中间特征图H^s^∈R^C×H×W^被共享给ego车辆。具有有损通信的共享特征图Hs将被LC感知修复网络恢复，但它们在一定程度上仍然有噪声，而ego特征图H^s^是完美的.<p>针对这一问题，该文考虑恢复特征图的不确定性，提出一种不确定性感知的车间注意力融合方法。<p>在该模块中,共享特征图将由两个 1 × 1 卷积层计算，分别产生两个特征向量 K^s^ 和 V^s^，其中它们都具有相同的大小，{Ks， Vs} ∈ R^C×H×W^，另一个特征向量 Q^e^ 直接从自我车辆而不是其他车辆获得.</p><script type="math/tex; mode=display">
\mathbf{A}^{inter}=\sum_i^N\mathrm{softmax}\left(\frac{\mathbf{Q}^e\mathbf{K}_i^{s\text{T}} }{ \sqrt { d _ k ^ s }}\right)\mathbf{V}_i^s,</script><p><strong>Efficient Implementation</strong><p>采用两个连续的纵横交错（CC）注意力模块来实现点云数据中的V2V注意力,而不是缩放的点积注意力,减小计算量.<p><strong>在获得车内注意力和车间注意力后，将它们分别输入到最大池化层和平均池化层，以获得丰富的空间信息，然后将它们串联为具有ReLU激活函数的二维卷积层的输入</strong>。<p><strong>总结</strong><p>感觉融合了cobev和coalign两篇文章.在代码上主要使用了ego的q和其他车的k,v计算注意力,而且计算注意力使用Criss-Cross Attention,然后使用max和mean再通过一个卷积.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>V2V_AttFusion</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, feature_dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(V2V_AttFusion, self).__init__()</span><br><span class=line>        </span><br><span class=line></span><br><span class=line>        self.cov_att = nn.Sequential(</span><br><span class=line>                nn.Conv2d(in_channels=feature_dim, out_channels=feature_dim, kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>),</span><br><span class=line>                nn.BatchNorm2d(feature_dim,eps=<span class=number>1e-5</span>, momentum=<span class=number>0.01</span>, affine=<span class=literal>True</span>),</span><br><span class=line>                nn.ReLU()</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line>        self.CCNet = CrissCrossAttention(feature_dim)</span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, x, record_len</span>):</span></span><br><span class=line></span><br><span class=line>        split_x = self.regroup(x, record_len)  <span class=comment>#x =[5, 64, 100, 352], record_len=[3,2]</span></span><br><span class=line></span><br><span class=line>        out = []</span><br><span class=line>        att = []</span><br><span class=line>        <span class=keyword>for</span> xx <span class=keyword>in</span> split_x:<span class=comment>#split_x[0] [num_car, C, W, H]</span></span><br><span class=line></span><br><span class=line>            <span class=string>''' CCNet: Criss-Cross Attention Module: attention for ego vehicle feature + cav feature '''</span></span><br><span class=line></span><br><span class=line>            ego_q, ego_k, ego_v = xx[<span class=number>0</span>:<span class=number>1</span>], xx[<span class=number>0</span>:<span class=number>1</span>], xx[<span class=number>0</span>:<span class=number>1</span>] </span><br><span class=line>            <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(xx[:,<span class=number>0</span>,<span class=number>0</span>,<span class=number>0</span>])):</span><br><span class=line>                att_vehicle = self.CCNet(ego_q, xx[i:i+<span class=number>1</span>], xx[i:i+<span class=number>1</span>])</span><br><span class=line>                att.append(att_vehicle)</span><br><span class=line></span><br><span class=line>            pooling_max = torch.<span class=built_in>max</span>(torch.cat(att, dim=<span class=number>0</span>), dim=<span class=number>0</span>, keepdim=<span class=literal>True</span>)[<span class=number>0</span>]</span><br><span class=line>            pooling_ave = torch.mean(torch.cat(att, dim=<span class=number>0</span>), dim=<span class=number>0</span>, keepdim=<span class=literal>True</span>)[<span class=number>0</span>]</span><br><span class=line></span><br><span class=line>            fuse_fea = pooling_max + pooling_ave</span><br><span class=line></span><br><span class=line>            fuse_att = fuse_fea</span><br><span class=line>            fuse_att = self.cov_att(fuse_att)</span><br><span class=line></span><br><span class=line>            out.append(fuse_att) <span class=comment>#[[1, 64, 100, 352], [1, 64, 100, 352]]</span></span><br><span class=line>            <span class=comment># torch.cuda.empty_cache()</span></span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> torch.cat(out, dim=<span class=number>0</span>) <span class=comment>#[2, 64, 100, 352]</span></span><br><span class=line></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>regroup</span>(<span class=params>self, x, record_len</span>):</span></span><br><span class=line>        cum_sum_len = torch.cumsum(record_len, dim=<span class=number>0</span>)</span><br><span class=line>        split_x = torch.tensor_split(x, cum_sum_len[:-<span class=number>1</span>].cpu())</span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> split_x</span><br><span class=line></span><br><span class=line><span class=class><span class=keyword>class</span> <span class=title>CrissCrossAttention</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=string>""" Criss-Cross Attention Module</span></span><br><span class=line><span class=string></span></span><br><span class=line><span class=string>    reference: https://github.com/speedinghzl/CCNet</span></span><br><span class=line><span class=string>    </span></span><br><span class=line><span class=string>    """</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, in_dim</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(CrissCrossAttention,self).__init__()</span><br><span class=line></span><br><span class=line></span><br><span class=line>        self.query_conv = nn.Sequential(</span><br><span class=line>                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class=number>1</span>),</span><br><span class=line>                nn.BatchNorm2d(in_dim,eps=<span class=number>1e-5</span>, momentum=<span class=number>0.01</span>, affine=<span class=literal>True</span>),</span><br><span class=line>                nn.ReLU()</span><br><span class=line>            )</span><br><span class=line>        self.key_conv = nn.Sequential(</span><br><span class=line>                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class=number>1</span>),</span><br><span class=line>                nn.BatchNorm2d(in_dim,eps=<span class=number>1e-5</span>, momentum=<span class=number>0.01</span>, affine=<span class=literal>True</span>),</span><br><span class=line>                nn.ReLU()</span><br><span class=line>            )</span><br><span class=line>        self.value_conv = nn.Sequential(</span><br><span class=line>                nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=<span class=number>1</span>),</span><br><span class=line>                nn.BatchNorm2d(in_dim,eps=<span class=number>1e-5</span>, momentum=<span class=number>0.01</span>, affine=<span class=literal>True</span>),</span><br><span class=line>                nn.ReLU()</span><br><span class=line>            )</span><br><span class=line></span><br><span class=line></span><br><span class=line>        self.softmax = Softmax(dim=<span class=number>3</span>)</span><br><span class=line>        self.INF = INF</span><br><span class=line>        self.gamma = nn.Parameter(torch.zeros(<span class=number>1</span>))</span><br><span class=line></span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self, query, key, value</span>):</span></span><br><span class=line>        m_batchsize, _, height, width = query.size()</span><br><span class=line></span><br><span class=line>        </span><br><span class=line>        proj_query = self.query_conv(query)</span><br><span class=line>        proj_query_H = proj_query.permute(<span class=number>0</span>,<span class=number>3</span>,<span class=number>1</span>,<span class=number>2</span>).contiguous().view(m_batchsize*width,-<span class=number>1</span>,height).permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)</span><br><span class=line>        proj_query_W = proj_query.permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>).contiguous().view(m_batchsize*height,-<span class=number>1</span>,width).permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)</span><br><span class=line></span><br><span class=line>        </span><br><span class=line>        proj_key = self.key_conv(key)</span><br><span class=line>        proj_key_H = proj_key.permute(<span class=number>0</span>,<span class=number>3</span>,<span class=number>1</span>,<span class=number>2</span>).contiguous().view(m_batchsize*width,-<span class=number>1</span>,height)</span><br><span class=line>        proj_key_W = proj_key.permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>).contiguous().view(m_batchsize*height,-<span class=number>1</span>,width)</span><br><span class=line>        </span><br><span class=line>        </span><br><span class=line>        proj_value = self.value_conv(value)</span><br><span class=line>        proj_value_H = proj_value.permute(<span class=number>0</span>,<span class=number>3</span>,<span class=number>1</span>,<span class=number>2</span>).contiguous().view(m_batchsize*width,-<span class=number>1</span>,height)</span><br><span class=line>        proj_value_W = proj_value.permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>).contiguous().view(m_batchsize*height,-<span class=number>1</span>,width)</span><br><span class=line>        energy_H = (torch.bmm(proj_query_H, proj_key_H)+self.INF(m_batchsize, height, width)).view(m_batchsize,width,height,height).permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>)</span><br><span class=line>        energy_W = torch.bmm(proj_query_W, proj_key_W).view(m_batchsize,height,width,width)</span><br><span class=line>        concate = self.softmax(torch.cat([energy_H, energy_W], <span class=number>3</span>))</span><br><span class=line></span><br><span class=line>        att_H = concate[:,:,:,<span class=number>0</span>:height].permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>).contiguous().view(m_batchsize*width,height,height)</span><br><span class=line>        att_W = concate[:,:,:,height:height+width].contiguous().view(m_batchsize*height,width,width)</span><br><span class=line>        out_H = torch.bmm(proj_value_H, att_H.permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)).view(m_batchsize,width,-<span class=number>1</span>,height).permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>3</span>,<span class=number>1</span>)</span><br><span class=line>        out_W = torch.bmm(proj_value_W, att_W.permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>1</span>)).view(m_batchsize,height,-<span class=number>1</span>,width).permute(<span class=number>0</span>,<span class=number>2</span>,<span class=number>1</span>,<span class=number>3</span>)</span><br><span class=line></span><br><span class=line></span><br><span class=line>        <span class=keyword>return</span> self.gamma*(out_H + out_W) + value</span><br></pre></table></figure><h3 id=Adaptive-Feature-Fusion-for-Cooperative-Perception-using-LiDAR-Point-Clouds-2023><a title="Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds 2023" class=headerlink href=#Adaptive-Feature-Fusion-for-Cooperative-Perception-using-LiDAR-Point-Clouds-2023></a>Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds 2023</h3><h5 id=Abs-3><a class=headerlink href=#Abs-3 title=Abs></a>Abs</h5><p>提出了具有<strong>可训练特征选择模块的自适应特征融合模型</strong>。我们提出的一个模型空间自适应特征融合（S-AdaFusion）在OPV2V数据集的两个子集上优于所有其他最先进的（SOTA）:用于车辆检测的默认CARLA Towns和用于域适应的Culver City.<blockquote><p>此外，以前的研究只测试了车辆检测的协同感知。然而，行人在交通事故中受重伤的可能性要大得多。<p>我们使用 CODD 数据集评估了车辆和行人检测的协同感知性能。我们的架构在 CODD 数据集上实现了比其他现有模型更高的车辆和行人检测平均精度 （AP）。实验表明，与传统的单车感知过程相比，协同感知也提高了行人检测的准确率。</blockquote><p>LiDAR 可以生成包含准确深度信息的点云数据，并且受外部照明条件的影响较小。然而，<strong>远离LiDAR的点云非常稀疏，这使得探测其他物体变得更加困难。</strong><p>感知到的信息可以包括 GPS 和各种传感器数据，包括<strong>雷达、摄像头和激光雷达数据</strong>。协同感知有助于弥补当前视觉感知技术的局限性，例如分辨率有限、天气效应和盲点。<p>首先，4个CAV处理其LiDAR点云，并在其本地系统中并行提取中间特征图。接下来，其他三个CAV将其提取的特征图连同LiDAR姿态信息一起广播到CAV1。然后，CAV1 将三个特征图投影到自己的坐标系，并将信息与自己的感知信息聚合在一起，用于 3D 目标检测。<p>根据CAV之间共享的数据类型，现有文献中发现了三种数据融合方法：1）早期融合聚合了来自其他CAV的原始输入传感器数据;<p>2）中间融合聚合了来自其他CAV的处理后的特征图;<p>3）后期融合聚合了来自其他CAV的目标检测的预测输出。<blockquote><p>在最近的研究中，与早期和晚期融合方法相比，中间特征融合已被证明是最有效的融合方法。我们假设通过实现有效的特征选择和融合模块，降低计算成本，可以进一步改进中间融合方法，以实现实时感知和更高的准确性.</blockquote><p>我们提出了几种具有可训练神经网络的特征融合模型，这些模型可以从多个CAV中自适应地选择特征<p>这项工作的贡献如下：1）我们创建了一个具有中间融合的轻量级协作感知架构;2）利用3D CNN和自适应特征融合进行协同感知，提出3种可训练的特征融合模型进行协同感知;3）我们使用两个公共合作感知基准数据集（OPV2V数据集[27]和CODD数据集[2]）验证了所提出的模型，用于多个任务，包括a）车辆检测，b）行人检测和c）领域适应;4）我们尝试了不同数量的CAVs，以观察其对合作感知的影响<p>从形式上讲，合作感知的问题可以描述如下。我们将来自一组周围 CAV 的原始输入数据（相机数据和 LiDAR 数据）表示为 I = {I1， I2， . . . ， In}，表示为 V = {v1， v2， . . . ， vn}。输入数据的相应提取中间特征集表示为 F = {F1， F2， . . . ， Fn}，其中目标检测器预测的输出集表示为 O = {O1， O2， . . . ， On}。在传统的视觉感知过程中，AV vi 接收来自摄像头和激光雷达等传感器的原始数据 Ii。然后对这些数据进行处理以提取特征图 Fi，以用于计算模型中用于预测对象作为输出 Oi。在协同感知中，应用额外的数据融合步骤来聚合来自其他车辆的数据，以改善感知。<p>中间融合是利用已处理的中间特征表示 F 的折衷方案。因此，准确和优化地集成和处理从不同位置获得的信息对于有效的特征融合以实现准确的目标检测至关重要。<p>Marvasti等将3D LiDAR点云扭曲为鸟瞰图（BEV），并应用2D CNN提取每个连接的AV中的中间特征。来自 CAV 的特征图被投影到自我车辆的坐标系上。然后将这些与自我车辆的特征图聚合在一起。在中，只使用了两个CAV，求和使重叠具有更大的权重，而在现实生活中，CAV的数量各不相同。我们计算重叠处的平均值。<p>Chen等提出了特征级融合方案，选择重叠处的最大值来表示中间特征。<p>上面提到的模型使用简单的约简算子，例如求和、最大池化或平均池化。这些算子能够处理重叠处的信息，并以可以忽略不计的计算成本融合特征图。然而，由于缺乏信息选择和数据相关性的识别，所选择的特征不一定是最好的。<p>在V2VNe中，应用图神经网络（GNN）基于地质坐标表示CAV地图，以促进数据融合。GNN 将从多辆车接收到的信息与车辆的内部状态（根据其自己的传感器数据计算）聚合在一起，以计算更新的中间表示。<p>Xu等提出了AttFuse，并<strong>利用自注意力模型融合了中间特征图</strong>。<strong>V2X-Vit 和 CoBEVT 中使用了 Transformer，用于与中间特征融合的协同感知</strong>。<p>我们探索了特征融合模型，这些模型可以有效和高效地利用CAV的多个特征图。<p><strong>Feature Learning and Feature Fusion</strong><p>注意力机制在解决计算机视觉任务中已经证明了它的实用性。通过在神经网络中加入一个小模块，该模型可以利用通道和/或空间信息，并增强提取的表示。<p>在协同感知中，通过特征通道连接中间特征图会随着CAV数量的增加而大大增加计算成本。<strong>因此,与其连接特征图并创建超大型特征提取网络，不如将特征图与几何和地理信息聚合起来更有效</strong>。<p><strong>Overview of the Proposed Framework</strong><p>整个网络以点云为输入，分5个步骤对数据进行处理：<p>1）特征编码利用支柱特征网络（PFN）将点云转换为伪图像;<p>2）中间特征提取利用<strong>二维金字塔网络从伪图像中提取多尺度特征</strong>;<p>3）<strong>特征投影将CAV的中间特征图投射到与LiDAR姿态信息协调的ego车辆上</strong>;<p>4）中间特征融合<strong>生成具有特征融合网络的组合特征图</strong>;<p>5）3D目标检测<strong>使用单次检测器（SSD）对边界框进行回归，并预测检测到的对象的类别</strong>。<p><strong>Feature Encoding</strong><p>维度为（n×4）的输入点云由n个点组成，每个点具有属性（x、y、z）坐标和强度。<p>点云被编码为柱子，其高度等于z轴上的点云高度。每个柱子中的点都增加了 5 个额外的属性，包括柱子中所有点的算术平均值的偏移量和柱子中心的偏移量。将点云数据转换为P柱，每个柱子具有N个点和D个特征然后将 PointNet 应用于柱子上以提取特征并生成大小的张量 （C~in~ × P ）。具有 C~in~ 特征的柱子被投射回原始位置，以生成大小的伪像 （C~in~ × 2H × 2W）。我们在这里使用 2H 和 2W，因为我们在下一个特征提取步骤中将特征图下采样为 （C × H × W ）。<p><strong>Feature Extraction</strong><p>FPN 包含三个用于多分辨率特征提取的下采样模块。每个模块包含一个 2D 卷积层、一个批量归一化层和一个 ReLU 激活函数。<p>然后，对从三个下采样模块获得的三个特征图进行上采样和连接。由CNN对得到的多尺度特征图进行细化，以生成大小为F ′ ∈ RC×H×W的特征图。<p><img alt=image-20240108181057303 data-src=https://s2.loli.net/2024/01/08/R7C6MQ8PWzI13JK.png><p><strong>Feature Projection</strong><p>从<strong>不同CAV中提取的特征图具有不同的地质位置和方向。因此，需要将它们转换为接收机的坐标系，以进行特征融合和目标检测</strong>。CAV 传播特征图及其 LiDAR 姿态信息，其中包含（X、Y、Z、滚动、偏航、俯仰）。一旦自我车辆从相邻的CAV接收到数据，它就会被投射到自我车辆的坐标系和时间戳中。<p><strong>Feature Fusion</strong><p>来自不同 CAV 的投影中间特征图被扩展为 4D 张量并连接起来进行进一步处理。在特征通道上连接特征图会生成具有 nC 通道的 3D 张量，这将增加计算复杂性和特征融合和细化的成本。因此，<strong>我们将特征图聚合为一个 4D 张量 F ∈ R^n×C×H×W^，其中 n 是 CAV 的最大数量。为了融合地质坐标系中的重叠特征图，我们提出了空间和通道特征融合模型</strong><p>我们提出的特征融合模型分为空间特征融合和通道特征融合。<p><strong>Spatial-wise Feature Fusion</strong>。为了融合特征图，将直接的约简算子（如Max或Mean）应用于重叠的特征，如图3a所示。本文将这两种融合方法称为MaxFusion和AvgFusion，它们分别在通道轴上计算最大池化和平均池化，得到融合特征图Ffusion ∈ R^1×C×H×W^<p>受信道注意力模块<strong>SENet</strong>的启发,提出了一种信道自适应特征融合（C-AdaFusion）模块，该模块可以利用<strong>信道信息选择和融合中间特征图</strong>。<p><img alt=image-20240108204441345 data-src=C:\Users\proanimer\AppData\Roaming\Typora\typora-user-images\image-20240108204441345.png><p>首先，通过计算沿第一通道轴的最大池化和平均池化，将输入特征图F ∈ R^n×C×H×W^分解为Smax ∈ R^1×C×H×W^和Savg ∈ R^1×C×H×W^。<p>将两个特征图连接在一起，得到一个四维张量F~spatial~ ∈ R^2×C×H×W^，其中包含来自原始级联中间特征图的两种空间信息。然后,利用<strong>具有ReLU激活函数的三维卷积进行进一步的特征选择和降维</strong>,输入通道数和输出通道数分别等于2和1.<p><strong>Channel-wise Feature Fusion</strong>CNN 在从表示中提取特征并减小其维度方面表现非常出色。对于输入的 4D 张量 F ∈ R^n×C×H×W^，可以使用 3D CNN 提取通道特征并减少输入特征通道的数量。3D CNN 的输入通道数将等于单个输出通道表示组合特征集的最大 CAV 数。<p><img alt=image-20240108204452386 data-src=https://s2.loli.net/2024/01/08/W5qG8EdRyZwz6lM.png><p>根据这两种不同的attention提出了c-AdaFusion.利用 3D 自适应最大池化和平均池化来提取两个通道描述符 C~max~ ∈ R^n×1×1×1^ 和 C~avg~ ∈ R^n×1×1×1^。然后，将两个向量连接起来，分别通过两个具有ReLU和Sigmoid激活函数的线性层。输入特征图F ∈ R^n×C×H×W^ 通道相乘学习的通道权重 F ′ 通道∈ R^n×1×1×1^，以生成新的特征表示 F ′ ∈ R^n×C×H×W^。融合特征图F~fusion~ ∈ R^1×C×H×W^是利用通道缩减3D CNN得到的。<p><img alt=image-20240108185354500 data-src=https://s2.loli.net/2024/01/08/9eTsclkiWp3Z4fV.png><p><strong>总结</strong><p>主要是利用了通道以及空间的fusion.我是打算再结合transformer做的.<h4 id=where2comm-2022><a title="where2comm 2022" class=headerlink href=#where2comm-2022></a>where2comm 2022</h4><h5 id=abs-1><a class=headerlink href=#abs-1 title=abs></a>abs</h5><p>多智能体协同感知可以使智能体通过通信相互共享互补信息，从而显著提升感知性能。<p>这不可避免地导致感知性能和通信带宽之间的基本权衡。为了解决这一瓶颈问题，<strong>我们提出了一种空间置信度图，该图反映了感知信息的空间异质性。它使智能体能够只共享空间稀疏但感知上至关重要的信息，从而有助于沟通的位置</strong>。<p>基于这种新颖的空间置信度图，我们提出了一种通信效率高的协作感知框架Where2comm。有两个明显的优势：<p>i）它考虑了语用压缩，并通过专注于感知关键区域来使用更少的通信来实现更高的感知性能;<p>ii）通过动态调整通信所涉及的空间区域，可以处理不同的通信带宽。为了评估 Where2comm，我们考虑了在真实世界和模拟场景中的 3D 目标检测，在四个数据集上使用两种模态（摄像头/激光雷达）和两种代理类型（汽车/无人机）：OPV2V、V2X-Sim、DAIR-V2X 和我们原始的 CoPerception-UAV。<h5 id=intro><a class=headerlink href=#intro title=intro></a>intro</h5><p>协作感知使多个智能体能够相互共享互补的感知信息，从而促进更全面的感知。它提供了一个新的方向，从根本上克服了单智能体感知的一些不可避免的局限性，如遮挡和远距离问题。在广泛的实际应用中迫切需要相关的方法和系统，例如<strong>车联网通信辅助自动驾驶</strong>、<strong>多机器人仓库自动化系统</strong>和用于<strong>搜救</strong>的多无人机。为了实现协作感知，最近的工作贡献了高质量的数据集和有效的协同方法。<p>在这个新兴领域,目前最大的挑战是如何优化感知性能和通信带宽之间的权衡。现实世界中的<strong>通信系统总是受到限制,以至于它们几乎无法承受巨大的实时通信消耗</strong>,例如通过完整的原始观测或大量特征。<p>以前的工作都做出了一个合理的假设：<strong>一旦两个智能体合作，他们就有义务平等地共享所有空间区域的感知信息。这种不必要的假设会极大地浪费带宽，因为很大一部分空间区域可能包含与感知任务无关的信息</strong>。为了填补这一空白，我们考虑了一种<strong>新颖的空间置信度感知沟通策略</strong>。其<strong>核心思想是为每个智能体启用空间置信度图，其中每个元素都反映了相应空间区域的感知临界水平</strong>。根据此地图，智能体决定要通信的空间区域（位置）。<p>Where2comm 包括三个关键模块：<p>i） 空间置信度生成器,它<strong>生成空间置信度图以指示感知关键区域</strong>;<p>ii）空间置信度感知通信模块,<strong>利用空间置信度图通过新型消息打包决定在何处进行通信</strong>，以及<strong>通过新型通信图构建来向谁进行通信</strong>;<p>iii）空间置信感知消息融合模块,<strong>该模块使用新颖的置信感知多头注意力来融合从其他智能体接收到的所有消息,从而升级每个智能体的特征图</strong>。<p>Where2comm有两个明显的优势。首先在特征层面促进了语用压缩，并通过专注于感知关键区域，<strong>使用更少的通信来实现更高的感知性能</strong>。其次，<strong>它适应各种通信带宽和通信轮数</strong>，而以前的型号只能处理一个预定义的通信带宽和固定数量的通信轮数。<p><strong>问题定义</strong><p>考虑场景中的 N 个代理。让 X~i~ 和 Y~i~ 分别是第 i 个智能体的观察和感知监督。协作感知的目标是实现所有智能体的感知性能最大化，作为总通信点 B 和通信轮 K 的函数</p><script type="math/tex; mode=display">
\xi_{\Phi}(B,K)=\arg\max_{\theta,\mathcal{P}}\sum_{i=1}^{N}g\left(\Phi_{\theta}\left(\mathcal{X}_{i},\{\mathcal{P}_{i\rightarrow j}^{(K)}\}_{j=1}^{N}\right),\mathcal{Y}_{i}\right),\mathrm{~s.t.~}\sum_{k=1}^{K}\sum_{i=1}^{N}|\mathcal{P}_{i\rightarrow j}^{(k)}|\leq B,</script><p>在这项工作中，我们考虑了3D目标检测的感知任务，并提出了三个贡献：<p>i）我们通过设计紧凑的消息和稀疏的通信图使通信更加高效;<p>ii）我们通过实施更全面的信息融合来提高感知性能;<p>iii） 我们通过动态调整沟通地点和人员，使整个系统能够适应不同的沟通条件。<p><img alt=image-20240109171737063 data-src=https://s2.loli.net/2024/01/09/1fg5jPZS6lUhmxz.png><p><strong>observation encoder</strong>从传感器数据中提取特征图。Where2comm 接受单模态/多模态输入，例如 RGB 图像和 3D 点云。这项工作采用鸟瞰图（BEV）中的特征表示，其中所有智能体将其个人感知信息投射到同一个全局坐标系，避免了复杂的坐标变换，并支持更好的共享跨智能体协作。<p><strong>Spatial confidence generator</strong>空间置信度图<strong>反映了各个空间区域的感知临界水平</strong>。直观地说,对于物体检测任务,包含<strong>物体的区域比背景区域更关键</strong>。在协作过程中,由于视野有限,有对象的区域可以帮助恢复误检对象;可以<strong>省略背景区域以节省宝贵的带宽。因此,我们用检测置信度图表示空间置信度图，其中感知临界水平高的区域是包含置信度得分高的对象的区域。</strong><p>传感器位置编码表示每个智能体的传感器与其观察点之间的物理距离。<strong>它采用以感应距离和特征尺寸为条件的标准位置编码功能</strong>。在输入到transformer之前，将这些特征与每个位置的位置编码相加。<p>与现有不使用注意力机制或仅使用智能体级注意力的融合模块相比，所提出的融合所采用的<strong>每位置注意力机制强调位置特异性特征交互。它使特征融合更具针对性</strong>。与同样使用基于每个位置注意力的融合模块的方法相比，所提出的融合模块利用了具有两个额外先验的多头注意力，<strong>包括空间置信度图和感知距离</strong>。两者都有助于注意力学习，以偏爱高质量和关键功能<p>空间置信度感知消息融合的目标是通过聚合从其他代理接收到的消息来增强每个代理的功能。为了实现这一点，我们采用了 transformer 架构，该架构利用多头注意力来融合来自每个单独空间位置的多个智能体的相应特征<p><strong>Spatial confidence-aware message fusion</strong></p><script type="math/tex; mode=display">
\mathbf{C}_i^{(k)}=\Phi_{\mathrm{generator}}(\mathcal{F}_i^{(k)})\in[0,1]^{H\times W}</script><script type="math/tex; mode=display">
\mathbf{W}_{j\rightarrow i}^{(k)}=\mathrm{MHA}_{\mathrm{W}}\left(\mathcal{F}_{i}^{(k)},\mathcal{Z}_{j\rightarrow i}^{(k)},\mathcal{Z}_{j\rightarrow i}^{(k)}\right)\odot\mathbf{C}_{j}^{(k)}\in\mathbb{R}^{H\times W},</script><script type="math/tex; mode=display">
\mathcal{F}_{i}^{(k+1)}=\mathrm{FFN}\left(\sum_{j\in\mathcal{N}_{i}\bigcup\{i\}}\mathbf{W}_{j\rightarrow i}^{(k)}\odot\mathcal{Z}_{j\rightarrow i}^{(k)}\right)\in\mathbb{R}^{H\times W\times D},</script><p><strong>总结</strong>很好的一篇论文,利用了其他车的输出作为confidence map再结合多头注意力进行协同感知.<h3 id=V2X-ViT-Vehicle-to-Everything-Cooperative-Perception-with-Vision-Transformer-2022><a title="V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer 2022" class=headerlink href=#V2X-ViT-Vehicle-to-Everything-Cooperative-Perception-with-Vision-Transformer-2022></a>V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer 2022</h3><p>这篇文章主要强调车与异构设备之间进行协同感知,提出新的改进的注意力机制,所以这里就不着重讲了.<ul><li>我们提出了第一个用于V2X感知的统一Transformer架构（V2X-ViT），该架构可以捕获V2X系统的异构性，对各种噪声具有很强的鲁棒性。<li>此外，所提模型在具有挑战性的协同检测任务中取得了最先进的性能。– 我们提出了一种新型的异构多智能体注意力模块（HMSA），用于异构智能体之间的自适应信息融合。<li>我们提出了一种新的多尺度窗口注意力模块 （MSwin），该模块可同时并行捕获局部和全局空间特征交互。– 我们构建了 V2XSet，这是一个用于 V2X 感知的新的大规模开放仿真数据集，它明确地解释了不完美的现实世界条件。</ul><p><strong>OPV2V</strong>本文提出了Attentive Fusion<p>由于来自不同联网车辆的传感器观测结果可能带有不同的噪声水平（例如，由于车辆之间的距离），因此，一种<strong>既能关注重要观测值又能忽略中断观测值的方法对于稳健的检测至关重要</strong>。因此，我们提出了一个中间融合管道来捕捉相邻连接车辆特征之间的相互作用，帮助网络关注关键观测。<p>首先广播每个CAV的相对姿态和外在性,以构建一个空间图,其中每个节点都是通信范围内的一个CAV,每个边代表一对节点之间的通信通道。<p>构建图形后，将在组中选择一辆 ego 车辆. 所有相邻的 CAV 将自己的点云投射到自我车辆的 LiDAR 框架上，并根据投影的点云提取特征。这里的特征提取器可以是现有 3D 对象检测器的backbone.<p>采用自注意力模型来融合这些解压缩特征。同一特征图中的每个特征向量（对应于原始点云中的某些空间区域。因此<strong>，简单地展平特征图并计算特征的加权总和将破坏空间相关性。取而代之的是，我们为特征图中的每个特征向量构建一个局部图，其中边是为来自不同连接车辆的相同空间位置的特征向量构建的。</strong><h3 id=Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds><a title="Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds" class=headerlink href=#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds></a>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</h3><h4 id=Abs-4><a class=headerlink href=#Abs-4 title=Abs></a>Abs</h4><p>自动驾驶汽车可能会因为检测和识别不准确而做出错误的决定。因此，<strong>智能车辆可以将自身数据与其他车辆的数据相结合，增强感知能力，从而提高探测精度和驾驶安全性</strong>。然而，<strong>多车协同感知需要整合真实世界的场景，而原始传感器数据交换的流量远远超过了现有车辆网络的带宽。</strong>据我们所知，我们是<strong>第一个对原始数据层面的合作感知进行研究，以提高自动驾驶系统的检测能力</strong>。<p>在这项工作中，我们<strong>以激光雷达三维点云为依托，融合从联网车辆的不同位置和角度收集的传感器数据</strong>。我们提出了一种基于点云的三维物体检测方法，可用于多种对齐点。<p>然而，到目前为止，人类驾驶的汽车与自动驾驶汽车之间的大多数比较都是不平衡的，包含各种不公平的因素。自动驾驶汽车不会感到疲劳、情绪衰弱，如愤怒或沮丧。但是，它们无法像细心和经验丰富的人类驾驶员那样，在不确定和模糊的情况下做出熟练的反应或预测<h3 id=Proposed-Solution><a title="Proposed Solution" class=headerlink href=#Proposed-Solution></a>Proposed Solution</h3><p>为了解决这个问题，我们研究了其中一个基础类别，即原始数据的低级融合。<strong>原始传感数据是自动驾驶汽车上所有传感器的组成部分，因此非常适合在不同制造商的不同汽车之间传输。因此，不同数据处理算法的异质性不会影响车辆之间共享数据的准确性</strong>。由于自动驾驶本身就是一项至关重要的任务，与车辆的集成度如此之高，即使是一个小小的检测错误也可能导致灾难性的事故。因此，我们需要自动驾驶汽车尽可能清晰地感知环境。为了实现这一最终目标，它们需要一个强大而可靠的感知系统。<p>在此过程中，我们要解决以下两个主要问题：(1) 我们需要在车辆之间共享的数据类型，以及 (2) 需要传输的数据量与接收车辆实际需要的数据量。<p>第一个问题是<strong>汽车数据集中的可共享数据</strong>。第二个问题<strong>是每辆车产生的数据量巨大。由于每辆自动驾驶汽车每天都会收集超过 1000GB 的数据</strong>，因此只收集区域数据变得更加困难。同样，重建附近感知系统从不同位置和角度收集的共享数据也是另一大挑战。<p>在不同类型的原始数据中，我们建议使用激光雷达（LiDAR）点云作为解决方案，原因如下：<ul><li>与二维图像和视频相比，<strong>激光雷达点云具有空间维度的优势</strong>。<li>在保留感知对象的准确模型的同时，对实体或私人数据（如人脸和车牌号码）进行本机混淆。<li>由于数据是由点而不是像素组成的，因此在图像和视频融合过程中具有多功能性。<strong>对于图像或视频融合来说，要求有一个清晰的重叠区，而点云数据则不需要重叠区，这使得点云数据成为一种更稳健的选择，尤其是在考虑到汽车的不同视角时</strong>。</ul><h5 id=贡献><a class=headerlink href=#贡献 title=贡献></a>贡献</h5><p>不准确的物体检测和识别是实现强大而有效的感知系统的主要障碍。自动驾驶汽车最终会屈服于这种能力，无法实现预期结果，这对自动驾驶是不安全的。为了解决这些问题，我们提出了一种解决方案，即<strong>自动驾驶车辆将自身的感知数据与其他联网车辆的感知数据相结合，以帮助增强感知能力</strong>。我们还认为，如前所述，<strong>数据冗余是解决这一问题的方法，我们可以通过自动驾驶车辆之间的数据共享和组合来实现</strong>。我们提出的 Cooper 系统可以提高探测性能和驾驶体验，从而提供保护和安全。具体来说，我们的贡献如下:<ul><li>我们提出了稀疏点云物体检测（SPOD）方法，用于<strong>检测低密度点云数据中的物体</strong>。虽然 SPOD 是针对低密度点云设计的，但它也适用于高密度激光雷达数据。<li>我们展示了所提出的 Cooper 系统如何通过扩大感知区域和提高检测精度来超越单个感知。<li>我们证明，可以利用现有的车载网络技术来促进车辆之间兴趣区域 (ROI) 激光雷达数据的传输，从而实现合作感知。</ul><p>鉴于当前自动驾驶汽车数据融合领域的前景和工作，我们需要更进一步，定义我们眼中的合作传感。我们认为，自动驾驶汽车的合作传感将带来一系列挑战和益处，这将是发展过程中不可避免的一部分。<h3 id=F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds><a title="F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds" class=headerlink href=#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds></a>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</h3><h4 id=Abs-5><a class=headerlink href=#Abs-5 title=Abs></a>Abs</h4><p>自动驾驶汽车在很大程度上依赖于传感器来完善对周围环境的感知，然而，就目前的技术水平而言，汽车所使用的数据仅限于来自自身传感器的数据.车辆和/或边缘服务器之间的数据共享受到可用网络带宽和自动驾驶应用严格的实时性限制。<p>为了解决这些问题，我们为联网自动驾驶汽车提出了<strong>基于点云特征的合作感知框架</strong>（F-Cooper），以实现更高的目标检测精度。<p>基于特征的数据不仅足以满足训练过程的需要，我们还利用特征的固有小尺寸来实现实时边缘计算，而不会面临网络拥塞的风险。<p>我们的实验结果表明，通过融合特征，我们能够获得更好的物体检测结果，20 米内的检测结果提高了约 10%，更远距离的检测结果提高了 30%，同时还能以较低的通信延迟实现更快的边缘计算，在某些特征选择中只需 71 毫秒。<h4 id=Intro><a class=headerlink href=#Intro title=Intro></a>Intro</h4><p>互联自动驾驶汽车（CAV）为改善道路安全提供了一个前景广阔的解决方案。这有赖于车辆能够实时感知路况并精确探测物体。<p>然而，准确和实时的感知在现场具有挑战性。它需要处理来自各种传感器的大量连续数据流，并有严格的时间要求。此外，车辆的感知精度往往会受到传感器有限视角和范围的影响。<h4 id=Proposed-Solution-1><a title="Proposed Solution" class=headerlink href=#Proposed-Solution-1></a>Proposed Solution</h4><p>我们提出的方法可以提高自动驾驶车辆的检测精度，同时不会带来太多的计算开销。一个有益的启示是，现代自动驾驶车辆的物体检测技术，<strong>无论是基于图像的还是基于三维激光雷达数据的，通常都采用卷积神经网络（CNN）来处理原始数据，并利用区域建议网络（RPN）来检测物体</strong>。我们认为，特征图的能力尚未得到充分挖掘，特别是对于自动驾驶车辆上生成的 3D LiDAR 数据，因为特征图仅用于单个车辆的物体检测。<p>为此，我们引入了基于特征的协同感知（FCooper）框架，利用特征级融合实现端到端的三维物体检测，从而提高检测精度。我们的 F-Cooper 框架支持两种不同的融合方案：<strong>体素特征融合和空间特征融合</strong>。<p>与原始数据级融合解决方案[3]相比，<strong>前者实现了几乎相同的检测精度提升，而后者则提供了动态调整待传输特征图大小的能力</strong>。F-Cooper 的独特之处在于它可以在车载和路边边缘系统上部署和执行。<p>除了能够<strong>提高检测精度</strong>外，<strong>特征融合所需的数据大小仅为原始数据的百分之一</strong>。对于一个典型的激光雷达传感器来说，每个激光雷达帧包含约 100,000 个点，约为 4 MB。对于任何现有的无线网络基础设施来说，如此庞大的数据量都将成为沉重的负担。<p>要确认特征对融合的有用性，我们必须回答以下三个基本问题。<p>1) 特征是否具备融合的必要手段？<br>2) 我们能否通过特征在自动驾驶车辆之间有效地交流数据？<br>3) 如果特征满足前面两个要求，那么我们从自动驾驶车辆中获取特征图的难度有多大？<h4 id=Fusion-Characteristics><a title="Fusion Characteristics" class=headerlink href=#Fusion-Characteristics></a>Fusion Characteristics</h4><p>受致力于融合不同层生成的特征图的研究成果（如特征金字塔网络（FPN） 和级联 R-CNN [2]）的启发，我们发现<strong>在不同的特征图中检测物体是可能的。例如，FPN 采用自上而下的金字塔结构特征图进行检测。这些网络非常善于复合特征融合的效率</strong>。<p>从这些著作中汲取灵感，我们假设兼容融合的汽车<strong>将使用相同的检测模型</strong>。这一点非常重要，因为我们看到只有最可靠的检测模型才会被用于自动驾驶。有了这个假设，我们现在来看看融合的特点<h4 id=Compression-and-Transmission><a title="Compression and Transmission" class=headerlink href=#Compression-and-Transmission></a>Compression and Transmission</h4><p>与原始数据相比，特征地图的另一个优势在于车辆之间的传输过程。原始数据可能有多种不同的格式，但它们都能达到一个目的，那就是保留所捕获数据的原始状态。例如，从驾驶过程中获取的激光雷达数据将存储驾驶过程中沿途的所有点云。不过，这种存储格式<strong>会将不必要的数据与基本数据一起记录下来</strong>；而特征地图则避免了这一问题.<p>在 CNN 网络处理原始数据的过程中，所有无关数据都会被网络过滤掉，只留下可能被网络用于物体检测的信息。<strong>这些特征图存储在稀疏矩阵中，只存储被认为有用的数据，任何被过滤掉的数据在矩阵中都存储为 0。</strong><p>通过<strong>无损压缩（如 gzip 压缩方法），数据大小的优势会进一步扩大，如文献[14]所示。再加上稀疏矩阵的特性，我们就能将二者结合起来，实现压缩后的特征数据不超过 1 MB</strong>，使特征数据成为部署 On-Edge 融合的最佳选择。<h4 id=Generic-and-Inherent-Properties><a title="Generic and Inherent Properties" class=headerlink href=#Generic-and-Inherent-Properties></a>Generic and Inherent Properties</h4><p>所有自动驾驶车辆都必须根据传感器生成的数据做出决策。<strong>原始数据由车辆上的物理传感器生成，然后传送到车载计算设备。在那里，原始数据通过基于 CNN 的深度学习网络进行处理，最终做出驾驶决策。</strong>在此过程中，<strong>我们可以提取提取的特征进行共享。这样，我们就能有效地获得原始数据的特征图，而无需额外的计算时间或车载计算设备的功率</strong>。迄今为止，几乎所有已知的自动驾驶车辆都使用了基于 CNN 的网络，因此特征提取是通用的，在融合之前无需进一步处理。<p>得益于自动驾驶车辆处理数据的方式，我们能够直接从原始激光雷达点云数据中提取处理后的特征图进行融合，因为这本身就提供了位置数据。<strong>只要激光雷达传感器已经按照自动驾驶所需的标准进行了校准，那么我们就能获得能够保留所有物体与车辆相对位置的特征地图</strong>。<p>为了融合两辆汽车的三维特征，设计了两种融合范式：体素特征融合和空间特征融合。在范式 I 中，首先融合两组体素特征，然后生成空间特征图。<h4 id=Voxel-Features-Fusion><a title="Voxel Features Fusion" class=headerlink href=#Voxel-Features-Fusion></a>Voxel Features Fusion</h4><p>与位图中的像素一样，体素代表三维空间中规则立方体上的一个数值。在一个体素内，可能有零个或多个由激光雷达传感器生成的点云。对于至少包含一个点的任何体素，VoxelNet 的 VFE 层可以生成一个体素特征<p>假设原始激光雷达检测区域被划分为一个体素网格。<p>在这些体素中，我们将获得绝大多数空体素，而剩余的体素则包含关键信息。所有非空的体素都会通过一系列全连接层进行转换，并转化为长度为 128 的固定大小的矢量。固定大小的向量通常被称为特征图。<blockquote><p>为了提高内存/计算效率，我们将非空体素的特征保存到哈希表中，并将体素坐标作为哈希键。由于我们的重点主要是自动驾驶，因此我们只将非空体素存储到哈希表中。</blockquote><p>在 VFF 中，我们明确地将来自两个输入的所有体素的特征结合起来。具体来说，来自汽车 1 的体素 3 和来自汽车 2 的体素 5 共享相同的校准位置。<blockquote><p>虽然两辆车的物理位置不同，但它们共享同一个配准的三维空间，不同的偏移量表示每辆车在所述三维标定空间中的相对物理位置。为此，我们采用了element-wise maxout来融合体素 3 和体素 5。</blockquote><p><img alt=image-20231129091921213 data-src=https://s2.loli.net/2023/11/30/pKu3lxqZ489NEk2.png><p>受卷积神经网络的启发，使用 maxout 进行潜在规模选择，提取明显的特征，同时抑制对三维空间检测无益的特征，从而实现更小的数据量。在我们的实验中，我们使用 maxout 来决定在比较车辆间的数据时哪个特征最突出。<h4 id=Spatial-Feature-Fusion><a title="Spatial Feature Fusion" class=headerlink href=#Spatial-Feature-Fusion></a>Spatial Feature Fusion</h4><p>VFF 需要考虑两辆车所有体素的特征，这涉及车辆之间的大量数据交换。<strong>为了进一步减少网络流量，同时保持基于特征融合的优势，我们设计了一种空间特征融合（SFF）方案</strong>。与 VFF 相比，SFF 融合的是空间特征图，与体素特征图相比，空间特征图更为稀疏，因此更容易压缩以进行通信。<p>与 VFF 不同，我们对每辆车上的体素特征进行预处理，以获得空间特征。接下来，将两个源空间特征融合在一起，并将融合后的空间特征转发给 RPN，以进行区域建议和目标检测。<p><img alt=image-20231129210539499 data-src=https://s2.loli.net/2023/11/29/W7JioLQIcyADPuk.png><p>特征学习网络的输出是一个稀疏张量，其形状为 128 × 10 × 400 × 352。为了整合所有体素特征，我们采用了三个三维卷积层，依次获得语义信息更多的较小特征图，大小为 64 × 2 × 400 × 352。然而，生成的特征无法满足传统区域建议网络的形状要求。为此，必须将输出重新塑造为 128 × 400 × 352 大小的三维特征图，然后才能将其转发给 RPN。<p>对于 SFF，我们生成一个更大的检测范围，大小为 W × H，其中 W > W1，H > H1。接下来，对重叠区域进行融合，同时保留非重叠区域的原始特征。假设 GPS 将汽车 1 的实际位置记录为 (x1，y1)，汽车 2 的实际位置记录为 (x2，y2)，如果 x2 + H1, y2 - W1 2 属于 2 号车的特征图，而左上角代表 1 号车的特征图，那么我们就可以得到左上角的位置。那么我们就很容易确定重叠区域。与 VFF 采用 maxout 策略类似，我们在 SFF 中也采用了 maxout 来融合重叠的空间特征。<p><img alt=image-20231129215351930 data-src=https://s2.loli.net/2023/11/29/7O2RQfyAFrawYEJ.png style=zoom:50%;><p>最后，我们采用区域建议网络在融合特征图上提出潜在区域。<blockquote><p>SENet 等最新研究表明，不同的通道具有不同的权重。也就是说，特征图中的某些通道对分类/检测的贡献更大，而其他通道则是多余或不需要的。</blockquote><p>选择从全部 128 个通道中选择部分通道进行传输。我们假定自动驾驶汽车装配了与实际应用中相同的训练有素的检测模型。<h4 id=使用融合特征进行目标检测><a class=headerlink href=#使用融合特征进行目标检测 title=使用融合特征进行目标检测></a>使用融合特征进行目标检测</h4><p>为了检测车辆，我们将合成特征图输入区域建议网络（RPN）进行对象建议。然后应用损失函数进行网络训练。<h4 id=区域建议网络><a class=headerlink href=#区域建议网络 title=区域建议网络></a>区域建议网络</h4><p>RPN:区域建议网络。不管是采用体素融合范式还是空间融合范式，当我们得到空间特征图后，都会将其送入区域提议网络（RPN）。通过 RPN 网络后，我们将得到两个损失函数的输出结果：<p>(1) 提议感兴趣区域的概率分数 p∈ [0, 1]；<p>(2) 提议区域的位置 P = (Px , Pw , Pz , Pl , Pw , Ph, Pθ ) ，其中 Px , Py , Pz 表示提议区域的中心，(Pl , Pw , Ph, Pθ ) 分别表示长度、宽度、高度和旋转角度。<h4 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h4><p>损失函数由两部分组成：分类损失 Lcls 和回归损失 Lreg。<p><img alt=img data-src=https://img-blog.csdnimg.cn/20190827164053851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E4MTIwNzM0Nzk=,size_16,color_FFFFFF,t_70><p>ground-truth bounding box,即gt-box表示为G = Gx , Gy, Gz, Gl , Gw , Gh, Gθ 其中，Gx , Gy , Gz 表示方框的中心点，（Gl , Gw , Gh, Gθ ）分别表示长度、宽度、高度和偏航旋转角<p>输出的值包括<p><img alt=image-20231129221138541 data-src=https://s2.loli.net/2023/11/29/kgRJEplj1QUroaB.png><p><img alt=image-20231129225123172 data-src=https://s2.loli.net/2023/11/29/NVbn6aGFShDjPke.png><p>损失可以表示为<p><img alt=image-20231129225143741 data-src=https://s2.loli.net/2023/11/29/oSfcDkQhd3g1zyF.png><h4 id=数据集><a class=headerlink href=#数据集 title=数据集></a>数据集</h4><p><strong>KITTI</strong><p>由于我们的重点是三维物体检测，因此我们使用了 KITTI 数据集提供的三维 Velodyne 点云数据。<p>云点数据<strong>每帧提供 100K 个点</strong>，并存储在二进制浮点矩阵中。<strong>数据包括每个点的三维位置和相关的反射率信息</strong>。<strong>但是，由于 KITTI 数据是由单个车辆记录的，我们必须利用同一记录中的不同时间段来模拟由两辆车生成的数据</strong>。因此，KITTI 数据只适用于某些测试场景。<p>为了解决这个问题，我们在两辆名为汤姆和杰瑞（T&J）的车辆上安装了必要的传感器，如激光雷达（Velodyne VLP-16）、摄像头（Allied Vision Mako G-319C）、雷达（Delphi ESR 2.5）、IMU&GPS（Xsens MTi-G-710 套件）和边缘计算设备（Nvidia Drive PX2），以便在我们学校的校园内收集所需的数据。我们的车辆配有 16 波束 Velodyn 激光雷达传感器，以二进制原始以太网数据包的形式存储数据。由于我们的车辆可以相互独立移动，因此我们能够用两辆车在真实环境中测试各种场景。<h4 id=训练细节><a class=headerlink href=#训练细节 title=训练细节></a>训练细节</h4><p>在停车场环境中，我们将距离车辆 20 米以内的物体视为高优先级物体，20 米以外的物体视为低优先级物体。<p>由于我们的激光雷达传感器只有 16 个光束，因此与更高端的激光雷达传感器相比，得到的点云数据相对稀疏。为了减轻稀疏数据带来的负面影响，我们将探测范围限制在沿 <strong>X、Y 和 Z 轴[0,70.4]X[-40,40] X [-3,1]</strong> 。我们不使用超出探测范围的数据。除了车辆的检测范围外，我们还<strong>将体素大小设置为 vD = 0.4 米、vH = 0.2 米、vW = 0.2 米，因此 D1 = 10、H1 = 400、W1 = 352</strong>。在我们的实验中，F-Cooper 框架在配备 GeForce GTX 1080 Ti GPU 的计算机上运行。<p>为了评估 F-Cooper，我们在实验中收集并测试了 200 多组数据。根据处理激光雷达数据的方法，我们将测试分为四类，方法（1）到（3）均来自相同的检测模型：（1）作为基线的非融合，（2）带有 VFF 的 F-Cooper，（3）带有 SFF 的 F-Cooper，以及（4）原始点云融合方法 - Cooper。特征融合在上述四种情况中随机进行，重点放在繁忙的校园停车场，因为由于遮挡物较多，这是最困难的情况。<p>后面结合OpenCood这个项目代码学习学习 <a href=https://github.com/DerrickXuNu/OpenCOOD rel=noopener target=_blank>ICRA 2022] An opensource framework for cooperative detection. Official implementation for OPV2V. (github.com)</a><h3 id=Learning-Distilled-Collaboration-Graph-for-Multi-Agent-Perception-2021><a title="Learning Distilled Collaboration Graph for Multi-Agent Perception 2021" class=headerlink href=#Learning-Distilled-Collaboration-Graph-for-Multi-Agent-Perception-2021></a>Learning Distilled Collaboration Graph for Multi-Agent Perception 2021</h3><h4 id=abs-2><a class=headerlink href=#abs-2 title=abs></a>abs</h4><p>为了促进多智能体感知更好的性能-带宽权衡，我们提出了一种新的提取协作图（DiscoGraph）来建模智能体之间的可训练、姿势感知和自适应协作。我们的主要新颖之处在于两个方面。<p>关键是用了类似蒸馏概念,使用小模型代替大模型.<p><img alt=image-20240518104759638 data-src=https://s2.loli.net/2024/05/18/Jt7BYCRpnbIWvPL.png><p><img alt=image-20240518104810361 data-src=https://s2.loli.net/2024/05/18/hRt384OIHjLkFyw.png><h3 id=Spatio-Temporal-Domain-Awareness-for-Multi-Agent-Collaborative-Perception><a title="Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception" class=headerlink href=#Spatio-Temporal-Domain-Awareness-for-Multi-Agent-Collaborative-Perception></a>Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception</h3><h4 id=abstract><a class=headerlink href=#abstract title=abstract></a>abstract</h4><p>作为 “车对车 “通信的一种潜在应用，多代理协作感知可以显著提高自动驾驶车辆的感知性能，而不是单代理感知。然而，在这项新兴研究中，要实现实用的信息共享仍面临一些挑战。在本文中，<strong>我们提出了一种新颖的协作感知框架 SCOPE，它能以端到端的方式聚合路面代理的时空感知特征。</strong><p>具体来说，SCOPE 有三个独特的优势：i) 它考虑了时间背景的有效语义线索，以增强目标代理的当前表征；ii) 它聚合了来自异构代理的感知关键空间信息，并通过多尺度特征交互克服定位误差；iii) 它通过自适应融合范式，根据目标代理的互补贡献，整合了目标代理的多源表征。<p>为了全面评估 SCOPE，我们在三个数据集上考虑了协作 3D 物体检测任务的真实世界和模拟场景。广泛的实验表明了我们的方法的优越性和所提出组件的必要性。<p><img alt=image-20240508150656435 data-src=https://s2.loli.net/2024/05/08/CInhm62SfjXLK5w.png></p><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2025\01\10\协同感知算法-四-大模型、多模态以及新趋势\ rel=bookmark>协同感知算法(四):大模型、多模态以及新趋势</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\06\30\协作感知算法-三\ rel=bookmark>协作感知算法:三</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\23\协同感知数据集介绍\ rel=bookmark>协同感知数据集和代码库介绍</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2024\05\17\协同感知算法-二\ rel=bookmark>协同感知学习(二)</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/11/30/%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E7%AE%97%E6%B3%95-%E4%B8%80/ title=协同感知学习(一)>https://www.sekyoro.top/2023/11/30/协同感知算法-一/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/collaborative-perception/ rel=tag><i class="fa fa-tag"></i> collaborative perception</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/11/30/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%8D%8F%E5%90%8C%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88%E7%AE%97%E6%B3%95/ rel=prev title=图像特征协同感知融合算法> <i class="fa fa-chevron-left"></i> 图像特征协同感知融合算法 </a></div><div class=post-nav-item><a href=/2023/12/03/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%8E%E9%87%8D%E6%9E%84/ rel=next title=设计模式与重构> 设计模式与重构 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-3"><a class=nav-link href=#OPV2V-An-Open-Benchmark-Dataset-and-Fusion-Pipeline-for-Perception-with-Vehicle-to-Vehicle-Communication><span class=nav-number>1.</span> <span class=nav-text>OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abs><span class=nav-number>1.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D><span class=nav-number>1.2.</span> <span class=nav-text>融合方法介绍</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#F-cooper><span class=nav-number>2.</span> <span class=nav-text>F-cooper</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#V2VNet-Vehicle-to-Vehicle-Communication-for-Joint-Perception-and-Prediction-2020><span class=nav-number>3.</span> <span class=nav-text>V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction 2020</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Abs><span class=nav-number>3.0.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E6%80%BB%E7%BB%93><span class=nav-number>3.0.2.</span> <span class=nav-text>总结</span></a></ol></ol><li class="nav-item nav-level-3"><a class=nav-link href=#DiscoNet><span class=nav-number>4.</span> <span class=nav-text>DiscoNet</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#CoBEVT-2022><span class=nav-number>5.</span> <span class=nav-text>CoBEVT 2022</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Related-Work><span class=nav-number>5.1.</span> <span class=nav-text>Related Work</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Fused-Axial-Attention><span class=nav-number>5.2.</span> <span class=nav-text>Fused Axial Attention</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SinBEVT-for-Single-agent-BEV-Feature-Computation><span class=nav-number>5.3.</span> <span class=nav-text>SinBEVT for Single-agent BEV Feature Computation</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#FuseBEVT-for-Multi-agent-BEV-Feature-Fusion><span class=nav-number>5.3.1.</span> <span class=nav-text>FuseBEVT for Multi-agent BEV Feature Fusion</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#%E6%80%BB%E7%BB%93-1><span class=nav-number>5.3.2.</span> <span class=nav-text>总结</span></a></ol></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Robust-Collaborative-3D-Object-Detection-in-Presence-of-Pose-Errors-CoAlign-2022><span class=nav-number>6.</span> <span class=nav-text>Robust Collaborative 3D Object Detection in Presence of Pose Errors (CoAlign) 2022</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Abs-1><span class=nav-number>6.0.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Agent-Object-Pose-Graph-Optimization><span class=nav-number>6.0.2.</span> <span class=nav-text>Agent-Object Pose Graph Optimization</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#Multiscale-Feature-Fusion><span class=nav-number>6.0.3.</span> <span class=nav-text>Multiscale Feature Fusion</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Learning-for-Vehicle-to-Vehicle-Cooperative-Perception-under-Lossy-Communication-V2VAM-2023><span class=nav-number>7.</span> <span class=nav-text>Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication (V2VAM) 2023</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Abs-2><span class=nav-number>7.0.1.</span> <span class=nav-text>Abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Adaptive-Feature-Fusion-for-Cooperative-Perception-using-LiDAR-Point-Clouds-2023><span class=nav-number>8.</span> <span class=nav-text>Adaptive Feature Fusion for Cooperative Perception using LiDAR Point Clouds 2023</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#Abs-3><span class=nav-number>8.0.1.</span> <span class=nav-text>Abs</span></a></ol><li class="nav-item nav-level-4"><a class=nav-link href=#where2comm-2022><span class=nav-number>8.1.</span> <span class=nav-text>where2comm 2022</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#abs-1><span class=nav-number>8.1.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-5"><a class=nav-link href=#intro><span class=nav-number>8.1.2.</span> <span class=nav-text>intro</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#V2X-ViT-Vehicle-to-Everything-Cooperative-Perception-with-Vision-Transformer-2022><span class=nav-number>9.</span> <span class=nav-text>V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer 2022</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Cooper-Cooperative-Perception-for-Connected-Autonomous-Vehicles-based-on-3D-Point-Clouds><span class=nav-number>10.</span> <span class=nav-text>Cooper: Cooperative Perception for Connected Autonomous Vehicles based on 3D Point Clouds</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Abs-4><span class=nav-number>10.1.</span> <span class=nav-text>Abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Proposed-Solution><span class=nav-number>11.</span> <span class=nav-text>Proposed Solution</span></a><ol class=nav-child><li class="nav-item nav-level-5"><a class=nav-link href=#%E8%B4%A1%E7%8C%AE><span class=nav-number>11.0.1.</span> <span class=nav-text>贡献</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#F-Cooper-Feature-based-Cooperative-Perception-for-Autonomous-Vehicle-Edge-Computing-System-Using-3D-Point-Clouds><span class=nav-number>12.</span> <span class=nav-text>F-Cooper: Feature based Cooperative Perception for Autonomous Vehicle Edge Computing System Using 3D Point Clouds</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Abs-5><span class=nav-number>12.1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Intro><span class=nav-number>12.2.</span> <span class=nav-text>Intro</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Proposed-Solution-1><span class=nav-number>12.3.</span> <span class=nav-text>Proposed Solution</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Fusion-Characteristics><span class=nav-number>12.4.</span> <span class=nav-text>Fusion Characteristics</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Compression-and-Transmission><span class=nav-number>12.5.</span> <span class=nav-text>Compression and Transmission</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Generic-and-Inherent-Properties><span class=nav-number>12.6.</span> <span class=nav-text>Generic and Inherent Properties</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Voxel-Features-Fusion><span class=nav-number>12.7.</span> <span class=nav-text>Voxel Features Fusion</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Spatial-Feature-Fusion><span class=nav-number>12.8.</span> <span class=nav-text>Spatial Feature Fusion</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E4%BD%BF%E7%94%A8%E8%9E%8D%E5%90%88%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B><span class=nav-number>12.9.</span> <span class=nav-text>使用融合特征进行目标检测</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E5%8C%BA%E5%9F%9F%E5%BB%BA%E8%AE%AE%E7%BD%91%E7%BB%9C><span class=nav-number>12.10.</span> <span class=nav-text>区域建议网络</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>12.11.</span> <span class=nav-text>损失函数</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%95%B0%E6%8D%AE%E9%9B%86><span class=nav-number>12.12.</span> <span class=nav-text>数据集</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82><span class=nav-number>12.13.</span> <span class=nav-text>训练细节</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Learning-Distilled-Collaboration-Graph-for-Multi-Agent-Perception-2021><span class=nav-number>13.</span> <span class=nav-text>Learning Distilled Collaboration Graph for Multi-Agent Perception 2021</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abs-2><span class=nav-number>13.1.</span> <span class=nav-text>abs</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Spatio-Temporal-Domain-Awareness-for-Multi-Agent-Collaborative-Perception><span class=nav-number>14.</span> <span class=nav-text>Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#abstract><span class=nav-number>14.1.</span> <span class=nav-text>abstract</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>256</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>219</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/collaborative-perception/ rel=tag>collaborative perception</a><span class=tag-list-count>5</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2026</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>3.7m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>55:26</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>document.addEventListener("DOMContentLoaded", function() {
    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {
        var pvContainer = document.getElementById("busuanzi_container_site_pv");
        if (pvContainer && pvContainer.style.display !== "none") {
            var pvElement = document.getElementById("busuanzi_value_site_pv");
            if (pvElement) {
                pvElement.innerHTML = parseInt(pvElement.innerHTML) + countOffset;
                clearInterval(int);
            }
        }
        
        var uvContainer = document.getElementById("busuanzi_container_site_uv");
        if (uvContainer && window.getComputedStyle(uvContainer).display !== "none")
        {
            var uvElement = document.getElementById("busuanzi_value_site_uv");
            if (uvElement) {
                uvElement.innerHTML = parseInt(uvElement.innerHTML) + countOffset; // 加上初始数据 
                clearInterval(int); // 停止检测
            }
        }
    }
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	 '.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
  
  // Reinitialize TagCanvas for tag cloud
  if (typeof TagCanvas !== 'undefined' && document.getElementById('resCanvas')) {
    try {
      TagCanvas.textFont = 'Trebuchet MS, Helvetica';
      TagCanvas.textColour = '#333';
      TagCanvas.textHeight = 20;
      TagCanvas.outlineColour = '#E2E1D1';
      TagCanvas.maxSpeed = 0.3;
      TagCanvas.freezeActive = true;
      TagCanvas.outlineMethod = 'block';
      TagCanvas.minBrightness = 0.2;
      TagCanvas.depth = 0.92;
      TagCanvas.pulsateTo = 0.6;
      TagCanvas.initial = [0.1,-0.1];
      TagCanvas.decel = 0.98;
      TagCanvas.reverse = true;
      TagCanvas.hideTags = false;
      TagCanvas.shadow = '#ccf';
      TagCanvas.shadowBlur = 3;
      TagCanvas.weight = false;
      TagCanvas.imageScale = null;
      TagCanvas.fadeIn = 1000;
      TagCanvas.clickToFront = 600;
      TagCanvas.lock = false;
      TagCanvas.Start('resCanvas');
      TagCanvas.tc['resCanvas'].Wheel(true);
    } catch(e) {
      console.log('TagCanvas initialization failed:', e);
    }
  }
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>