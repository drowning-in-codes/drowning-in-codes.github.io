<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content=主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO. name=description><meta content=article property=og:type><meta content=目标检测学习_P3 property=og:title><meta content=https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content=主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO. property=og:description><meta content=zh_CN property=og:locale><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png property=og:image><meta content=https://s2.loli.net/2023/11/29/63TqhgSLFPbIKki.png property=og:image><meta content=https://s2.loli.net/2023/11/30/AGzmK4HVNhW95rq.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png property=og:image><meta content=https://s2.loli.net/2023/11/30/nKCZulO46oeVdJb.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png property=og:image><meta content=https://miro.medium.com/v2/resize:fit:770/1*f0p4it3vSVV_qeTJq5Jv1Q.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-box-scales.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/featurized-image-pyramid.png property=og:image><meta content=https://s2.loli.net/2023/11/29/SaJdTtXjVy5qx7f.png property=og:image><meta content=https://upload-images.jianshu.io/upload_images/18299912-aa79ebef839e6772.png?imageMogr2/auto-orient/strip|imageView2/2/w/611/format/webp property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/retina-net.png property=og:image><meta content=https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/overfeat-training.png property=og:image><meta content=https://s2.loli.net/2023/11/29/nHdKf2JCcxV6sry.png property=og:image><meta content=https://s2.loli.net/2023/11/29/b1BWorKMJTxf2R4.png property=og:image><meta content=https://s2.loli.net/2023/11/29/3DqjYcTbewiCrE6.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/focal-loss.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502194822347.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502194817818.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502200729086.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505142247768.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505144614664.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505153729910.png property=og:image><meta content=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505153747011.png property=og:image><meta content=2023-11-01T13:29:52.000Z property=article:published_time><meta content=2024-05-05T11:39:39.458Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="object detection" property=article:tag><meta content=summary name=twitter:card><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo.png name=twitter:image><link href=https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>目标检测学习_P3 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>目标检测学习_P3</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-11-01 21:29:52" datetime=2023-11-01T21:29:52+08:00>2023-11-01</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2024-05-05 19:39:39" datetime=2024-05-05T19:39:39+08:00 itemprop=dateModified>2024-05-05</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>12k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>11 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>主要写写one-stage的网络模型,涉及到SSD,RetinaNet,YOLO.<br><span id=more></span><h2 id=YOLO><a class=headerlink href=#YOLO title=YOLO></a>YOLO</h2><p>YOLO模型是构建快速实时物体探测器的第一次尝试。因为YOLO不经历区域建议步骤，并且只在有限数量的边界框上进行预测，所以它能够超快速地进行推理。<ol><li><p>残差块</p> <p>首先，将图像划分为不同的网格。每个网格的尺寸为S x S。将输入图像转换为网格的过程如下图所示。每个网格单元将检测其中出现的对象。</p><li><p>边界框线性回归</p></ol><p>边界框是高亮显示图像中具有某些属性（如宽度（bw）、高度（bh）和类别（如人、汽车、红绿灯等）的对象的轮廓，由字母C表示。边界框的中心（bx）。YOLO使用单边界框回归来预测对象的高度、宽度、中心和类别。<ol><li><p>IOU</p> <p>并集交集（IOU）是一种用于对象检测的工具，用于解释方框如何重叠。YOLO使用IOU完美地围绕对象的完美输出框。网格中的每个单元负责预测边界框及其置信度得分。如果预测的边界框与实际框相同，则IOU等于1。此技术可以消除与实际框不相等的边界框。</p></ol><p>YOLOv2:YOLOv2于2017年发布，其架构对YOLO进行了几次迭代改进，包括BatchNorm、更高分辨率和锚盒。<p>YOLOv3：于2018年发布，YOLOv3在以前的模型的基础上，为边界框预测添加客观性分数，为主干层添加连接性，并在三个不同的级别进行预测，以提高对较小对象的性能。<p>YOLOv4:YOLOv4由Alexey Bochkovskiy于2020年4月发布，其中引入了改进的功能聚合、“免费包”（带增强）、漏洞激活等改进。<p>YOLOv5:由Glenn Jocher于2020年6月发布，YOLOv5与之前的所有版本不同，因为它是PyTorch实现，而不是原始暗网的分支。与YOLO v4一样，YOLO v5具有CSP脊椎和PA-NET颈部。主要改进包括马赛克数据扩展和自动学习边界框锚定。<p>PP-YOLO：百度基于YOLO v3于2020年8月发布。PP-YOLO的主要目标是实现一种具有相对平衡的效率和有效性的对象检测器，该检测器可以直接用于当前的应用场景，而不是设计新的检测模型。<p>Scaled YOLOv4:发布于2020年11月，作者：王、博奇科夫斯基和廖。该模型使用跨阶段部分网络来增加网络大小，同时保持YOLOv4的准确性和速度。<p>PP-YOLOv2：再次由百度团队撰写并于2021年4月发布，它对PP-YOLO进行了小修改，以获得更好的性能，包括添加错误激活功能和路径聚合网络。<p>流程:<ol><li>预训练一个CNN用于图像分类任务<li>将输入图像分为SxS的块,如果一个物体的中心落入一个块cell中，该块“负责”检测该物体的存在.包括预测<strong>每个块预测碰撞盒的位置</strong>,<strong>置信度</strong>以及<strong>包含物体的概率</strong><li>位置就是(x,y,w,h),x,y是相对于cell的offset,w,h被归一化<li>置信度是<code>Pr(containing an object) x IoU(pred, truth)</code>; 其中<code>Pr</code> = 概率<li>如果一个cell包含物体,它会预测一个概率,表示这个物体属于每一类的概率Pr(the object belongs to the class C_i | containing an object),在该阶段模型仅预测每个cell的一组类概率,而与bbox无关<li>最终,一张图像包含SXSXB个bbox,每个bbox包含四个预测位置以及置信度和K个条件概率.所以预测的值shape是SXSX(5B+K)</ol><h3 id=Network-Architecture><a title="Network Architecture" class=headerlink href=#Network-Architecture></a><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo.png>Network Architecture</h3><p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png><blockquote><p>作为一个单级对象检测器，YOLO速度极快，但由于候选边界框的数量有限，它不善于识别形状不规则的对象或一组小对象。</blockquote><h3 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h3><p><img alt=image-20231129233153863 data-src=https://s2.loli.net/2023/11/29/63TqhgSLFPbIKki.png><p>损失由两部分组成，边界框偏移预测的定位损失和条件类概率的分类损失。这两部分都计算为误差平方和。<p><img alt=image-20231130101219411 data-src=https://s2.loli.net/2023/11/30/AGzmK4HVNhW95rq.png><p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png><h3 id=YOLOV2改进><a class=headerlink href=#YOLOV2改进 title=YOLOV2改进></a>YOLOV2改进</h3><p>应用了多种修改以使YOLO预测更准确、更快，包括：<p>1.BatchNorm有助于：在所有卷积层上添加批次范数，从而显著提高收敛性。<p>2.图像分辨率很重要：用高分辨率图像微调基本模型可以提高检测性能。<p>3.卷积锚盒检测：YOLOv2不是在整个特征图上预测具有完全连接层的边界盒位置，而是使用卷积层来预测锚盒的位置，就像在更快的R-CNN中一样。空间位置的预测和类概率是解耦的。总体而言，这一变化导致mAP略有下降，但召回率有所上升。<p>4.框维度的K-均值聚类：与使用手工挑选的锚框大小的更快的R-CNN不同，YOLOv2对训练数据进行K-均值集群，以在锚框维度上找到良好的先验。距离度量是根据IoU分数设计的：<p><img alt=image-20231130103908962 data-src=https://s2.loli.net/2023/11/30/nKCZulO46oeVdJb.png><p>通过聚类生成的锚框在固定数量的框的条件下提供更好的平均IoU。<p>5.直接位置预测：YOLOv2以一种不会与中心位置偏离太多的方式来制定边界框预测。如果盒子位置预测可以将盒子放置在图像的任何部分，就像在区域提案网络中一样，那么模型训练可能会变得不稳定。<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png style=zoom:50%;><p>6.添加细粒度特性：YOLOv2添加了一个直通层，将细粒度特性从早期层带到最后一个输出层。该穿透层的机制类似于ResNet中的身份映射，以从以前的层中提取更高维度的特征。这将使性能提高1%。<p>7.多尺度训练：为了训练模型对不同大小的输入图像具有鲁棒性，每10个批次随机采样一个新大小的输入维度。由于YOLOv2的conv层将输入维度下采样因子为32，因此新采样的大小是32的倍数。<p>8.轻量级基础模型：为了更快地进行预测，YOLOv2采用了轻量级基础模型DarkNet-19，该模型有19个conv层和5个最大池化层。关键是在3x3 conv层之间插入平均池和1x1 conv滤波器。<h2 id=SSD><a class=headerlink href=#SSD title=SSD></a>SSD</h2><p>Single Shot Detector（SSD；Liu等人，2016）是<strong>首次尝试使用卷积神经网络的金字塔特征层次来有效检测各种大小的对象之一</strong>。<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png><p>该模型以图像作为输入，<strong>该图像通过具有不同大小滤波器（10x10、5x5和3x3）的多个卷积层。使用来自网络不同位置的卷积层的特征图来预测边界框</strong>。它们由具有3x3滤波器的特定卷积层处理，称为额外特征层，以产生一组类似于快速R-CNN的锚框的边界框。<p>与需要对象建议的方法相比，SSD 非常简单，因为它<strong>完全省去了建议生成和随后的像素或特征重采样阶段</strong>，并将所有计算封装在一个网络中。<p><img alt=img data-src=https://miro.medium.com/v2/resize:fit:770/1*f0p4it3vSVV_qeTJq5Jv1Q.png><p>此模型<strong>主要由基础网络组成，其后是几个多尺度特征块</strong>。 <strong>基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络</strong>。<p>单发多框检测论文中选用了在分类层之前截断的VGG (<a href=http://zh.d2l.ai/chapter_references/zreferences.html#id98 rel=noopener target=_blank>Liu <em>et al.</em>, 2016</a>)，现在也常用ResNet替代。 我们可以设计基础网络，使它输出的高和宽较大。 这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。<p><strong>接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小</strong>（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。<p>通过深度神经网络分层表示图像的多尺度目标检测的设计。 由于接近顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。 简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。<h4 id=default-box的生成><a title="default box的生成" class=headerlink href=#default-box的生成></a>default box的生成</h4><p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-box-scales.png></p><script type="math/tex; mode=display">
\begin{aligned}
\text{level index: } &\ell = 1, \dots, L \\
\text{scale of boxes: } &s_\ell = s_\text{min} + \frac{s_\text{max} - s_\text{min}}{L - 1} (\ell - 1) \\
\text{aspect ratio: } &r \in \{1, 2, 3, 1/2, 1/3\}\\
\text{additional scale: } & s'_\ell = \sqrt{s_\ell s_{\ell + 1}} \text{ when } r = 1 \text{thus, 6 boxes in total.}\\
\text{width: } &w_\ell^r = s_\ell \sqrt{r} \\
\text{height: } &h_\ell^r = s_\ell / \sqrt{r} \\
\text{center location: } & (x^i_\ell, y^j_\ell) = (\frac{i+0.5}{m}, \frac{j+0.5}{n})
\end{aligned}</script><script type="math/tex; mode=display">
\mathcal{L}_\text{cls} = -\sum_{i \in \text{pos}} \mathbb{1}_{ij}^k \log(\hat{c}_i^k) - \sum_{i \in \text{neg}} \log(\hat{c}_i^0)\text{, where }\hat{c}_i^k = \text{softmax}(c_i^k)</script><p>其中1表示对于k类bbox与gt-box是否match<h4 id=损失函数-1><a class=headerlink href=#损失函数-1 title=损失函数></a>损失函数</h4><script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}_\text{loc} &= \sum_{i,j} \sum_{m\in\{x, y, w, h\}} \mathbb{1}_{ij}^\text{match}
 L_1^\text{smooth}(d_m^i - t_m^j)^2\\
L_1^\text{smooth}(x) &= \begin{cases}
    0.5 x^2             & \text{if } \vert x \vert < 1\\
    \vert x \vert - 0.5 & \text{otherwise}
\end{cases} \\
t^j_x &= (g^j_x - p^i_x) / p^i_w \\
t^j_y &= (g^j_y - p^i_y) / p^i_h \\
t^j_w &= \log(g^j_w / p^i_w) \\
t^j_h &= \log(g^j_h / p^i_h)
\end{aligned}</script><p>此外SSD使用了NMS和HHM优化训练过程.<blockquote><p>NMS:非最大值抑制有助于避免重复检测同一实例。在我们为同一对象类别获得一组匹配的边界框之后：根据置信度得分对所有边界框进行排序。丢弃置信度分数较低的方框。当存在任何剩余的边界框时，重复以下操作：<strong>贪婪地选择得分最高的边界框。跳过具有高IoU（即大于0.5）的剩余框，使用之前选择的框</strong>。<p>HNM:有些负类很容易被错误分类。我们可以在训练循环中明确地找到那些假阳性样本，并将它们包含在训练数据中，以改进分类器。</blockquote><h3 id=连结多尺度的预测><a class=headerlink href=#连结多尺度的预测 title=连结多尺度的预测></a>连结多尺度的预测</h3><p><strong>单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量</strong>。<p>在不同的尺度下,特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。 因此，不同尺度下预测输出的形状可能会有所不同。<p>除了批量大小这一维度外，其他三个维度都具有不同的尺寸。 为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br></pre><td class=code><pre><span class=line><span class=keyword>import</span> torch</span><br><span class=line><span class=keyword>import</span> torchvision</span><br><span class=line><span class=keyword>from</span> torch <span class=keyword>import</span> nn</span><br><span class=line><span class=keyword>from</span> torch.nn <span class=keyword>import</span> functional <span class=keyword>as</span> F</span><br><span class=line><span class=keyword>from</span> d2l <span class=keyword>import</span> torch <span class=keyword>as</span> d2l</span><br><span class=line></span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>cls_predictor</span>(<span class=params>num_inputs, num_anchors, num_classes</span>):</span></span><br><span class=line>    <span class=keyword>return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class=number>1</span>),</span><br><span class=line>                     kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>)</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>x, block</span>):</span></span><br><span class=line>    <span class=keyword>return</span> block(x)</span><br><span class=line></span><br><span class=line>Y1 = forward(torch.zeros((<span class=number>2</span>, <span class=number>8</span>, <span class=number>20</span>, <span class=number>20</span>)), cls_predictor(<span class=number>8</span>, <span class=number>5</span>, <span class=number>10</span>))</span><br><span class=line>Y2 = forward(torch.zeros((<span class=number>2</span>, <span class=number>16</span>, <span class=number>10</span>, <span class=number>10</span>)), cls_predictor(<span class=number>16</span>, <span class=number>3</span>, <span class=number>10</span>))</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>flatten_pred</span>(<span class=params>pred</span>):</span></span><br><span class=line>    <span class=keyword>return</span> torch.flatten(pred.permute(<span class=number>0</span>, <span class=number>2</span>, <span class=number>3</span>, <span class=number>1</span>), start_dim=<span class=number>1</span>)</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>concat_preds</span>(<span class=params>preds</span>):</span></span><br><span class=line>    <span class=keyword>return</span> torch.cat([flatten_pred(p) <span class=keyword>for</span> p <span class=keyword>in</span> preds], dim=<span class=number>1</span>)</span><br><span class=line>concat_preds([Y1, Y2]).shape</span><br></pre></table></figure><h4 id=高和宽减半块><a class=headerlink href=#高和宽减半块 title=高和宽减半块></a>高和宽减半块</h4><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>down_sample_blk</span>(<span class=params>in_channels, out_channels</span>):</span></span><br><span class=line>    blk = []</span><br><span class=line>    <span class=keyword>for</span> _ <span class=keyword>in</span> <span class=built_in>range</span>(<span class=number>2</span>):</span><br><span class=line>        blk.append(nn.Conv2d(in_channels, out_channels,</span><br><span class=line>                             kernel_size=<span class=number>3</span>, padding=<span class=number>1</span>))</span><br><span class=line>        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class=line>        blk.append(nn.ReLU())</span><br><span class=line>        in_channels = out_channels</span><br><span class=line>    blk.append(nn.MaxPool2d(<span class=number>2</span>))</span><br><span class=line>    <span class=keyword>return</span> nn.Sequential(*blk)</span><br></pre></table></figure><h4 id=FPN><a class=headerlink href=#FPN title=FPN></a>FPN</h4><p>Feature Pyramid Networks for Object Detection<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/featurized-image-pyramid.png><p>在看多尺度特征的时候注意到了这篇文章.提出了一个利用深度卷积神经网络固有的多尺度金字塔结构来以极小的计算量构建特征金字塔的网络结构<p><img alt=image-20231129231413439 data-src=https://s2.loli.net/2023/11/29/SaJdTtXjVy5qx7f.png><p><img alt=img data-src=https://upload-images.jianshu.io/upload_images/18299912-aa79ebef839e6772.png?imageMogr2/auto-orient/strip|imageView2/2/w/611/format/webp><ul><li>自下而上的路径是正常的前馈计算。<li>自上而下的路径朝着相反的方向发展，通过横向连接将粗糙但语义更强的特征图添加回更大尺寸的先前金字塔级别。</ul><p>首先，更高级别的特征在空间上更粗糙地上采样，使其大2倍。对于图像放大，本文使用了最近邻上采样。虽然有许多图像放大算法，例如使用deconv，但<strong>采用另一种图像缩放方法可能会也可能不会提高RetinaNet的性能</strong>。<p>较大的特征图<strong>经过1x1 conv层以减小通道尺寸</strong>。<p>最后，通过<strong>元素相加</strong>将这两个特征图合并。<p>根据消融研究，特征化图像金字塔设计的组件的重要性等级如下：1x1横向连接>跨多层检测对象>自上而下的富集>金字塔表示（与仅使用最底层相比）。<p>与SSD中一样，<strong>通过对每个合并的特征图进行预测，可以在所有金字塔级别中进行检测</strong>。因为预测共享相同的分类器和框回归器，所以它们都形成为具有相同的通道维度d=256。<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/retina-net.png><h4 id=OverFeat><a class=headerlink href=#OverFeat title=OverFeat></a>OverFeat</h4><p>[<a href=https://pdfs.semanticscholar.org/f2c2/fbc35d0541571f54790851de9fcd1adde085.pdf rel=noopener target=_blank>overfeat</a>]<p>Overfeat是将目标检测、定位和分类任务集成到一个卷积神经网络中的先驱模型。主要思想是（i）<strong>以滑动窗口的方式在图像的多个尺度的区域上的不同位置进行图像分类</strong>，以及（ii）使用<strong>在相同卷积层上训练的回归器来预测边界框位置</strong>。<p>（1）用一个共享的CNN（ConvNet）来同时处理图像分类，定位，检测三个任务，可以提升三个任务的表现。<p>（2）用CNN有效地实现了一个多尺度的，滑动窗口的方法，来处理任务。<p>（3）提出了一种方法，通过累积预测来求bounding boxes（而不是传统的非极大值抑制）<p><a href=https://blog.csdn.net/Gentleman_Qin/article/details/84836122 rel=noopener target=_blank>OverFeat——全卷积首次用于检测问题 (目标检测)(深度学习)(ICLR 2014）_overfeat是做什么的-CSDN博客</a><p><img alt=img data-src=https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/overfeat-training.png><h2 id=RetinaNet><a class=headerlink href=#RetinaNet title=RetinaNet></a>RetinaNet</h2><h3 id=Focal-Loss-for-Dense-Object-Detection><a title="Focal Loss for Dense Object Detection" class=headerlink href=#Focal-Loss-for-Dense-Object-Detection></a>Focal Loss for Dense Object Detection</h3><p>在损失函数上进行改进.对象检测模型训练的一<strong>个问题是不包含对象的背景和包含感兴趣对象的前景之间的极端不平衡。焦点损失被设计为在硬的、容易被错误分类的例子（即具有噪声纹理或部分对象的背景）上分配更多的权重，并对容易被加权的例子（例如明显为空的背景）进行加权。</strong><h3 id=Abs><a class=headerlink href=#Abs title=Abs></a>Abs</h3><p>迄今为止，准确率最高的物体检测器都是基于 R-CNN 推广的两阶段方法，即对稀疏的候选物体位置集进行分类。<strong>相比之下，应用于对可能的物体位置进行规则、密集采样的单阶段检测器有可能更快、更简单，但迄今为止，其准确性仍落后于两阶段检测器。在本文中，我们将探讨出现这种情况的原因。</strong><p>我们发现，dense detectors训练过程中遇到的前景-背景类别极度不平衡是主要原因。我们建议通过重塑标准交叉熵损失来解决这种类别不平衡问题，从而降低分类良好示例的损失权重。<h3 id=Focal-Loss><a title="Focal Loss" class=headerlink href=#Focal-Loss></a>Focal Loss</h3><p>焦点损失（Focal Loss）的设计目<strong>的是解决在训练过程中前景类和背景类之间极度不平衡（例如 1:1000）的单阶段物体检测问题</strong>。我们从用于二元分类的交叉熵（CE）损失开始引入焦点损失<p><img alt=image-20231129231823763 data-src=https://s2.loli.net/2023/11/29/nHdKf2JCcxV6sry.png><p>在上述公式中，y∈{±1} 表示地面实况类别，p∈[0, 1]是模型对标签 y = 1 的类别的估计概率。<p>我们定义 p~t~<p><img alt=image-20231129231934979 data-src=https://s2.loli.net/2023/11/29/b1BWorKMJTxf2R4.png><p>重写 CE(p, y) = CE(p~t~) = - log(pt)。<p>我们建议在交叉熵损失中加入一个调制因子 (1 - p~t~)γ ，可调聚焦参数 γ ≥ 0。<p><img alt=image-20231129231711234 data-src=https://s2.loli.net/2023/11/29/3DqjYcTbewiCrE6.png><p>我们注意到焦点损失的两个特性。(1) 当一个例子被错误分类且 p~t~ 较小时，调制因子接近 1，损失不受影响。<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/focal-loss.png style=zoom:67%;><h4 id=BackBone><a class=headerlink href=#BackBone title=BackBone></a>BackBone</h4><p>我们采用特征金字塔网络（FPN）作为 RetinaNet 的骨干网络。<p>简而言之，FPN 利用自上而下的路径和横向连接增强了标准卷积网络，因此该网络能从单一分辨率的输入图像中有效构建丰富的多尺度特征金字塔。金字塔的每一层都可用于检测不同尺度的物体。<strong>FPN 可以改进全卷积网络 (FCN) [23] 的多尺度预测，这体现在它对 RPN [28] 和 DeepMask 式提案 [24] 以及快速 R-CNN [10] 或 Mask R-CNN [14] 等两阶段检测器的增益上</strong>。继 <strong>Feature pyramid networks for object detection</strong>之后，我们在 ResNet 架构<strong>Deep residual learning for image recognition</strong>. 的基础上构建了 FPN。我们构建了一个 P3 到 P7 级的金字塔，其中 l 表示金字塔级别（Pl 的分辨率比输入低 2l）。与FPN 一样，所有金字塔层级都有 C = 256 个通道。虽然许多设计选择并不重要，但我们<strong>强调使用 FPN 主干网才是关键；使用仅来自最后 ResNet 层的特征进行的初步实验得出的 AP 值较低。</strong><h4 id=anchors><a class=headerlink href=#anchors title=anchors></a>anchors</h4><p>我们使用了与中 RPN 变体类似的平移不变锚点框。锚点在金字塔 P3 到 P7 层的面积分别为 32^2^ 到 512^2^。与文献一样，我们在每个金字塔层使用了三种纵横比的锚点{1:2, 1:1, 2:1}。为了获得比更密集的比例覆盖，我们在每个层级添加了尺寸为{2^0^, 2^1/3^, 2^2/3^}的锚点，这些锚点是原始的 3 种宽高比锚点的集合。这改进了我们的 AP 设置。每个级别总共有 A = 9 个锚点，相对于网络的输入图像，这些锚点覆盖了 32-813 个像素的范围。<h2 id=YOLO-v1-v9><a title="YOLO v1~v9" class=headerlink href=#YOLO-v1-v9></a>YOLO v1~v9</h2><p>总结一下YOLO的演变.<p>YOLO一开始,没有使用选择搜索和RPN生成region proposal,而是直接在图像上划分得到多个bbox.输出的结果包括位置和分类概率. 位置是相对于bbox的位移,w,h经过了归一化.<p>假设图片被均匀分为SxS个cell,每个cell中预测得到B个bbox,那么输出结果是SxSx(5B+K),K是预测的K类物体概率,5B包括4B(x,y,w,h)以及置信度Pr(containing an object) x IoU(pred, truth) 假设一个cell只包含一个物体.<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-responsible-predictor.png style=zoom:50%;><p>YOLO2改进,使用了Batchnorm,使用高分辨率图像finetune,使用卷积而不是全连接替代header. 使用K聚集挑选anchor box的尺寸. 输出的位置也变成了直接的位置而不是便宜.多尺度训练和更细粒度的特征.<p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolov2-loc-prediction.png style=zoom:50%;><p>YOLO3在YOLO2上加了一堆tricks,包括使用logstic回归预测置信度,使用多个不同的logistic分类器对每类得到概率,在原本Darknet上加RestNet. 多尺度预测加跨层连接.<p>之后的YOLOv4并不是所谓官方的版本,在我看来还是网上加了一堆Tricks,<ol><li><p>将CSP（Channel and Spatial Pyramid）结构融入Darknet53中，生成了新的主干网络CSPDarknet53</p><li><p>采用SPP（Spatial Pyramid Pooling）空间金字塔池化来扩大感受野</p><li><p>在Neck部分引入PAN结构，即FPN+PAN的形式</p><li><p>4.引入Mish激活函数</p><li><p>引入Mosaic数据增强</p></ol><p>6.训练时采用CIOU_loss ，同时预测时采用DIOU_nms<p><img alt=image-20240502194822347 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502194822347.png style=zoom:67%;><p><img alt=image-20240502194817818 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502194817818.png style=zoom:50%;><blockquote><p>这里也要提一句,目前深度学习貌似有进入到了大模型阶段,之前这种”小农”时代的往上加层数加残差魔改模型加点数的做法有点过时了,至少在学术界.而在工业界,其中的瓶颈貌似也不是模型的预测精度问题.</blockquote><p>YOLOv5版本 UltralyticsLLC 公司推出的，是在YOLOv4的基础上做了少许的修补，由于改进比较小，譬如：<ol><li><p>将v4版本骨干网络中的CSP（Channel and Spatial Pyramid）结构拓展到了NECK结构中。</p><li><p>增加了FOCUS操作，但是后续6.1版本中又剔除掉了该操作，使用一个6x6的卷积进行了替代。</p><li><p>使用SPPF结构代替了SPP。</p></ol><p>YOLOv6是美团提出,1.骨干网Yo络由CSPDarknet换为了EfficientRep<p>2.Neck是基于Rep和PAN构建了Rep-PAN<p>3.检测头部分模仿YOLOX，进行了解耦操作，并进行了少许优化。<p>YOLOv7是YOLOv4团队的续作，主要是针对模型结构重参化和动态标签分配问题进行了优化。<p>YOLOv7检测算法的思路是与YOLOv4、v5类似。<p>主要改动：<p>1.提出了计划的模型结构重参化。<p>2.借鉴了YOLOv5、Scale YOLOv4、YOLOX，“拓展”和“复合缩放”方法，以便高效的利用参数和计算量。<p>3.提出了一种新的标签分配方法。<br>YOLOv8版本 UltralyticsLLC 公司推出的是，利用了与YOLOv5类似的代码，但采用了新的结构，其中使用相同的代码来支持分类、实例分割和对象检测等任务类型。模型仍然使用相同的YOLOv5 YAML格式初始化，数据集格式也保持不变。<p>YOLOv8目标检测算法相较于前几代YOLO系列算法具有如下的几点优势：<ul><li><p>更友好的安装/运行方式</p><li><p>速度更快、准确率更高</p><li>新的backbone，将YOLOv5中的C3更换为C2F<li>YOLO系列第一次尝试使用anchor-free</ul><p>目前YOLOv9也出来了,通过引入“可编程梯度信息”（Programmable Gradient Information, PGI）和一种新的轻量级网络架构“通用高效层聚合网络”（Generalized Efficient Layer Aggregation Network, GELAN）来解决深层网络中的信息瓶颈问题。<p><img alt=image-20240502200729086 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240502200729086.png><h2 id=DETR><a class=headerlink href=#DETR title=DETR></a>DETR</h2><h3 id=摘要><a class=headerlink href=#摘要 title=摘要></a>摘要</h3><p>我们提出的新方法将目标检测视为一个直接的集合预测问题。我们的方法简化了检测流水线，有效地消除了对许多手工设计组件的需求，如非最大抑制程序或锚点生成，这些组件明确地编码了我们对任务的先验知识。<p>新框架被称为 DEtection TRansformer 或 DETR，其主要成分是基于集合的全局损失（通过两方匹配强制进行唯一预测）和transformers编码器-解码器架构。DETR 给定了一小组固定的已学对象查询，通过推理对象之间的关系和全局图像上下文，直接并行输出最终的预测结果。<h3 id=引言><a class=headerlink href=#引言 title=引言></a>引言</h3><p>目标检测的目标是为每个感兴趣的物体预测一组边界框和类别标签。<p>现代检测器以一种间接的方式来解决这个集合预测任务,它们通过在大量候选区域、锚框或窗口中心上定义替代的回归和分类问题来实现这一目标。<p><img alt=image-20240505142247768 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505142247768.png><p>他们的性能在很大程度上受到后处理步骤的影响,这些步骤用于合并近似重复的预测,也受到锚框集合的设计和将目标框分配给锚框的启发式方法的影响.<p>为了简化这些流程管道,我们提出了一种直接的集合预测方法来绕过替代任务.这种端到端的理念已经在诸如机器翻译或语音识别等复杂结构预测任务中取得了重大进展,但在目标检测领域尚未得到应用:<strong>以前的尝试要么增加了其他形式的先验知识,要么在具有挑战性的基准测试中未能证明其比强基线算法的性能更加出色</strong>。<p><img alt=image-20240505144614664 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505144614664.png><p>DETR 的整体架构非常简单，它包含三个主要组件：用于提取紧凑特征表示的 CNN 主干网、编码器-解码器转换器以及进行最终检测预测的简单前馈网络 (FFN)。<p><strong>骨干网络</strong>: 利用传统CNN骨干处理图像得到CXHXW,C=2048,H, W = H0/32 , W0/32<p><strong>Transformer encoder</strong>:1x1 卷积将高级激活图 f 的通道维度从 C 减小到更小的维度d.<p>编码器希望输入一个序列，<strong>因此我们将 z0 的空间维度折叠为一个维度,从而得到一个 d×HW 的特征图</strong>。每个编码器层都采用标准架构，由多头自注意模块和前馈网络（FFN）组成。由于变换器架构是permutation-invariant的，因此我们用固定位置编码对其进行补充，并将其添加到每个注意层的输入中<p><strong>Transformer decoder</strong>:解码器遵循变换器的标准架构,利用多头自编码和编码器-解码器关注机制变换大小为 d 的 N 个嵌入式数据.<p><strong>Prediction feed-forward networks (FFNs)</strong>最终预测结果由一个具有 ReLU 激活函数和隐藏维度 d 的 3 层感知器和一个线性投影层计算得出。<p>由于我们预测的是一组固定大小的 N 个边界框，而 N 通常远大于图像中感兴趣物体的实际数量，因此我们使用了一个额外的特殊类标签∅ 来表示在一个插槽中没有检测到任何物体。<p><strong>Auxiliary decoding losses.</strong>发现在训练过程中在解码器中使用辅助损失]很有帮助，尤其是可以帮助模型输出每一类对象的正确数量。<p>我们在每个解码器层后添加预测 FFN 和Hungarian损失。所有预测 FFN 都共享参数。我们使用额外的共享层规范来规范不同解码器层的预测 FFNs 输入。<h2 id=Deformable-DETR><a class=headerlink href=#Deformable-DETR title=Deformable-DETR></a>Deformable-DETR</h2><p>最近有人提出了 DETR 方法，该方法无需在物体检测中使用许多手工设计的组件，而且性能良好。<strong>然而，由于transformer注意模块在处理图像特征图时的局限性，它存在收敛速度慢和特征空间分辨率有限的问题。为了缓解这些问题，我们提出了可变形 DETR</strong>，其注意力模块只关注参考点周围的一小部分关键采样点。可变形 DETR 比 DETR 性能更好（尤其是在处理小物体时），训练历时减少了 10 倍。在 COCO 基准上进行的大量实验证明了我们方法的有效性。<p><img alt=image-20240505153729910 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505153729910.png><p>尽管 DETR 的设计很有趣，性能也不错，但它也有自己的问题：(1) 与现有的物体检测器相比，它需要更长的训练历时才能收敛。.(2) DETR 检测小物体的性能相对较低。现代物体检测器通常利用多尺度特征，从高分辨率特征图中检测小物体。同时，高分辨率特征图会导致 DETR 的复杂性。<p>上述问题主要归咎于 Transformer 组件在处理图像特征图时的缺陷。在初始化时，注意力模块会对特征图中的所有像素施加几乎一致的注意力权重。需要经过长时间的训练才能学会将注意力权重集中在稀疏的有意义的位置上。另一方面，Transformer 编码器中的注意力权重计算是像素数的二次计算。因此，处理高分辨率特征图的计算和内存复杂度非常高。<p>可变形 DETR，它缓解了 DETR 收敛慢和复杂度高的问题。它结合了可变形卷积的稀疏空间采样和变形器的关系建模能力。<p><img alt=image-20240505153747011 data-src=https://proanimer-img.oss-cn-shanghai.aliyuncs.com/alimg/image-20240505153747011.png></p><script type="math/tex; mode=display">
\operatorname{DeformAttn}(\boldsymbol{z}_q,p_q,x)=\sum W_m\big[\sum A_{mqk}\cdot W_m'x(p_q+\Delta p_{mqk})\big] \\
\text{MSDeformAttn}(z_q,\hat{p}_q,\{x^l\}_{l=1}^L)=\sum_{m=1}^MW_m\Big[\sum_{l=1}^L\sum_{k=1}^KA_{mlqk}\cdot W_m^{\prime}x^l(\phi_l(\hat{p}_q)+\Delta p_{mlqk})\Big]</script><p>给定一个输入特征图$ x∈R^{C×H×W}$ ,让 q 标示一个具有内容特征 $z<em>{q}$ 和二维参考点 $p</em>{q}$ 的查询元素.m 表示注意力头，k 表示采样键，K 是总采样键数.<p>∆p$<em>{mqk}$ 和 A$</em>{mqk}$ 分别表示第 m 个注意头中第 k 个采样点的采样偏移和注意权重<p>我们用提出的多尺度可变形注意力模块取代了 DETR 中处理特征图的 Transformer 注意力模块。编码器的输入和输出都是分辨率相同的多尺度特征图。在编码器中，我们从 ResNet中 C3 到 C5 阶段（通过 1 × 1 卷积转换）的输出特征图中提取多尺度特征图 {xl}$^{L-1}_{l=1}$ （L = 4），其中 Cl 的分辨率比输入图像低 2^l^。<p>解码器中有交叉注意模块和自我注意模块。这两种注意模块的查询元素都是对象查询。在交叉注意模块中，对象查询从特征图中提取特征，而关键元素则来自编码器输出的特征图。在自我关注模块中，对象查询相互影响，其中的关键要素是对象查询。<p>由于我们提出的可变形注意力模块旨在处理卷积特征图这一关键要素，因此我们只将每个交叉注意力模块替换为多尺度可变形注意力模块，而自注意力模块则保持不变。<h3 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h3><ol><li><a href=https://arxiv.org/pdf/2402.13616 rel=noopener target=_blank>2402.13616 (arxiv.org)</a><li><a href=https://arxiv.org/pdf/2304.00501v5 rel=noopener target=_blank>A Comprehensive Review of YOLO: From YOLOv1 and Beyond (arxiv.org)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\10\21\目标检测-初识\ rel=bookmark>目标检测_初识</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\12\22\目标检测综述\ rel=bookmark>目标检测综述</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/11/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AD%A6%E4%B9%A0-P3/ title=目标检测学习_P3>https://www.sekyoro.top/2023/11/01/目标检测学习-P3/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/object-detection/ rel=tag><i class="fa fa-tag"></i> object detection</a></div><div class=post-nav><div class=post-nav-item><a title="3D Object Detection Learning" href=/2023/10/30/3D-Object-Detection-Learning/ rel=prev> <i class="fa fa-chevron-left"></i> 3D Object Detection Learning </a></div><div class=post-nav-item><a href=/2023/11/02/%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/ rel=next title=图像融合论文阅读> 图像融合论文阅读 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#YOLO><span class=nav-number>1.</span> <span class=nav-text>YOLO</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Network-Architecture><span class=nav-number>1.1.</span> <span class=nav-text>Network Architecture</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>1.2.</span> <span class=nav-text>损失函数</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#YOLOV2%E6%94%B9%E8%BF%9B><span class=nav-number>1.3.</span> <span class=nav-text>YOLOV2改进</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#SSD><span class=nav-number>2.</span> <span class=nav-text>SSD</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#default-box%E7%9A%84%E7%94%9F%E6%88%90><span class=nav-number>2.0.1.</span> <span class=nav-text>default box的生成</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1><span class=nav-number>2.0.2.</span> <span class=nav-text>损失函数</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%BF%9E%E7%BB%93%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9A%84%E9%A2%84%E6%B5%8B><span class=nav-number>2.1.</span> <span class=nav-text>连结多尺度的预测</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#%E9%AB%98%E5%92%8C%E5%AE%BD%E5%87%8F%E5%8D%8A%E5%9D%97><span class=nav-number>2.1.1.</span> <span class=nav-text>高和宽减半块</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#FPN><span class=nav-number>2.1.2.</span> <span class=nav-text>FPN</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#OverFeat><span class=nav-number>2.1.3.</span> <span class=nav-text>OverFeat</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#RetinaNet><span class=nav-number>3.</span> <span class=nav-text>RetinaNet</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#Focal-Loss-for-Dense-Object-Detection><span class=nav-number>3.1.</span> <span class=nav-text>Focal Loss for Dense Object Detection</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Abs><span class=nav-number>3.2.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#Focal-Loss><span class=nav-number>3.3.</span> <span class=nav-text>Focal Loss</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#BackBone><span class=nav-number>3.3.1.</span> <span class=nav-text>BackBone</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#anchors><span class=nav-number>3.3.2.</span> <span class=nav-text>anchors</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#YOLO-v1-v9><span class=nav-number>4.</span> <span class=nav-text>YOLO v1~v9</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#DETR><span class=nav-number>5.</span> <span class=nav-text>DETR</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%91%98%E8%A6%81><span class=nav-number>5.1.</span> <span class=nav-text>摘要</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%BC%95%E8%A8%80><span class=nav-number>5.2.</span> <span class=nav-text>引言</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#Deformable-DETR><span class=nav-number>6.</span> <span class=nav-text>Deformable-DETR</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>6.1.</span> <span class=nav-text>参考资料</span></a></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>185</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>187</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a></ul></div><div class="motion-element announcement"><div class=title>注意</div><p class=content>由于最近图床更新,可能有些图片显示不了.如果发现了有些图片无法显示影响阅读的,还烦请联系我,我有空补上.<p class=date>2023-10-6</div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/object-detection/ rel=tag>object detection</a><span class=tag-list-count>3</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>1.3m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>20:24</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>