<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"0F9LEEVW82","apiKey":"78839e9f9be09d081c5c4da81975cd19","indexName":"sekyoblog_sec","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="2023年的目标检测综述A comprehensive review of object detection with deep learning以及3D Object Detection for Autonomous Driving: A Comprehensive Survey,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下." name=description><meta content=article property=og:type><meta content=目标检测综述 property=og:title><meta content=https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="2023年的目标检测综述A comprehensive review of object detection with deep learning以及3D Object Detection for Autonomous Driving: A Comprehensive Survey,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下." property=og:description><meta content=zh_CN property=og:locale><meta content=https://i.imgur.com/lJA5oxG.png property=og:image><meta content=https://i.imgur.com/tIKAWn2.png property=og:image><meta content=https://i.imgur.com/GyXhKPL.png property=og:image><meta content=https://i.imgur.com/D9dXGsH.png property=og:image><meta content=https://i.imgur.com/or3OwoO.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png property=og:image><meta content=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png property=og:image><meta content=https://i.imgur.com/02X39qA.png property=og:image><meta content=https://i.imgur.com/lS0F3vq.png property=og:image><meta content=https://i.imgur.com/HiTj4TF.png property=og:image><meta content=https://i.imgur.com/7LLjGj3.png property=og:image><meta content=https://i.imgur.com/Vl1RhDd.png property=og:image><meta content=https://i.imgur.com/O8n1SeP.png property=og:image><meta content=https://i.imgur.com/r7H1L1T.png property=og:image><meta content=https://i.imgur.com/jFknBKH.png property=og:image><meta content=https://i.imgur.com/6IcyJdX.png property=og:image><meta content=https://i.imgur.com/TkWPitq.png property=og:image><meta content=2023-12-22T11:38:36.000Z property=article:published_time><meta content=2023-12-23T12:04:10.000Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="object detection" property=article:tag><meta content=summary name=twitter:card><meta content=https://i.imgur.com/lJA5oxG.png name=twitter:image><link href=https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>目标检测综述 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>目标检测综述</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-12-22 19:38:36" datetime=2023-12-22T19:38:36+08:00>2023-12-22</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2023-12-23 20:04:10" datetime=2023-12-23T20:04:10+08:00 itemprop=dateModified>2023-12-23</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>21k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>19 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>2023年的目标检测综述<strong>A comprehensive review of object detection with deep learning</strong>以及<strong>3D Object Detection for Autonomous Driving: A Comprehensive Survey</strong>,之前写了一些单阶段和双阶段的2D目标检测,可以好好回顾一下.</p><span id=more></span><h2 id=Abs><a class=headerlink href=#Abs title=Abs></a>Abs</h2><p>本综述详细介绍了物体检测及其各个方面。随着用于检测物体的深度学习算法逐渐发展，物体检测模型的性能也有了显著提高。但是，这并不意味着在深度学习出现之前已经发展了几十年的传统物体检测方法已经过时。<strong>在某些情况下，具有全局特征的传统方法是更优的选择</strong>。<strong>本综述论文首先简要概述了物体检测，然后介绍了物体检测框架、骨干卷积神经网络、常见数据集概述以及评估指标</strong>。此外，还详细研究了物体检测问题和应用。还<strong>讨论了设计深度神经网络的一些未来研究挑战</strong>。最后，比较了对象检测模型在 PASCAL VOC 和 MS COCO 数据集上的性能，并得出结论。<h2 id=Intro><a class=headerlink href=#Intro title=Intro></a>Intro</h2><p>物体检测的主要目的是检测特定类别的视觉物体，如电视/显示器、书籍、猫、人类等，并使用边界框定位它们，然后将它们归入特定物体的类别中。<p>通用对象检测还有其他几个术语，例如通用对象类别检测、对象类别检测、类别级对象检测和对象类别检测。它也侧重于识别一些预设类别的实例.<p>物体检测的发展通常分为两个历史阶段。<strong>2014 年之前是传统方法阶段，2014 年之后则是基于深度学习的方法阶段</strong>。本文将重点讨论基于深度学习的方法。由于 CNN 在物体检测算法的实施中发挥着重要作用，因此本文将利用 CNN 来获得最佳结果。这两个阶段的架构在精度、速度和硬件资源方面各不相同。将 CNN 与传统技术相比，CNN 具有更好的架构和更强的表现力。<p>在讨论基于深度学习的物体检测算法之前，重要的是要了解传统技术的工作原理，并知道为什么基于深度学习的方法要优越得多。这将有助于研究人员更好地理解现代物体检测方法。<h3 id=传统方法><a class=headerlink href=#传统方法 title=传统方法></a>传统方法</h3><p>传统目标检测方法分为三个阶段。这些阶段及其各自的缺点如下:<p><strong>区域选择</strong> – 由于对象具有不同的大小和纵横比，因此它们可能出现在图像的不同区域。因此，在第一阶段，必须确定物体的区域。因此，使用多尺度滑动窗口方法检查整个图像以检测物体。然而，这种方法的计算成本很高，并且还会导致大量非必要的选择。<p><strong>特征提取</strong> – 定位对象后，执行特征提取过程以提供可靠的表示。<strong>HOG、Haar-like 、SIFT</strong> 等方法用于提取特征以进行目标识别，以提供有意义的表示。然而，由于对比鲜明的背景、照明环境和透视差异，手动构建一个能够正确识别各种对象的综合特征描述符是极其困难的。<p><strong>分类</strong> – 在这个阶段，使用分类器（如 Adaboost ）来识别目标对象，并构建更有条理、更有意义的视觉感知模型。<p>从以上几点可以清楚地看出，<strong>在传统方法中，手工制作的特征并不总是足以正确表示对象</strong>。除此之外，用于生成边界框的滑动窗口方法在计算上成本高昂且效率低下。传统的技术包括HOG、SIFT 、Haar、VJ检测器和其他算法，如。在HOG 中，识别一个物体需要很长时间，因为它采用滑动窗口方法来提取特征。SIFT算法速度极慢，计算成本高，也不擅长光照变化。在VJ检测器中，训练持续时间非常长，仅限于二元分类。因此，深度学习技术正在被用于克服传统方法的问题<p>深度学习的出现有可能解决传统技术的一些局限性。最近，深度学习方法在自动从数据中学习特征表示方面变得突出。这些方法显著改善了目标检测。基于深度学习的方法有 <strong>Faster RCNN、SSD、YOLO</strong> 等等。<p><img alt=image-20231222203515697 data-src=https://i.imgur.com/lJA5oxG.png><p>由于深度 CNN 具有很高的特征表示能力，因此它们被用于对象检测架构。有两种类型的探测器：两级和一级探测器.<p>两阶段目标检测框架<strong>将目标定位和目标分类任务分开</strong>。简单来说，<strong>首先在对象所处的地方生成区域建议，然后根据其特定类别对该区域进行分类</strong>。这就是为什么它被称为两阶段的原因。两级目标探测器的主要优点是检测精度高，缺点是检测速度慢。<h4 id=RCNN><a class=headerlink href=#RCNN title=RCNN></a>RCNN</h4><p>基于区域的卷积神经网络（RCNN）在使用深度学习方法检测目标方面进行了深入研究。其架构如图所示。RCNN的过程在下面分四个阶段进行解释<p><img alt=image-20231222204652846 data-src=https://i.imgur.com/tIKAWn2.png><p>第 1 阶段 :使用<strong>选择性搜索方法提取区域建议</strong>。选择性搜索<strong>根据不同的比例、外壳、纹理和颜色模式来识别这些区域</strong>。它<strong>从每张图像中提取大约 2000 个区域</strong><p>第 2 阶段 – 由于全连接层需要固定长度的输入向量，因此<strong>所有这些区域建议都重新缩放为相同的图像大小以匹配 CNN 输入大小</strong>。<strong>使用 CNN 提取每个候选区域的特征</strong>。<p>第 3 阶段 – 提取特征后，<strong>使用 SVM 分类器检测对象是否存在于每个区域</strong>中。<p>第 4 阶段 – 最后，对于图像中的每个已识别对象，使用线性回归模型在其周围生成更紧密的边界框。尽管RCNN在目标检测方面取得了很大的进步，但仍然存在一些局限性，如目标检测速度慢、多阶段流水线训练和选择性搜索方法的僵化。<h4 id=SPP-Net><a class=headerlink href=#SPP-Net title=SPP-Net></a>SPP-Net</h4><p>由于 RCNN 为每张图像生成 2000 个区域建议，因此从这些区域提取 CNN 特征是主要障碍。<strong>固定输入大小的约束只是因为全连接层</strong>。因此，<strong>为了克服这一困难，引入了一种称为空间金字塔池化网络层（SPP-Net）的新技术</strong>。<strong>将 SPP 层添加到最终卷积层的顶部，以生成全连接层的固定长度特征</strong>，无论 RoI（感兴趣区域）的大小如何，并且不会重新缩放它，这可能会导致信息丢失(相当于替代RCNN中的warp操作).<p><img alt=image-20231222205054546 data-src=https://i.imgur.com/GyXhKPL.png><p>通过使用SPPNet层，RCNN的速度有了很大的提高，而检测质量没有任何损失。这是因为卷积层<strong>只需要在完整的测试图像上运行一次，就可以为随机大小的区域建议创建固定长度的特征</strong>。这里 SPP 层的输出是 256×M-d 向量。256 是卷积滤波器的数量，M 是bin的数量。全连接层接收固定长度的维向量。<h4 id=Fast-RCNN><a title="Fast RCNN" class=headerlink href=#Fast-RCNN></a>Fast RCNN</h4><p>尽管SPPNet在效率和准确性方面优于RCNN，但它仍然存在一些问题，例如它大致遵循与RCNN相同的过程，包括网络微调、特征提取和边界框回归。<p>Girshick， R. 在 RCNN 和 SPPNet 方面表现出进一步的改进，并提出了一种名为 Fast RCNN 的新探测器 。<strong>它允许对检测器进行端到端训练，同时学习 softmax 分类器和特定于类的边界框回归，同时进行多任务损失，而不是像在 RCNN 和 SPPNet 中那样单独训练它们</strong>。<p>在 Fast RCNN 中，它不是对每张图像执行 2000 次 CNN，<strong>而是只运行一次并获取所有感兴趣的区域。然后，在最终卷积层和初始全连接层之间添加RoI池化层，从而提取出所有区域建议的固定长度向量特征</strong>。<p>1st – Fast RCNN 获取完整的输入图像并将其传递给 CNN 以生成特征图。<p>第 2 个 – 感兴趣区域 （RoI） 是使用选择性搜索方法生成的。<p>第三 – 在提取的 RoI 上应用 RoI 池化层以生成固定长度的特征向量。它确保所有区域都具有相同的量级。<p>第 4 次 – 然后<strong>将提取的特征发送到全连接层，同时使用 softmax 层和线性回归层进行分类和定位</strong>。<p>Fast RCNN消耗的计算时间更少，检测精度更高。然而，<strong>它基于传统的区域建议方法，使用选择性搜索方法，使其非常耗时</strong>。<h4 id=Faster-RCNN><a title="Faster RCNN" class=headerlink href=#Faster-RCNN></a>Faster RCNN</h4><p>尽管Fast RCNN在速度和准确性方面取得了长足的进步，<strong>但它使用选择性搜索方法生成了2000个region proposals，这是一个非常缓慢的过程</strong>。任，S.等人致力于这个问题，并开发了一种新的检测器，名为Faster RCNN，作为第一个端到端深度学习检测器。<strong>它还通过将传统的region proposal算法（如选择性搜索、多尺度组合分组或边缘框）替换为称为区域建议网络（RPN）的CNN</strong>，提高了Fast RCNN的检测速度。<p>a） CNN 将图像作为输入，并提供图像的特征图作为输出。<p>b） <strong>RPN 应用于生成的特征图，返回对象建议 （RoI） 及其对象性分数</strong>。<p>c） 提取 RoI 后，将 RoI 池化层应用于其，以将所有提案置于固定维度。<p>d） <strong>将派生的特征向量提供给连续的全连接层中，顶部有一层 softmax 和回归层，用于对对象的边界框进行分类和输出</strong><p><img alt=image-20231222205738677 data-src=https://i.imgur.com/D9dXGsH.png style=zoom:67%;><p>RPN 的工作 – 区域提案网络是一个完全卷积网络，它连接到骨干网络的最后一个卷积层 。<strong>它接收特征图，并使用这些特征图上的滑动窗口输出多个对象建议</strong>。<strong>在每个窗口上，网络生成 k 个不同大小和纵横比的锚框</strong>（也称为参考框）。<p><strong>只有从锚点框获得的特征是特定于类的，而不是锚点的位置</strong>。<p>每个对象提案由 4 个坐标和一个分数组成，用于确定对象是否存在。<strong>每个锚点映射到一个低维向量，并传递给两个全连接层，一个是对象类别分类层，另一个是box回归层</strong><h4 id=Feature-pyramid-network><a title="Feature pyramid network" class=headerlink href=#Feature-pyramid-network></a>Feature pyramid network</h4><p>Lin， T. Y. et al. 提出了特征金字塔网络 （FPN）<p>DCNN 固有的多尺度金字塔层次结构，以低成本构建特征金字塔。它将任何大小的图像作为输入，并在多个级别输出相同大小的特征图。这种方法在许多应用中显示出相当大的增强。<p><img alt=image-20231222210741517 data-src=https://i.imgur.com/or3OwoO.png><p>FPN 不是对象检测器。它是一种特征提取器，与对象检测器结合使用。<strong>FPN的架构使用自上而下的通路和横向连接将语义上较强的低分辨率特征与语义较弱的高分辨率特征相结合。FPN使用CNN架构的序列，通过横向连接构建自下而上的路径和自上而下的路径。在自下而上的路径（红色）中，图像作为输入传递给 CNN，它使用池化层将特征图设置为相同的大小。对于FPN的每个阶段（即每个分辨率级别），定义了一个金字塔级别</strong><p>在自上而下的路径（以蓝色显示）中，<strong>通过将特征图上采样回与自下而上部分相同的大小来使用更高分辨率的特征。然后使用横向连接，这些特征通过自下而上途径的特征进行增强</strong>。每个横向连接都从自下而上和自上而下路径组合了相同大小的特征图<p>FPN 的过程为生成具有大量语义内容的多尺度特征图提供了广泛的解决方案。F<strong>PN 不依赖于 CNN 的架构，可以强制执行到对象检测的不相同阶段</strong>，例如 RPN、Fast RCNN.尽管DCNN具有强大的表征能力，但<strong>有必要通过金字塔表示来解决多尺度挑战</strong><h4 id=Mask-RCNN><a title="Mask RCNN" class=headerlink href=#Mask-RCNN></a>Mask RCNN</h4><p>He， K. et al.设计了一款名为Mask RCNN的目标检测器，这是对Faster RCNN的增强，用于解决进行目标检测和语义分割作业的实例分割问题。这两个任务是自力更生的过程。Mask RCNN 的目标是执行像素级分割。蒙版RCNN检查每个像素并估计它是否是对象的一部分。<p>Mask R-CNN 遵循 Faster R-CNN 的架构;两者都使用相同的RPN，<strong>但区别在于掩码RCNN对每个对象提案有三个输出，即.class标签，边界框偏移量和对象检测掩码</strong>。在 Mask RCNN 中，RoIAlign 层用于将提取的特征与对象的输入位置相关联。RoIAlign 层的目的是修复 RoI 池化层中的错位问题。它无需测量 RoI 阈值，而是使用双线性插值来评估每个采样点的实际特征值。Mask RCNN 在实例分割方面实现了最先进的性能<p><strong>基于区域提案的框架由各个阶段组成，这些阶段相互连接并分别进行训练。这些是区域建议生成、使用 CNN 提取特征、分类和边界框回归</strong>。尽管这些方法能够实现高精度，但仍存在一些与实时速度相关的问题。这个问题可以通过统一的阶段检测器来克服，<strong>方法是删除区域建议阶段，并在单个CNN中实现特征提取、建议回归和预测</strong><p>损失或成本函数，如Hinge损失、L1 和 L2 损失、对数损失 [52]是预期输出和预测输出之间差异的度量。建议读者参考相应的物体检测器论文以获取更多信息<h3 id=One-stage-object-detectors><a title="One-stage object detectors" class=headerlink href=#One-stage-object-detectors></a>One-stage object detectors</h3><p>单阶段对象检测框架使用 DCNN <strong>同时进行定位和分类</strong>，而无需将它们划分为两个部分。<p>在这种情况下，只需要通过神经网络进行一次传递。它具有前馈神经网络，可<strong>以一次预测所有边界框。它们将图像像素直接映射到边界框坐标和类概率</strong>。<h4 id=DetectorNet><a class=headerlink href=#DetectorNet title=DetectorNet></a>DetectorNet</h4><p>Szegedy， C.等将DetectorNet框架实现为回归问题。<p>它能够学习特征进行分类并获取一些几何信息。它<strong>使用 AlexNet 作为骨干网络，并将 softmax 层替换为回归层</strong>。为了预测前景像素，DetectorNet 将输入图像分割成coarse grid。它的训练过程非常缓慢，因为网络要针对每种对象类型和掩码类型进行训练。此外，DetectorNet 无法处理类似类的多个对象。当它与多尺度从粗到细方法结合使用时，基于 DNN 的对象掩码回归会产生出色的结果<h4 id=Overfeat><a class=headerlink href=#Overfeat title=Overfeat></a>Overfeat</h4><p>Sermanet， P.等提出了一种统一的结构，即<strong>使用卷积网络通过多尺度滑动窗口方法进行定位、分类和检测</strong>。它是最强大的目标检测框架之一，应用于 ImageNet 大规模视觉识别挑战赛 2013 （ILSVRC），在检测和定位方面排名第一 。它是<strong>第一个基于全卷积深度网络的单级检测器，它通过全卷积层使用单次前向通道来检测物体</strong>。<p>OverFeat 充当后来出现的算法的基础模型，即 YOLO 及其版本、SSD 等。主要区别在于<strong>分类器和回归器的训练是在 OverFeat 中连续完成的</strong><h4 id=YOLO><a class=headerlink href=#YOLO title=YOLO></a>YOLO</h4><p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png><p>You Only Look Once（YOLO）是由Redmon， J.等人设计的单级目标检测器，<strong>其中目标检测作为回归问题进行。它预测对象的边界框的坐标，并确定它所关联的类别的可能性</strong>。由于仅使用单个网络，因此可以实现端到端优化。它<strong>使用有限的候选区域选择直接预测检测。与基于区域的方法不同，这些方法使用来自特定区域的特征</strong>，而YOLO广泛使用来自整个图像的特征<p>在YOLO目标检测中，图像被划分为S×S网格;每个网格由五个元组（x、y、w、h 和置信度分数）组成。单个对象的置信度分数基于概率。这个分数是给每个类的，无论哪个类的概率很高，该类都会优先。<p>边界框的参数宽度 （W） 和高度 （H） 是根据对象的大小来预测的。从重叠的边界框中，选择具有最高 IOU 的框，并删除其余框。<p>YOLOv2是YOLOv1的增强版本，由Redmon， J.等人]给出。在这个版本中，应用了不同的思想<strong>，如批量归一化、卷积锚框、高分辨率分类器</strong>、<strong>细粒度特征和多尺度训练</strong>来提高 YOLO 的性能。它使用 Darknet-19 作为包含 19 个卷积层和 5 个最大池化层的骨干分类，这些层需要更少的过程来分析图像，同时实现最佳精度<p>YOLOv3 是 YOLOv2 的渐进形式，它使用<strong>逻辑回归来估计每个边界框的客观性分数。边界框中包含多个类，为了预测这些类，使用了多标签分类</strong>。它还使用二进制交叉熵损失、数据增强技术和批量归一化。YOLOv3 使用一个名为 Darknet-53 的健壮特征提取器<p>YOLOv4 是一种先进的目标检测器，比以前所有版本的YOLO更准确、更快。它包括一种称为“Bag of freebies”的方法，该方法在不影响推理时间的情况下增加了训练时间。该方法利用<strong>数据增强技术、自对抗训练、交叉小批量归一化 （CmBN）、CIoU 损失 、DropBlock 正则化、余弦退火调度器来改进训练。YOLOv4 还包含了那些只影响推理时间的方法，称为“Bag of specials”;它包括 Mish 激活、多输入加权残差连接 （MiWRC）、SPP 模块 [26]、PAN 路径聚合模块 [58]、跨级部分连接 （CSP）和空间注意力模块模块</strong>。YOLOv4 可以在单个 GPU 上训练，并使用遗传算法来选择超参数<p>在 YOLOv4 发布后不久，Ultralytics 公司推出了 YOLOv5 存储库，与以前的 YOLO 模型相比，它有相当大的增强,由于 YOLOv5 不是作为同行评议的研究发表的，因此它引起了许多关于其合法性的争论;但它仍然被用于各种应用，并在产生模型可靠性的同时提供有效的结果。它以 140 fps 的推理速度运行。YOLOv5使用PyTorch，这使得模型的部署更快、更容易、更准确[60]。虽然 YOLOv4 和 YOLOv5 框架相似，因此很难比较它们之间的区别，但后来，YOLOv5 在某些情况下获得了比 YOLOv4 更高的性能。YOLOv5 模型有五种类型：nano、small、medium、large、extralarge。根据数据集选择模型类型。此外，YOLOv5 模型的轻量级模型随 6.0 版本发布;推理速度提高至 1666 fps.<h4 id=SSD><a class=headerlink href=#SSD title=SSD></a>SSD</h4><p><img alt=img data-src=https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/SSD-architecture.png><p>SSD是一种用于多个类别的快速单次多box检测器，由Liu， W.等人实现。它构建了一个统一的检测器框架<p>该框架与 YOLO 一样快，与 Faster-RCNN 一样准确。SSD的设计结合了YOLO模型的回归思想和Faster R-CNN算法的锚定过程。通过使用 YOLO 的回归，SSD 降低了神经网络的计算复杂性，以确保实时性能。通过锚点程序，SSD能够提取各种大小和纵横比的特征，以确保检测精度。SSD 使用 VGG-16 作为骨干检测器<p>SSD的过程基于前馈CNN，该CNN为这些框中是否存在对象类实例生成固定大小和对象性分数的边界框，然后应用NMS（非最大抑制）进行最终检测。它还使用RPN的概念来获得快速的检测速度，同时保持高检测质量。通过一些辅助数据增强和硬负挖掘方法，SSD 在各种基准数据集上实现了最先进的性能<h3 id=Backbone-networks><a title="Backbone networks" class=headerlink href=#Backbone-networks></a>Backbone networks</h3><p>DCNN作为目标检测模型的骨干网络。为了改善特征表示行为，网络的结构变得更加复杂，这意味着网络层会变得更深，其参数也会增加。骨干CNN用于提取基于DCNN的目标检测系统的特征。<p><strong>骨干网络作为目标检测方法的主要特征提取器，将图像作为输入，并为每个输入图像生成特征图作为输出</strong>。根据精度和效率的需要;可以使用密集连接的骨干网，如ResNet 、ResNext等。当需要高精度和构建精确的应用程序时，需要复杂的主干网。<p>AlexNet是一种重要的CNN架构，由5个卷积层和3个全连接层组成。在为图像提供固定大小（224 × 224）的输入后，网络一遍又一遍地卷积并汇集激活，然后将结果传输到完全连接的层。该网络在ImageNet上进行训练，并结合了多种正则化方法，例如数据增强，dropout等。为了加速数据处理，提高收敛速度，首次使用了ReLu激活函数和GPU。<h3 id=ZFNet><a class=headerlink href=#ZFNet title=ZFNet></a>ZFNet</h3><p>在AlexNet取得成功之后，研究人员想知道卷积层可视化背后的机制，以了解CNN如何学习特征以及如何检查每层图像特征图的差异。<p>因此，Zeiler， M. D. et al. 设计了一种<strong>使用反卷积层、解池层和ReLU非线性来可视化特征图的方法。与 AlexNet 一样，第一层的滤波器大小为 11×11，步幅为 4，但在 ZFNet 中，它减少到 7×7，步幅设置为 2 而不是 4</strong>。这样做的原因是第一层的滤波器包含频率信息的变化;它可以是高的，也可以是低的，并且具有非常小的中频百分比。该方法的性能优于AlexNet，并证明了网络的深度会影响深度学习模型的性能<h4 id=VGGNet><a class=headerlink href=#VGGNet title=VGGNet></a>VGGNet</h4><p>VGG 进一步将AlexNet的深度扩大到16-19层，从而细化了网络的特征表示。VGG16 和 VGG19 是两种流行的 VGG 网络架构。在每一层中，它采用大小为 3×3 的内核，步幅为 1。小内核和步幅更有利于提取图像中物体位置的细节。它的好处是通过合并额外的卷积层来扩展网络的深度。最小化参数可以提高网络的特征表示能力<h4 id=GoogLeNet-或-inception-v1><a title="GoogLeNet 或 inception v1" class=headerlink href=#GoogLeNet-或-inception-v1></a>GoogLeNet 或 inception v1</h4><p>GoogleNet 的主要目的Inception v1 架构旨在通过降低计算成本来实现高精度。向网络添加 1×1 卷积层，其深度增加。这种滤波器大小首先用于名为Network-in-Network的技术，主要用作降维以消除计算瓶颈并增加网络的宽度和高度。<p>GoogleNet 是一个 22 层的深度架构，是 ILSVRC 2014 竞赛的获胜者。基于这一思路，作者开发了一个具有降维功能的初始模块。通过使用 inception 模块，GoogLeNet 参数的数量减少了。Inception 模块由 1x1、3x3 和 5x5 滤波器大小的卷积层和相互平行组装的最大池化层组成。Inception v2 系列是第一个提出批量归一化的网络 ，从而实现快速训练<h4 id=ResNet><a class=headerlink href=#ResNet title=ResNet></a>ResNet</h4><p>随着网络深度的增加，可能会出现精度在达到饱和点后下降的情况。这被称为退化问题，为了解决这个问题，提出了一个残差学习（ResNet）模块。与早期设计的架构（如AlexNet 和VGGNet）相比，它的计算复杂度更低。通常使用层数为50和101层的ResNet骨干网络。在 ResNet50 中，使用跳过连接来保留更深层的梯度，并且精度有所提高。在 ResNet101 中，该模块的性能与 VGG 网络相同，但参数数量较少，遵循 GoogLeNet 中的全局平均池化和瓶颈<h4 id=DenseNet><a class=headerlink href=#DenseNet title=DenseNet></a>DenseNet</h4><p>Huang， G. et al. 提出了由密集块组成的 DenseNet 架构，该架构以前馈方式将每一层与其他层连接起来，从而带来特征重用、参数有效性和隐式深度监督等好处。DenseNet 减少了梯度消失的问题<h3 id=Problems-of-object-detection-and-its-solutions><a title="Problems of object detection and its solutions" class=headerlink href=#Problems-of-object-detection-and-its-solutions></a>Problems of object detection and its solutions</h3><h4 id=Small-object-detection><a title="Small object detection" class=headerlink href=#Small-object-detection></a>Small object detection</h4><p>检测小尺寸物体是物体检测中最困难的问题之一。Faster RCNN 和 YOLO等目标检测算法在检测小尺寸物体方面不足。在深度卷积神经网络中，由于独立特征层在实际图像中仅占据很小的像素尺寸，因此缺乏足够的知识。由于低分辨率的小尺寸物体携带有限的上下文细节，因此很难检测到它们。为了克服这个问题，可以<strong>通过增强生成更多的数据，或者可以提高模型的输入分辨率</strong>等<h4 id=Multi-scale-object-detection><a title="Multi-scale object detection" class=headerlink href=#Multi-scale-object-detection></a>Multi-scale object detection</h4><p>在目标检测领域，多尺度目标检测是一项具有挑战性的任务。<strong>深度CNN的每一层都会生成特征图，而这些特征图生成的信息是相互独立的。多尺度对象的判别细节可以出现在骨干网络的任一层中，而对于小尺度对象，它出现在初始层中，并在后面的层中消散</strong>。在目标检测算法（一级和两级）中，预测是从最顶层进行的，这给检测多尺度对象（通常是小对象）的方式造成了障碍。为了克服这个困难;该文提出<strong>信息融合与DCNNs分层结构相结合的多层检测和特征融合</strong><p>代表性方法包括多尺度深度CNN 、深度监督目标检测（DSOD）和SSD。为了提高多尺度目标检测的可靠性，可以合并多层特征融合和多层检测。这包括特征金字塔网络（FPN）、反卷积单次检测器（DSSD）、尺度可转移检测网络（STDN）、与对象先验网络的反向连接（RON）、自上而下的调制（TDM）等几个具有代表性的框架。<h4 id=Intraclass-variation><a title="Intraclass variation" class=headerlink href=#Intraclass-variation></a>Intraclass variation</h4><p>类内variation是指<strong>同一类的不同图像之间发生的variation</strong>。<strong>它们的形状、大小、颜色、材料、质地等各不相同</strong>。对象实例看起来很灵活，可以在缩放和旋转方面轻松转换。这些被称为内在因素。外部因素也会产生一些明显的影响。<strong>它包括照明不当、天气条件、照明、低质量相机等。这种差异可能由多种因素引起</strong>，如遮挡、照明、位置、透视等。这个问题可以通过验证训练数据是否具有良好的多样性（包括上述所有因素）来克服<h4 id=Class-imbalance><a title="Class imbalance" class=headerlink href=#Class-imbalance></a>Class imbalance</h4><p>类之间的不规则数据分布称为类不平衡。简单来说，<strong>可以说当类包含不成比例数量的实例时，即在一个数据集中比另一个数据集中的标本多</strong>。从对象检测的角度来看，类不平衡可以分为两种类型：前景-背景不平衡和前景-前景不平衡。前者发生在训练过程中，与数据集中的类别数量无关。后者是指在样本数量范围内批次水平的不平衡，涉及正类。<strong>一般来说，一级目标探测器的精度低于两级目标探测器，其背后的原因之一是类别不平衡。为了解决这个问题，可以对类进行上采样和下采样，或者使用合成少数过采样技术（SMOTE）等生成合成数据</strong><h4 id=Generalization-issues><a title="Generalization issues" class=headerlink href=#Generalization-issues></a>Generalization issues</h4><p>当模型欠拟合或过拟合时，就会出现目标检测中的泛化问题。欠拟合可以在训练阶段的初始阶段识别出来，这个问题可以通过增加训练周期的数量或模型的复杂性来解决。对于过拟合，我们可以使用重要的方法，例如<strong>增加训练数据、提前停止、正则化方法（L1、L2）或丢弃层</strong><h2 id=3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey><a title="3D Object Detection for Autonomous Driving: A Comprehensive Survey" class=headerlink href=#3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey></a>3D Object Detection for Autonomous Driving: A Comprehensive Survey</h2><h3 id=abs><a class=headerlink href=#abs title=abs></a>abs</h3><p>近年来，自动驾驶因其减轻驾驶员负担和提高驾驶安全性的潜力而受到越来越多的关注。在现代自动驾驶管道中，感知系统是不可或缺的组成部分，<strong>旨在准确估计周围环境的状态，并为预测和规划提供可靠的观测结果。3D 物体检测旨在预测自动驾驶汽车附近 3D 物体的位置、大小和类别</strong>，是感知系统的重要组成部分。本文综述了自动驾驶三维目标检测的研究进展。首先，我们<strong>介绍了3D目标检测的背景，并讨论了该任务的挑战。其次，我们从模型和感官输入方面对3D目标检测的进展进行了全面调查，包括基于激光雷达、基于相机和多模态检测方法。我们还对每类方法的潜力和挑战进行了深入分析</strong>。此外，我们还系统地研究了3D目标检测在驾驶系统中的应用。最后，对三维目标检测方法进行了性能分析，进一步总结了多年来的研究趋势，并展望了该领域的未来发展方向。<h3 id=intro><a class=headerlink href=#intro title=intro></a>intro</h3><p>自动驾驶技术已广泛应用于许多场景，包括自动驾驶卡车、机器人出租车、送货机器人等，能够减少人为错误并增强道路安全性。作为自动驾驶系统的核心组成部分，汽车感知帮助自动驾驶汽车通过感官输入了解周围环境。<strong>感知系统通常以多模态数据（摄像头图像、激光雷达扫描仪点云、高清地图等）为输入，预测道路上关键要素的几何和语义信息。高质量的感知结果可作为目标跟踪、轨迹预测和路径规划等后续步骤的可靠观测</strong><p><img alt=image-20231222223259144 data-src=https://i.imgur.com/02X39qA.png><p>3D 对象检测旨在根据感官输入预测驾驶场景中 3D 对象的边界框。3D 目标检测的一般公式可以表示为</p><script type="math/tex; mode=display">
\begin{equation}\mathcal{B}=f_{det}(\mathcal{I}_{sensor}),\end{equation}</script><p>f~det~ 是 3D 对象检测模型，I~sensor~ 是一个或多个感官输入,B = {B1， · · · ， BN } 是场景中 N 个 3D 对象的集合。<p>如何表示 3D 对象 Bi 是此任务中的一个关键问题，因为它决定了应为以下预测和规划步骤提供哪些 3D 信息。在大多数情况下，3D 对象表示为包含此对象的 3D 长方体，</p><script type="math/tex; mode=display">
\begin{equation}B=[x_c,y_c,z_c,l,w,h,\theta,class],\end{equation}</script><p>其中 （x~c~， y~c~， z~c~） 是长方体的 3D 中心坐标，l、w、h 分别是长方体的长度、宽度和高度，θ 是长方体在地平面上的航向角，即偏航角，class 表示 3D 对象的类别，例如汽车、卡车、行人、骑自行车的人。此外也有其他模型使用了更多参数的.<p><strong>Sensory inputs</strong><p>有许多类型的传感器可以为 3D 物体检测提供原始数据。<strong>在传感器中，雷达、摄像头和LiDAR（光探测和测距）传感器是三种最广泛采用的传感类型</strong>。雷达具有较长的探测范围，并且对不同的天气条件具有鲁棒性。由于多普勒效应，雷达可以提供额外的速度测量。摄像头价格便宜且易于获取，对于理解语义（例如交通标志的类型）至关重要。尽管价格便宜，但相机在用于 3D 物体检测方面存在固有的局限性<strong>。相机只能捕获外观信息，无法直接获取场景的 3D 结构信息</strong>。另一方面，<strong>3D物体检测通常需要在3D空间中进行精确定位，而从图像中估计的3D信息（例如深度）通常具有较大的误差</strong>。图像的变形通常<strong>容易受到极端天气和时间条件的影响</strong>。在<strong>夜间或雾天从图像中检测物体比在晴天检测要困难得多</strong>，这导致了实现自动驾驶的足够鲁棒性的挑战。<p><img alt=image-20231222225107201 data-src=https://i.imgur.com/lS0F3vq.png><p>作为替代解决方案，<strong>LiDAR 传感器可以通过发射激光束然后测量其反射信息来获得场景的细粒度 3D 结构。</strong>一个激光雷达传感器发射 m 束并在一个扫描周期内进行 n 次测量，可以产生 I~range~ ∈ R^m×n×3^ 的距离图像,其中,范围图像的<strong>每个像素都包含球面坐标系中的距离 r、方位角α和倾角φ以及反射强度</strong>。<p><strong>范围(Range)图像是LiDAR传感器获得的原始数据格式，可以通过将球面坐标转换为笛卡尔坐标来进一步转换为点云。</strong><p>点云可以表示为 I~point~ ∈ R^N×3^，其中 N 表示场景中的点数，每个点有 3 个 xyz 坐标通道。<strong>距离图像和点云都包含由LiDAR传感器直接获取的精确3D信息</strong>。<p>因此，<strong>与相机相比，LiDAR 传感器更适合检测 3D 空间中的物体，并且 LiDAR 传感器也不太容易受到时间和天气变化的影响</strong>。然而，LiDAR 传感器比摄像头贵得多，这可能会限制在驾驶场景中的应用<p>2D 目标检测旨在<strong>在图像上生成 2D 边界框</strong>，是计算机视觉中的一个基本问题。<p>3D 目标检测方法借鉴了 2D 对应物的许多设计范式：<strong>region proposals生成和细化、锚点、非最大抑制等</strong>。然而，从很多方面来看，3D目标检测并不是2D目标检测方法对3D空间的简单改变。<p>（1）3D目标检测方法必须处理异构数据表示<strong>。点云检测需要新型算子和网络来处理不规则的点数据</strong>，而点云和图像的检测需要特殊的融合机制。<p>（2） 3D 目标检测方法通常<strong>利用不同的投影视图来生成对象预测</strong>。<p>与从透视图检测对象的 2D 对象检测方法相反，<strong>3D 方法必须考虑不同的视图来检测 3D 对象，例如从鸟瞰图、点视图和圆柱视图</strong>。（3）3D物体检测对物体在3D空间中的精确定位有很高的要求。分米级的定位误差可能导致行人和骑自行车的人等小物体的检测失败，而在二维物体检测中，几个像素的定位误差仍可能在预测边界框和地面实况边界框之间保持较高的交并 （IoU）。因此，精确的 3D 几何信息对于从点云或图像进行 3D 物体检测是必不可少的。<p>（1） LiDAR 和 RGB-D 传感器的点云分布不同。在室内场景中，点相对均匀地分布在扫描表面上，大多数 3D 对象在其表面上接收到足够数量的点。然而，在驾驶场景中，大多数点都落在 LiDAR 传感器的附近，而那些远离传感器的 3D 物体只会获得几个点。因此，在驾驶场景中，<strong>特别需要处理各种点云密度的三维物体，并准确检测那些远距离和稀疏的物体。</strong><p>（2）驾驶场景下的检测对推理时延有特殊要求。<strong>驾驶场景中的感知必须是实时的</strong>，以避免事故发生。因此，这些方法必须具有计算效率，否则它们将无法应用于实际应用。<h4 id=Datasets><a class=headerlink href=#Datasets title=Datasets></a>Datasets</h4><p><img alt=image-20231222230257242 data-src=https://i.imgur.com/HiTj4TF.png><h3 id=LiDAR-based-3D-Object-Detection><a title="LiDAR-based 3D Object Detection" class=headerlink href=#LiDAR-based-3D-Object-Detection></a>LiDAR-based 3D Object Detection</h3><p>我们将介绍基于LiDAR数据的3D目标检测方法，即点云或距离图像。回顾和分析了基于不同数据表示的基于 LiDAR 的 3D 目标检测模型，包括<strong>基于点</strong>、<strong>基于网格</strong>、<strong>基于点体素</strong>和<strong>基于距离</strong>的方法。(point-based, grid-based, point-voxel based, and range-based)<p><img alt=image-20231222230749845 data-src=https://i.imgur.com/7LLjGj3.png><h4 id=Data-representations-for-3D-object-detection><a title="Data representations for 3D object detection" class=headerlink href=#Data-representations-for-3D-object-detection></a>Data representations for 3D object detection</h4><p><strong>与像素有规律地分布在图像平面上的图像相比，点云是一种稀疏且不规则的 3D 表示</strong>，需要专门设计的模型进行特征提取。<strong>范围图像是一种密集而紧凑的表示形式，但范围像素包含 3D 信息而不是 RGB 值</strong>。因此，在范围图像上直接应用传统的卷积网络可能不是最佳解决方案。另一方面，自动驾驶场景中的检测通常具有实时推理的要求。因此，如何开发一个既<strong>能有效处理点云或范围图像数据又能保持高效率的模型</strong>，仍然是研究界面临的一个公开挑战。<h4 id=Point-based-3D-object-detection><a title="Point-based 3D object detection" class=headerlink href=#Point-based-3D-object-detection></a>Point-based 3D object detection</h4><p>基于点的3D目标检测方法通常继承了点云深度学习技术的成功，并<strong>提出了直接从原始点检测3D对象的多种架构</strong>。<p>点云首先通过基于<strong>点的骨干网络</strong>，在该网络中，点逐渐采样，点云操作学习特征。然后，根据下采样点和特征预测 3D 边界框。<p><img alt=image-20231222233252147 data-src=https://i.imgur.com/Vl1RhDd.png><p>基于点的 3D 对象检测器有两个基本组件：<strong>点云采样</strong>和<strong>特征学习</strong>。<p><strong>Point Cloud Sampling</strong>:PointNet++中的<strong>最远点采样（FPS）已被广泛用于基于点的检测器中</strong>，其中最远的点是<strong>从原始点集中依次选择的</strong>。PointRCNN 是一项开创性的工作，<strong>它采用 FPS 逐步对输入点云进行下采样，并从下采样点生成 3D 建议</strong>。类似的设计范式也被用于后续的许多工作，并进行了改进，如分割引导滤波、特征空间采样、随机采样、基于体素的采样(voxel-based sampling)和坐标细化(coordinate refinement)。<p><strong>Point Cloud Feature Learning</strong>:具体来说，首先通过球查询(ball query)在预定义的半径内收集上下文点。然后，通过多层感知器和maxpooling对上下文点和特征进行聚合，得到新的特征。还有其他使用不同点云算子的工作，包括图算子，注意力算子和Transformer。<p>基于点的检测器的表示能力主要受两个因素的限制：<strong>特征学习中采用的上下文点数和上下文半径</strong>。增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。<p><strong>增加上下文点的数量将获得更多的表示能力，但代价是增加大量的内存消耗</strong>。球查询中合适的上下文半径也是一个重要因素：如果半径太小，上下文信息可能不足，如果半径过大，细粒度的 3D 信息可能会丢失。必须仔细确定这两个因素，以平衡检测模型的有效性和效率。可并行进行随机均匀采样，效率高。然而，考虑到LiDAR扫描中的点不是均匀分布的，<strong>随机均匀采样可能倾向于对那些高点云密度的区域进行过度采样，而对那些稀疏区域进行采样不足，这通常会导致与最远点采样相比性能较差。</strong><p><strong>最远点采样及其变体可以通过从现有点集中依次选择最远的点来获得更均匀的采样结果。然而，最远点采样本质上是一种顺序算法，不能变得高度并行</strong>。因此，最远点采样通常很耗时，并且无法进行实时检测。<h4 id=Grid-based-3D-object-detection><a title="Grid-based 3D object detection" class=headerlink href=#Grid-based-3D-object-detection></a>Grid-based 3D object detection</h4><p><img alt=image-20231223135445200 data-src=https://i.imgur.com/O8n1SeP.png><p>基于网格的 3D 对象检测器首先<strong>将点云栅格化为离散的网格表示</strong>，即<strong>体素、柱子和鸟瞰图 （BEV） 特征图</strong>。然后，他们<strong>应用传统的 2D 卷积神经网络或 3D 稀疏神经网络</strong>从网格中提取特征。最后，可以从BEV网格单元中检测到3D物体。基于网格的 3D 目标检测图如图 所示。基于网格的检测器有两个基本组件：<strong>基于网格的表示</strong>和<strong>基于网格的神经网络</strong><p><strong>Grid-based representations</strong>:<p><strong>voxels</strong>。如果<strong>将检测空间栅格化为规则的 3D 格网</strong>，则体素就是格网像元。<strong>如果点云落入此格网像元中，则体素可以为非空。</strong>由于点云分布稀疏，因此 3D 空间中的大多数体素单元格都是空的，不包含任何点。<strong>在实际应用中，只有那些非空体素才会被存储并用于特征提取。</strong>VoxelNet是一项开创性的工作，它利用稀疏的体素网格，提出了一种新的体素特征编码（VFE）层，从体素单元内的点中提取特征。<p>随后的一系列工作采用了类似的体素编码策略。此外，还有两类方法试图改进 3D 目标检测的体素表示：（1） <strong>多视图体素</strong>。一些方法从不同的视角提出了动态体素化和融合方案，例如鸟瞰图和透视图、圆柱面和球面视角、距离视角。（2）<strong>多尺度体素</strong>。一些论文生成不同尺度的体素或使用可重构的体素<p><strong>Pillars</strong>:柱子可以看作是特殊的体素，<strong>其中体素大小在垂直方向上是无限的</strong>。<strong>支柱特征可以通过PointNet从点聚合，然后散射回来</strong>，构建2D BEV图像进行特征提取。PointPillars 是一部开创性的著作，它引入了Pillar表示<p><strong>BEV feature maps</strong>:鸟瞰图特征图是一种密集的 2D 表示，其中<strong>每个像素对应于一个特定区域</strong>，<strong>并对该区域中的点信息进行编码</strong>。BEV特征图可以通过将3D特征投影到鸟瞰图中，从体素和柱子中获取，也可以通过汇总像素区域内的点统计数据，直接从原始点云中获取。<p><strong>Grid-based neural networks</strong>:基于网格的网络主要有两种类型：<strong>用于 BEV 特征图</strong>和<strong>Pillar的 2D 卷积神经网络</strong>，以及<strong>用于体素的 3D 稀疏神经网络</strong>。<p>2D convolutional neural networks:传统的 2D 卷积神经网络可以应用于 BEV 特征图，以从鸟瞰图检测 3D 对象。在大多数作品中，2D网络架构通常都是从2D目标检测中的成功设计中改编而来的,比如ResNet，区域建议网络（RPN）和特征金字塔网络（FPN）<p>3D 稀疏神经网络。3D稀疏卷积神经网络基于两个专门的3D卷积算子：sparse convolutions和submanifold convolutions，<strong>它们只能在那些非空体素上有效地进行3D卷积。与在整个体素空间上执行标准3D卷积相比，稀疏卷积算子效率更高</strong>，可以获得实时推理速度。<p>SECOND是一项开创性的工作，它使用基于 GPU 的哈希表实现了这两个稀疏算子，并构建了一个稀疏卷积网络来提取 3D 体素特征。<p>这种网络架构已在许多工程中得到应用，并成为基于体素的探测器中使用最广泛的骨干网络。还有一系列工作试图改进稀疏算子，将扩展为两级检测器，并将Transformer架构引入基于体素的检测。<p>与 BEV 特征图和Pillar等 2D 表示相比，体素包含更结构化的 3D 信息。此外，<strong>深度体素特征可以通过 3D 稀疏网络学习</strong>。然而，<strong>3D 神经网络会带来额外的时间和内存成本</strong>。<strong>BEV 特征图是最有效的网格表示，可直接将点云投影到 2D 伪图像中，而无需专门的 3D 运算符</strong>，如稀疏卷积或柱状编码。<strong>2D检测技术也可以无缝应用于BEV特征图，无需太多修改。</strong>基于BEV的检测方法通常可以获得高效率和实时推理速度。但是，<strong>简单地汇总像素区域内的点统计数据会丢失太多的 3D 信息</strong>，与基于体素的检测相比，这会导致检测结果的准确性较低。基于Pillar的检测方法利用 PointNet 对Pillar内的 3D 点信息进行编码，然后将特征分散回 2D 伪图像中以实现高效检测，从而平衡了 3D 目标检测的有效性和效率<p><strong>challenges of the grid-based detection methods</strong>:所有基于网格的方法都必须面对的一个关键问题是选择适当大小的网格单元。<strong>网格表示本质上是点云的离散格式</strong>，通过将连续的点坐标转换为离散的网格索引。<p>量化过程不可避免地会丢失一些 3D 信息<strong>，其有效性很大程度上取决于网格单元的大小：网格尺寸越小，网格就越高分辨率，因此可以保留更细粒度的细节，这对于准确的 3D 对象检测至关重要</strong>,然而，减小网格单元的大小会导致 2D 网格表示（如 BEV 特征图或支柱）的内存消耗呈二次增加。至于像体素这样的 3D 网格表示，问题可能会变得更加严重。因此，<strong>如何平衡较小网格尺寸带来的功效和内存增加影响的效率仍然是所有基于网格的三维目标检测方法面临的一个悬而未决的挑战</strong>。<h4 id=Point-voxel-based-3D-object-detection><a title="Point-voxel based 3D object detection" class=headerlink href=#Point-voxel-based-3D-object-detection></a><strong>Point-voxel based 3D object detection</strong></h4><p>基于点体素的方法采用混合架构，<strong>利用点和体素进行 3D 对象检测</strong>。这些方法可以分为两类：单阶段和两阶段检测框架。<p><strong>Single-stage point-voxel detection frameworks.</strong><p>基于单级点体素的 3D 目标检测器<strong>试图将点和体素的特征与骨干网络中的点到体素和体素到点变换联系起来</strong><p><strong>点包含细粒度的几何信息，体素的计算效率很高，在特征提取阶段将它们组合在一起自然会从这两种表示中受益</strong>。在骨干网中利用点-体素特征融合的思想已被许多著作探索，其贡献包括点-体素卷积，辅助点网络和多尺度特征融合.<p><strong>Two-stage point-voxel detection frameworks</strong><p>基于点体素的两级 3D 对象检测器<strong>针对不同的检测阶段采用不同的数据表示</strong>。<p>具体来说，在第一阶段，采用<strong>基于体素的检测框架来生成一组 3D 对象建议</strong>,在第二阶段，<strong>首先从输入点云中对关键点进行采样，然后通过新的点算子从关键点进一步细化3D建议</strong>。<p>基于点体素的方法自然可以受益于从点获得的细粒度 3D 形状和结构信息以及体素带来的计算效率。然而，这些方法仍然存在一些挑战。对于混合点-体素骨干网，点-体素特征的融合一般<strong>依赖于体素-点和点-体素变换机制，可以带来不可忽视的时间成本</strong>。对于两阶段点体素检测框架，一个关键的挑战是<strong>如何有效地聚合 3D 提案的点特征，因为现有的模块和运算符通常非常耗时</strong>。综上所述，与纯基于体素的检测方法相比，基于点体素的检测方法可以获得更好的检测精度，但代价是增加了推理时间。<h4 id=Range-based-3D-object-detection><a title="Range-based 3D object detection" class=headerlink href=#Range-based-3D-object-detection></a>Range-based 3D object detection</h4><p>范围图像是一种密集而紧凑的 2D 表示，其中<strong>每个像素都包含 3D 距离信息，而不是 RGB 值</strong>。基于距离的方法从两个方面解决了检测问题：<strong>设计适合距离图像的新模型和算子</strong>，以及<strong>选择合适的视图进行检测</strong>。<p><img alt=image-20231223162320834 data-src=https://i.imgur.com/r7H1L1T.png style=zoom:67%;><p><strong>Range-based detection models</strong><p>由于距离图像是与RGB图像一样的2D表示，因此基于范围的3D对象检测器可以自然地借用2D对象检测中的模型来处理范围图像。<p>LaserNet 是一项开创性的工作，它利用深层聚合网络 （DLA-Net）从距离图像中获取多尺度特征并检测 3D 对象。一些论文还采用了其他 2D 目标检测架构<p><strong>Range-based operators</strong><p>距离图像的像素包含 3D 距离信息而不是颜色值，<strong>因此传统 2D 网络架构中的标准卷积算子对于基于范围的检测来说不是最佳选择</strong>，因为滑动窗口中的像素在 3D 空间中可能彼此相距很远。一些工作采用新颖的算子来有效地从范围像素中提取特征，包括范围扩展卷积、图算子和元核卷积<p><strong>Views for range-based detection</strong><p>范围图像是从范围视图 （RV） 捕获的，理想情况下，范围视图是点云的球面投影。<p>然而，从距离视图进行检测时，<strong>不可避免地会受到球面投影带来的遮挡和尺度变化问题的影响</strong>。<p>为了规避这些问题，许多方法都在<strong>利用其他视图来预测3D物体，例如中采用的圆柱视图（CYV），在中采用的距离视图、鸟瞰视图（BEV）和/或点视图（PV）的组合</strong><p><strong>Analysis: potentials and challenges of the range-based methods</strong><p>范围图像是一种密集而紧凑的 2D 表示，因此<strong>传统或专用的 2D 卷积可以无缝地应用于范围图像</strong>，这使得特征提取过程非常高效.然而，<strong>与鸟瞰图检测相比，距离图检测容易受到遮挡和尺度变化的影响</strong>。因此，<strong>从距离视图中提取特征</strong>，<strong>从鸟瞰图进行目标检测</strong>，成为基于距离的3D目标检测最实用的解决方案。<h3 id=Learning-objectives-for-3D-object-detection><a title="Learning objectives for 3D object detection" class=headerlink href=#Learning-objectives-for-3D-object-detection></a>Learning objectives for 3D object detection</h3><p>学习目标在对象检测中至关重要。<strong>由于 3D 物体相对于整个检测范围非常小，因此在 3D 检测中非常需要特殊的机制来增强小物体的定位</strong>。另一方面，<strong>考虑到点云稀疏且物体通常具有不完整的形状</strong>，准确估计 3D 物体的中心和大小是一项长期存在的挑战。<h4 id=Anchor-based-3D-object-detection><a title="Anchor-based 3D object detection" class=headerlink href=#Anchor-based-3D-object-detection></a>Anchor-based 3D object detection</h4><p>锚点(anchors)是具有固定形状的预定义长方体,可以放置在 3D 空间中。<p><img alt=image-20231223174714325 data-src=https://i.imgur.com/jFknBKH.png><p>3D 对象可以基于与真实值具有高交集 （IoU） 的正锚点进行预测。将从锚点配置和损失函数方面介绍基于锚点的三维目标检测方法。<p>Anchor configurations:基于锚点的 3D 对象检测方法通常<strong>从鸟瞰图检测 3D 对象</strong>,其中 3D 锚框放置在 BEV 特征图的每个网格单元上。3D 锚点通常对每个类别具有固定大小，因为同一类别的对象具有相似的大小<p>Loss functions:基于锚点的方法利用分类损失Lcls来学习正负锚点，利用回归损失Lreg来学习基于正锚点的物体的大小和位置。此外，L~θ~ 用于学习物体的航向角。</p><script type="math/tex; mode=display">
\begin{equation}L_{det}=L_{cls}+L_{reg}+L_\theta.\end{equation}</script><p>回归目标可以进一步应用于这些正锚点,以学习 3D 对象的大小和位置.</p><script type="math/tex; mode=display">
\begin{equation}\begin{aligned}\Delta x&=\frac{x^g-x^a}{d^a},\Delta y=\frac{y^g-y^a}{d^a},\Delta z=\frac{z^g-z^a}{h^a},\\\Delta l&=\log(\frac{l^g}{l^a}),\Delta w=\log(\frac{w^g}{w^a}),\Delta h=\log(\frac{h^g}{h^a}),\end{aligned}\end{equation}</script><script type="math/tex; mode=display">
\begin{equation}L_{cls}^{bce}=-[q\cdot\log(p)+(1-q)\cdot\log(1-p)]\end{equation}</script><p>p 是每个锚点的预测概率，如果锚点为正，则目标 q 为 1，否则为 0</p><script type="math/tex; mode=display">
\begin{equation}d^a=\sqrt{(l^a)^2+(w^a)^2}\end{equation}</script><p>此外还有使用Focal Loss,</p><script type="math/tex; mode=display">
\begin{equation}L_{cls}^{focal}=-\alpha(1-p)^\gamma\log(p),\end{equation}</script><p>使用SmoothL1 loss用于回归</p><script type="math/tex; mode=display">
\begin{equation}L_{reg}=\sum_{\begin{array}{c}u\in\{x,y,z,l,w,h\},\\v\in\{\Delta x,\Delta y,\Delta z,\Delta l,\Delta w,\Delta h\}\end{array}}\text{SmoothL1}(u-v).\end{equation}</script><p>为了学习航向角 θ，弧度方向偏移量可以直接用 SmoothL1 损失回归</p><script type="math/tex; mode=display">
\begin{equation}\begin{aligned}\Delta\theta&=\theta^g-\theta^a,\\L_\theta&=\text{SmoothL}1(\theta-\Delta\theta).\end{aligned}\end{equation}</script><p>然而，由于回归范围较大,直接回归弧度偏移通常很困难,另外<strong>,基于bin的航向估计</strong>是学习航向角的较好解，其中<strong>首先将角度空间划分为bin,并采用基于bin的分类L~dir~和残差回归</strong>.</p><script type="math/tex; mode=display">
\begin{equation}L_\theta=L_{dir}+\text{SmoothL}1(\theta-\Delta\theta^{\prime}),\end{equation}</script><p>正弦函数也可用于对弧度偏移进行编码</p><script type="math/tex; mode=display">
\begin{equation}\Delta\theta=\sin(\theta^g-\theta^a),\end{equation}</script><p>除了分别学习<strong>物体大小</strong>、<strong>位置</strong>和<strong>方向</strong>的损失函数外，将所有物体参数视为一个整体的交并（IoU）损失也可以应用于3D物体检测</p><script type="math/tex; mode=display">
\begin{equation}L_{IoU}=1-IoU(b^g,b),\end{equation}</script><p>其中 C^G^ ~i~ 和 C~I~ 分别是地面实况和预测长方体的第 i 个角。<p>基于锚点的方法可以从同一类别的 3D 对象应该具有相似形状的先验知识中受益，因此它们可以在 <strong>3D 锚点的帮助下生成准确的对象预测</strong>。然而，<strong>由于3D物体相对于检测范围相对较小，因此需要大量的锚点来确保整个检测范围的完全覆盖</strong>，例如，在KITTI 数据集的中使用了大约70k个锚点。<strong>此外，对于那些非常小的物体，如行人和骑自行车的人，应用基于锚点的分配可能非常具有挑战性</strong>。考虑到锚点通常放置在每个网格单元的中心，如果网格单元较大而单元中的对象较小，则该单元的锚点可能与小对象具有较低的 IoU，这可能会阻碍训练过程。<h3 id=Anchor-free-3D-object-detection><a title="Anchor-free 3D object detection" class=headerlink href=#Anchor-free-3D-object-detection></a>Anchor-free 3D object detection</h3><p>无anchor box方法消除了复杂的anchor box设计，可以灵活地应用于不同的视图，例如鸟瞰图、点视图和范围视图。<p><img alt=image-20231223190845168 data-src=https://i.imgur.com/6IcyJdX.png><p>基于锚点和无锚点方法之间的<strong>主要区别在于正样本和负样本的选择</strong>。<p><strong>Grid-based assignment</strong>:与依赖于带有锚点的 IoU 来确定正负样本的基于锚点的方法相比，无锚点方法<strong>利用各种基于grid的分配策略来评估 BEV 网格单元、Pillar和体素</strong><p>PIXOR是一项开创性的工作，它<strong>利用地面实况 3D 物体内部的网格单元作为正例，而其他则作为负例。</strong>Pillar-based object detection for autonomous driving.采用了这种内部对象分配策略，并在中通过选择最接近对象中心的网格单元进一步改进。CenterPoint利用每个对象中心的高斯核来分配正标签。<p>损失函数上,分类损失基本不变.但回归损失改变如下</p><script type="math/tex; mode=display">
\begin{equation}\Delta=[dx,dy,z^g,\log(l^g),\log(w^g),\log(h^g),\sin(\theta^g),\cos(\theta^g)],\end{equation}</script><p>DX 和 DY 是正grid cells和对象中心之间的偏移量。<p><strong>Point-based assignment.</strong><p>大多数基于点的检测方法<strong>采用无锚点和基于点的分配策略</strong>，其中<strong>首先对点进行分割，然后选择 3D 对象内部或附近的前景点作为正样本</strong>，最后从这些前景点中学习 3D 边界框。<strong>大多数基于点的检测器都采用了这种前景点分割策略，并进行了改进，例如增加了中心度分数</strong><p><strong>Range-based assignment</strong><p>无锚点分配也可以用于范围图像。<strong>一种常见的解决方案是选择 3D 对象内部的范围像素作为正样本</strong>。与其他回归目标基于全局三维坐标系的方法不同，<strong>基于范围的方法采用以对象为中心的坐标系进行回归</strong>。<p><strong>Set-to-set assignment.</strong>DETR是一种颇具影响力的 2D 检测方法，它引入了<strong>一种集到集的分配策略</strong>，通过匈牙利算法自动将预测分配给相应的地面实况</p><script type="math/tex; mode=display">
\begin{equation}\mathcal{M}^*=\underset{\mathcal{M}}{\operatorname*{argmin}}\sum_{(i\to j)\in\mathcal{M}}L_{det}(b_i^g,b_j),\end{equation}</script><p>其中 M 是从每个阳性样本到 3D 对象的一对一映射。<h3 id=3D-object-detection-with-auxiliary-tasks><a title="3D object detection with auxiliary tasks" class=headerlink href=#3D-object-detection-with-auxiliary-tasks></a>3D object detection with auxiliary tasks</h3><p>许多方法都采用辅助任务来增强空间特征，<strong>并为精确的 3D 目标检测提供隐式指导。常用的辅助任务包括语义分割、交集并集预测、对象形状补全和对象部分估计</strong>。<p><img alt=image-20231223195729269 data-src=https://i.imgur.com/TkWPitq.png><p><strong>Semantic segmentation</strong>:语义分割可以从3个方面帮助3D目标检测：（1）前景分割可以提供对象位置的隐性信息。在大多数基于点的 3D 目标检测器 中，逐点前景分割已被广泛采用，用于生成提案。（2）空间特征可以通过分割来增强。在文献[347]中，利用语义上下文编码器来增强具有语义知识的空间特征。（3）语义分割可以作为预处理步骤，过滤掉背景样本，使3D目标检测更加高效。<p><strong>IoU prediction</strong>:交集并集 （IoU） 可以作为纠正对象置信度分数的有用监督信号。Cia-ssd: Confident iou-aware single-stage object detector from point cloud. In: AAAI<strong>提出了一个辅助分支来预测每个检测到的 3D 对象的 IoU 分数 S~IoU~</strong>。</p><script type="math/tex; mode=display">
\begin{equation}S_{conf}=S_{cls}\cdot(S_{IoU})^\beta,\end{equation}</script><p>在推理过程中,来自传统分类分支的原始置信度分数 S~con~f = S~cls~ 被 IoU 分数 SIoU 进一步校正.其中，超参数β控制抑制低 IoU 预测和增强高 IoU 预测的程度。<strong>通过 IoU 校正,更容易选择高质量的 3D 对象作为最终预测</strong>。<p><strong>Object shape completion</strong><p>由于LiDAR传感器的性质，<strong>远处物体通常只在其表面上接收到几个点，因此3D物体通常是稀疏和不完整的</strong>。提高检测性能的一种直接方法是从稀疏点云中完成物体形状。完整的形状可以为准确和稳健的检测提供更多有用的信息<strong>。在3D检测中已经提出了许多形状补全技术，包括形状解码器、形状特征和概率占用网格</strong><p><strong>Object part estimation.</strong><p>识别对象内部的零件信息有助于 3D 对象检测，因为它可以显示对象的更细粒度的 3D 结构信息。<p>3D 对象检测与许多其他 3D 感知和生成任务具有内在关联性。与独立训练 3D 目标检测器相比，3D 检测和分割的多任务学习更有利，形状补全也有助于 3D 目标检测。还有其他任务可以帮助提高 3D 对象检测器的性能。例如，场景流估计可以识别静态和移动对象，在点云序列中跟踪相同的 3D 对象可以更准确地估计该对象。因此，将更多的感知任务集成到现有的3D目标检测管道中将是有希望的。</p><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=popular-posts-header>相关文章</div><ul class=popular-posts><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\11\01\目标检测学习-P3\ rel=bookmark>目标检测学习_P3</a></div><li class=popular-posts-item><div class=popular-posts-title><a href=\2023\10\21\目标检测-初识\ rel=bookmark>目标检测_初识</a></div></ul><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/12/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BB%BC%E8%BF%B0/ title=目标检测综述>https://www.sekyoro.top/2023/12/22/目标检测综述/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/object-detection/ rel=tag><i class="fa fa-tag"></i> object detection</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/12/04/Python%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8B%E8%B7%AF/ rel=prev title=Python的工程化之路> <i class="fa fa-chevron-left"></i> Python的工程化之路 </a></div><div class=post-nav-item><a href=/2023/12/24/Github-bot%E5%88%9B%E5%BB%BA/ rel=next title=Github_bot创建> Github_bot创建 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#Abs><span class=nav-number>1.</span> <span class=nav-text>Abs</span></a><li class="nav-item nav-level-2"><a class=nav-link href=#Intro><span class=nav-number>2.</span> <span class=nav-text>Intro</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95><span class=nav-number>2.1.</span> <span class=nav-text>传统方法</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#RCNN><span class=nav-number>2.1.1.</span> <span class=nav-text>RCNN</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SPP-Net><span class=nav-number>2.1.2.</span> <span class=nav-text>SPP-Net</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Fast-RCNN><span class=nav-number>2.1.3.</span> <span class=nav-text>Fast RCNN</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Faster-RCNN><span class=nav-number>2.1.4.</span> <span class=nav-text>Faster RCNN</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Feature-pyramid-network><span class=nav-number>2.1.5.</span> <span class=nav-text>Feature pyramid network</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Mask-RCNN><span class=nav-number>2.1.6.</span> <span class=nav-text>Mask RCNN</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#One-stage-object-detectors><span class=nav-number>2.2.</span> <span class=nav-text>One-stage object detectors</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#DetectorNet><span class=nav-number>2.2.1.</span> <span class=nav-text>DetectorNet</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Overfeat><span class=nav-number>2.2.2.</span> <span class=nav-text>Overfeat</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#YOLO><span class=nav-number>2.2.3.</span> <span class=nav-text>YOLO</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#SSD><span class=nav-number>2.2.4.</span> <span class=nav-text>SSD</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Backbone-networks><span class=nav-number>2.3.</span> <span class=nav-text>Backbone networks</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#ZFNet><span class=nav-number>2.4.</span> <span class=nav-text>ZFNet</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#VGGNet><span class=nav-number>2.4.1.</span> <span class=nav-text>VGGNet</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#GoogLeNet-%E6%88%96-inception-v1><span class=nav-number>2.4.2.</span> <span class=nav-text>GoogLeNet 或 inception v1</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#ResNet><span class=nav-number>2.4.3.</span> <span class=nav-text>ResNet</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#DenseNet><span class=nav-number>2.4.4.</span> <span class=nav-text>DenseNet</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Problems-of-object-detection-and-its-solutions><span class=nav-number>2.5.</span> <span class=nav-text>Problems of object detection and its solutions</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Small-object-detection><span class=nav-number>2.5.1.</span> <span class=nav-text>Small object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Multi-scale-object-detection><span class=nav-number>2.5.2.</span> <span class=nav-text>Multi-scale object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Intraclass-variation><span class=nav-number>2.5.3.</span> <span class=nav-text>Intraclass variation</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Class-imbalance><span class=nav-number>2.5.4.</span> <span class=nav-text>Class imbalance</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Generalization-issues><span class=nav-number>2.5.5.</span> <span class=nav-text>Generalization issues</span></a></ol></ol><li class="nav-item nav-level-2"><a class=nav-link href=#3D-Object-Detection-for-Autonomous-Driving-A-Comprehensive-Survey><span class=nav-number>3.</span> <span class=nav-text>3D Object Detection for Autonomous Driving: A Comprehensive Survey</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#abs><span class=nav-number>3.1.</span> <span class=nav-text>abs</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#intro><span class=nav-number>3.2.</span> <span class=nav-text>intro</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Datasets><span class=nav-number>3.2.1.</span> <span class=nav-text>Datasets</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#LiDAR-based-3D-Object-Detection><span class=nav-number>3.3.</span> <span class=nav-text>LiDAR-based 3D Object Detection</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Data-representations-for-3D-object-detection><span class=nav-number>3.3.1.</span> <span class=nav-text>Data representations for 3D object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Point-based-3D-object-detection><span class=nav-number>3.3.2.</span> <span class=nav-text>Point-based 3D object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Grid-based-3D-object-detection><span class=nav-number>3.3.3.</span> <span class=nav-text>Grid-based 3D object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Point-voxel-based-3D-object-detection><span class=nav-number>3.3.4.</span> <span class=nav-text>Point-voxel based 3D object detection</span></a><li class="nav-item nav-level-4"><a class=nav-link href=#Range-based-3D-object-detection><span class=nav-number>3.3.5.</span> <span class=nav-text>Range-based 3D object detection</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Learning-objectives-for-3D-object-detection><span class=nav-number>3.4.</span> <span class=nav-text>Learning objectives for 3D object detection</span></a><ol class=nav-child><li class="nav-item nav-level-4"><a class=nav-link href=#Anchor-based-3D-object-detection><span class=nav-number>3.4.1.</span> <span class=nav-text>Anchor-based 3D object detection</span></a></ol><li class="nav-item nav-level-3"><a class=nav-link href=#Anchor-free-3D-object-detection><span class=nav-number>3.5.</span> <span class=nav-text>Anchor-free 3D object detection</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#3D-object-detection-with-auxiliary-tasks><span class=nav-number>3.6.</span> <span class=nav-text>3D object detection with auxiliary tasks</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>215</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>16</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>197</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a><li class=links-of-blogroll-item><a href=https://www.lllomh.com/ rel=noopener target=_blank title=https://www.lllomh.com/>芈渡</a><li class=links-of-blogroll-item><a href=https://protool-ten.vercel.app/ rel=noopener target=_blank title=https://protool-ten.vercel.app/>protools</a></ul></div><div class="motion-element announcement"><div class=title></div><p class=content><p class=date></div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/object-detection/ rel=tag>object detection</a><span class=tag-list-count>3</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>2m</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>30:46</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>