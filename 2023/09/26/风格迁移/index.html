<!doctypehtml><html lang=zh-CN><script defer src=/live2d-widget/autoload.js></script><meta charset=UTF-8><meta content=width=device-width,initial-scale=1,maximum-scale=2 name=viewport><meta content=#222 name=theme-color><meta content="Hexo 5.4.0" name=generator><link href=/images/blog_32px.png rel=apple-touch-icon sizes=180x180><link href=/images/blog_32px.png rel=icon sizes=32x32 type=image/png><link href=/images/blog_16px.png rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg rel=mask-icon><meta content=EPrJAp11bJwHULpQUaSNSZ8_3RcvTsPDAEGOME4pl1w name=google-site-verification><!-- Google tag (gtag.js) --><!-- 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VB21D8MKKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VB21D8MKKW');
</script> --><!-- google adsense in head.swig --><script async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4034523802263123></script><meta content=7226864CE87CE9DE8C008385273846FF name=msvalidate.01><meta content=code-fjFXVtiL7j name=baidu-site-verification><link href=/css/main.css rel=stylesheet><link as=style href=https://fonts.googleapis.com/css?family=Roboto%20Mono,Roboto:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext onload=this.rel='stylesheet' rel=preload><link as=style href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css onload=this.rel='stylesheet' rel=preload><link href=https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap rel=stylesheet><link href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/pace-js@1/pace.min.js></script><script id=hexo-configurations>var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sekyoro.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":240,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"3G9PZZIKCH","apiKey":"8eb71f5ca3167e9ef3487882f10cfaad","indexName":"SekyoroSearch","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};</script><link href=//cdn.bootcss.com/animate.css/3.5.0/animate.min.css rel=stylesheet><meta content="将一个图像中的风格应用在另一图像之上，即风格迁移（style transfer）这里我们需要两张输入图像：一张是内容图像，另一张是风格图像。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。" name=description><meta content=article property=og:type><meta content=风格迁移 property=og:title><meta content=https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/index.html property=og:url><meta content=Sekyoro的博客小屋 property=og:site_name><meta content="将一个图像中的风格应用在另一图像之上，即风格迁移（style transfer）这里我们需要两张输入图像：一张是内容图像，另一张是风格图像。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。" property=og:description><meta content=zh_CN property=og:locale><meta content=https://zh.d2l.ai/_images/neural-style.svg property=og:image><meta content=https://s2.loli.net/2023/10/06/W4Ey98RPmMslXwJ.png property=og:image><meta content=https://s2.loli.net/2023/10/06/S5DUiCZE8K9cwnA.png property=og:image><meta content=https://s2.loli.net/2023/10/06/9UaWgiykmJt5Lfz.png property=og:image><meta content=https://s2.loli.net/2023/10/06/H2QAYtzVKRXMNmw.png property=og:image><meta content=https://s2.loli.net/2023/10/06/Pg1q59j4WnuSCIo.png property=og:image><meta content=https://s2.loli.net/2023/10/06/X1qoLVJPAZbYIHr.png property=og:image><meta content=https://s2.loli.net/2023/10/06/ivem7EW1CjXrZN8.png property=og:image><meta content=https://s2.loli.net/2023/10/06/HvpYWiA2uUdz9xe.png property=og:image><meta content=2023-09-26T03:00:48.000Z property=article:published_time><meta content=2023-11-06T14:12:50.905Z property=article:modified_time><meta content=Sekyoro property=article:author><meta content="style transfer" property=article:tag><meta content=summary name=twitter:card><meta content=https://zh.d2l.ai/_images/neural-style.svg name=twitter:image><link href=https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/ rel=canonical><script id=page-configurations>// https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };</script><title>风格迁移 | Sekyoro的博客小屋</title><noscript><style>.use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }</style></noscript><link href=/atom.xml rel=alternate title=Sekyoro的博客小屋 type=application/atom+xml><body itemscope itemtype=http://schema.org/WebPage><canvas style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" class=fireworks></canvas><script defer src=https://cdn.bootcss.com/animejs/2.2.0/anime.min.js></script><script defer src=/js/src/fireworks.js></script><div class="container use-motion"><div class=headband></div><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div aria-label=切换导航栏 class=toggle><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class=site-meta><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <h1 class=site-title>Sekyoro的博客小屋</h1> <span class=logo-line-after><i></i></span> </a></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu" id=menu><li class="menu-item menu-item-home"><a href=/ rel=section><i class="fa fa-home fa-fw"></i>首页</a><li class="menu-item menu-item-about"><a href=/about/ rel=section><i class="fa fa-user fa-fw"></i>关于</a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section><i class="fa fa-tags fa-fw"></i>标签</a><li class="menu-item menu-item-categories"><a href=/categories/ rel=section><i class="fa fa-th fa-fw"></i>分类</a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section><i class="fa fa-archive fa-fw"></i>归档</a><li class="menu-item menu-item-bangumis"><a href=/bangumis/ rel=section><i class="fa fa-film fa-fw"></i>追番</a><li class="menu-item menu-item-resume"><a href=/resume/ rel=section><i class="fa fa-file-pdf fa-fw"></i>简历</a><li class="menu-item menu-item-materials"><a href=/materials/ rel=section><i class="fa fa-book fa-fw"></i>学习资料</a><li class="menu-item menu-item-sitemap"><a href=/sitemap.xml rel=section><i class="fa fa-sitemap fa-fw"></i>站点地图</a><li class="menu-item menu-item-search"><a class=popup-trigger role=button><i class="fa fa-search fa-fw"></i>搜索 </a></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon> <i class="fa fa-search"></i> </span><div class=search-input-container></div><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span></div><div class=algolia-results><div id=algolia-stats></div><div id=algolia-hits></div><div class=algolia-pagination id=algolia-pagination></div></div></div></div></div></header><a class="book-mark-link book-mark-link-fixed" role=button></a><main class=main><div class=main-inner><div class=content-wrap><div class="content post posts-expand"><article class=post-block itemscope itemtype=http://schema.org/Article lang=zh-CN><link href=https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta content=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg itemprop=image> <meta content=Sekyoro itemprop=name> <meta content=什么也无法舍弃的人，什么也做不了. itemprop=description> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content=Sekyoro的博客小屋 itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title>风格迁移</h1><div class=post-meta><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" title="创建时间：2023-09-26 11:00:48" datetime=2023-09-26T11:00:48+08:00>2023-09-26</time> </span><span class=post-meta-item> <span class=post-meta-item-icon> <i class="far fa-calendar-check"></i> </span> <span class=post-meta-item-text>更新于</span> <time title="修改时间：2023-11-06 22:12:50" datetime=2023-11-06T22:12:50+08:00 itemprop=dateModified>2023-11-06</time> </span><span style="display: none;" class=post-meta-item id=busuanzi_container_page_pv title=阅读次数> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=post-meta-item-text>阅读次数：</span> <span id=busuanzi_value_page_pv></span> </span><br><span class=post-meta-item title=本文字数> <span class=post-meta-item-icon> <i class="far fa-file-word"></i> </span> <span class=post-meta-item-text>本文字数：</span> <span>13k</span> </span><span class=post-meta-item title=阅读时长> <span class=post-meta-item-icon> <i class="far fa-clock"></i> </span> <span class=post-meta-item-text>阅读时长 ≈</span> <span>12 分钟</span> </span></div></header><div class=post-body itemprop=articleBody><p>将一个图像中的风格应用在另一图像之上，即<em>风格迁移</em>（style transfer）这里我们需要两张输入图像：一张是<em>内容图像</em>，另一张是<em>风格图像</em>。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像。</p><span id=more></span><h2 id=使用预训练模型进行迁移><a class=headerlink href=#使用预训练模型进行迁移 title=使用预训练模型进行迁移></a>使用预训练模型进行迁移</h2><p><img alt=风格迁移 data-src=https://zh.d2l.ai/_images/neural-style.svg><p>首先，我们初始化合成图像，例如将其初始化为内容图像。<p>该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。<p>接下来，我们通过前向传播（实线箭头方向）计算风格迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。 风格迁移常用的损失函数由3部分组成：<ol><li><em>内容损失</em>使合成图像与内容图像在内容特征上接近；<li><em>风格损失</em>使合成图像与风格图像在风格特征上接近；<li><em>全变分损失</em>则有助于减少合成图像中的噪点。</ol><p>最后，当模型训练结束时，我们输出风格迁移的模型参数，即得到最终的合成图像。<p>使用一个预训练模型,比如VGG提取内容与特征.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>pretrained_net = torchvision.models.vgg19(pretrained=<span class=literal>True</span>)</span><br></pre></table></figure><p>为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出。<p>一般来说，<strong>越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。</strong> <strong>为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，即<em>内容层</em>，来输出图像的内容特征</strong>。<p>我们从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为<em>风格层</em>。 VGG网络使用了5个卷积块。可以选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre><td class=code><pre><span class=line>style_layers, content_layers = [<span class=number>0</span>, <span class=number>5</span>, <span class=number>10</span>, <span class=number>19</span>, <span class=number>28</span>], [<span class=number>25</span>]</span><br><span class=line>net = nn.Sequential(*[pretrained_net.features[i] <span class=keyword>for</span> i <span class=keyword>in</span></span><br><span class=line>                      <span class=built_in>range</span>(<span class=built_in>max</span>(content_layers + style_layers) + <span class=number>1</span>)])</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>extract_features</span>(<span class=params>X, content_layers, style_layers</span>):</span></span><br><span class=line>    contents = []</span><br><span class=line>    styles = []</span><br><span class=line>    <span class=keyword>for</span> i <span class=keyword>in</span> <span class=built_in>range</span>(<span class=built_in>len</span>(net)):</span><br><span class=line>        X = net[i](X)</span><br><span class=line>        <span class=comment># forward pass 如果是选定的风格层或者是内容层 添加到列表中	</span></span><br><span class=line>        <span class=keyword>if</span> i <span class=keyword>in</span> style_layers:</span><br><span class=line>            styles.append(X)</span><br><span class=line>        <span class=keyword>if</span> i <span class=keyword>in</span> content_layers:</span><br><span class=line>            contents.append(X)</span><br><span class=line>    <span class=keyword>return</span> contents, styles</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_contents</span>(<span class=params>image_shape, device</span>):</span></span><br><span class=line>    content_X = preprocess(content_img, image_shape).to(device)</span><br><span class=line>    contents_Y, _ = extract_features(content_X, content_layers, style_layers)</span><br><span class=line>    <span class=keyword>return</span> content_X, contents_Y</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_styles</span>(<span class=params>image_shape, device</span>):</span></span><br><span class=line>    style_X = preprocess(style_img, image_shape).to(device) <span class=comment># 得到tensor数据 放入网络中 将输出这张图像的内容和风格</span></span><br><span class=line>    _, styles_Y = extract_features(style_X, content_layers, style_layers)</span><br><span class=line>    <span class=keyword>return</span> style_X, styles_Y</span><br></pre></table></figure><p><code>get_contents</code>函数对内容图像抽取内容特征； <code>get_styles</code>函数对风格图像抽取风格特征。 因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容特征和风格特征。<p>由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用<code>extract_features</code>函数来抽取合成图像的内容特征和风格特征.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br></pre><td class=code><pre><span class=line>rgb_mean = torch.tensor([<span class=number>0.485</span>, <span class=number>0.456</span>, <span class=number>0.406</span>])</span><br><span class=line>rgb_std = torch.tensor([<span class=number>0.229</span>, <span class=number>0.224</span>, <span class=number>0.225</span>])</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>preprocess</span>(<span class=params>img, image_shape</span>):</span></span><br><span class=line>    transforms = torchvision.transforms.Compose([</span><br><span class=line>        torchvision.transforms.Resize(image_shape),</span><br><span class=line>        torchvision.transforms.ToTensor(),</span><br><span class=line>        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])</span><br><span class=line>    <span class=keyword>return</span> transforms(img).unsqueeze(<span class=number>0</span>)</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>postprocess</span>(<span class=params>img</span>):</span></span><br><span class=line>    img = img[<span class=number>0</span>].to(rgb_std.device)</span><br><span class=line>    img = torch.clamp(img.permute(<span class=number>1</span>, <span class=number>2</span>, <span class=number>0</span>) * rgb_std + rgb_mean, <span class=number>0</span>, <span class=number>1</span>)</span><br><span class=line>    <span class=keyword>return</span> torchvision.transforms.ToPILImage()(img.permute(<span class=number>2</span>, <span class=number>0</span>, <span class=number>1</span>))</span><br></pre></table></figure><p>preprocess与postprocess分别将PIL数据转为tensor,tensor转为PIL数据.<h3 id=损失函数><a class=headerlink href=#损失函数 title=损失函数></a>损失函数</h3><p><strong>内容损失</strong><p>容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。 平方误差函数的两个输入均为<code>extract_features</code>函数计算所得到的内容层的输出<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>content_loss</span>(<span class=params>Y_hat, Y</span>):</span></span><br><span class=line>    <span class=comment># 我们从动态计算梯度的树中分离目标：</span></span><br><span class=line>    <span class=comment># 这是一个规定的值，而不是一个变量。</span></span><br><span class=line>    <span class=keyword>return</span> torch.square(Y_hat - Y.detach()).mean()</span><br></pre></table></figure><p><strong>风格损失</strong><p>风格损失与内容损失类似，也通过平方误差函数衡量合成图像与风格图像在风格上的差异。 为了表达风格层输出的风格，我们先通过<code>extract_features</code>函数计算风格层的输出。 假设该输出的样本数为1，通道数为c，高和宽分别为h和w，我们可以将此输出转换为矩阵X，其有c行和hw列。 这个矩阵可以被看作由c个长度为hw的向量x1,…,xc组合而成的。其中向量xi代表了通道i上的风格特征。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>gram</span>(<span class=params>X</span>):</span></span><br><span class=line>    num_channels, n = X.shape[<span class=number>1</span>], X.numel() // X.shape[<span class=number>1</span>]</span><br><span class=line>    X = X.reshape((num_channels, n))</span><br><span class=line>    <span class=keyword>return</span> torch.matmul(X, X.T) / (num_channels * n)</span><br></pre></table></figure><p>风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出。这里假设基于风格图像的格拉姆矩阵<code>gram_Y</code>已经预先计算好了<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>style_loss</span>(<span class=params>Y_hat, gram_Y</span>):</span></span><br><span class=line>    <span class=keyword>return</span> torch.square(gram(Y_hat) - gram_Y.detach()).mean()</span><br></pre></table></figure><p><strong>全变分损失</strong><p>有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。 一种常见的去噪方法是<em>全变分去噪</em>（total variation denoising）： 假设xi,j表示坐标(i,j)处的像素值，降低全变分损失<code>∑i,j|xi,j−xi+1,j|+|xi,j−xi,j+1|</code><p>能够尽可能使邻近的像素值相似。<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>tv_loss</span>(<span class=params>Y_hat</span>):</span></span><br><span class=line>    <span class=keyword>return</span> <span class=number>0.5</span> * (torch.<span class=built_in>abs</span>(Y_hat[:, :, <span class=number>1</span>:, :] - Y_hat[:, :, :-<span class=number>1</span>, :]).mean() +torch.<span class=built_in>abs</span>(Y_hat[:, :, :, <span class=number>1</span>:] - Y_hat[:, :, :, :-<span class=number>1</span>]).mean())</span><br></pre></table></figure><p><strong>损失函数</strong><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>content_weight, style_weight, tv_weight = <span class=number>1</span>, <span class=number>1e3</span>, <span class=number>10</span></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>compute_loss</span>(<span class=params>X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram</span>):</span></span><br><span class=line>    <span class=comment># 分别计算内容损失、风格损失和全变分损失</span></span><br><span class=line>    contents_l = [content_loss(Y_hat, Y) * content_weight <span class=keyword>for</span> Y_hat, Y <span class=keyword>in</span> <span class=built_in>zip</span>(contents_Y_hat, contents_Y)]</span><br><span class=line>    styles_l = [style_loss(Y_hat, Y) * style_weight <span class=keyword>for</span> Y_hat, Y <span class=keyword>in</span> <span class=built_in>zip</span>(styles_Y_hat, styles_Y_gram)]</span><br><span class=line>    tv_l = tv_loss(X) * tv_weight</span><br><span class=line>    <span class=comment># 对所有损失求和</span></span><br><span class=line>    l = <span class=built_in>sum</span>(<span class=number>10</span> * styles_l + contents_l + [tv_l])</span><br><span class=line>    <span class=keyword>return</span> contents_l, styles_l, tv_l, l</span><br></pre></table></figure><p>contents_l表示内容损失,<h3 id=初始化合成图像><a class=headerlink href=#初始化合成图像 title=初始化合成图像></a>初始化合成图像</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>get_inits</span>(<span class=params>X, device, lr, styles_Y</span>):</span></span><br><span class=line>    gen_img = SynthesizedImage(X.shape).to(device)</span><br><span class=line>    gen_img.weight.data.copy_(X.data)</span><br><span class=line>    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)</span><br><span class=line>    styles_Y_gram = [gram(Y) <span class=keyword>for</span> Y <span class=keyword>in</span> styles_Y]</span><br><span class=line>    <span class=keyword>return</span> gen_img(), styles_Y_gram, trainer <span class=comment># 返回与内容图像一样的数据与计算的风格gram特征</span></span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre><td class=code><pre><span class=line><span class=class><span class=keyword>class</span> <span class=title>SynthesizedImage</span>(<span class=params>nn.Module</span>):</span></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>__init__</span>(<span class=params>self, img_shape, **kwargs</span>):</span></span><br><span class=line>        <span class=built_in>super</span>(SynthesizedImage, self).__init__(**kwargs)</span><br><span class=line>        self.weight = nn.Parameter(torch.rand(*img_shape))</span><br><span class=line></span><br><span class=line>    <span class=function><span class=keyword>def</span> <span class=title>forward</span>(<span class=params>self</span>):</span></span><br><span class=line>        <span class=keyword>return</span> self.weight</span><br></pre></table></figure><h3 id=训练模型><a class=headerlink href=#训练模型 title=训练模型></a>训练模型</h3><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>train</span>(<span class=params>X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch</span>):</span></span><br><span class=line>    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y) <span class=comment>#生成原本图片 作为变量 并计算风格特征</span></span><br><span class=line>    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, <span class=number>0.8</span>)</span><br><span class=line>    animator = d2l.Animator(xlabel=<span class=string>'epoch'</span>, ylabel=<span class=string>'loss'</span>,</span><br><span class=line>                            xlim=[<span class=number>10</span>, num_epochs],</span><br><span class=line>                            legend=[<span class=string>'content'</span>, <span class=string>'style'</span>, <span class=string>'TV'</span>],</span><br><span class=line>                            ncols=<span class=number>2</span>, figsize=(<span class=number>7</span>, <span class=number>2.5</span>))</span><br><span class=line>    <span class=keyword>for</span> epoch <span class=keyword>in</span> <span class=built_in>range</span>(num_epochs):</span><br><span class=line>        trainer.zero_grad()</span><br><span class=line>        contents_Y_hat, styles_Y_hat = extract_features(</span><br><span class=line>            X, content_layers, style_layers) <span class=comment># 获取内容和风格</span></span><br><span class=line>        contents_l, styles_l, tv_l, l = compute_loss(</span><br><span class=line>            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)</span><br><span class=line>        l.backward()</span><br><span class=line>        trainer.step()</span><br><span class=line>        scheduler.step()</span><br><span class=line>        <span class=keyword>if</span> (epoch + <span class=number>1</span>) % <span class=number>10</span> == <span class=number>0</span>:</span><br><span class=line>            animator.axes[<span class=number>1</span>].imshow(postprocess(X))</span><br><span class=line>            animator.add(epoch + <span class=number>1</span>, [<span class=built_in>float</span>(<span class=built_in>sum</span>(contents_l)),</span><br><span class=line>                                     <span class=built_in>float</span>(<span class=built_in>sum</span>(styles_l)), <span class=built_in>float</span>(tv_l)])</span><br><span class=line>    <span class=keyword>return</span> X</span><br></pre></table></figure><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line>device, image_shape = d2l.try_gpu(), (<span class=number>300</span>, <span class=number>450</span>)</span><br><span class=line>net = net.to(device)</span><br><span class=line>content_X, contents_Y = get_contents(image_shape, device)</span><br><span class=line>_, styles_Y = get_styles(image_shape, device) <span class=comment># 获取content_image和内容特征 内容特征就是传到预训练模型得到的相应层输出  以及style_image的样式特征</span></span><br><span class=line>output = train(content_X, contents_Y, styles_Y, device, <span class=number>0.3</span>, <span class=number>500</span>, <span class=number>50</span>) <span class=comment># 训练传入X和其内容与样式特征</span></span><br></pre></table></figure><p>合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩。例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理<h3 id=结果展示><a class=headerlink href=#结果展示 title=结果展示></a>结果展示</h3><p><img alt=image-20231006173228071 data-src=https://s2.loli.net/2023/10/06/W4Ey98RPmMslXwJ.png><p><img alt=image-20231006173234821 data-src=https://s2.loli.net/2023/10/06/S5DUiCZE8K9cwnA.png><p><img alt=image-20231006173243962 data-src=https://s2.loli.net/2023/10/06/9UaWgiykmJt5Lfz.png><p>可以尝试改动style_weight,看看风格变换.比如style_weight增大,发现style loss太小了,而且会影响content loss<p><img alt=image-20231006173251615 data-src=https://s2.loli.net/2023/10/06/H2QAYtzVKRXMNmw.png><ul><li>风格迁移常用的损失函数由3部分组成：（1）<strong>内容损失使合成图像与内容图像在内容特征上接近</strong>；（2）<strong>风格损失令合成图像与风格图像在风格特征上接近</strong>；（3）<strong>全变分损失则有助于减少合成图像中的噪点</strong>。<li>我们<strong>可以通过预训练的卷积神经网络来抽取图像的特征</strong>，并通过最小化损失函数来不断更新合成图像来作为模型参数。<li>我们使用<strong>gram矩阵表达风格层输出的风格</strong></ul><h2 id=使用GAN进行风格迁移><a class=headerlink href=#使用GAN进行风格迁移 title=使用GAN进行风格迁移></a>使用GAN进行风格迁移</h2><p>GANs是生成艺术图像的好方法。另一种有趣的技术是所谓的风格转换，它获取一个内容图像，然后用不同的风格重新绘制，从风格图像中应用过滤器。<p>工作方式如下:<p>我们从随机噪声图像开始（或从内容图像开始，但为了理解起见，从随机噪声开始更容易）<p>我们的目标是创建这样一个图像，<strong>它将接近内容图像和风格图像</strong>。这将由两个损失函数确定：基于CNN在<strong>当前图像和内容图像的某些层提取的特征来计算内容损失</strong>,<strong>使用Gram矩阵巧妙地计算当前图像和风格图像之间的风格损失</strong><p>为了使图像更平滑并去除噪声，我们还引入了全变分损失，它计算相邻像素之间的平均距离<p>优化方式使用梯度下降（或一些其他优化算法）调整当前图像，以最小化总损失，总损失是所有三个损失的加权和。<p>在代码上与之前的差别是使用高斯分布采样得到的噪音作为需要优化的参数,<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br></pre><td class=code><pre><span class=line>img_style = load_image(<span class=string>'images/style.jpg'</span>)</span><br><span class=line>img_content = load_image(<span class=string>'images/image.jpg'</span>)</span><br><span class=line></span><br><span class=line>img_result = np.random.uniform(size=(img_size,img_size,<span class=number>3</span>))</span><br><span class=line>vgg = tf.keras.applications.VGG16(include_top=<span class=literal>False</span>, weights=<span class=string>'imagenet'</span>)</span><br><span class=line>vgg.trainable = <span class=literal>False</span></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>layer_extractor</span>(<span class=params>layers</span>):</span></span><br><span class=line>    outputs = [vgg.get_layer(x).output <span class=keyword>for</span> x <span class=keyword>in</span> layers]</span><br><span class=line>    model = tf.keras.Model([vgg.<span class=built_in>input</span>],outputs)</span><br><span class=line>    <span class=keyword>return</span> model </span><br><span class=line>content_layers = [<span class=string>'block4_conv2'</span>] </span><br><span class=line>content_extractor = layer_extractor(content_layers)</span><br><span class=line></span><br><span class=line>content_target = content_extractor(preprocess_input(tf.expand_dims(img_content,axis=<span class=number>0</span>)))</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>content_loss</span>(<span class=params>img</span>):</span></span><br><span class=line>    z = content_extractor(preprocess_input(tf.expand_dims(<span class=number>255</span>*img,axis=<span class=number>0</span>))) </span><br><span class=line>    <span class=keyword>return</span> <span class=number>0.5</span>*tf.reduce_sum((z-content_target)**<span class=number>2</span>)</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>gram_matrix</span>(<span class=params>x</span>):</span></span><br><span class=line>  result = tf.linalg.einsum(<span class=string>'bijc,bijd->bcd'</span>, x, x)</span><br><span class=line>  input_shape = tf.shape(x)</span><br><span class=line>  num_locations = tf.cast(input_shape[<span class=number>1</span>]*input_shape[<span class=number>2</span>], tf.float32)</span><br><span class=line>  <span class=keyword>return</span> result/(num_locations)</span><br><span class=line></span><br><span class=line>style_layers = [<span class=string>'block1_conv1'</span>,<span class=string>'block2_conv1'</span>,<span class=string>'block3_conv1'</span>,<span class=string>'block4_conv1'</span>]</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>style_extractor</span>(<span class=params>img</span>):</span></span><br><span class=line>    <span class=keyword>return</span> [gram_matrix(x) <span class=keyword>for</span> x <span class=keyword>in</span> layer_extractor(style_layers)(img)]</span><br><span class=line></span><br><span class=line>style_target = style_extractor(preprocess_input(tf.expand_dims(img_style,axis=<span class=number>0</span>)))</span><br><span class=line></span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>style_loss</span>(<span class=params>img</span>):</span></span><br><span class=line>    z = style_extractor(preprocess_input(tf.expand_dims(<span class=number>255</span>*img,axis=<span class=number>0</span>)))</span><br><span class=line>    loss = tf.add_n([tf.reduce_mean((x-target)**<span class=number>2</span>) </span><br><span class=line>                           <span class=keyword>for</span> x,target <span class=keyword>in</span> <span class=built_in>zip</span>(z,style_target)])</span><br><span class=line>    <span class=keyword>return</span> loss / <span class=built_in>len</span>(style_layers)</span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>variation_loss</span>(<span class=params>img</span>):</span></span><br><span class=line>  img = tf.cast(img,tf.float32)</span><br><span class=line>  x_var = img[ :, <span class=number>1</span>:, :] - img[ :, :-<span class=number>1</span>, :]</span><br><span class=line>  y_var = img[ <span class=number>1</span>:, :, :] - img[ :-<span class=number>1</span>, :, :]</span><br><span class=line>  <span class=keyword>return</span> tf.reduce_sum(tf.<span class=built_in>abs</span>(x_var)) + tf.reduce_sum(tf.<span class=built_in>abs</span>(y_var))</span><br><span class=line>  </span><br><span class=line><span class=function><span class=keyword>def</span> <span class=title>total_loss_var</span>(<span class=params>img</span>):</span></span><br><span class=line>    <span class=keyword>return</span> content_loss(img)+<span class=number>150</span>*style_loss(img)+<span class=number>30</span>*variation_loss(img)</span><br><span class=line></span><br><span class=line>img.assign(clip(np.random.normal(-<span class=number>0.3</span>,<span class=number>0.3</span>,size=img_content.shape)+img_content/<span class=number>255.0</span>))</span><br><span class=line></span><br><span class=line>train(img,loss_fn=total_loss_var)</span><br></pre></table></figure><p>注意,提取风格的层数一般选择每个特征块的前面几层,而提取内容的层数一般选择特征块的后面几块.<p>风格迁移与迁移学习存在不可区分的关系,因为我们将一些知识从一个神经网络模型转移到另一个。在迁移学习中，我们通常从预先训练的模型开始，该模型已经在一些大型图像数据集（如ImageNet）上进行了训练。<p>预训练模型比如:<ul><li>VGG-16/VGG-19，它们是相对简单的模型，仍然提供良好的精度。经常将VGG作为第一次尝试是了解迁移学习如何运作的好选择。<li>ResNet是微软研究院于2015年提出的一系列模型。它们有更多的层，因此占用更多的资源。<li>MobileNet是一系列尺寸较小的型号，适用于移动设备。如果你缺乏资源，并且可能会牺牲一点准确性，那么就使用它们。</ul><p>使用pytorch加载预训练模型.<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>vgg = torchvision.models.vgg16(pretrained=<span class=literal>True</span>)</span><br><span class=line>vgg</span><br></pre></table></figure><p>结构如下,可以看见有<code>features</code>,<code>avgpool</code>以及<code>classifier</code><figure class="highlight routeros"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br></pre><td class=code><pre><span class=line>VGG(</span><br><span class=line>  (features): Sequential(</span><br><span class=line>    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (1): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (3): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (4): MaxPool2d(<span class=attribute>kernel_size</span>=2, <span class=attribute>stride</span>=2, <span class=attribute>padding</span>=0, <span class=attribute>dilation</span>=1, <span class=attribute>ceil_mode</span>=<span class=literal>False</span>)</span><br><span class=line>    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (6): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (8): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (9): MaxPool2d(<span class=attribute>kernel_size</span>=2, <span class=attribute>stride</span>=2, <span class=attribute>padding</span>=0, <span class=attribute>dilation</span>=1, <span class=attribute>ceil_mode</span>=<span class=literal>False</span>)</span><br><span class=line>    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (11): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (13): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (15): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (16): MaxPool2d(<span class=attribute>kernel_size</span>=2, <span class=attribute>stride</span>=2, <span class=attribute>padding</span>=0, <span class=attribute>dilation</span>=1, <span class=attribute>ceil_mode</span>=<span class=literal>False</span>)</span><br><span class=line>    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (18): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (20): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (22): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (23): MaxPool2d(<span class=attribute>kernel_size</span>=2, <span class=attribute>stride</span>=2, <span class=attribute>padding</span>=0, <span class=attribute>dilation</span>=1, <span class=attribute>ceil_mode</span>=<span class=literal>False</span>)</span><br><span class=line>    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (25): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (27): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class=line>    (29): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (30): MaxPool2d(<span class=attribute>kernel_size</span>=2, <span class=attribute>stride</span>=2, <span class=attribute>padding</span>=0, <span class=attribute>dilation</span>=1, <span class=attribute>ceil_mode</span>=<span class=literal>False</span>)</span><br><span class=line>  )</span><br><span class=line>  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class=line>  (classifier): Sequential(</span><br><span class=line>    (0): Linear(<span class=attribute>in_features</span>=25088, <span class=attribute>out_features</span>=4096, <span class=attribute>bias</span>=<span class=literal>True</span>)</span><br><span class=line>    (1): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (2): Dropout(<span class=attribute>p</span>=0.5, <span class=attribute>inplace</span>=<span class=literal>False</span>)</span><br><span class=line>    (3): Linear(<span class=attribute>in_features</span>=4096, <span class=attribute>out_features</span>=4096, <span class=attribute>bias</span>=<span class=literal>True</span>)</span><br><span class=line>    (4): ReLU(<span class=attribute>inplace</span>=<span class=literal>True</span>)</span><br><span class=line>    (5): Dropout(<span class=attribute>p</span>=0.5, <span class=attribute>inplace</span>=<span class=literal>False</span>)</span><br><span class=line>    (6): Linear(<span class=attribute>in_features</span>=4096, <span class=attribute>out_features</span>=1000, <span class=attribute>bias</span>=<span class=literal>True</span>)</span><br><span class=line>  )</span><br><span class=line>)</span><br></pre></table></figure><p><strong>查看每一层后的结果</strong><p>利用<code>torchinfo</code>库查看<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br></pre><td class=code><pre><span class=line>summary(vgg,input_size=(<span class=number>1</span>,<span class=number>3</span>,<span class=number>224</span>,<span class=number>224</span>))</span><br></pre></table></figure><p><img alt=image-20231006173311040 data-src=https://s2.loli.net/2023/10/06/Pg1q59j4WnuSCIo.png><p>可以看到输入一张3通道224的图像特征层输出是512通道的宽高为7的特征<p><strong>提取图像特征</strong><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre><span class=line>res = vgg.features(sample_image).cpu()</span><br><span class=line>plt.figure(figsize=(<span class=number>15</span>,<span class=number>3</span>))</span><br><span class=line>plt.imshow(res.detach().view(<span class=number>512</span>,-<span class=number>1</span>).T)</span><br><span class=line><span class=built_in>print</span>(res.size())</span><br></pre></table></figure><p>利用feature层提取特征,然后利用预训练模型提取的特征,直接拿一个简单的Linear层作为分类器进行训练,比如:<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre><span class=line>vgg_dataset = torch.utils.data.TensorDataset(feature_tensor,label_tensor.to(torch.long))</span><br><span class=line>train_ds, test_ds = torch.utils.data.random_split(vgg_dataset,[<span class=number>700</span>,<span class=number>100</span>])</span><br><span class=line>train_loader = torch.utils.data.DataLoader(train_ds,batch_size=<span class=number>32</span>)</span><br><span class=line>test_loader = torch.utils.data.DataLoader(test_ds,batch_size=<span class=number>32</span>)</span><br><span class=line></span><br><span class=line>net = torch.nn.Sequential(torch.nn.Linear(<span class=number>512</span>*<span class=number>7</span>*<span class=number>7</span>,<span class=number>2</span>),torch.nn.LogSoftmax()).to(device)</span><br><span class=line></span><br><span class=line>history = train(net,train_loader,test_loader)</span><br></pre></table></figure><p>net就是简单的线性层加一个激活函数<p><strong>常用的加载数据流程</strong><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre><td class=code><pre><span class=line>std_normalize = transforms.Normalize(mean=[<span class=number>0.485</span>, <span class=number>0.456</span>, <span class=number>0.406</span>],</span><br><span class=line>                          std=[<span class=number>0.229</span>, <span class=number>0.224</span>, <span class=number>0.225</span>])</span><br><span class=line>trans = transforms.Compose([</span><br><span class=line>        transforms.Resize(<span class=number>256</span>),</span><br><span class=line>        transforms.CenterCrop(<span class=number>224</span>),</span><br><span class=line>        transforms.ToTensor(), </span><br><span class=line>        std_normalize])</span><br><span class=line>dataset = torchvision.datasets.ImageFolder(<span class=string>'data/PetImages'</span>,transform=trans)</span><br><span class=line>trainset, testset = torch.utils.data.random_split(dataset,[<span class=number>20000</span>,<span class=built_in>len</span>(dataset)-<span class=number>20000</span>])</span><br></pre></table></figure><p>如果图像在一个文件夹中,利用<code>ImageFolder</code>得到dataset.然后使用<code>dl = torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=True)</code>用于循环每个batch处理.<p>可以在训练过程中使用原始VGG-16网络作为一个整体来避免手动预计算特征.如下<ul><li><strong>将最终分类器替换为将产生所需数量的类的分类</strong>器。<li><strong>冻结卷积特征提取器的权重，使得它们不被训练</strong>。建议最初进行这种冻结，因为否则未经训练的分类器层可能会破坏卷积提取器的原始预训练权重。冻结权重可以通过将所有参数的requires_grad属性设置为False来实现</ul><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line><span class=keyword>for</span> x <span class=keyword>in</span> vgg.features.parameters():</span><br><span class=line>    x.requires_grad = <span class=literal>False</span></span><br></pre></table></figure><p>如果您的对象在视觉上与普通的ImageNet图像不同，则这种功能组合可能无法发挥最佳效果。因此，开始训练卷积层也是有意义的。 为此，我们可以解冻之前冻结的卷积滤波器参数。不过一般都会采用一些微调方法,比如LoRA等.<h3 id=其他方向><a class=headerlink href=#其他方向 title=其他方向></a>其他方向</h3><p>domain knowledge,domain adaption或者是transfer learning,本质上都是像提取一些特征,这种特征能在多个domain上使用.我们可以考虑利用这种特征进行可视化或者对抗攻击等.<p>比如利用预训练模型作为分类器,尝试从下从正态分布采样得到噪声图,然后作为输入,优化这个输入使其被分类为想要的分类.这样的图像虽然被正确分类了但人眼还是能明显看出差别.<p><img alt=image-20231006173320659 data-src=https://s2.loli.net/2023/10/06/X1qoLVJPAZbYIHr.png><blockquote><p>这种噪音对我们来说没有多大意义，但很可能它包含了很多需要的类别(比如猫)特有的低级别过滤器。然而，由于有很多方法可以优化输入以获得理想的结果，因此优化算法没有动机找到视觉上可理解的模式.<p>为了让它看起来不那么像噪音，我们可以在损失函数中引入一个附加项——变化损失。它测量图像的相邻像素的相似程度。如果我们将这个项添加到损失函数中，它将迫使优化器找到噪声较小的解决方案，从而具有更多可识别的细节</blockquote><figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br></pre><td class=code><pre><span class=line><span class=function><span class=keyword>def</span> <span class=title>total_loss</span>(<span class=params>target,res</span>):</span></span><br><span class=line>    <span class=keyword>return</span> <span class=number>10</span>*tf.reduce_mean(keras.metrics.sparse_categorical_crossentropy(target,res)) + \</span><br><span class=line>           <span class=number>0.005</span>*tf.image.total_variation(x,res)</span><br><span class=line></span><br><span class=line>optimize(x,target,loss_fn=total_loss)</span><br></pre></table></figure><p>也就是分类的损失加上全变分损失.全变分损失目的是减小噪声,得到图像图下<p><img alt=image-20231006173331956 data-src=https://s2.loli.net/2023/10/06/ivem7EW1CjXrZN8.png><p>对抗攻击就利用一张本身是狗分类也确实是狗的图片,对这张图片进行优化<figure class="highlight python"><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br></pre><td class=code><pre><span class=line>x = tf.Variable(np.expand_dims(img,axis=<span class=number>0</span>).astype(np.float32)/<span class=number>255.0</span>)</span><br><span class=line>optimize(x,target,epochs=<span class=number>100</span>)</span><br></pre></table></figure><p><img alt=image-20231006173338372 data-src=https://s2.loli.net/2023/10/06/HvpYWiA2uUdz9xe.png><p>在pytorch中使用<code>autograd</code>计算梯度.<a href=https://www.w3cschool.cn/article/4182917.html rel=noopener target=_blank>pytorch 中autograd.grad()函数的用法说明 | w3cschool笔记</a><p>最后推荐一下微软的Ai for beginners的课程,质量比较高,此外还有李沐的d2l,台湾李宏毅老师的深度学习课程以及fast.ai课程,都是比较好的.<blockquote><p>我们能够在预先训练的CNN中可视化猫（以及任何其他物体）的理想图像，<strong>使用梯度下降优化来调整输入图像而不是权重</strong>。获得有意义的图像的主要技巧是使用<strong>变化损失作为额外的损失函数</strong>，这会<strong>使图像看起来更平滑</strong>。</blockquote><h3 id=参考资料><a class=headerlink href=#参考资料 title=参考资料></a>参考资料</h3><ol><li><a href=https://zh.d2l.ai/chapter_computer-vision/neural-style.html rel=noopener target=_blank>13.12. 风格迁移 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a><li><a href=https://github.com/microsoft/AI-For-Beginners/tree/main/lessons/4-ComputerVision/10-GANs rel=noopener target=_blank>AI-For-Beginners/lessons/4-ComputerVision/10-GANs at main · microsoft/AI-For-Beginners (github.com)</a><li><a href=https://arxiv.org/abs/1508.06576 rel=noopener target=_blank>[1508.06576] A Neural Algorithm of Artistic Style (arxiv.org)</a><li><a href=https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/08-TransferLearning/README.md rel=noopener target=_blank>AI-For-Beginners/lessons/4-ComputerVision/08-TransferLearning/README.md at main · microsoft/AI-For-Beginners (github.com)</a></ol><link href=/css/spoiler.css rel=stylesheet><script async src=/js/spoiler.js></script></div><div><div><div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div></div></div><div class=reward-container><div>感谢阅读.</div><button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">打赏</button><div style="display: none;" id=qr><div style="display: inline-block;"><img alt="Sekyoro 微信支付" src=/images/wechatpay.png><p>微信支付</div></div></div><div><ul class=post-copyright><li class=post-copyright-author><strong>本文作者： </strong>Sekyoro<li class=post-copyright-link><strong>本文链接：</strong> <a href=https://www.sekyoro.top/2023/09/26/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/ title=风格迁移>https://www.sekyoro.top/2023/09/26/风格迁移/</a><li class=post-copyright-license><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ rel=noopener target=_blank><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</ul></div><div class=followme><p>欢迎关注我的其它发布渠道<div class=social-list><div class=social-item><a class=social-link href=/images/wxqrcode.png target=_blank> <span class=icon> <i class="fab fa-weixin"></i> </span> <span class=label>WeChat</span> </a></div><div class=social-item><a class=social-link href=/images/website.png target=_blank> <span class=icon> <i class="fa fa-user"></i> </span> <span class=label>PersonalWebsite</span> </a></div><div class=social-item><a class=social-link href=https://my-astro-git-main-drowning-in-codes.vercel.app target=_blank> <span class=icon> <i class="fas fa-share"></i> </span> <span class=label>杂鱼分享</span> </a></div><div class=social-item><a class=social-link href=/atom.xml target=_blank> <span class=icon> <i class="fa fa-rss"></i> </span> <span class=label>RSS</span> </a></div></div></div><footer class=post-footer><div class=post-tags><a href=/tags/style-transfer/ rel=tag><i class="fa fa-tag"></i> style transfer</a></div><div class=post-nav><div class=post-nav-item><a href=/2023/09/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/ rel=prev title=论文阅读_自动驾驶(一)> <i class="fa fa-chevron-left"></i> 论文阅读_自动驾驶(一) </a></div><div class=post-nav-item><a href=/2023/09/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/ rel=next title=大模型微调> 大模型微调 <i class="fa fa-chevron-right"></i> </a></div></div></footer></article></div><!-- 评论区 --><div class=comments><div data-id=city data-uid=MTAyMC81MzE5Ny8yOTY3Mg== id=lv-container></div></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class=sidebar><div class=sidebar-inner><!-- canvas粒子时钟 --><div><canvas id=canvas style=width:60%;>当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();</script><!-- require APlayer --><link href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css rel=stylesheet><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><!-- require MetingJS --><script src=/js/meting-js.js></script><ul class="sidebar-nav motion-element"><li class=sidebar-nav-toc>文章目录<li class=sidebar-nav-overview>站点概览</ul><!--noindex--><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class=nav><li class="nav-item nav-level-2"><a class=nav-link href=#%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB><span class=nav-number>1.</span> <span class=nav-text>使用预训练模型进行迁移</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0><span class=nav-number>1.1.</span> <span class=nav-text>损失函数</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%88%E6%88%90%E5%9B%BE%E5%83%8F><span class=nav-number>1.2.</span> <span class=nav-text>初始化合成图像</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B><span class=nav-number>1.3.</span> <span class=nav-text>训练模型</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA><span class=nav-number>1.4.</span> <span class=nav-text>结果展示</span></a></ol><li class="nav-item nav-level-2"><a class=nav-link href=#%E4%BD%BF%E7%94%A8GAN%E8%BF%9B%E8%A1%8C%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB><span class=nav-number>2.</span> <span class=nav-text>使用GAN进行风格迁移</span></a><ol class=nav-child><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%85%B6%E4%BB%96%E6%96%B9%E5%90%91><span class=nav-number>2.1.</span> <span class=nav-text>其他方向</span></a><li class="nav-item nav-level-3"><a class=nav-link href=#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99><span class=nav-number>2.2.</span> <span class=nav-text>参考资料</span></a></ol></ol></div></div><!--/noindex--><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt=Sekyoro class=site-author-image itemprop=image src=https://i.loli.net/2021/05/17/YqoavnXdGTpPO9R.jpg><p class=site-author-name itemprop=name>Sekyoro<div class=site-description itemprop=description>什么也无法舍弃的人，什么也做不了.</div></div><div class="site-state-wrap motion-element"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>154</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-categories"><a href=/categories/> <span class=site-state-item-count>17</span> <span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/> <span class=site-state-item-count>178</span> <span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class=links-of-author-item> <a title="Personal Website → http://proanimer.com" href=http://proanimer.com/ rel=noopener target=_blank><i class="fab fa-internet-explorer fa-fw"></i>Personal Website</a> </span><span class=links-of-author-item> <a title="GitHub → https://github.com/drowning-in-codes" href=https://github.com/drowning-in-codes rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class=links-of-author-item> <a title="E-Mail → mailto:bukalala174@gmail.com" href=mailto:bukalala174@gmail.com rel=noopener target=_blank><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class=links-of-author-item> <a title="wxPublicAccount → https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd" href=https://mp.weixin.qq.com/s?__biz=Mzg3ODY1MDkzMg==&mid=2247483770&idx=1&sn=fdf88faab01d5c219ac609570a21c9d6&chksm=cf113221f866bb373938cfca03cf095ff4fe1e4dc37d68ef5de4cd4876ee1260fca0c015a4d6&token=1096259873&lang=zh_CN#rd rel=noopener target=_blank><i class="fab fa-weixin fa-fw"></i>wxPublicAccount</a> </span><span class=links-of-author-item> <a title="RSS → /atom.xml" href=/atom.xml><i class="fa fa-rss fa-fw"></i>RSS</a> </span><span class=links-of-author-item> <a title="CSDN → https://blog.csdn.net/aqwca" href=https://blog.csdn.net/aqwca rel=noopener target=_blank><i class="fa fa-handshake fa-fw"></i>CSDN</a> </span><span class=links-of-author-item> <a title="杂鱼分享 → https://my-astro-git-main-drowning-in-codes.vercel.app" href=https://my-astro-git-main-drowning-in-codes.vercel.app/ rel=noopener target=_blank><i class="fas fa-share fa-fw"></i>杂鱼分享</a> </span></div><div class="links-of-blogroll motion-element"><div class=links-of-blogroll-title><i class="fa fa-link fa-fw"></i> 友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=http://myqhs.top/ rel=noopener target=_blank title=http://myqhs.top/>myqhs</a></ul></div><div class="motion-element announcement"><div class=title>注意</div><p class=content>由于最近图床更新,可能有些图片显示不了.如果发现了有些图片无法显示影响阅读的,还烦请联系我,我有空补上.<p class=date>2023-10-6</div></div><meting-js id=6856787487 order=random server=netease type=playlist> </meting-js><div class=widget-wrap><h3 class=widget-title style=margin:0>文章词云</h3><div class="widget tagcloud" id=myCanvasContainer><canvas height=250 id=resCanvas style=width:100% width=250><ul class=tag-list itemprop=keywords><li class=tag-list-item><a class=tag-list-link href=/tags/style-transfer/ rel=tag>style transfer</a><span class=tag-list-count>1</span></ul></canvas></div></div><script id=clustrmaps src=https://clustrmaps.com/map_v2.js?d=xQdGTxqARTBiNIwX2aUban-ixkj2s6VaZQWo-aVCgY8&cl=ffffff&w=a></script><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i><span>0%</span></div><!-- 边栏 --></div></aside><div id=sidebar-dimmer></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>© Wed Apr 08 2020 08:00:00 GMT+0800 (中国标准时间) – <span itemprop=copyrightYear>2023</span><span class=with-love> <i class="fa fa-heart"></i> </span><span class=author itemprop=copyrightHolder>Sekyoro</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-chart-area"></i> </span><span title=站点总字数>954k</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点阅读时长>14:28</span></div><script async src=https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container_site_pv>总访问量<span id=busuanzi_value_site_pv></span>次</span><span class=post-meta-divider>|</span><span id=busuanzi_container_site_uv>总访客数<span id=busuanzi_value_site_uv></span>人</span><span class=post-meta-divider>|</span><!-- 不蒜子计数初始值纠正 --><script>$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});</script><div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("04/08/2021 20:00:00");
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);</script></div><div class=busuanzi-count><script async data-pjax src=https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span style="display: none;" class=post-meta-item id=busuanzi_container_site_uv> <span class=post-meta-item-icon> <i class="fa fa-user"></i> </span> <span class=site-uv title=总访客量> <span id=busuanzi_value_site_uv></span> </span> </span><span class=post-meta-divider>|</span><span style="display: none;" class=post-meta-item id=busuanzi_container_site_pv> <span class=post-meta-item-icon> <i class="fa fa-eye"></i> </span> <span class=site-pv title=总访问量> <span id=busuanzi_value_site_pv></span> </span> </span></div></div></footer></div><script color=0,0,255 count=99 opacity=0.5 src=/lib/canvas-nest/canvas-nest.min.js zindex=-1></script><script src=/lib/anime.min.js></script><script src=https://cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js></script><script src=https://cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.min.js></script><script src=https://cdn.jsdelivr.net/npm/velocity-animate@1/velocity.ui.min.js></script><script src=/js/utils.js></script><script src=/js/motion.js></script><script src=/js/schemes/pisces.js></script><script src=/js/next-boot.js></script><script src=/js/bookmark.js></script><script>var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax',
	'.widget-wrap'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
 
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});</script><script data-pjax>(function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();</script><script src=https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js></script><script src=/js/algolia-search.js></script><script data-pjax>document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});</script><div id=pjax><script charset=utf-8 defer src=/js/outdate.js></script></div><script charset=utf-8 defer src=/js/tagcanvas.js></script><script charset=utf-8 defer src=/js/tagcloud.js></script><script>NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});</script><script>var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });</script><script src=/js/src/activate-power-mode.min.js></script><script>POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);</script>